[
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "optim",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "optim",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Variable",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Function",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Function",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Function",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "torch.utils.data",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DistributedSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "RandomSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "SequentialSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DistributedSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "RandomSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "SequentialSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "torch.optim",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.optim",
        "description": "torch.optim",
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "Optimizer",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "Action",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "ArgumentParser",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "multiprocessing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "queue",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "queue",
        "description": "queue",
        "detail": "queue",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "utils.dataset",
        "description": "utils.dataset",
        "isExtraImport": true,
        "detail": "utils.dataset",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "utils.dataset",
        "description": "utils.dataset",
        "isExtraImport": true,
        "detail": "utils.dataset",
        "documentation": {}
    },
    {
        "label": "Forwarder",
        "importPath": "utils.network",
        "description": "utils.network",
        "isExtraImport": true,
        "detail": "utils.network",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "utils.network",
        "description": "utils.network",
        "isExtraImport": true,
        "detail": "utils.network",
        "documentation": {}
    },
    {
        "label": "Forwarder",
        "importPath": "utils.network",
        "description": "utils.network",
        "isExtraImport": true,
        "detail": "utils.network",
        "documentation": {}
    },
    {
        "label": "PathGrammar",
        "importPath": "utils.grammar",
        "description": "utils.grammar",
        "isExtraImport": true,
        "detail": "utils.grammar",
        "documentation": {}
    },
    {
        "label": "PoissonModel",
        "importPath": "utils.length_model",
        "description": "utils.length_model",
        "isExtraImport": true,
        "detail": "utils.length_model",
        "documentation": {}
    },
    {
        "label": "Viterbi",
        "importPath": "utils.viterbi",
        "description": "utils.viterbi",
        "isExtraImport": true,
        "detail": "utils.viterbi",
        "documentation": {}
    },
    {
        "label": "Viterbi",
        "importPath": "utils.viterbi",
        "description": "utils.viterbi",
        "isExtraImport": true,
        "detail": "utils.viterbi",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "sqrt",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "sqrt",
        "importPath": "math",
        "description": "math",
        "isExtraImport": true,
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "softmax",
        "importPath": "scipy.special",
        "description": "scipy.special",
        "isExtraImport": true,
        "detail": "scipy.special",
        "documentation": {}
    },
    {
        "label": "Parameter",
        "importPath": "torch.nn.parameter",
        "description": "torch.nn.parameter",
        "isExtraImport": true,
        "detail": "torch.nn.parameter",
        "documentation": {}
    },
    {
        "label": "Module",
        "importPath": "torch.nn.modules.module",
        "description": "torch.nn.modules.module",
        "isExtraImport": true,
        "detail": "torch.nn.modules.module",
        "documentation": {}
    },
    {
        "label": "Module",
        "importPath": "torch.nn.modules.module",
        "description": "torch.nn.modules.module",
        "isExtraImport": true,
        "detail": "torch.nn.modules.module",
        "documentation": {}
    },
    {
        "label": "copy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "copy",
        "description": "copy",
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "MultiStageModel",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "BatchGenerator",
        "importPath": "batch_gen",
        "description": "batch_gen",
        "isExtraImport": true,
        "detail": "batch_gen",
        "documentation": {}
    },
    {
        "label": "BatchGenerator",
        "importPath": "batch_gen",
        "description": "batch_gen",
        "isExtraImport": true,
        "detail": "batch_gen",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "SingleStageModel",
        "importPath": "layers",
        "description": "layers",
        "isExtraImport": true,
        "detail": "layers",
        "documentation": {}
    },
    {
        "label": "GCNStageModel",
        "importPath": "layers",
        "description": "layers",
        "isExtraImport": true,
        "detail": "layers",
        "documentation": {}
    },
    {
        "label": "exchange_time",
        "importPath": "layers",
        "description": "layers",
        "isExtraImport": true,
        "detail": "layers",
        "documentation": {}
    },
    {
        "label": "CfgNode",
        "importPath": "fvcore.common.config",
        "description": "fvcore.common.config",
        "isExtraImport": true,
        "detail": "fvcore.common.config",
        "documentation": {}
    },
    {
        "label": "CfgNode",
        "importPath": "fvcore.common.config",
        "description": "fvcore.common.config",
        "isExtraImport": true,
        "detail": "fvcore.common.config",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "collections",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "collections",
        "description": "collections",
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "abc",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "abc",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "pathmgr",
        "importPath": "slowfast.utils.env",
        "description": "slowfast.utils.env",
        "isExtraImport": true,
        "detail": "slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "pathmgr",
        "importPath": "slowfast.utils.env",
        "description": "slowfast.utils.env",
        "isExtraImport": true,
        "detail": "slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "pathmgr",
        "importPath": "slowfast.utils.env",
        "description": "slowfast.utils.env",
        "isExtraImport": true,
        "detail": "slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "pathmgr",
        "importPath": "slowfast.utils.env",
        "description": "slowfast.utils.env",
        "isExtraImport": true,
        "detail": "slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "pathmgr",
        "importPath": "slowfast.utils.env",
        "description": "slowfast.utils.env",
        "isExtraImport": true,
        "detail": "slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "pathmgr",
        "importPath": "slowfast.utils.env",
        "description": "slowfast.utils.env",
        "isExtraImport": true,
        "detail": "slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "setup_environment",
        "importPath": "slowfast.utils.env",
        "description": "slowfast.utils.env",
        "isExtraImport": true,
        "detail": "slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "pathmgr",
        "importPath": "slowfast.utils.env",
        "description": "slowfast.utils.env",
        "isExtraImport": true,
        "detail": "slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "pathmgr",
        "importPath": "slowfast.utils.env",
        "description": "slowfast.utils.env",
        "isExtraImport": true,
        "detail": "slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "pathmgr",
        "importPath": "slowfast.utils.env",
        "description": "slowfast.utils.env",
        "isExtraImport": true,
        "detail": "slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "pathmgr",
        "importPath": "slowfast.utils.env",
        "description": "slowfast.utils.env",
        "isExtraImport": true,
        "detail": "slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "pathmgr",
        "importPath": "slowfast.utils.env",
        "description": "slowfast.utils.env",
        "isExtraImport": true,
        "detail": "slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "pathmgr",
        "importPath": "slowfast.utils.env",
        "description": "slowfast.utils.env",
        "isExtraImport": true,
        "detail": "slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "pathmgr",
        "importPath": "slowfast.utils.env",
        "description": "slowfast.utils.env",
        "isExtraImport": true,
        "detail": "slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "pathmgr",
        "importPath": "slowfast.utils.env",
        "description": "slowfast.utils.env",
        "isExtraImport": true,
        "detail": "slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "pathmgr",
        "importPath": "slowfast.utils.env",
        "description": "slowfast.utils.env",
        "isExtraImport": true,
        "detail": "slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "setup_environment",
        "importPath": "slowfast.utils.env",
        "description": "slowfast.utils.env",
        "isExtraImport": true,
        "detail": "slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "pathmgr",
        "importPath": "slowfast.utils.env",
        "description": "slowfast.utils.env",
        "isExtraImport": true,
        "detail": "slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "pathmgr",
        "importPath": "slowfast.utils.env",
        "description": "slowfast.utils.env",
        "isExtraImport": true,
        "detail": "slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "pathmgr",
        "importPath": "slowfast.utils.env",
        "description": "slowfast.utils.env",
        "isExtraImport": true,
        "detail": "slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "pathmgr",
        "importPath": "slowfast.utils.env",
        "description": "slowfast.utils.env",
        "isExtraImport": true,
        "detail": "slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "Registry",
        "importPath": "fvcore.common.registry",
        "description": "fvcore.common.registry",
        "isExtraImport": true,
        "detail": "fvcore.common.registry",
        "documentation": {}
    },
    {
        "label": "Registry",
        "importPath": "fvcore.common.registry",
        "description": "fvcore.common.registry",
        "isExtraImport": true,
        "detail": "fvcore.common.registry",
        "documentation": {}
    },
    {
        "label": "Registry",
        "importPath": "fvcore.common.registry",
        "description": "fvcore.common.registry",
        "isExtraImport": true,
        "detail": "fvcore.common.registry",
        "documentation": {}
    },
    {
        "label": "Registry",
        "importPath": "fvcore.common.registry",
        "description": "fvcore.common.registry",
        "isExtraImport": true,
        "detail": "fvcore.common.registry",
        "documentation": {}
    },
    {
        "label": "itertools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "itertools",
        "description": "itertools",
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "chain",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "chain",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "chain",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "chain",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "chain",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "chain",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "permutations",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "combinations",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "slowfast.utils.logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "slowfast.utils.logging",
        "description": "slowfast.utils.logging",
        "detail": "slowfast.utils.logging",
        "documentation": {}
    },
    {
        "label": "cv2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cv2",
        "description": "cv2",
        "detail": "cv2",
        "documentation": {}
    },
    {
        "label": "getTickCount",
        "importPath": "cv2",
        "description": "cv2",
        "isExtraImport": true,
        "detail": "cv2",
        "documentation": {}
    },
    {
        "label": "getTickFrequency",
        "importPath": "cv2",
        "description": "cv2",
        "isExtraImport": true,
        "detail": "cv2",
        "documentation": {}
    },
    {
        "label": "getTickCount",
        "importPath": "cv2",
        "description": "cv2",
        "isExtraImport": true,
        "detail": "cv2",
        "documentation": {}
    },
    {
        "label": "getTickFrequency",
        "importPath": "cv2",
        "description": "cv2",
        "isExtraImport": true,
        "detail": "cv2",
        "documentation": {}
    },
    {
        "label": "torchvision.io",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.io",
        "description": "torchvision.io",
        "detail": "torchvision.io",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "PIL",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "PIL",
        "description": "PIL",
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageEnhance",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageOps",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageFilter",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageEnhance",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageOps",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageFilter",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "torchvision",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision",
        "description": "torchvision",
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "slowfast.datasets.transform",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "slowfast.datasets.transform",
        "description": "slowfast.datasets.transform",
        "detail": "slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "g_pathmgr",
        "importPath": "iopath.common.file_io",
        "description": "iopath.common.file_io",
        "isExtraImport": true,
        "detail": "iopath.common.file_io",
        "documentation": {}
    },
    {
        "label": "g_pathmgr",
        "importPath": "iopath.common.file_io",
        "description": "iopath.common.file_io",
        "isExtraImport": true,
        "detail": "iopath.common.file_io",
        "documentation": {}
    },
    {
        "label": "g_pathmgr",
        "importPath": "iopath.common.file_io",
        "description": "iopath.common.file_io",
        "isExtraImport": true,
        "detail": "iopath.common.file_io",
        "documentation": {}
    },
    {
        "label": "PathManagerFactory",
        "importPath": "iopath.common.file_io",
        "description": "iopath.common.file_io",
        "isExtraImport": true,
        "detail": "iopath.common.file_io",
        "documentation": {}
    },
    {
        "label": "g_pathmgr",
        "importPath": "iopath.common.file_io",
        "description": "iopath.common.file_io",
        "isExtraImport": true,
        "detail": "iopath.common.file_io",
        "documentation": {}
    },
    {
        "label": "g_pathmgr",
        "importPath": "iopath.common.file_io",
        "description": "iopath.common.file_io",
        "isExtraImport": true,
        "detail": "iopath.common.file_io",
        "documentation": {}
    },
    {
        "label": "g_pathmgr",
        "importPath": "iopath.common.file_io",
        "description": "iopath.common.file_io",
        "isExtraImport": true,
        "detail": "iopath.common.file_io",
        "documentation": {}
    },
    {
        "label": "PathManagerFactory",
        "importPath": "iopath.common.file_io",
        "description": "iopath.common.file_io",
        "isExtraImport": true,
        "detail": "iopath.common.file_io",
        "documentation": {}
    },
    {
        "label": "g_pathmgr",
        "importPath": "iopath.common.file_io",
        "description": "iopath.common.file_io",
        "isExtraImport": true,
        "detail": "iopath.common.file_io",
        "documentation": {}
    },
    {
        "label": "functools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "functools",
        "description": "functools",
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "lru_cache",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "reduce",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "lru_cache",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "default_collate",
        "importPath": "torch.utils.data._utils.collate",
        "description": "torch.utils.data._utils.collate",
        "isExtraImport": true,
        "detail": "torch.utils.data._utils.collate",
        "documentation": {}
    },
    {
        "label": "default_collate",
        "importPath": "torch.utils.data._utils.collate",
        "description": "torch.utils.data._utils.collate",
        "isExtraImport": true,
        "detail": "torch.utils.data._utils.collate",
        "documentation": {}
    },
    {
        "label": "DistributedSampler",
        "importPath": "torch.utils.data.distributed",
        "description": "torch.utils.data.distributed",
        "isExtraImport": true,
        "detail": "torch.utils.data.distributed",
        "documentation": {}
    },
    {
        "label": "DistributedSampler",
        "importPath": "torch.utils.data.distributed",
        "description": "torch.utils.data.distributed",
        "isExtraImport": true,
        "detail": "torch.utils.data.distributed",
        "documentation": {}
    },
    {
        "label": "DistributedSampler",
        "importPath": "torch.utils.data.distributed",
        "description": "torch.utils.data.distributed",
        "isExtraImport": true,
        "detail": "torch.utils.data.distributed",
        "documentation": {}
    },
    {
        "label": "DistributedSampler",
        "importPath": "torch.utils.data.distributed",
        "description": "torch.utils.data.distributed",
        "isExtraImport": true,
        "detail": "torch.utils.data.distributed",
        "documentation": {}
    },
    {
        "label": "RandomSampler",
        "importPath": "torch.utils.data.sampler",
        "description": "torch.utils.data.sampler",
        "isExtraImport": true,
        "detail": "torch.utils.data.sampler",
        "documentation": {}
    },
    {
        "label": "Sampler",
        "importPath": "torch.utils.data.sampler",
        "description": "torch.utils.data.sampler",
        "isExtraImport": true,
        "detail": "torch.utils.data.sampler",
        "documentation": {}
    },
    {
        "label": "RandomSampler",
        "importPath": "torch.utils.data.sampler",
        "description": "torch.utils.data.sampler",
        "isExtraImport": true,
        "detail": "torch.utils.data.sampler",
        "documentation": {}
    },
    {
        "label": "Sampler",
        "importPath": "torch.utils.data.sampler",
        "description": "torch.utils.data.sampler",
        "isExtraImport": true,
        "detail": "torch.utils.data.sampler",
        "documentation": {}
    },
    {
        "label": "ShortCycleBatchSampler",
        "importPath": "slowfast.datasets.multigrid_helper",
        "description": "slowfast.datasets.multigrid_helper",
        "isExtraImport": true,
        "detail": "slowfast.datasets.multigrid_helper",
        "documentation": {}
    },
    {
        "label": "ShortCycleBatchSampler",
        "importPath": "slowfast.datasets.multigrid_helper",
        "description": "slowfast.datasets.multigrid_helper",
        "isExtraImport": true,
        "detail": "slowfast.datasets.multigrid_helper",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "torchvision.transforms",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Compose",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Lambda",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Compose",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "Lambda",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "NormalizeVideo",
        "importPath": "torchvision.transforms._transforms_video",
        "description": "torchvision.transforms._transforms_video",
        "isExtraImport": true,
        "detail": "torchvision.transforms._transforms_video",
        "documentation": {}
    },
    {
        "label": "RandomCropVideo",
        "importPath": "torchvision.transforms._transforms_video",
        "description": "torchvision.transforms._transforms_video",
        "isExtraImport": true,
        "detail": "torchvision.transforms._transforms_video",
        "documentation": {}
    },
    {
        "label": "RandomHorizontalFlipVideo",
        "importPath": "torchvision.transforms._transforms_video",
        "description": "torchvision.transforms._transforms_video",
        "isExtraImport": true,
        "detail": "torchvision.transforms._transforms_video",
        "documentation": {}
    },
    {
        "label": "NormalizeVideo",
        "importPath": "torchvision.transforms._transforms_video",
        "description": "torchvision.transforms._transforms_video",
        "isExtraImport": true,
        "detail": "torchvision.transforms._transforms_video",
        "documentation": {}
    },
    {
        "label": "RandomCropVideo",
        "importPath": "torchvision.transforms._transforms_video",
        "description": "torchvision.transforms._transforms_video",
        "isExtraImport": true,
        "detail": "torchvision.transforms._transforms_video",
        "documentation": {}
    },
    {
        "label": "RandomHorizontalFlipVideo",
        "importPath": "torchvision.transforms._transforms_video",
        "description": "torchvision.transforms._transforms_video",
        "isExtraImport": true,
        "detail": "torchvision.transforms._transforms_video",
        "documentation": {}
    },
    {
        "label": "Charades",
        "importPath": "pytorchvideo.data",
        "description": "pytorchvideo.data",
        "isExtraImport": true,
        "detail": "pytorchvideo.data",
        "documentation": {}
    },
    {
        "label": "LabeledVideoDataset",
        "importPath": "pytorchvideo.data",
        "description": "pytorchvideo.data",
        "isExtraImport": true,
        "detail": "pytorchvideo.data",
        "documentation": {}
    },
    {
        "label": "SSv2",
        "importPath": "pytorchvideo.data",
        "description": "pytorchvideo.data",
        "isExtraImport": true,
        "detail": "pytorchvideo.data",
        "documentation": {}
    },
    {
        "label": "make_clip_sampler",
        "importPath": "pytorchvideo.data",
        "description": "pytorchvideo.data",
        "isExtraImport": true,
        "detail": "pytorchvideo.data",
        "documentation": {}
    },
    {
        "label": "Charades",
        "importPath": "pytorchvideo.data",
        "description": "pytorchvideo.data",
        "isExtraImport": true,
        "detail": "pytorchvideo.data",
        "documentation": {}
    },
    {
        "label": "LabeledVideoDataset",
        "importPath": "pytorchvideo.data",
        "description": "pytorchvideo.data",
        "isExtraImport": true,
        "detail": "pytorchvideo.data",
        "documentation": {}
    },
    {
        "label": "SSv2",
        "importPath": "pytorchvideo.data",
        "description": "pytorchvideo.data",
        "isExtraImport": true,
        "detail": "pytorchvideo.data",
        "documentation": {}
    },
    {
        "label": "make_clip_sampler",
        "importPath": "pytorchvideo.data",
        "description": "pytorchvideo.data",
        "isExtraImport": true,
        "detail": "pytorchvideo.data",
        "documentation": {}
    },
    {
        "label": "LabeledVideoPaths",
        "importPath": "pytorchvideo.data.labeled_video_paths",
        "description": "pytorchvideo.data.labeled_video_paths",
        "isExtraImport": true,
        "detail": "pytorchvideo.data.labeled_video_paths",
        "documentation": {}
    },
    {
        "label": "LabeledVideoPaths",
        "importPath": "pytorchvideo.data.labeled_video_paths",
        "description": "pytorchvideo.data.labeled_video_paths",
        "isExtraImport": true,
        "detail": "pytorchvideo.data.labeled_video_paths",
        "documentation": {}
    },
    {
        "label": "ApplyTransformToKey",
        "importPath": "pytorchvideo.transforms",
        "description": "pytorchvideo.transforms",
        "isExtraImport": true,
        "detail": "pytorchvideo.transforms",
        "documentation": {}
    },
    {
        "label": "RandomShortSideScale",
        "importPath": "pytorchvideo.transforms",
        "description": "pytorchvideo.transforms",
        "isExtraImport": true,
        "detail": "pytorchvideo.transforms",
        "documentation": {}
    },
    {
        "label": "ShortSideScale",
        "importPath": "pytorchvideo.transforms",
        "description": "pytorchvideo.transforms",
        "isExtraImport": true,
        "detail": "pytorchvideo.transforms",
        "documentation": {}
    },
    {
        "label": "UniformCropVideo",
        "importPath": "pytorchvideo.transforms",
        "description": "pytorchvideo.transforms",
        "isExtraImport": true,
        "detail": "pytorchvideo.transforms",
        "documentation": {}
    },
    {
        "label": "UniformTemporalSubsample",
        "importPath": "pytorchvideo.transforms",
        "description": "pytorchvideo.transforms",
        "isExtraImport": true,
        "detail": "pytorchvideo.transforms",
        "documentation": {}
    },
    {
        "label": "ApplyTransformToKey",
        "importPath": "pytorchvideo.transforms",
        "description": "pytorchvideo.transforms",
        "isExtraImport": true,
        "detail": "pytorchvideo.transforms",
        "documentation": {}
    },
    {
        "label": "RandomShortSideScale",
        "importPath": "pytorchvideo.transforms",
        "description": "pytorchvideo.transforms",
        "isExtraImport": true,
        "detail": "pytorchvideo.transforms",
        "documentation": {}
    },
    {
        "label": "ShortSideScale",
        "importPath": "pytorchvideo.transforms",
        "description": "pytorchvideo.transforms",
        "isExtraImport": true,
        "detail": "pytorchvideo.transforms",
        "documentation": {}
    },
    {
        "label": "UniformCropVideo",
        "importPath": "pytorchvideo.transforms",
        "description": "pytorchvideo.transforms",
        "isExtraImport": true,
        "detail": "pytorchvideo.transforms",
        "documentation": {}
    },
    {
        "label": "UniformTemporalSubsample",
        "importPath": "pytorchvideo.transforms",
        "description": "pytorchvideo.transforms",
        "isExtraImport": true,
        "detail": "pytorchvideo.transforms",
        "documentation": {}
    },
    {
        "label": "torchvision.transforms.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.transforms.functional",
        "description": "torchvision.transforms.functional",
        "detail": "torchvision.transforms.functional",
        "documentation": {}
    },
    {
        "label": "gaussian_filter",
        "importPath": "scipy.ndimage",
        "description": "scipy.ndimage",
        "isExtraImport": true,
        "detail": "scipy.ndimage",
        "documentation": {}
    },
    {
        "label": "gaussian_filter",
        "importPath": "scipy.ndimage",
        "description": "scipy.ndimage",
        "isExtraImport": true,
        "detail": "scipy.ndimage",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "av",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "av",
        "description": "av",
        "detail": "av",
        "documentation": {}
    },
    {
        "label": "decord",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "decord",
        "description": "decord",
        "detail": "decord",
        "documentation": {}
    },
    {
        "label": "VideoReader",
        "importPath": "decord",
        "description": "decord",
        "isExtraImport": true,
        "detail": "decord",
        "documentation": {}
    },
    {
        "label": "cpu",
        "importPath": "decord",
        "description": "decord",
        "isExtraImport": true,
        "detail": "decord",
        "documentation": {}
    },
    {
        "label": "VideoReader",
        "importPath": "decord",
        "description": "decord",
        "isExtraImport": true,
        "detail": "decord",
        "documentation": {}
    },
    {
        "label": "cpu",
        "importPath": "decord",
        "description": "decord",
        "isExtraImport": true,
        "detail": "decord",
        "documentation": {}
    },
    {
        "label": "torch.nn.init",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "importPath": "slowfast.models.common",
        "description": "slowfast.models.common",
        "isExtraImport": true,
        "detail": "slowfast.models.common",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "importPath": "slowfast.models.common",
        "description": "slowfast.models.common",
        "isExtraImport": true,
        "detail": "slowfast.models.common",
        "documentation": {}
    },
    {
        "label": "drop_path",
        "importPath": "slowfast.models.common",
        "description": "slowfast.models.common",
        "isExtraImport": true,
        "detail": "slowfast.models.common",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "importPath": "slowfast.models.common",
        "description": "slowfast.models.common",
        "isExtraImport": true,
        "detail": "slowfast.models.common",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "importPath": "slowfast.models.common",
        "description": "slowfast.models.common",
        "isExtraImport": true,
        "detail": "slowfast.models.common",
        "documentation": {}
    },
    {
        "label": "drop_path",
        "importPath": "slowfast.models.common",
        "description": "slowfast.models.common",
        "isExtraImport": true,
        "detail": "slowfast.models.common",
        "documentation": {}
    },
    {
        "label": "NaiveSyncBatchNorm1d",
        "importPath": "pytorchvideo.layers.batch_norm",
        "description": "pytorchvideo.layers.batch_norm",
        "isExtraImport": true,
        "detail": "pytorchvideo.layers.batch_norm",
        "documentation": {}
    },
    {
        "label": "NaiveSyncBatchNorm3d",
        "importPath": "pytorchvideo.layers.batch_norm",
        "description": "pytorchvideo.layers.batch_norm",
        "isExtraImport": true,
        "detail": "pytorchvideo.layers.batch_norm",
        "documentation": {}
    },
    {
        "label": "NaiveSyncBatchNorm1d",
        "importPath": "pytorchvideo.layers.batch_norm",
        "description": "pytorchvideo.layers.batch_norm",
        "isExtraImport": true,
        "detail": "pytorchvideo.layers.batch_norm",
        "documentation": {}
    },
    {
        "label": "NaiveSyncBatchNorm3d",
        "importPath": "pytorchvideo.layers.batch_norm",
        "description": "pytorchvideo.layers.batch_norm",
        "isExtraImport": true,
        "detail": "pytorchvideo.layers.batch_norm",
        "documentation": {}
    },
    {
        "label": "slowfast.models.losses",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "slowfast.models.losses",
        "description": "slowfast.models.losses",
        "detail": "slowfast.models.losses",
        "documentation": {}
    },
    {
        "label": "slowfast.utils.distributed",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "slowfast.utils.distributed",
        "description": "slowfast.utils.distributed",
        "detail": "slowfast.utils.distributed",
        "documentation": {}
    },
    {
        "label": "X3D",
        "importPath": "slowfast.models.video_model_builder",
        "description": "slowfast.models.video_model_builder",
        "isExtraImport": true,
        "detail": "slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "MViT",
        "importPath": "slowfast.models.video_model_builder",
        "description": "slowfast.models.video_model_builder",
        "isExtraImport": true,
        "detail": "slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "ResNet",
        "importPath": "slowfast.models.video_model_builder",
        "description": "slowfast.models.video_model_builder",
        "isExtraImport": true,
        "detail": "slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "SlowFast",
        "importPath": "slowfast.models.video_model_builder",
        "description": "slowfast.models.video_model_builder",
        "isExtraImport": true,
        "detail": "slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "_POOL1",
        "importPath": "slowfast.models.video_model_builder",
        "description": "slowfast.models.video_model_builder",
        "isExtraImport": true,
        "detail": "slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "_TEMPORAL_KERNEL_BASIS",
        "importPath": "slowfast.models.video_model_builder",
        "description": "slowfast.models.video_model_builder",
        "isExtraImport": true,
        "detail": "slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "X3D",
        "importPath": "slowfast.models.video_model_builder",
        "description": "slowfast.models.video_model_builder",
        "isExtraImport": true,
        "detail": "slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "MViT",
        "importPath": "slowfast.models.video_model_builder",
        "description": "slowfast.models.video_model_builder",
        "isExtraImport": true,
        "detail": "slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "ResNet",
        "importPath": "slowfast.models.video_model_builder",
        "description": "slowfast.models.video_model_builder",
        "isExtraImport": true,
        "detail": "slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "SlowFast",
        "importPath": "slowfast.models.video_model_builder",
        "description": "slowfast.models.video_model_builder",
        "isExtraImport": true,
        "detail": "slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "_POOL1",
        "importPath": "slowfast.models.video_model_builder",
        "description": "slowfast.models.video_model_builder",
        "isExtraImport": true,
        "detail": "slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "_TEMPORAL_KERNEL_BASIS",
        "importPath": "slowfast.models.video_model_builder",
        "description": "slowfast.models.video_model_builder",
        "isExtraImport": true,
        "detail": "slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "NaiveSyncBatchNorm1d",
        "importPath": "slowfast.models.batchnorm_helper",
        "description": "slowfast.models.batchnorm_helper",
        "isExtraImport": true,
        "detail": "slowfast.models.batchnorm_helper",
        "documentation": {}
    },
    {
        "label": "get_norm",
        "importPath": "slowfast.models.batchnorm_helper",
        "description": "slowfast.models.batchnorm_helper",
        "isExtraImport": true,
        "detail": "slowfast.models.batchnorm_helper",
        "documentation": {}
    },
    {
        "label": "get_norm",
        "importPath": "slowfast.models.batchnorm_helper",
        "description": "slowfast.models.batchnorm_helper",
        "isExtraImport": true,
        "detail": "slowfast.models.batchnorm_helper",
        "documentation": {}
    },
    {
        "label": "SubBatchNorm3d",
        "importPath": "slowfast.models.batchnorm_helper",
        "description": "slowfast.models.batchnorm_helper",
        "isExtraImport": true,
        "detail": "slowfast.models.batchnorm_helper",
        "documentation": {}
    },
    {
        "label": "NaiveSyncBatchNorm1d",
        "importPath": "slowfast.models.batchnorm_helper",
        "description": "slowfast.models.batchnorm_helper",
        "isExtraImport": true,
        "detail": "slowfast.models.batchnorm_helper",
        "documentation": {}
    },
    {
        "label": "get_norm",
        "importPath": "slowfast.models.batchnorm_helper",
        "description": "slowfast.models.batchnorm_helper",
        "isExtraImport": true,
        "detail": "slowfast.models.batchnorm_helper",
        "documentation": {}
    },
    {
        "label": "get_norm",
        "importPath": "slowfast.models.batchnorm_helper",
        "description": "slowfast.models.batchnorm_helper",
        "isExtraImport": true,
        "detail": "slowfast.models.batchnorm_helper",
        "documentation": {}
    },
    {
        "label": "SubBatchNorm3d",
        "importPath": "slowfast.models.batchnorm_helper",
        "description": "slowfast.models.batchnorm_helper",
        "isExtraImport": true,
        "detail": "slowfast.models.batchnorm_helper",
        "documentation": {}
    },
    {
        "label": "Nonlocal",
        "importPath": "slowfast.models.nonlocal_helper",
        "description": "slowfast.models.nonlocal_helper",
        "isExtraImport": true,
        "detail": "slowfast.models.nonlocal_helper",
        "documentation": {}
    },
    {
        "label": "Nonlocal",
        "importPath": "slowfast.models.nonlocal_helper",
        "description": "slowfast.models.nonlocal_helper",
        "isExtraImport": true,
        "detail": "slowfast.models.nonlocal_helper",
        "documentation": {}
    },
    {
        "label": "Nonlocal",
        "importPath": "slowfast.models.nonlocal_helper",
        "description": "slowfast.models.nonlocal_helper",
        "isExtraImport": true,
        "detail": "slowfast.models.nonlocal_helper",
        "documentation": {}
    },
    {
        "label": "Nonlocal",
        "importPath": "slowfast.models.nonlocal_helper",
        "description": "slowfast.models.nonlocal_helper",
        "isExtraImport": true,
        "detail": "slowfast.models.nonlocal_helper",
        "documentation": {}
    },
    {
        "label": "SoftTargetCrossEntropyLoss",
        "importPath": "pytorchvideo.losses.soft_target_cross_entropy",
        "description": "pytorchvideo.losses.soft_target_cross_entropy",
        "isExtraImport": true,
        "detail": "pytorchvideo.losses.soft_target_cross_entropy",
        "documentation": {}
    },
    {
        "label": "SoftTargetCrossEntropyLoss",
        "importPath": "pytorchvideo.losses.soft_target_cross_entropy",
        "description": "pytorchvideo.losses.soft_target_cross_entropy",
        "isExtraImport": true,
        "detail": "pytorchvideo.losses.soft_target_cross_entropy",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "to_2tuple",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "Swish",
        "importPath": "pytorchvideo.layers.swish",
        "description": "pytorchvideo.layers.swish",
        "isExtraImport": true,
        "detail": "pytorchvideo.layers.swish",
        "documentation": {}
    },
    {
        "label": "Swish",
        "importPath": "pytorchvideo.layers.swish",
        "description": "pytorchvideo.layers.swish",
        "isExtraImport": true,
        "detail": "pytorchvideo.layers.swish",
        "documentation": {}
    },
    {
        "label": "slowfast.utils.lr_policy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "slowfast.utils.lr_policy",
        "description": "slowfast.utils.lr_policy",
        "detail": "slowfast.utils.lr_policy",
        "documentation": {}
    },
    {
        "label": "ROIAlign",
        "importPath": "detectron2.layers",
        "description": "detectron2.layers",
        "isExtraImport": true,
        "detail": "detectron2.layers",
        "documentation": {}
    },
    {
        "label": "ROIAlign",
        "importPath": "detectron2.layers",
        "description": "detectron2.layers",
        "isExtraImport": true,
        "detail": "detectron2.layers",
        "documentation": {}
    },
    {
        "label": "create_csn",
        "importPath": "pytorchvideo.models.csn",
        "description": "pytorchvideo.models.csn",
        "isExtraImport": true,
        "detail": "pytorchvideo.models.csn",
        "documentation": {}
    },
    {
        "label": "create_csn",
        "importPath": "pytorchvideo.models.csn",
        "description": "pytorchvideo.models.csn",
        "isExtraImport": true,
        "detail": "pytorchvideo.models.csn",
        "documentation": {}
    },
    {
        "label": "create_res_basic_head",
        "importPath": "pytorchvideo.models.head",
        "description": "pytorchvideo.models.head",
        "isExtraImport": true,
        "detail": "pytorchvideo.models.head",
        "documentation": {}
    },
    {
        "label": "create_res_roi_pooling_head",
        "importPath": "pytorchvideo.models.head",
        "description": "pytorchvideo.models.head",
        "isExtraImport": true,
        "detail": "pytorchvideo.models.head",
        "documentation": {}
    },
    {
        "label": "create_res_basic_head",
        "importPath": "pytorchvideo.models.head",
        "description": "pytorchvideo.models.head",
        "isExtraImport": true,
        "detail": "pytorchvideo.models.head",
        "documentation": {}
    },
    {
        "label": "create_res_roi_pooling_head",
        "importPath": "pytorchvideo.models.head",
        "description": "pytorchvideo.models.head",
        "isExtraImport": true,
        "detail": "pytorchvideo.models.head",
        "documentation": {}
    },
    {
        "label": "create_2plus1d_bottleneck_block",
        "importPath": "pytorchvideo.models.r2plus1d",
        "description": "pytorchvideo.models.r2plus1d",
        "isExtraImport": true,
        "detail": "pytorchvideo.models.r2plus1d",
        "documentation": {}
    },
    {
        "label": "create_r2plus1d",
        "importPath": "pytorchvideo.models.r2plus1d",
        "description": "pytorchvideo.models.r2plus1d",
        "isExtraImport": true,
        "detail": "pytorchvideo.models.r2plus1d",
        "documentation": {}
    },
    {
        "label": "create_2plus1d_bottleneck_block",
        "importPath": "pytorchvideo.models.r2plus1d",
        "description": "pytorchvideo.models.r2plus1d",
        "isExtraImport": true,
        "detail": "pytorchvideo.models.r2plus1d",
        "documentation": {}
    },
    {
        "label": "create_r2plus1d",
        "importPath": "pytorchvideo.models.r2plus1d",
        "description": "pytorchvideo.models.r2plus1d",
        "isExtraImport": true,
        "detail": "pytorchvideo.models.r2plus1d",
        "documentation": {}
    },
    {
        "label": "create_bottleneck_block",
        "importPath": "pytorchvideo.models.resnet",
        "description": "pytorchvideo.models.resnet",
        "isExtraImport": true,
        "detail": "pytorchvideo.models.resnet",
        "documentation": {}
    },
    {
        "label": "create_resnet",
        "importPath": "pytorchvideo.models.resnet",
        "description": "pytorchvideo.models.resnet",
        "isExtraImport": true,
        "detail": "pytorchvideo.models.resnet",
        "documentation": {}
    },
    {
        "label": "create_bottleneck_block",
        "importPath": "pytorchvideo.models.resnet",
        "description": "pytorchvideo.models.resnet",
        "isExtraImport": true,
        "detail": "pytorchvideo.models.resnet",
        "documentation": {}
    },
    {
        "label": "create_resnet",
        "importPath": "pytorchvideo.models.resnet",
        "description": "pytorchvideo.models.resnet",
        "isExtraImport": true,
        "detail": "pytorchvideo.models.resnet",
        "documentation": {}
    },
    {
        "label": "create_slowfast",
        "importPath": "pytorchvideo.models.slowfast",
        "description": "pytorchvideo.models.slowfast",
        "isExtraImport": true,
        "detail": "pytorchvideo.models.slowfast",
        "documentation": {}
    },
    {
        "label": "create_slowfast",
        "importPath": "pytorchvideo.models.slowfast",
        "description": "pytorchvideo.models.slowfast",
        "isExtraImport": true,
        "detail": "pytorchvideo.models.slowfast",
        "documentation": {}
    },
    {
        "label": "create_multiscale_vision_transformers",
        "importPath": "pytorchvideo.models.vision_transformers",
        "description": "pytorchvideo.models.vision_transformers",
        "isExtraImport": true,
        "detail": "pytorchvideo.models.vision_transformers",
        "documentation": {}
    },
    {
        "label": "create_multiscale_vision_transformers",
        "importPath": "pytorchvideo.models.vision_transformers",
        "description": "pytorchvideo.models.vision_transformers",
        "isExtraImport": true,
        "detail": "pytorchvideo.models.vision_transformers",
        "documentation": {}
    },
    {
        "label": "Swish",
        "importPath": "pytorchvideo.models.x3d",
        "description": "pytorchvideo.models.x3d",
        "isExtraImport": true,
        "detail": "pytorchvideo.models.x3d",
        "documentation": {}
    },
    {
        "label": "create_x3d",
        "importPath": "pytorchvideo.models.x3d",
        "description": "pytorchvideo.models.x3d",
        "isExtraImport": true,
        "detail": "pytorchvideo.models.x3d",
        "documentation": {}
    },
    {
        "label": "create_x3d_bottleneck_block",
        "importPath": "pytorchvideo.models.x3d",
        "description": "pytorchvideo.models.x3d",
        "isExtraImport": true,
        "detail": "pytorchvideo.models.x3d",
        "documentation": {}
    },
    {
        "label": "Swish",
        "importPath": "pytorchvideo.models.x3d",
        "description": "pytorchvideo.models.x3d",
        "isExtraImport": true,
        "detail": "pytorchvideo.models.x3d",
        "documentation": {}
    },
    {
        "label": "create_x3d",
        "importPath": "pytorchvideo.models.x3d",
        "description": "pytorchvideo.models.x3d",
        "isExtraImport": true,
        "detail": "pytorchvideo.models.x3d",
        "documentation": {}
    },
    {
        "label": "create_x3d_bottleneck_block",
        "importPath": "pytorchvideo.models.x3d",
        "description": "pytorchvideo.models.x3d",
        "isExtraImport": true,
        "detail": "pytorchvideo.models.x3d",
        "documentation": {}
    },
    {
        "label": "SE",
        "importPath": "slowfast.models.operators",
        "description": "slowfast.models.operators",
        "isExtraImport": true,
        "detail": "slowfast.models.operators",
        "documentation": {}
    },
    {
        "label": "Swish",
        "importPath": "slowfast.models.operators",
        "description": "slowfast.models.operators",
        "isExtraImport": true,
        "detail": "slowfast.models.operators",
        "documentation": {}
    },
    {
        "label": "SE",
        "importPath": "slowfast.models.operators",
        "description": "slowfast.models.operators",
        "isExtraImport": true,
        "detail": "slowfast.models.operators",
        "documentation": {}
    },
    {
        "label": "Swish",
        "importPath": "slowfast.models.operators",
        "description": "slowfast.models.operators",
        "isExtraImport": true,
        "detail": "slowfast.models.operators",
        "documentation": {}
    },
    {
        "label": "slowfast.utils.weight_init_helper",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "slowfast.utils.weight_init_helper",
        "description": "slowfast.utils.weight_init_helper",
        "detail": "slowfast.utils.weight_init_helper",
        "documentation": {}
    },
    {
        "label": "MultiScaleBlock",
        "importPath": "slowfast.models.attention",
        "description": "slowfast.models.attention",
        "isExtraImport": true,
        "detail": "slowfast.models.attention",
        "documentation": {}
    },
    {
        "label": "MultiScaleBlock",
        "importPath": "slowfast.models.attention",
        "description": "slowfast.models.attention",
        "isExtraImport": true,
        "detail": "slowfast.models.attention",
        "documentation": {}
    },
    {
        "label": "round_width",
        "importPath": "slowfast.models.utils",
        "description": "slowfast.models.utils",
        "isExtraImport": true,
        "detail": "slowfast.models.utils",
        "documentation": {}
    },
    {
        "label": "validate_checkpoint_wrapper_import",
        "importPath": "slowfast.models.utils",
        "description": "slowfast.models.utils",
        "isExtraImport": true,
        "detail": "slowfast.models.utils",
        "documentation": {}
    },
    {
        "label": "round_width",
        "importPath": "slowfast.models.utils",
        "description": "slowfast.models.utils",
        "isExtraImport": true,
        "detail": "slowfast.models.utils",
        "documentation": {}
    },
    {
        "label": "validate_checkpoint_wrapper_import",
        "importPath": "slowfast.models.utils",
        "description": "slowfast.models.utils",
        "isExtraImport": true,
        "detail": "slowfast.models.utils",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "abc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "abc",
        "description": "abc",
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABCMeta",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABCMeta",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractclassmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "csv",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "csv",
        "description": "csv",
        "detail": "csv",
        "documentation": {}
    },
    {
        "label": "pprint",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pprint",
        "description": "pprint",
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "object_detection_evaluation",
        "importPath": "slowfast.utils.ava_evaluation",
        "description": "slowfast.utils.ava_evaluation",
        "isExtraImport": true,
        "detail": "slowfast.utils.ava_evaluation",
        "documentation": {}
    },
    {
        "label": "standard_fields",
        "importPath": "slowfast.utils.ava_evaluation",
        "description": "slowfast.utils.ava_evaluation",
        "isExtraImport": true,
        "detail": "slowfast.utils.ava_evaluation",
        "documentation": {}
    },
    {
        "label": "object_detection_evaluation",
        "importPath": "slowfast.utils.ava_evaluation",
        "description": "slowfast.utils.ava_evaluation",
        "isExtraImport": true,
        "detail": "slowfast.utils.ava_evaluation",
        "documentation": {}
    },
    {
        "label": "standard_fields",
        "importPath": "slowfast.utils.ava_evaluation",
        "description": "slowfast.utils.ava_evaluation",
        "isExtraImport": true,
        "detail": "slowfast.utils.ava_evaluation",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tqdm",
        "description": "tqdm",
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "Timer",
        "importPath": "fvcore.common.timer",
        "description": "fvcore.common.timer",
        "isExtraImport": true,
        "detail": "fvcore.common.timer",
        "documentation": {}
    },
    {
        "label": "Timer",
        "importPath": "fvcore.common.timer",
        "description": "fvcore.common.timer",
        "isExtraImport": true,
        "detail": "fvcore.common.timer",
        "documentation": {}
    },
    {
        "label": "Timer",
        "importPath": "fvcore.common.timer",
        "description": "fvcore.common.timer",
        "isExtraImport": true,
        "detail": "fvcore.common.timer",
        "documentation": {}
    },
    {
        "label": "Timer",
        "importPath": "fvcore.common.timer",
        "description": "fvcore.common.timer",
        "isExtraImport": true,
        "detail": "fvcore.common.timer",
        "documentation": {}
    },
    {
        "label": "slowfast.utils.misc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "slowfast.utils.misc",
        "description": "slowfast.utils.misc",
        "detail": "slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "get_class_names",
        "importPath": "slowfast.utils.misc",
        "description": "slowfast.utils.misc",
        "isExtraImport": true,
        "detail": "slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "get_class_names",
        "importPath": "slowfast.utils.misc",
        "description": "slowfast.utils.misc",
        "isExtraImport": true,
        "detail": "slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "get_class_names",
        "importPath": "slowfast.utils.misc",
        "description": "slowfast.utils.misc",
        "isExtraImport": true,
        "detail": "slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "get_class_names",
        "importPath": "slowfast.utils.misc",
        "description": "slowfast.utils.misc",
        "isExtraImport": true,
        "detail": "slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "get_class_names",
        "importPath": "slowfast.utils.misc",
        "description": "slowfast.utils.misc",
        "isExtraImport": true,
        "detail": "slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "get_class_names",
        "importPath": "slowfast.utils.misc",
        "description": "slowfast.utils.misc",
        "isExtraImport": true,
        "detail": "slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "launch_job",
        "importPath": "slowfast.utils.misc",
        "description": "slowfast.utils.misc",
        "isExtraImport": true,
        "detail": "slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "launch_job",
        "importPath": "slowfast.utils.misc",
        "description": "slowfast.utils.misc",
        "isExtraImport": true,
        "detail": "slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "loader",
        "importPath": "slowfast.datasets",
        "description": "slowfast.datasets",
        "isExtraImport": true,
        "detail": "slowfast.datasets",
        "documentation": {}
    },
    {
        "label": "cv2_transform",
        "importPath": "slowfast.datasets",
        "description": "slowfast.datasets",
        "isExtraImport": true,
        "detail": "slowfast.datasets",
        "documentation": {}
    },
    {
        "label": "cv2_transform",
        "importPath": "slowfast.datasets",
        "description": "slowfast.datasets",
        "isExtraImport": true,
        "detail": "slowfast.datasets",
        "documentation": {}
    },
    {
        "label": "loader",
        "importPath": "slowfast.datasets",
        "description": "slowfast.datasets",
        "isExtraImport": true,
        "detail": "slowfast.datasets",
        "documentation": {}
    },
    {
        "label": "cv2_transform",
        "importPath": "slowfast.datasets",
        "description": "slowfast.datasets",
        "isExtraImport": true,
        "detail": "slowfast.datasets",
        "documentation": {}
    },
    {
        "label": "cv2_transform",
        "importPath": "slowfast.datasets",
        "description": "slowfast.datasets",
        "isExtraImport": true,
        "detail": "slowfast.datasets",
        "documentation": {}
    },
    {
        "label": "loader",
        "importPath": "slowfast.datasets",
        "description": "slowfast.datasets",
        "isExtraImport": true,
        "detail": "slowfast.datasets",
        "documentation": {}
    },
    {
        "label": "loader",
        "importPath": "slowfast.datasets",
        "description": "slowfast.datasets",
        "isExtraImport": true,
        "detail": "slowfast.datasets",
        "documentation": {}
    },
    {
        "label": "loader",
        "importPath": "slowfast.datasets",
        "description": "slowfast.datasets",
        "isExtraImport": true,
        "detail": "slowfast.datasets",
        "documentation": {}
    },
    {
        "label": "get_name_convert_func",
        "importPath": "slowfast.utils.c2_model_loading",
        "description": "slowfast.utils.c2_model_loading",
        "isExtraImport": true,
        "detail": "slowfast.utils.c2_model_loading",
        "documentation": {}
    },
    {
        "label": "get_name_convert_func",
        "importPath": "slowfast.utils.c2_model_loading",
        "description": "slowfast.utils.c2_model_loading",
        "isExtraImport": true,
        "detail": "slowfast.utils.c2_model_loading",
        "documentation": {}
    },
    {
        "label": "get_name_convert_func",
        "importPath": "slowfast.utils.c2_model_loading",
        "description": "slowfast.utils.c2_model_loading",
        "isExtraImport": true,
        "detail": "slowfast.utils.c2_model_loading",
        "documentation": {}
    },
    {
        "label": "get_name_convert_func",
        "importPath": "slowfast.utils.c2_model_loading",
        "description": "slowfast.utils.c2_model_loading",
        "isExtraImport": true,
        "detail": "slowfast.utils.c2_model_loading",
        "documentation": {}
    },
    {
        "label": "torch.distributed",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.distributed",
        "description": "torch.distributed",
        "detail": "torch.distributed",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    cat_all_gather",
        "importPath": "pytorchvideo.layers.distributed",
        "description": "pytorchvideo.layers.distributed",
        "isExtraImport": true,
        "detail": "pytorchvideo.layers.distributed",
        "documentation": {}
    },
    {
        "label": "get_local_process_group",
        "importPath": "pytorchvideo.layers.distributed",
        "description": "pytorchvideo.layers.distributed",
        "isExtraImport": true,
        "detail": "pytorchvideo.layers.distributed",
        "documentation": {}
    },
    {
        "label": "get_local_rank",
        "importPath": "pytorchvideo.layers.distributed",
        "description": "pytorchvideo.layers.distributed",
        "isExtraImport": true,
        "detail": "pytorchvideo.layers.distributed",
        "documentation": {}
    },
    {
        "label": "get_local_size",
        "importPath": "pytorchvideo.layers.distributed",
        "description": "pytorchvideo.layers.distributed",
        "isExtraImport": true,
        "detail": "pytorchvideo.layers.distributed",
        "documentation": {}
    },
    {
        "label": "get_world_size",
        "importPath": "pytorchvideo.layers.distributed",
        "description": "pytorchvideo.layers.distributed",
        "isExtraImport": true,
        "detail": "pytorchvideo.layers.distributed",
        "documentation": {}
    },
    {
        "label": "init_distributed_training",
        "importPath": "pytorchvideo.layers.distributed",
        "description": "pytorchvideo.layers.distributed",
        "isExtraImport": true,
        "detail": "pytorchvideo.layers.distributed",
        "documentation": {}
    },
    {
        "label": "# noqa\r\n    cat_all_gather",
        "importPath": "pytorchvideo.layers.distributed",
        "description": "pytorchvideo.layers.distributed",
        "isExtraImport": true,
        "detail": "pytorchvideo.layers.distributed",
        "documentation": {}
    },
    {
        "label": "get_local_process_group",
        "importPath": "pytorchvideo.layers.distributed",
        "description": "pytorchvideo.layers.distributed",
        "isExtraImport": true,
        "detail": "pytorchvideo.layers.distributed",
        "documentation": {}
    },
    {
        "label": "get_local_rank",
        "importPath": "pytorchvideo.layers.distributed",
        "description": "pytorchvideo.layers.distributed",
        "isExtraImport": true,
        "detail": "pytorchvideo.layers.distributed",
        "documentation": {}
    },
    {
        "label": "get_local_size",
        "importPath": "pytorchvideo.layers.distributed",
        "description": "pytorchvideo.layers.distributed",
        "isExtraImport": true,
        "detail": "pytorchvideo.layers.distributed",
        "documentation": {}
    },
    {
        "label": "get_world_size",
        "importPath": "pytorchvideo.layers.distributed",
        "description": "pytorchvideo.layers.distributed",
        "isExtraImport": true,
        "detail": "pytorchvideo.layers.distributed",
        "documentation": {}
    },
    {
        "label": "init_distributed_training",
        "importPath": "pytorchvideo.layers.distributed",
        "description": "pytorchvideo.layers.distributed",
        "isExtraImport": true,
        "detail": "pytorchvideo.layers.distributed",
        "documentation": {}
    },
    {
        "label": "atexit",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "atexit",
        "description": "atexit",
        "detail": "atexit",
        "documentation": {}
    },
    {
        "label": "builtins",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "builtins",
        "description": "builtins",
        "detail": "builtins",
        "documentation": {}
    },
    {
        "label": "decimal",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "decimal",
        "description": "decimal",
        "detail": "decimal",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "simplejson",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "simplejson",
        "description": "simplejson",
        "detail": "simplejson",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "average_precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "confusion_matrix",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "average_precision_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "confusion_matrix",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "slowfast.datasets.ava_helper",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "slowfast.datasets.ava_helper",
        "description": "slowfast.datasets.ava_helper",
        "detail": "slowfast.datasets.ava_helper",
        "documentation": {}
    },
    {
        "label": "parse_bboxes_file",
        "importPath": "slowfast.datasets.ava_helper",
        "description": "slowfast.datasets.ava_helper",
        "isExtraImport": true,
        "detail": "slowfast.datasets.ava_helper",
        "documentation": {}
    },
    {
        "label": "parse_bboxes_file",
        "importPath": "slowfast.datasets.ava_helper",
        "description": "slowfast.datasets.ava_helper",
        "isExtraImport": true,
        "detail": "slowfast.datasets.ava_helper",
        "documentation": {}
    },
    {
        "label": "slowfast.utils.metrics",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "slowfast.utils.metrics",
        "description": "slowfast.utils.metrics",
        "detail": "slowfast.utils.metrics",
        "documentation": {}
    },
    {
        "label": "evaluate_ava",
        "importPath": "slowfast.utils.ava_eval_helper",
        "description": "slowfast.utils.ava_eval_helper",
        "isExtraImport": true,
        "detail": "slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "read_csv",
        "importPath": "slowfast.utils.ava_eval_helper",
        "description": "slowfast.utils.ava_eval_helper",
        "isExtraImport": true,
        "detail": "slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "read_exclusions",
        "importPath": "slowfast.utils.ava_eval_helper",
        "description": "slowfast.utils.ava_eval_helper",
        "isExtraImport": true,
        "detail": "slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "read_labelmap",
        "importPath": "slowfast.utils.ava_eval_helper",
        "description": "slowfast.utils.ava_eval_helper",
        "isExtraImport": true,
        "detail": "slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "evaluate_ava",
        "importPath": "slowfast.utils.ava_eval_helper",
        "description": "slowfast.utils.ava_eval_helper",
        "isExtraImport": true,
        "detail": "slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "read_csv",
        "importPath": "slowfast.utils.ava_eval_helper",
        "description": "slowfast.utils.ava_eval_helper",
        "isExtraImport": true,
        "detail": "slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "read_exclusions",
        "importPath": "slowfast.utils.ava_eval_helper",
        "description": "slowfast.utils.ava_eval_helper",
        "isExtraImport": true,
        "detail": "slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "read_labelmap",
        "importPath": "slowfast.utils.ava_eval_helper",
        "description": "slowfast.utils.ava_eval_helper",
        "isExtraImport": true,
        "detail": "slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "psutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "psutil",
        "description": "psutil",
        "detail": "psutil",
        "documentation": {}
    },
    {
        "label": "activation_count",
        "importPath": "fvcore.nn.activation_count",
        "description": "fvcore.nn.activation_count",
        "isExtraImport": true,
        "detail": "fvcore.nn.activation_count",
        "documentation": {}
    },
    {
        "label": "activation_count",
        "importPath": "fvcore.nn.activation_count",
        "description": "fvcore.nn.activation_count",
        "isExtraImport": true,
        "detail": "fvcore.nn.activation_count",
        "documentation": {}
    },
    {
        "label": "flop_count",
        "importPath": "fvcore.nn.flop_count",
        "description": "fvcore.nn.flop_count",
        "isExtraImport": true,
        "detail": "fvcore.nn.flop_count",
        "documentation": {}
    },
    {
        "label": "flop_count",
        "importPath": "fvcore.nn.flop_count",
        "description": "fvcore.nn.flop_count",
        "isExtraImport": true,
        "detail": "fvcore.nn.flop_count",
        "documentation": {}
    },
    {
        "label": "pyplot",
        "importPath": "matplotlib",
        "description": "matplotlib",
        "isExtraImport": true,
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "pyplot",
        "importPath": "matplotlib",
        "description": "matplotlib",
        "isExtraImport": true,
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "slowfast.utils.multiprocessing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "slowfast.utils.multiprocessing",
        "description": "slowfast.utils.multiprocessing",
        "detail": "slowfast.utils.multiprocessing",
        "documentation": {}
    },
    {
        "label": "slowfast.datasets.utils",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "slowfast.datasets.utils",
        "description": "slowfast.datasets.utils",
        "detail": "slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "pack_pathway_output",
        "importPath": "slowfast.datasets.utils",
        "description": "slowfast.datasets.utils",
        "isExtraImport": true,
        "detail": "slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "get_sequence",
        "importPath": "slowfast.datasets.utils",
        "description": "slowfast.datasets.utils",
        "isExtraImport": true,
        "detail": "slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "pack_pathway_output",
        "importPath": "slowfast.datasets.utils",
        "description": "slowfast.datasets.utils",
        "isExtraImport": true,
        "detail": "slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "tensor_normalize",
        "importPath": "slowfast.datasets.utils",
        "description": "slowfast.datasets.utils",
        "isExtraImport": true,
        "detail": "slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "pack_pathway_output",
        "importPath": "slowfast.datasets.utils",
        "description": "slowfast.datasets.utils",
        "isExtraImport": true,
        "detail": "slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "get_sequence",
        "importPath": "slowfast.datasets.utils",
        "description": "slowfast.datasets.utils",
        "isExtraImport": true,
        "detail": "slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "pack_pathway_output",
        "importPath": "slowfast.datasets.utils",
        "description": "slowfast.datasets.utils",
        "isExtraImport": true,
        "detail": "slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "tensor_normalize",
        "importPath": "slowfast.datasets.utils",
        "description": "slowfast.datasets.utils",
        "isExtraImport": true,
        "detail": "slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "slowfast.utils.checkpoint",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "slowfast.utils.checkpoint",
        "description": "slowfast.utils.checkpoint",
        "detail": "slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "get_cfg",
        "importPath": "slowfast.config.defaults",
        "description": "slowfast.config.defaults",
        "isExtraImport": true,
        "detail": "slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "get_cfg",
        "importPath": "slowfast.config.defaults",
        "description": "slowfast.config.defaults",
        "isExtraImport": true,
        "detail": "slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "assert_and_infer_cfg",
        "importPath": "slowfast.config.defaults",
        "description": "slowfast.config.defaults",
        "isExtraImport": true,
        "detail": "slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "c2_msra_fill",
        "importPath": "fvcore.nn.weight_init",
        "description": "fvcore.nn.weight_init",
        "isExtraImport": true,
        "detail": "fvcore.nn.weight_init",
        "documentation": {}
    },
    {
        "label": "c2_xavier_fill",
        "importPath": "fvcore.nn.weight_init",
        "description": "fvcore.nn.weight_init",
        "isExtraImport": true,
        "detail": "fvcore.nn.weight_init",
        "documentation": {}
    },
    {
        "label": "c2_msra_fill",
        "importPath": "fvcore.nn.weight_init",
        "description": "fvcore.nn.weight_init",
        "isExtraImport": true,
        "detail": "fvcore.nn.weight_init",
        "documentation": {}
    },
    {
        "label": "c2_xavier_fill",
        "importPath": "fvcore.nn.weight_init",
        "description": "fvcore.nn.weight_init",
        "isExtraImport": true,
        "detail": "fvcore.nn.weight_init",
        "documentation": {}
    },
    {
        "label": "c2_msra_fill",
        "importPath": "fvcore.nn.weight_init",
        "description": "fvcore.nn.weight_init",
        "isExtraImport": true,
        "detail": "fvcore.nn.weight_init",
        "documentation": {}
    },
    {
        "label": "c2_xavier_fill",
        "importPath": "fvcore.nn.weight_init",
        "description": "fvcore.nn.weight_init",
        "isExtraImport": true,
        "detail": "fvcore.nn.weight_init",
        "documentation": {}
    },
    {
        "label": "torch.multiprocessing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.multiprocessing",
        "description": "torch.multiprocessing",
        "detail": "torch.multiprocessing",
        "documentation": {}
    },
    {
        "label": "Predictor",
        "importPath": "slowfast.visualization.predictor",
        "description": "slowfast.visualization.predictor",
        "isExtraImport": true,
        "detail": "slowfast.visualization.predictor",
        "documentation": {}
    },
    {
        "label": "Predictor",
        "importPath": "slowfast.visualization.predictor",
        "description": "slowfast.visualization.predictor",
        "isExtraImport": true,
        "detail": "slowfast.visualization.predictor",
        "documentation": {}
    },
    {
        "label": "ActionPredictor",
        "importPath": "slowfast.visualization.predictor",
        "description": "slowfast.visualization.predictor",
        "isExtraImport": true,
        "detail": "slowfast.visualization.predictor",
        "documentation": {}
    },
    {
        "label": "scale",
        "importPath": "slowfast.datasets.cv2_transform",
        "description": "slowfast.datasets.cv2_transform",
        "isExtraImport": true,
        "detail": "slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "scale_boxes",
        "importPath": "slowfast.datasets.cv2_transform",
        "description": "slowfast.datasets.cv2_transform",
        "isExtraImport": true,
        "detail": "slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "scale",
        "importPath": "slowfast.datasets.cv2_transform",
        "description": "slowfast.datasets.cv2_transform",
        "isExtraImport": true,
        "detail": "slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "scale_boxes",
        "importPath": "slowfast.datasets.cv2_transform",
        "description": "slowfast.datasets.cv2_transform",
        "isExtraImport": true,
        "detail": "slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "build_model",
        "importPath": "slowfast.models",
        "description": "slowfast.models",
        "isExtraImport": true,
        "detail": "slowfast.models",
        "documentation": {}
    },
    {
        "label": "build_model",
        "importPath": "slowfast.models",
        "description": "slowfast.models",
        "isExtraImport": true,
        "detail": "slowfast.models",
        "documentation": {}
    },
    {
        "label": "build_model",
        "importPath": "slowfast.models",
        "description": "slowfast.models",
        "isExtraImport": true,
        "detail": "slowfast.models",
        "documentation": {}
    },
    {
        "label": "build_model",
        "importPath": "slowfast.models",
        "description": "slowfast.models",
        "isExtraImport": true,
        "detail": "slowfast.models",
        "documentation": {}
    },
    {
        "label": "build_model",
        "importPath": "slowfast.models",
        "description": "slowfast.models",
        "isExtraImport": true,
        "detail": "slowfast.models",
        "documentation": {}
    },
    {
        "label": "build_model",
        "importPath": "slowfast.models",
        "description": "slowfast.models",
        "isExtraImport": true,
        "detail": "slowfast.models",
        "documentation": {}
    },
    {
        "label": "build_model",
        "importPath": "slowfast.models",
        "description": "slowfast.models",
        "isExtraImport": true,
        "detail": "slowfast.models",
        "documentation": {}
    },
    {
        "label": "misc",
        "importPath": "slowfast.utils",
        "description": "slowfast.utils",
        "isExtraImport": true,
        "detail": "slowfast.utils",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "slowfast.utils",
        "description": "slowfast.utils",
        "isExtraImport": true,
        "detail": "slowfast.utils",
        "documentation": {}
    },
    {
        "label": "misc",
        "importPath": "slowfast.utils",
        "description": "slowfast.utils",
        "isExtraImport": true,
        "detail": "slowfast.utils",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "slowfast.utils",
        "description": "slowfast.utils",
        "isExtraImport": true,
        "detail": "slowfast.utils",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "slowfast.utils",
        "description": "slowfast.utils",
        "isExtraImport": true,
        "detail": "slowfast.utils",
        "documentation": {}
    },
    {
        "label": "slowfast.visualization.utils",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "slowfast.visualization.utils",
        "description": "slowfast.visualization.utils",
        "detail": "slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "process_cv2_inputs",
        "importPath": "slowfast.visualization.utils",
        "description": "slowfast.visualization.utils",
        "isExtraImport": true,
        "detail": "slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "TaskInfo",
        "importPath": "slowfast.visualization.utils",
        "description": "slowfast.visualization.utils",
        "isExtraImport": true,
        "detail": "slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "get_layer",
        "importPath": "slowfast.visualization.utils",
        "description": "slowfast.visualization.utils",
        "isExtraImport": true,
        "detail": "slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "process_cv2_inputs",
        "importPath": "slowfast.visualization.utils",
        "description": "slowfast.visualization.utils",
        "isExtraImport": true,
        "detail": "slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "process_cv2_inputs",
        "importPath": "slowfast.visualization.utils",
        "description": "slowfast.visualization.utils",
        "isExtraImport": true,
        "detail": "slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "TaskInfo",
        "importPath": "slowfast.visualization.utils",
        "description": "slowfast.visualization.utils",
        "isExtraImport": true,
        "detail": "slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "get_layer",
        "importPath": "slowfast.visualization.utils",
        "description": "slowfast.visualization.utils",
        "isExtraImport": true,
        "detail": "slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "process_cv2_inputs",
        "importPath": "slowfast.visualization.utils",
        "description": "slowfast.visualization.utils",
        "isExtraImport": true,
        "detail": "slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "GetWeightAndActivation",
        "importPath": "slowfast.visualization.utils",
        "description": "slowfast.visualization.utils",
        "isExtraImport": true,
        "detail": "slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "process_layer_index_data",
        "importPath": "slowfast.visualization.utils",
        "description": "slowfast.visualization.utils",
        "isExtraImport": true,
        "detail": "slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "VideoVisualizer",
        "importPath": "slowfast.visualization.video_visualizer",
        "description": "slowfast.visualization.video_visualizer",
        "isExtraImport": true,
        "detail": "slowfast.visualization.video_visualizer",
        "documentation": {}
    },
    {
        "label": "VideoVisualizer",
        "importPath": "slowfast.visualization.video_visualizer",
        "description": "slowfast.visualization.video_visualizer",
        "isExtraImport": true,
        "detail": "slowfast.visualization.video_visualizer",
        "documentation": {}
    },
    {
        "label": "VideoVisualizer",
        "importPath": "slowfast.visualization.video_visualizer",
        "description": "slowfast.visualization.video_visualizer",
        "isExtraImport": true,
        "detail": "slowfast.visualization.video_visualizer",
        "documentation": {}
    },
    {
        "label": "VideoVisualizer",
        "importPath": "slowfast.visualization.video_visualizer",
        "description": "slowfast.visualization.video_visualizer",
        "isExtraImport": true,
        "detail": "slowfast.visualization.video_visualizer",
        "documentation": {}
    },
    {
        "label": "VideoVisualizer",
        "importPath": "slowfast.visualization.video_visualizer",
        "description": "slowfast.visualization.video_visualizer",
        "isExtraImport": true,
        "detail": "slowfast.visualization.video_visualizer",
        "documentation": {}
    },
    {
        "label": "VideoVisualizer",
        "importPath": "slowfast.visualization.video_visualizer",
        "description": "slowfast.visualization.video_visualizer",
        "isExtraImport": true,
        "detail": "slowfast.visualization.video_visualizer",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "MultipleLocator",
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "isExtraImport": true,
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "slowfast.visualization.tensorboard_vis",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "slowfast.visualization.tensorboard_vis",
        "description": "slowfast.visualization.tensorboard_vis",
        "detail": "slowfast.visualization.tensorboard_vis",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "make_grid",
        "importPath": "torchvision.utils",
        "description": "torchvision.utils",
        "isExtraImport": true,
        "detail": "torchvision.utils",
        "documentation": {}
    },
    {
        "label": "make_grid",
        "importPath": "torchvision.utils",
        "description": "torchvision.utils",
        "isExtraImport": true,
        "detail": "torchvision.utils",
        "documentation": {}
    },
    {
        "label": "benchmark_data_loading",
        "importPath": "slowfast.utils.benchmark",
        "description": "slowfast.utils.benchmark",
        "isExtraImport": true,
        "detail": "slowfast.utils.benchmark",
        "documentation": {}
    },
    {
        "label": "load_config",
        "importPath": "slowfast.utils.parser",
        "description": "slowfast.utils.parser",
        "isExtraImport": true,
        "detail": "slowfast.utils.parser",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "importPath": "slowfast.utils.parser",
        "description": "slowfast.utils.parser",
        "isExtraImport": true,
        "detail": "slowfast.utils.parser",
        "documentation": {}
    },
    {
        "label": "load_config",
        "importPath": "slowfast.utils.parser",
        "description": "slowfast.utils.parser",
        "isExtraImport": true,
        "detail": "slowfast.utils.parser",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "importPath": "slowfast.utils.parser",
        "description": "slowfast.utils.parser",
        "isExtraImport": true,
        "detail": "slowfast.utils.parser",
        "documentation": {}
    },
    {
        "label": "AsyncDemo",
        "importPath": "slowfast.visualization.async_predictor",
        "description": "slowfast.visualization.async_predictor",
        "isExtraImport": true,
        "detail": "slowfast.visualization.async_predictor",
        "documentation": {}
    },
    {
        "label": "AsyncVis",
        "importPath": "slowfast.visualization.async_predictor",
        "description": "slowfast.visualization.async_predictor",
        "isExtraImport": true,
        "detail": "slowfast.visualization.async_predictor",
        "documentation": {}
    },
    {
        "label": "AVAVisualizerWithPrecomputedBox",
        "importPath": "slowfast.visualization.ava_demo_precomputed_boxes",
        "description": "slowfast.visualization.ava_demo_precomputed_boxes",
        "isExtraImport": true,
        "detail": "slowfast.visualization.ava_demo_precomputed_boxes",
        "documentation": {}
    },
    {
        "label": "ThreadVideoManager",
        "importPath": "slowfast.visualization.demo_loader",
        "description": "slowfast.visualization.demo_loader",
        "isExtraImport": true,
        "detail": "slowfast.visualization.demo_loader",
        "documentation": {}
    },
    {
        "label": "VideoManager",
        "importPath": "slowfast.visualization.demo_loader",
        "description": "slowfast.visualization.demo_loader",
        "isExtraImport": true,
        "detail": "slowfast.visualization.demo_loader",
        "documentation": {}
    },
    {
        "label": "test",
        "importPath": "test_net",
        "description": "test_net",
        "isExtraImport": true,
        "detail": "test_net",
        "documentation": {}
    },
    {
        "label": "train",
        "importPath": "train_net",
        "description": "train_net",
        "isExtraImport": true,
        "detail": "train_net",
        "documentation": {}
    },
    {
        "label": "AVAMeter",
        "importPath": "slowfast.utils.meters",
        "description": "slowfast.utils.meters",
        "isExtraImport": true,
        "detail": "slowfast.utils.meters",
        "documentation": {}
    },
    {
        "label": "TestMeter",
        "importPath": "slowfast.utils.meters",
        "description": "slowfast.utils.meters",
        "isExtraImport": true,
        "detail": "slowfast.utils.meters",
        "documentation": {}
    },
    {
        "label": "AVAMeter",
        "importPath": "slowfast.utils.meters",
        "description": "slowfast.utils.meters",
        "isExtraImport": true,
        "detail": "slowfast.utils.meters",
        "documentation": {}
    },
    {
        "label": "EpochTimer",
        "importPath": "slowfast.utils.meters",
        "description": "slowfast.utils.meters",
        "isExtraImport": true,
        "detail": "slowfast.utils.meters",
        "documentation": {}
    },
    {
        "label": "TrainMeter",
        "importPath": "slowfast.utils.meters",
        "description": "slowfast.utils.meters",
        "isExtraImport": true,
        "detail": "slowfast.utils.meters",
        "documentation": {}
    },
    {
        "label": "ValMeter",
        "importPath": "slowfast.utils.meters",
        "description": "slowfast.utils.meters",
        "isExtraImport": true,
        "detail": "slowfast.utils.meters",
        "documentation": {}
    },
    {
        "label": "get_bn_modules",
        "importPath": "fvcore.nn.precise_bn",
        "description": "fvcore.nn.precise_bn",
        "isExtraImport": true,
        "detail": "fvcore.nn.precise_bn",
        "documentation": {}
    },
    {
        "label": "update_bn_stats",
        "importPath": "fvcore.nn.precise_bn",
        "description": "fvcore.nn.precise_bn",
        "isExtraImport": true,
        "detail": "fvcore.nn.precise_bn",
        "documentation": {}
    },
    {
        "label": "NativeScaler",
        "importPath": "timm.utils",
        "description": "timm.utils",
        "isExtraImport": true,
        "detail": "timm.utils",
        "documentation": {}
    },
    {
        "label": "slowfast.models.optimizer",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "slowfast.models.optimizer",
        "description": "slowfast.models.optimizer",
        "detail": "slowfast.models.optimizer",
        "documentation": {}
    },
    {
        "label": "slowfast.utils.checkpoint_amp",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "slowfast.utils.checkpoint_amp",
        "description": "slowfast.utils.checkpoint_amp",
        "detail": "slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "MixUp",
        "importPath": "slowfast.datasets.mixup",
        "description": "slowfast.datasets.mixup",
        "isExtraImport": true,
        "detail": "slowfast.datasets.mixup",
        "documentation": {}
    },
    {
        "label": "MultigridSchedule",
        "importPath": "slowfast.utils.multigrid",
        "description": "slowfast.utils.multigrid",
        "isExtraImport": true,
        "detail": "slowfast.utils.multigrid",
        "documentation": {}
    },
    {
        "label": "GradCAM",
        "importPath": "slowfast.visualization.gradcam_utils",
        "description": "slowfast.visualization.gradcam_utils",
        "isExtraImport": true,
        "detail": "slowfast.visualization.gradcam_utils",
        "documentation": {}
    },
    {
        "label": "WrongPredictionVis",
        "importPath": "slowfast.visualization.prediction_vis",
        "description": "slowfast.visualization.prediction_vis",
        "isExtraImport": true,
        "detail": "slowfast.visualization.prediction_vis",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "train",
        "description": "train",
        "isExtraImport": true,
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "predict",
        "description": "predict",
        "isExtraImport": true,
        "detail": "predict",
        "documentation": {}
    },
    {
        "label": "Centroid",
        "importPath": "centroid",
        "description": "centroid",
        "isExtraImport": true,
        "detail": "centroid",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "loss",
        "description": "loss",
        "isExtraImport": true,
        "detail": "loss",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "tensorboardX",
        "description": "tensorboardX",
        "isExtraImport": true,
        "detail": "tensorboardX",
        "documentation": {}
    },
    {
        "label": "onnxruntime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "onnxruntime",
        "description": "onnxruntime",
        "detail": "onnxruntime",
        "documentation": {}
    },
    {
        "label": "os.path",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.path",
        "description": "os.path",
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "Parallel",
        "importPath": "joblib",
        "description": "joblib",
        "isExtraImport": true,
        "detail": "joblib",
        "documentation": {}
    },
    {
        "label": "delayed",
        "importPath": "joblib",
        "description": "joblib",
        "isExtraImport": true,
        "detail": "joblib",
        "documentation": {}
    },
    {
        "label": "load_state_dict",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_state_dict",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "_load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_state_dict",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "_load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "_load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_state_dict",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_state_dict",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_state_dict",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "torch.utils.checkpoint",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.utils.checkpoint",
        "description": "torch.utils.checkpoint",
        "detail": "torch.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "ConvModule",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "constant_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "kaiming_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "ConvModule",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "constant_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "kaiming_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "CONV_LAYERS",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "build_norm_layer",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "constant_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "kaiming_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "ConvModule",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "build_norm_layer",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "constant_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "constant_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "kaiming_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "ConvModule",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "constant_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "kaiming_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "constant_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "kaiming_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "ConvModule",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "NonLocal3d",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "build_activation_layer",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "constant_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "kaiming_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "NonLocal3d",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "constant_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "kaiming_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "build_conv_layer",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "build_norm_layer",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "kaiming_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "constant_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "kaiming_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "normal_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "constant_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "kaiming_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "trunc_normal_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "constant_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "kaiming_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "constant_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "kaiming_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "constant_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "kaiming_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "constant_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "kaiming_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "constant_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "kaiming_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "xavier_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "normal_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "ConvModule",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "constant_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "kaiming_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "ConvModule",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "constant_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "kaiming_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "_BatchNorm",
        "importPath": "torch.nn.modules.batchnorm",
        "description": "torch.nn.modules.batchnorm",
        "isExtraImport": true,
        "detail": "torch.nn.modules.batchnorm",
        "documentation": {}
    },
    {
        "label": "_BatchNorm",
        "importPath": "torch.nn.modules.batchnorm",
        "description": "torch.nn.modules.batchnorm",
        "isExtraImport": true,
        "detail": "torch.nn.modules.batchnorm",
        "documentation": {}
    },
    {
        "label": "_BatchNorm",
        "importPath": "torch.nn.modules.batchnorm",
        "description": "torch.nn.modules.batchnorm",
        "isExtraImport": true,
        "detail": "torch.nn.modules.batchnorm",
        "documentation": {}
    },
    {
        "label": "Reduce",
        "importPath": "einops.layers.torch",
        "description": "einops.layers.torch",
        "isExtraImport": true,
        "detail": "einops.layers.torch",
        "documentation": {}
    },
    {
        "label": "Rearrange",
        "importPath": "einops.layers.torch",
        "description": "einops.layers.torch",
        "isExtraImport": true,
        "detail": "einops.layers.torch",
        "documentation": {}
    },
    {
        "label": "Rearrange",
        "importPath": "einops.layers.torch",
        "description": "einops.layers.torch",
        "isExtraImport": true,
        "detail": "einops.layers.torch",
        "documentation": {}
    },
    {
        "label": "Rearrange",
        "importPath": "einops.layers.torch",
        "description": "einops.layers.torch",
        "isExtraImport": true,
        "detail": "einops.layers.torch",
        "documentation": {}
    },
    {
        "label": "Rearrange",
        "importPath": "einops.layers.torch",
        "description": "einops.layers.torch",
        "isExtraImport": true,
        "detail": "einops.layers.torch",
        "documentation": {}
    },
    {
        "label": "Rearrange",
        "importPath": "einops.layers.torch",
        "description": "einops.layers.torch",
        "isExtraImport": true,
        "detail": "einops.layers.torch",
        "documentation": {}
    },
    {
        "label": "_BatchNorm",
        "importPath": "mmcv.utils",
        "description": "mmcv.utils",
        "isExtraImport": true,
        "detail": "mmcv.utils",
        "documentation": {}
    },
    {
        "label": "digit_version",
        "importPath": "mmcv.utils",
        "description": "mmcv.utils",
        "isExtraImport": true,
        "detail": "mmcv.utils",
        "documentation": {}
    },
    {
        "label": "_BatchNorm",
        "importPath": "mmcv.utils",
        "description": "mmcv.utils",
        "isExtraImport": true,
        "detail": "mmcv.utils",
        "documentation": {}
    },
    {
        "label": "_BatchNorm",
        "importPath": "mmcv.utils",
        "description": "mmcv.utils",
        "isExtraImport": true,
        "detail": "mmcv.utils",
        "documentation": {}
    },
    {
        "label": "SyncBatchNorm",
        "importPath": "mmcv.utils",
        "description": "mmcv.utils",
        "isExtraImport": true,
        "detail": "mmcv.utils",
        "documentation": {}
    },
    {
        "label": "_BatchNorm",
        "importPath": "mmcv.utils",
        "description": "mmcv.utils",
        "isExtraImport": true,
        "detail": "mmcv.utils",
        "documentation": {}
    },
    {
        "label": "SyncBatchNorm",
        "importPath": "mmcv.utils",
        "description": "mmcv.utils",
        "isExtraImport": true,
        "detail": "mmcv.utils",
        "documentation": {}
    },
    {
        "label": "_BatchNorm",
        "importPath": "mmcv.utils",
        "description": "mmcv.utils",
        "isExtraImport": true,
        "detail": "mmcv.utils",
        "documentation": {}
    },
    {
        "label": "collect_env",
        "importPath": "mmcv.utils",
        "description": "mmcv.utils",
        "isExtraImport": true,
        "detail": "mmcv.utils",
        "documentation": {}
    },
    {
        "label": "get_git_hash",
        "importPath": "mmcv.utils",
        "description": "mmcv.utils",
        "isExtraImport": true,
        "detail": "mmcv.utils",
        "documentation": {}
    },
    {
        "label": "checkpoint",
        "importPath": "torch.utils",
        "description": "torch.utils",
        "isExtraImport": true,
        "detail": "torch.utils",
        "documentation": {}
    },
    {
        "label": "pad_sequence",
        "importPath": "torch.nn.utils.rnn",
        "description": "torch.nn.utils.rnn",
        "isExtraImport": true,
        "detail": "torch.nn.utils.rnn",
        "documentation": {}
    },
    {
        "label": "pad_sequence",
        "importPath": "torch.nn.utils.rnn",
        "description": "torch.nn.utils.rnn",
        "isExtraImport": true,
        "detail": "torch.nn.utils.rnn",
        "documentation": {}
    },
    {
        "label": "num2words",
        "importPath": "num2words",
        "description": "num2words",
        "isExtraImport": true,
        "detail": "num2words",
        "documentation": {}
    },
    {
        "label": "num2words",
        "importPath": "num2words",
        "description": "num2words",
        "isExtraImport": true,
        "detail": "num2words",
        "documentation": {}
    },
    {
        "label": "gzip",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gzip",
        "description": "gzip",
        "detail": "gzip",
        "documentation": {}
    },
    {
        "label": "html",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "html",
        "description": "html",
        "detail": "html",
        "documentation": {}
    },
    {
        "label": "ftfy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ftfy",
        "description": "ftfy",
        "detail": "ftfy",
        "documentation": {}
    },
    {
        "label": "regex",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "regex",
        "description": "regex",
        "detail": "regex",
        "documentation": {}
    },
    {
        "label": "_triple",
        "importPath": "torch.nn.modules.utils",
        "description": "torch.nn.modules.utils",
        "isExtraImport": true,
        "detail": "torch.nn.modules.utils",
        "documentation": {}
    },
    {
        "label": "_triple",
        "importPath": "torch.nn.modules.utils",
        "description": "torch.nn.modules.utils",
        "isExtraImport": true,
        "detail": "torch.nn.modules.utils",
        "documentation": {}
    },
    {
        "label": "_pair",
        "importPath": "torch.nn.modules.utils",
        "description": "torch.nn.modules.utils",
        "isExtraImport": true,
        "detail": "torch.nn.modules.utils",
        "documentation": {}
    },
    {
        "label": "_ntuple",
        "importPath": "torch.nn.modules.utils",
        "description": "torch.nn.modules.utils",
        "isExtraImport": true,
        "detail": "torch.nn.modules.utils",
        "documentation": {}
    },
    {
        "label": "_triple",
        "importPath": "torch.nn.modules.utils",
        "description": "torch.nn.modules.utils",
        "isExtraImport": true,
        "detail": "torch.nn.modules.utils",
        "documentation": {}
    },
    {
        "label": "_ntuple",
        "importPath": "torch.nn.modules.utils",
        "description": "torch.nn.modules.utils",
        "isExtraImport": true,
        "detail": "torch.nn.modules.utils",
        "documentation": {}
    },
    {
        "label": "_pair",
        "importPath": "torch.nn.modules.utils",
        "description": "torch.nn.modules.utils",
        "isExtraImport": true,
        "detail": "torch.nn.modules.utils",
        "documentation": {}
    },
    {
        "label": "BuildExtension",
        "importPath": "torch.utils.cpp_extension",
        "description": "torch.utils.cpp_extension",
        "isExtraImport": true,
        "detail": "torch.utils.cpp_extension",
        "documentation": {}
    },
    {
        "label": "CUDAExtension",
        "importPath": "torch.utils.cpp_extension",
        "description": "torch.utils.cpp_extension",
        "isExtraImport": true,
        "detail": "torch.utils.cpp_extension",
        "documentation": {}
    },
    {
        "label": "scipy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "scipy",
        "description": "scipy",
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "ATTENTION",
        "importPath": "mmcv.cnn.bricks.registry",
        "description": "mmcv.cnn.bricks.registry",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.registry",
        "documentation": {}
    },
    {
        "label": "FEEDFORWARD_NETWORK",
        "importPath": "mmcv.cnn.bricks.registry",
        "description": "mmcv.cnn.bricks.registry",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.registry",
        "documentation": {}
    },
    {
        "label": "FFN",
        "importPath": "mmcv.cnn.bricks.transformer",
        "description": "mmcv.cnn.bricks.transformer",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.transformer",
        "documentation": {}
    },
    {
        "label": "build_dropout",
        "importPath": "mmcv.cnn.bricks.transformer",
        "description": "mmcv.cnn.bricks.transformer",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.transformer",
        "documentation": {}
    },
    {
        "label": "build_transformer_layer_sequence",
        "importPath": "mmcv.cnn.bricks.transformer",
        "description": "mmcv.cnn.bricks.transformer",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.transformer",
        "documentation": {}
    },
    {
        "label": "BaseModule",
        "importPath": "mmcv.runner.base_module",
        "description": "mmcv.runner.base_module",
        "isExtraImport": true,
        "detail": "mmcv.runner.base_module",
        "documentation": {}
    },
    {
        "label": "ast",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ast",
        "description": "ast",
        "detail": "ast",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "ast",
        "description": "ast",
        "isExtraImport": true,
        "detail": "ast",
        "documentation": {}
    },
    {
        "label": "checkpoint_wrapper",
        "importPath": "torch.distributed.algorithms._checkpoint",
        "description": "torch.distributed.algorithms._checkpoint",
        "isExtraImport": true,
        "detail": "torch.distributed.algorithms._checkpoint",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "mul",
        "importPath": "operator",
        "description": "operator",
        "isExtraImport": true,
        "detail": "operator",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "importPath": "mmcv",
        "description": "mmcv",
        "isExtraImport": true,
        "detail": "mmcv",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "mmcv.cnn.utils.weight_init",
        "description": "mmcv.cnn.utils.weight_init",
        "isExtraImport": true,
        "detail": "mmcv.cnn.utils.weight_init",
        "documentation": {}
    },
    {
        "label": "onnx",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "onnx",
        "description": "onnx",
        "detail": "onnx",
        "documentation": {}
    },
    {
        "label": "_LRScheduler",
        "importPath": "torch.optim.lr_scheduler",
        "description": "torch.optim.lr_scheduler",
        "isExtraImport": true,
        "detail": "torch.optim.lr_scheduler",
        "documentation": {}
    },
    {
        "label": "make_palette",
        "importPath": "tools.infer.infer",
        "description": "tools.infer.infer",
        "isExtraImport": true,
        "detail": "tools.infer.infer",
        "documentation": {}
    },
    {
        "label": "label_arr2img",
        "importPath": "tools.infer.infer",
        "description": "tools.infer.infer",
        "isExtraImport": true,
        "detail": "tools.infer.infer",
        "documentation": {}
    },
    {
        "label": "draw_action_label",
        "importPath": "tools.infer.infer",
        "description": "tools.infer.infer",
        "isExtraImport": true,
        "detail": "tools.infer.infer",
        "documentation": {}
    },
    {
        "label": "GuidedBackpropReLUModel",
        "importPath": "pytorch_grad_cam",
        "description": "pytorch_grad_cam",
        "isExtraImport": true,
        "detail": "pytorch_grad_cam",
        "documentation": {}
    },
    {
        "label": "GradCAM",
        "importPath": "pytorch_grad_cam",
        "description": "pytorch_grad_cam",
        "isExtraImport": true,
        "detail": "pytorch_grad_cam",
        "documentation": {}
    },
    {
        "label": "\\",
        "importPath": "pytorch_grad_cam",
        "description": "pytorch_grad_cam",
        "isExtraImport": true,
        "detail": "pytorch_grad_cam",
        "documentation": {}
    },
    {
        "label": "show_cam_on_image",
        "importPath": "pytorch_grad_cam.utils.image",
        "description": "pytorch_grad_cam.utils.image",
        "isExtraImport": true,
        "detail": "pytorch_grad_cam.utils.image",
        "documentation": {}
    },
    {
        "label": "\\",
        "importPath": "pytorch_grad_cam.utils.image",
        "description": "pytorch_grad_cam.utils.image",
        "isExtraImport": true,
        "detail": "pytorch_grad_cam.utils.image",
        "documentation": {}
    },
    {
        "label": "AblationLayerVit",
        "importPath": "pytorch_grad_cam.ablation_layer",
        "description": "pytorch_grad_cam.ablation_layer",
        "isExtraImport": true,
        "detail": "pytorch_grad_cam.ablation_layer",
        "documentation": {}
    },
    {
        "label": "types",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "types",
        "description": "types",
        "detail": "types",
        "documentation": {}
    },
    {
        "label": "MethodType",
        "importPath": "types",
        "description": "types",
        "isExtraImport": true,
        "detail": "types",
        "documentation": {}
    },
    {
        "label": "MethodType",
        "importPath": "types",
        "description": "types",
        "isExtraImport": true,
        "detail": "types",
        "documentation": {}
    },
    {
        "label": "get_model_complexity_info",
        "importPath": "mmcv.cnn.utils.flops_counter",
        "description": "mmcv.cnn.utils.flops_counter",
        "isExtraImport": true,
        "detail": "mmcv.cnn.utils.flops_counter",
        "documentation": {}
    },
    {
        "label": "get_model_complexity_info",
        "importPath": "mmcv.cnn.utils.flops_counter",
        "description": "mmcv.cnn.utils.flops_counter",
        "isExtraImport": true,
        "detail": "mmcv.cnn.utils.flops_counter",
        "documentation": {}
    },
    {
        "label": "FlopCountAnalysis",
        "importPath": "fvcore.nn",
        "description": "fvcore.nn",
        "isExtraImport": true,
        "detail": "fvcore.nn",
        "documentation": {}
    },
    {
        "label": "flop_count_table",
        "importPath": "fvcore.nn",
        "description": "fvcore.nn",
        "isExtraImport": true,
        "detail": "fvcore.nn",
        "documentation": {}
    },
    {
        "label": "FlopCountAnalysis",
        "importPath": "fvcore.nn",
        "description": "fvcore.nn",
        "isExtraImport": true,
        "detail": "fvcore.nn",
        "documentation": {}
    },
    {
        "label": "flop_count_table",
        "importPath": "fvcore.nn",
        "description": "fvcore.nn",
        "isExtraImport": true,
        "detail": "fvcore.nn",
        "documentation": {}
    },
    {
        "label": "clever_format",
        "importPath": "thop",
        "description": "thop",
        "isExtraImport": true,
        "detail": "thop",
        "documentation": {}
    },
    {
        "label": "clever_format",
        "importPath": "thop",
        "description": "thop",
        "isExtraImport": true,
        "detail": "thop",
        "documentation": {}
    },
    {
        "label": "platform",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "platform",
        "description": "platform",
        "detail": "platform",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "tempfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tempfile",
        "description": "tempfile",
        "detail": "tempfile",
        "documentation": {}
    },
    {
        "label": "uuid",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uuid",
        "description": "uuid",
        "detail": "uuid",
        "documentation": {}
    },
    {
        "label": "import_module",
        "importPath": "importlib",
        "description": "importlib",
        "isExtraImport": true,
        "detail": "importlib",
        "documentation": {}
    },
    {
        "label": "import_module",
        "importPath": "importlib",
        "description": "importlib",
        "isExtraImport": true,
        "detail": "importlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "addict",
        "description": "addict",
        "isExtraImport": true,
        "detail": "addict",
        "documentation": {}
    },
    {
        "label": "FormatCode",
        "importPath": "yapf.yapflib.yapf_api",
        "description": "yapf.yapflib.yapf_api",
        "isExtraImport": true,
        "detail": "yapf.yapflib.yapf_api",
        "documentation": {}
    },
    {
        "label": "collections.abc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "collections.abc",
        "description": "collections.abc",
        "detail": "collections.abc",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "getfullargspec",
        "importPath": "inspect",
        "description": "inspect",
        "isExtraImport": true,
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "write",
        "importPath": "asyncore",
        "description": "asyncore",
        "isExtraImport": true,
        "detail": "asyncore",
        "documentation": {}
    },
    {
        "label": "test",
        "importPath": "cgi",
        "description": "cgi",
        "isExtraImport": true,
        "detail": "cgi",
        "documentation": {}
    },
    {
        "label": "InceptionI3d",
        "importPath": "svtas.model.backbones.video",
        "description": "svtas.model.backbones.video",
        "isExtraImport": true,
        "detail": "svtas.model.backbones.video",
        "documentation": {}
    },
    {
        "label": "svtas.model.builder",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "svtas.model.builder",
        "description": "svtas.model.builder",
        "detail": "svtas.model.builder",
        "documentation": {}
    },
    {
        "label": "svtas.loader.builder",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "svtas.loader.builder",
        "description": "svtas.loader.builder",
        "detail": "svtas.loader.builder",
        "documentation": {}
    },
    {
        "label": "Config",
        "importPath": "svtas.utils.config",
        "description": "svtas.utils.config",
        "isExtraImport": true,
        "detail": "svtas.utils.config",
        "documentation": {}
    },
    {
        "label": "Config",
        "importPath": "svtas.utils.config",
        "description": "svtas.utils.config",
        "isExtraImport": true,
        "detail": "svtas.utils.config",
        "documentation": {}
    },
    {
        "label": "get_config",
        "importPath": "svtas.utils.config",
        "description": "svtas.utils.config",
        "isExtraImport": true,
        "detail": "svtas.utils.config",
        "documentation": {}
    },
    {
        "label": "Config",
        "importPath": "svtas.utils.config",
        "description": "svtas.utils.config",
        "isExtraImport": true,
        "detail": "svtas.utils.config",
        "documentation": {}
    },
    {
        "label": "get_config",
        "importPath": "svtas.utils.config",
        "description": "svtas.utils.config",
        "isExtraImport": true,
        "detail": "svtas.utils.config",
        "documentation": {}
    },
    {
        "label": "get_logger",
        "importPath": "svtas.utils.logger",
        "description": "svtas.utils.logger",
        "isExtraImport": true,
        "detail": "svtas.utils.logger",
        "documentation": {}
    },
    {
        "label": "setup_logger",
        "importPath": "svtas.utils.logger",
        "description": "svtas.utils.logger",
        "isExtraImport": true,
        "detail": "svtas.utils.logger",
        "documentation": {}
    },
    {
        "label": "get_logger",
        "importPath": "svtas.utils.logger",
        "description": "svtas.utils.logger",
        "isExtraImport": true,
        "detail": "svtas.utils.logger",
        "documentation": {}
    },
    {
        "label": "setup_logger",
        "importPath": "svtas.utils.logger",
        "description": "svtas.utils.logger",
        "isExtraImport": true,
        "detail": "svtas.utils.logger",
        "documentation": {}
    },
    {
        "label": "get_logger",
        "importPath": "svtas.utils.logger",
        "description": "svtas.utils.logger",
        "isExtraImport": true,
        "detail": "svtas.utils.logger",
        "documentation": {}
    },
    {
        "label": "get_logger",
        "importPath": "svtas.utils.logger",
        "description": "svtas.utils.logger",
        "isExtraImport": true,
        "detail": "svtas.utils.logger",
        "documentation": {}
    },
    {
        "label": "setup_logger",
        "importPath": "svtas.utils.logger",
        "description": "svtas.utils.logger",
        "isExtraImport": true,
        "detail": "svtas.utils.logger",
        "documentation": {}
    },
    {
        "label": "ExtractFeatureRunner",
        "importPath": "svtas.runner.extract_runner",
        "description": "svtas.runner.extract_runner",
        "isExtraImport": true,
        "detail": "svtas.runner.extract_runner",
        "documentation": {}
    },
    {
        "label": "ExtractOpticalFlowRunner",
        "importPath": "svtas.runner.extract_runner",
        "description": "svtas.runner.extract_runner",
        "isExtraImport": true,
        "detail": "svtas.runner.extract_runner",
        "documentation": {}
    },
    {
        "label": "cam_forward",
        "importPath": "tools.visualize.cam_forward_fn",
        "description": "tools.visualize.cam_forward_fn",
        "isExtraImport": true,
        "detail": "tools.visualize.cam_forward_fn",
        "documentation": {}
    },
    {
        "label": "VisualRunner",
        "importPath": "svtas.runner.visual_runner",
        "description": "svtas.runner.visual_runner",
        "isExtraImport": true,
        "detail": "svtas.runner.visual_runner",
        "documentation": {}
    },
    {
        "label": "infer",
        "importPath": "svtas.tasks.infer",
        "description": "svtas.tasks.infer",
        "isExtraImport": true,
        "detail": "svtas.tasks.infer",
        "documentation": {}
    },
    {
        "label": "test",
        "importPath": "svtas.tasks.test",
        "description": "svtas.tasks.test",
        "isExtraImport": true,
        "detail": "svtas.tasks.test",
        "documentation": {}
    },
    {
        "label": "train",
        "importPath": "svtas.tasks.train",
        "description": "svtas.tasks.train",
        "isExtraImport": true,
        "detail": "svtas.tasks.train",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "kind": 6,
        "importPath": "CDFL.utils.dataset",
        "description": "CDFL.utils.dataset",
        "peekOfCode": "class Dataset(object):\n    def __init__(self, base_path, video_list, label2index, shuffle = False):\n        self.features = dict()\n        self.transcript = dict()\n        self.shuffle = shuffle\n        self.idx = 0\n        # read features for each video\n        for video in video_list:\n            # video features\n            self.features[video] = np.load(base_path + '/features/' + video + '.npy')",
        "detail": "CDFL.utils.dataset",
        "documentation": {}
    },
    {
        "label": "Grammar",
        "kind": 6,
        "importPath": "CDFL.utils.grammar",
        "description": "CDFL.utils.grammar",
        "peekOfCode": "class Grammar(object):\n    # @context: tuple containing the previous label indices\n    # @label: the current label index\n    # @return: the log probability of label given context p(label|context)\n    def score(self, context, label): # score is a log probability\n        return 0.0\n    # @return: the number of classes\n    def n_classes(self):\n        return 0\n    # @return sequence start symbol",
        "detail": "CDFL.utils.grammar",
        "documentation": {}
    },
    {
        "label": "PathGrammar",
        "kind": 6,
        "importPath": "CDFL.utils.grammar",
        "description": "CDFL.utils.grammar",
        "peekOfCode": "class PathGrammar(Grammar):\n    def __init__(self, transcript_file, label2index_map):\n        self.num_classes = len(label2index_map)\n        transcripts = self._read_transcripts(transcript_file, label2index_map)\n        # generate successor sets\n        self.successors = dict()\n        for transcript in transcripts:\n            transcript = transcript + [self.end_symbol()]\n            for i in range(len(transcript)):\n                context = (self.start_symbol(),) + tuple(transcript[0:i])",
        "detail": "CDFL.utils.grammar",
        "documentation": {}
    },
    {
        "label": "SingleTranscriptGrammar",
        "kind": 6,
        "importPath": "CDFL.utils.grammar",
        "description": "CDFL.utils.grammar",
        "peekOfCode": "class SingleTranscriptGrammar(Grammar):\n    def __init__(self, transcript, n_classes):\n        self.num_classes = n_classes\n        transcript = transcript + [self.end_symbol()]\n        self.successors = dict()\n        for i in range(len(transcript)):\n            context = (self.start_symbol(),) + tuple(transcript[0:i])\n            self.successors[context] = set([transcript[i]]).union( self.successors.get(context, set()) )\n    def n_classes(self):\n        return self.num_classes",
        "detail": "CDFL.utils.grammar",
        "documentation": {}
    },
    {
        "label": "LengthModel",
        "kind": 6,
        "importPath": "CDFL.utils.length_model",
        "description": "CDFL.utils.length_model",
        "peekOfCode": "class LengthModel(object):\n    def n_classes(self):\n        return 0\n    def score(self, length, label):\n        return 0.0\n    def max_length(self):\n        return np.inf\nclass PoissonModel(LengthModel):\n    def __init__(self, model, max_length = 2000, renormalize = True):\n        super(PoissonModel, self).__init__()",
        "detail": "CDFL.utils.length_model",
        "documentation": {}
    },
    {
        "label": "PoissonModel",
        "kind": 6,
        "importPath": "CDFL.utils.length_model",
        "description": "CDFL.utils.length_model",
        "peekOfCode": "class PoissonModel(LengthModel):\n    def __init__(self, model, max_length = 2000, renormalize = True):\n        super(PoissonModel, self).__init__()\n        if type(model) == str:\n            self.mean_lengths = np.loadtxt(model)\n        else:\n            self.mean_lengths = model\n        self.num_classes = self.mean_lengths.shape[0]\n        self.max_len = max_length\n        self.poisson = np.zeros((max_length, self.num_classes))",
        "detail": "CDFL.utils.length_model",
        "documentation": {}
    },
    {
        "label": "Buffer",
        "kind": 6,
        "importPath": "CDFL.utils.network",
        "description": "CDFL.utils.network",
        "peekOfCode": "class Buffer(object):\n    def __init__(self, buffer_size, n_classes):\n        self.features = []\n        self.transcript = []\n        self.framelabels = []\n        self.instance_counts = []\n        self.label_counts = []\n        self.buffer_size = buffer_size\n        self.n_classes = n_classes\n        self.next_position = 0",
        "detail": "CDFL.utils.network",
        "documentation": {}
    },
    {
        "label": "DataWrapper",
        "kind": 6,
        "importPath": "CDFL.utils.network",
        "description": "CDFL.utils.network",
        "peekOfCode": "class DataWrapper(torch.utils.data.Dataset):\n    # for each frame in the sequence, create a subsequence of length window_size\n    def __init__(self, sequence, window_size = 21):\n        self.features = []\n        self.labels = []\n        # ensure window_size is odd\n        if window_size % 2 == 0:\n            window_size += 1\n        self.window_size = window_size\n        # extract temporal window around each frame of the sequence",
        "detail": "CDFL.utils.network",
        "documentation": {}
    },
    {
        "label": "Net",
        "kind": 6,
        "importPath": "CDFL.utils.network",
        "description": "CDFL.utils.network",
        "peekOfCode": "class Net(nn.Module):\n    def __init__(self, input_dim, hidden_size, n_classes):\n        super(Net, self).__init__()\n        self.n_classes = n_classes\n        self.gru = nn.GRU(input_dim, hidden_size, 1, bidirectional = False, batch_first = True)\n        self.fc = nn.Linear(hidden_size, n_classes)\n    def forward(self, x):\n        dummy, output = self.gru(x)\n        output = self.fc(output)\n        output = nn.functional.log_softmax(output, dim=2) # tensor is of shape (batch_size, 1, features)",
        "detail": "CDFL.utils.network",
        "documentation": {}
    },
    {
        "label": "Forwarder",
        "kind": 6,
        "importPath": "CDFL.utils.network",
        "description": "CDFL.utils.network",
        "peekOfCode": "class Forwarder(object):\n    def __init__(self, input_dimension, n_classes):\n        self.n_classes = n_classes\n        hidden_size = 64\n        self.net = Net(input_dimension, hidden_size, n_classes)\n        self.net.cuda()\n    def _forward(self, data_wrapper, batch_size = 512):\n        dataloader = torch.utils.data.DataLoader(data_wrapper, batch_size = batch_size, shuffle = False)\n        # output probability container\n        log_probs_list = []",
        "detail": "CDFL.utils.network",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "CDFL.utils.network",
        "description": "CDFL.utils.network",
        "peekOfCode": "class Trainer(Forwarder):\n    def __init__(self, decoder, input_dimension, n_classes, buffer_size, buffered_frame_ratio = 25):\n        super(Trainer, self).__init__(input_dimension, n_classes)\n        self.buffer = Buffer(buffer_size, n_classes)\n        self.decoder = decoder\n        self.buffered_frame_ratio = buffered_frame_ratio\n        self.criterion = nn.NLLLoss()\n        self.prior = np.ones((self.n_classes), dtype=np.float32) / self.n_classes\n        self.mean_lengths = np.ones((self.n_classes), dtype=np.float32)\n    def update_mean_lengths(self):",
        "detail": "CDFL.utils.network",
        "documentation": {}
    },
    {
        "label": "Viterbi",
        "kind": 6,
        "importPath": "CDFL.utils.viterbi",
        "description": "CDFL.utils.viterbi",
        "peekOfCode": "class Viterbi(object):\n    ### helper structure ###\n    class TracebackNode(object):\n        def __init__(self, label, predecessor, length = 0, boundary = False):\n            self.label = label\n            self.length = length\n            self.predecessor = predecessor\n            self.boundary = boundary\n    ### helper structure ###\n    class HypDict(dict):",
        "detail": "CDFL.utils.viterbi",
        "documentation": {}
    },
    {
        "label": "recog_file",
        "kind": 2,
        "importPath": "CDFL.eval",
        "description": "CDFL.eval",
        "peekOfCode": "def recog_file(filename, ground_truth_path):\n    # read ground truth\n    gt_file = ground_truth_path + re.sub('.*/','/',filename) + '.txt'\n    with open(gt_file, 'r') as f:\n        ground_truth = f.read().split('\\n')[0:-1]\n        f.close()\n    # read recognized sequence\n    with open(filename, 'r') as f:\n        recognized = f.read().split('\\n')[5].split() # framelevel recognition is in 6-th line of file\n        f.close()",
        "detail": "CDFL.eval",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "CDFL.eval",
        "description": "CDFL.eval",
        "peekOfCode": "parser = argparse.ArgumentParser()\nparser.add_argument('--recog_dir', default='results')\nparser.add_argument('--ground_truth_dir', default='data/groundTruth')\nargs = parser.parse_args()\nfilelist = glob.glob(args.recog_dir + '/P*')\nprint('Evaluate %d video files...' % len(filelist))\nn_frames = 0\nn_errors = 0\n# loop over all recognition files and evaluate the frame error\nfor filename in filelist:",
        "detail": "CDFL.eval",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "CDFL.eval",
        "description": "CDFL.eval",
        "peekOfCode": "args = parser.parse_args()\nfilelist = glob.glob(args.recog_dir + '/P*')\nprint('Evaluate %d video files...' % len(filelist))\nn_frames = 0\nn_errors = 0\n# loop over all recognition files and evaluate the frame error\nfor filename in filelist:\n    errors, frames = recog_file(filename, args.ground_truth_dir)\n    n_errors += errors\n    n_frames += frames",
        "detail": "CDFL.eval",
        "documentation": {}
    },
    {
        "label": "filelist",
        "kind": 5,
        "importPath": "CDFL.eval",
        "description": "CDFL.eval",
        "peekOfCode": "filelist = glob.glob(args.recog_dir + '/P*')\nprint('Evaluate %d video files...' % len(filelist))\nn_frames = 0\nn_errors = 0\n# loop over all recognition files and evaluate the frame error\nfor filename in filelist:\n    errors, frames = recog_file(filename, args.ground_truth_dir)\n    n_errors += errors\n    n_frames += frames\n# print frame accuracy (1.0 - frame error rate)",
        "detail": "CDFL.eval",
        "documentation": {}
    },
    {
        "label": "n_frames",
        "kind": 5,
        "importPath": "CDFL.eval",
        "description": "CDFL.eval",
        "peekOfCode": "n_frames = 0\nn_errors = 0\n# loop over all recognition files and evaluate the frame error\nfor filename in filelist:\n    errors, frames = recog_file(filename, args.ground_truth_dir)\n    n_errors += errors\n    n_frames += frames\n# print frame accuracy (1.0 - frame error rate)\nprint('frame accuracy: %f' % (1.0 - float(n_errors) / n_frames))",
        "detail": "CDFL.eval",
        "documentation": {}
    },
    {
        "label": "n_errors",
        "kind": 5,
        "importPath": "CDFL.eval",
        "description": "CDFL.eval",
        "peekOfCode": "n_errors = 0\n# loop over all recognition files and evaluate the frame error\nfor filename in filelist:\n    errors, frames = recog_file(filename, args.ground_truth_dir)\n    n_errors += errors\n    n_frames += frames\n# print frame accuracy (1.0 - frame error rate)\nprint('frame accuracy: %f' % (1.0 - float(n_errors) / n_frames))",
        "detail": "CDFL.eval",
        "documentation": {}
    },
    {
        "label": "decode",
        "kind": 2,
        "importPath": "CDFL.inference",
        "description": "CDFL.inference",
        "peekOfCode": "def decode(queue, log_probs, decoder, index2label):\n    while not queue.empty():\n        try:\n            video = queue.get(timeout = 3)\n            score, labels, segments = decoder.decode( log_probs[video] )\n            # save result\n            with open('results/' + video, 'w') as f:\n                f.write( '### Recognized sequence: ###\\n' )\n                f.write( ' '.join( [index2label[s.label] for s in segments] ) + '\\n' )\n                f.write( '### Score: ###\\n' + str(score) + '\\n')",
        "detail": "CDFL.inference",
        "documentation": {}
    },
    {
        "label": "stn_decode",
        "kind": 2,
        "importPath": "CDFL.inference",
        "description": "CDFL.inference",
        "peekOfCode": "def stn_decode(queue, log_probs, decoder, index2label, window, step):\n    while not queue.empty():\n        try:\n            video = queue.get(timeout = 3)\n            score, labels, segments = decoder.decode( log_probs[video])\n#             cum_segments = np.array([segment.length for segment in segments])\n#             cum_segments = np.cumsum(cum_segments)\n#             print('segments', cum_segments)\n#             print('labels', len(labels))\n#             labels = np.array(labels)",
        "detail": "CDFL.inference",
        "documentation": {}
    },
    {
        "label": "label2index",
        "kind": 5,
        "importPath": "CDFL.inference",
        "description": "CDFL.inference",
        "peekOfCode": "label2index = dict()\nindex2label = dict()\nwith open('data/mapping.txt', 'r') as f:\n    content = f.read().split('\\n')[0:-1]\n    for line in content:\n        label2index[line.split()[1]] = int(line.split()[0])\n        index2label[int(line.split()[0])] = line.split()[1]\n### read test data #############################################################\nwith open('data/split1.test', 'r') as f:\n    video_list = f.read().split('\\n')[0:-1]",
        "detail": "CDFL.inference",
        "documentation": {}
    },
    {
        "label": "index2label",
        "kind": 5,
        "importPath": "CDFL.inference",
        "description": "CDFL.inference",
        "peekOfCode": "index2label = dict()\nwith open('data/mapping.txt', 'r') as f:\n    content = f.read().split('\\n')[0:-1]\n    for line in content:\n        label2index[line.split()[1]] = int(line.split()[0])\n        index2label[int(line.split()[0])] = line.split()[1]\n### read test data #############################################################\nwith open('data/split1.test', 'r') as f:\n    video_list = f.read().split('\\n')[0:-1]\ndataset = Dataset('data', video_list, label2index, shuffle = False)",
        "detail": "CDFL.inference",
        "documentation": {}
    },
    {
        "label": "dataset",
        "kind": 5,
        "importPath": "CDFL.inference",
        "description": "CDFL.inference",
        "peekOfCode": "dataset = Dataset('data', video_list, label2index, shuffle = False)\n# load prior, length model, grammar, and network\nload_iteration = 100000\nlog_prior = np.log( np.loadtxt('results/prior.iter-' + str(load_iteration) + '.txt') )\ngrammar = PathGrammar('results/grammar.txt', label2index)\nlength_model = PoissonModel('results/lengths.iter-' + str(load_iteration) + '.txt', max_length = 2000)\nforwarder = Forwarder(dataset.input_dimension, dataset.n_classes)\nforwarder.load_model('results/network.iter-' + str(load_iteration) + '.net')\nwindow = 10\nstep = 5",
        "detail": "CDFL.inference",
        "documentation": {}
    },
    {
        "label": "load_iteration",
        "kind": 5,
        "importPath": "CDFL.inference",
        "description": "CDFL.inference",
        "peekOfCode": "load_iteration = 100000\nlog_prior = np.log( np.loadtxt('results/prior.iter-' + str(load_iteration) + '.txt') )\ngrammar = PathGrammar('results/grammar.txt', label2index)\nlength_model = PoissonModel('results/lengths.iter-' + str(load_iteration) + '.txt', max_length = 2000)\nforwarder = Forwarder(dataset.input_dimension, dataset.n_classes)\nforwarder.load_model('results/network.iter-' + str(load_iteration) + '.net')\nwindow = 10\nstep = 5\n# parallelization\nn_threads = 4",
        "detail": "CDFL.inference",
        "documentation": {}
    },
    {
        "label": "log_prior",
        "kind": 5,
        "importPath": "CDFL.inference",
        "description": "CDFL.inference",
        "peekOfCode": "log_prior = np.log( np.loadtxt('results/prior.iter-' + str(load_iteration) + '.txt') )\ngrammar = PathGrammar('results/grammar.txt', label2index)\nlength_model = PoissonModel('results/lengths.iter-' + str(load_iteration) + '.txt', max_length = 2000)\nforwarder = Forwarder(dataset.input_dimension, dataset.n_classes)\nforwarder.load_model('results/network.iter-' + str(load_iteration) + '.net')\nwindow = 10\nstep = 5\n# parallelization\nn_threads = 4\n# Viterbi decoder",
        "detail": "CDFL.inference",
        "documentation": {}
    },
    {
        "label": "grammar",
        "kind": 5,
        "importPath": "CDFL.inference",
        "description": "CDFL.inference",
        "peekOfCode": "grammar = PathGrammar('results/grammar.txt', label2index)\nlength_model = PoissonModel('results/lengths.iter-' + str(load_iteration) + '.txt', max_length = 2000)\nforwarder = Forwarder(dataset.input_dimension, dataset.n_classes)\nforwarder.load_model('results/network.iter-' + str(load_iteration) + '.net')\nwindow = 10\nstep = 5\n# parallelization\nn_threads = 4\n# Viterbi decoder\nviterbi_decoder = Viterbi(grammar, length_model, frame_sampling = 30)",
        "detail": "CDFL.inference",
        "documentation": {}
    },
    {
        "label": "length_model",
        "kind": 5,
        "importPath": "CDFL.inference",
        "description": "CDFL.inference",
        "peekOfCode": "length_model = PoissonModel('results/lengths.iter-' + str(load_iteration) + '.txt', max_length = 2000)\nforwarder = Forwarder(dataset.input_dimension, dataset.n_classes)\nforwarder.load_model('results/network.iter-' + str(load_iteration) + '.net')\nwindow = 10\nstep = 5\n# parallelization\nn_threads = 4\n# Viterbi decoder\nviterbi_decoder = Viterbi(grammar, length_model, frame_sampling = 30)\n# forward each video",
        "detail": "CDFL.inference",
        "documentation": {}
    },
    {
        "label": "forwarder",
        "kind": 5,
        "importPath": "CDFL.inference",
        "description": "CDFL.inference",
        "peekOfCode": "forwarder = Forwarder(dataset.input_dimension, dataset.n_classes)\nforwarder.load_model('results/network.iter-' + str(load_iteration) + '.net')\nwindow = 10\nstep = 5\n# parallelization\nn_threads = 4\n# Viterbi decoder\nviterbi_decoder = Viterbi(grammar, length_model, frame_sampling = 30)\n# forward each video\nlog_probs = dict()",
        "detail": "CDFL.inference",
        "documentation": {}
    },
    {
        "label": "window",
        "kind": 5,
        "importPath": "CDFL.inference",
        "description": "CDFL.inference",
        "peekOfCode": "window = 10\nstep = 5\n# parallelization\nn_threads = 4\n# Viterbi decoder\nviterbi_decoder = Viterbi(grammar, length_model, frame_sampling = 30)\n# forward each video\nlog_probs = dict()\nqueue = mp.Queue()\nfor i, data in enumerate(dataset):",
        "detail": "CDFL.inference",
        "documentation": {}
    },
    {
        "label": "step",
        "kind": 5,
        "importPath": "CDFL.inference",
        "description": "CDFL.inference",
        "peekOfCode": "step = 5\n# parallelization\nn_threads = 4\n# Viterbi decoder\nviterbi_decoder = Viterbi(grammar, length_model, frame_sampling = 30)\n# forward each video\nlog_probs = dict()\nqueue = mp.Queue()\nfor i, data in enumerate(dataset):\n    sequence, _ = data",
        "detail": "CDFL.inference",
        "documentation": {}
    },
    {
        "label": "n_threads",
        "kind": 5,
        "importPath": "CDFL.inference",
        "description": "CDFL.inference",
        "peekOfCode": "n_threads = 4\n# Viterbi decoder\nviterbi_decoder = Viterbi(grammar, length_model, frame_sampling = 30)\n# forward each video\nlog_probs = dict()\nqueue = mp.Queue()\nfor i, data in enumerate(dataset):\n    sequence, _ = data\n    video = list(dataset.features.keys())[i]\n    queue.put(video)",
        "detail": "CDFL.inference",
        "documentation": {}
    },
    {
        "label": "viterbi_decoder",
        "kind": 5,
        "importPath": "CDFL.inference",
        "description": "CDFL.inference",
        "peekOfCode": "viterbi_decoder = Viterbi(grammar, length_model, frame_sampling = 30)\n# forward each video\nlog_probs = dict()\nqueue = mp.Queue()\nfor i, data in enumerate(dataset):\n    sequence, _ = data\n    video = list(dataset.features.keys())[i]\n    queue.put(video)\n    log_probs[video] = forwarder.forward(sequence).data.cpu().numpy() - log_prior\n    log_probs[video] = log_probs[video] - np.max(log_probs[video])",
        "detail": "CDFL.inference",
        "documentation": {}
    },
    {
        "label": "log_probs",
        "kind": 5,
        "importPath": "CDFL.inference",
        "description": "CDFL.inference",
        "peekOfCode": "log_probs = dict()\nqueue = mp.Queue()\nfor i, data in enumerate(dataset):\n    sequence, _ = data\n    video = list(dataset.features.keys())[i]\n    queue.put(video)\n    log_probs[video] = forwarder.forward(sequence).data.cpu().numpy() - log_prior\n    log_probs[video] = log_probs[video] - np.max(log_probs[video])\n# Viterbi decoding\nprocs = []",
        "detail": "CDFL.inference",
        "documentation": {}
    },
    {
        "label": "queue",
        "kind": 5,
        "importPath": "CDFL.inference",
        "description": "CDFL.inference",
        "peekOfCode": "queue = mp.Queue()\nfor i, data in enumerate(dataset):\n    sequence, _ = data\n    video = list(dataset.features.keys())[i]\n    queue.put(video)\n    log_probs[video] = forwarder.forward(sequence).data.cpu().numpy() - log_prior\n    log_probs[video] = log_probs[video] - np.max(log_probs[video])\n# Viterbi decoding\nprocs = []\nfor i in range(n_threads):",
        "detail": "CDFL.inference",
        "documentation": {}
    },
    {
        "label": "procs",
        "kind": 5,
        "importPath": "CDFL.inference",
        "description": "CDFL.inference",
        "peekOfCode": "procs = []\nfor i in range(n_threads):\n    p = mp.Process(target = stn_decode, args = (queue, log_probs, viterbi_decoder, index2label, window, step) )\n#     p = mp.Process(target = decode, args = (queue, log_probs, viterbi_decoder, index2label) )\n    procs.append(p)\n    p.start()\nfor p in procs:\n    p.join()",
        "detail": "CDFL.inference",
        "documentation": {}
    },
    {
        "label": "os.environ[\"CUDA_VISIBLE_DEVICES\"]",
        "kind": 5,
        "importPath": "CDFL.train",
        "description": "CDFL.train",
        "peekOfCode": "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n### read label2index mapping and index2label mapping ###########################\nlabel2index = dict()\nindex2label = dict()\nwith open('data/mapping.txt', 'r') as f:\n    content = f.read().split('\\n')[0:-1]\n    for line in content:\n        label2index[line.split()[1]] = int(line.split()[0])\n        index2label[int(line.split()[0])] = line.split()[1]\n### read training data #########################################################",
        "detail": "CDFL.train",
        "documentation": {}
    },
    {
        "label": "label2index",
        "kind": 5,
        "importPath": "CDFL.train",
        "description": "CDFL.train",
        "peekOfCode": "label2index = dict()\nindex2label = dict()\nwith open('data/mapping.txt', 'r') as f:\n    content = f.read().split('\\n')[0:-1]\n    for line in content:\n        label2index[line.split()[1]] = int(line.split()[0])\n        index2label[int(line.split()[0])] = line.split()[1]\n### read training data #########################################################\nwith open('data/split1.train', 'r') as f:\n    video_list = f.read().split('\\n')[0:-1]",
        "detail": "CDFL.train",
        "documentation": {}
    },
    {
        "label": "index2label",
        "kind": 5,
        "importPath": "CDFL.train",
        "description": "CDFL.train",
        "peekOfCode": "index2label = dict()\nwith open('data/mapping.txt', 'r') as f:\n    content = f.read().split('\\n')[0:-1]\n    for line in content:\n        label2index[line.split()[1]] = int(line.split()[0])\n        index2label[int(line.split()[0])] = line.split()[1]\n### read training data #########################################################\nwith open('data/split1.train', 'r') as f:\n    video_list = f.read().split('\\n')[0:-1]\ndataset = Dataset('data', video_list, label2index, shuffle = True)",
        "detail": "CDFL.train",
        "documentation": {}
    },
    {
        "label": "dataset",
        "kind": 5,
        "importPath": "CDFL.train",
        "description": "CDFL.train",
        "peekOfCode": "dataset = Dataset('data', video_list, label2index, shuffle = True)\n### generate path grammar for inference ########################################\npaths = set()\nfor _, transcript in dataset:\n    paths.add( ' '.join([index2label[index] for index in transcript]) )\nwith open('results/grammar.txt', 'w') as f:\n    f.write('\\n'.join(paths) + '\\n')\n### actual nn-viterbi training #################################################\ndecoder = Viterbi(None, None, frame_sampling = 30) # (None, None): transcript-grammar and length-model are set for each training sequence separately, see trainer.train(...)\ntrainer = Trainer(decoder, dataset.input_dimension, dataset.n_classes, buffer_size = len(dataset), buffered_frame_ratio = 25)",
        "detail": "CDFL.train",
        "documentation": {}
    },
    {
        "label": "paths",
        "kind": 5,
        "importPath": "CDFL.train",
        "description": "CDFL.train",
        "peekOfCode": "paths = set()\nfor _, transcript in dataset:\n    paths.add( ' '.join([index2label[index] for index in transcript]) )\nwith open('results/grammar.txt', 'w') as f:\n    f.write('\\n'.join(paths) + '\\n')\n### actual nn-viterbi training #################################################\ndecoder = Viterbi(None, None, frame_sampling = 30) # (None, None): transcript-grammar and length-model are set for each training sequence separately, see trainer.train(...)\ntrainer = Trainer(decoder, dataset.input_dimension, dataset.n_classes, buffer_size = len(dataset), buffered_frame_ratio = 25)\nlearning_rate = 0.01\nwindow = 10",
        "detail": "CDFL.train",
        "documentation": {}
    },
    {
        "label": "decoder",
        "kind": 5,
        "importPath": "CDFL.train",
        "description": "CDFL.train",
        "peekOfCode": "decoder = Viterbi(None, None, frame_sampling = 30) # (None, None): transcript-grammar and length-model are set for each training sequence separately, see trainer.train(...)\ntrainer = Trainer(decoder, dataset.input_dimension, dataset.n_classes, buffer_size = len(dataset), buffered_frame_ratio = 25)\nlearning_rate = 0.01\nwindow = 10\nstep = 5\n# train for 100000 iterations\nfor i in range(100000):\n    sequence, transcript = dataset.get()\n    loss1, loss2 = trainer.train(sequence, transcript, batch_size=512, learning_rate=learning_rate, window=window, step=step)\n    # print some progress information",
        "detail": "CDFL.train",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "CDFL.train",
        "description": "CDFL.train",
        "peekOfCode": "trainer = Trainer(decoder, dataset.input_dimension, dataset.n_classes, buffer_size = len(dataset), buffered_frame_ratio = 25)\nlearning_rate = 0.01\nwindow = 10\nstep = 5\n# train for 100000 iterations\nfor i in range(100000):\n    sequence, transcript = dataset.get()\n    loss1, loss2 = trainer.train(sequence, transcript, batch_size=512, learning_rate=learning_rate, window=window, step=step)\n    # print some progress information\n    if (i+1) % 100 == 0:",
        "detail": "CDFL.train",
        "documentation": {}
    },
    {
        "label": "learning_rate",
        "kind": 5,
        "importPath": "CDFL.train",
        "description": "CDFL.train",
        "peekOfCode": "learning_rate = 0.01\nwindow = 10\nstep = 5\n# train for 100000 iterations\nfor i in range(100000):\n    sequence, transcript = dataset.get()\n    loss1, loss2 = trainer.train(sequence, transcript, batch_size=512, learning_rate=learning_rate, window=window, step=step)\n    # print some progress information\n    if (i+1) % 100 == 0:\n        print('Iteration %d, loss1: %f, loss2: %f, loss: %f' % (i+1, loss1, loss2, loss1 - loss2))",
        "detail": "CDFL.train",
        "documentation": {}
    },
    {
        "label": "window",
        "kind": 5,
        "importPath": "CDFL.train",
        "description": "CDFL.train",
        "peekOfCode": "window = 10\nstep = 5\n# train for 100000 iterations\nfor i in range(100000):\n    sequence, transcript = dataset.get()\n    loss1, loss2 = trainer.train(sequence, transcript, batch_size=512, learning_rate=learning_rate, window=window, step=step)\n    # print some progress information\n    if (i+1) % 100 == 0:\n        print('Iteration %d, loss1: %f, loss2: %f, loss: %f' % (i+1, loss1, loss2, loss1 - loss2))\n    # save model every 1000 iterations",
        "detail": "CDFL.train",
        "documentation": {}
    },
    {
        "label": "step",
        "kind": 5,
        "importPath": "CDFL.train",
        "description": "CDFL.train",
        "peekOfCode": "step = 5\n# train for 100000 iterations\nfor i in range(100000):\n    sequence, transcript = dataset.get()\n    loss1, loss2 = trainer.train(sequence, transcript, batch_size=512, learning_rate=learning_rate, window=window, step=step)\n    # print some progress information\n    if (i+1) % 100 == 0:\n        print('Iteration %d, loss1: %f, loss2: %f, loss: %f' % (i+1, loss1, loss2, loss1 - loss2))\n    # save model every 1000 iterations\n    if (i+1) % 1000 == 0:",
        "detail": "CDFL.train",
        "documentation": {}
    },
    {
        "label": "BatchGenerator",
        "kind": 6,
        "importPath": "DTGRM.batch_gen",
        "description": "DTGRM.batch_gen",
        "peekOfCode": "class BatchGenerator(object):\n    def __init__(self, num_classes, actions_dict, gt_path, features_path, sample_rate):\n        self.list_of_examples = list()\n        self.index = 0\n        self.num_classes = num_classes\n        self.actions_dict = actions_dict\n        self.gt_path = gt_path\n        self.features_path = features_path\n        self.sample_rate = sample_rate\n    def reset(self):",
        "detail": "DTGRM.batch_gen",
        "documentation": {}
    },
    {
        "label": "read_file",
        "kind": 2,
        "importPath": "DTGRM.eval",
        "description": "DTGRM.eval",
        "peekOfCode": "def read_file(path):\n    with open(path, 'r') as f:\n        content = f.read()\n        f.close()\n    return content\ndef get_labels_start_end_time(frame_wise_labels, bg_class=[\"background\"]):\n    labels = []\n    starts = []\n    ends = []\n    last_label = frame_wise_labels[0]",
        "detail": "DTGRM.eval",
        "documentation": {}
    },
    {
        "label": "get_labels_start_end_time",
        "kind": 2,
        "importPath": "DTGRM.eval",
        "description": "DTGRM.eval",
        "peekOfCode": "def get_labels_start_end_time(frame_wise_labels, bg_class=[\"background\"]):\n    labels = []\n    starts = []\n    ends = []\n    last_label = frame_wise_labels[0]\n    if frame_wise_labels[0] not in bg_class:\n        labels.append(frame_wise_labels[0])\n        starts.append(0)\n    for i in range(len(frame_wise_labels)):\n        if frame_wise_labels[i] != last_label:",
        "detail": "DTGRM.eval",
        "documentation": {}
    },
    {
        "label": "levenstein",
        "kind": 2,
        "importPath": "DTGRM.eval",
        "description": "DTGRM.eval",
        "peekOfCode": "def levenstein(p, y, norm=False):\n    m_row = len(p)    \n    n_col = len(y)\n    D = np.zeros([m_row+1, n_col+1], np.float)\n    for i in range(m_row+1):\n        D[i, 0] = i\n    for i in range(n_col+1):\n        D[0, i] = i\n    for j in range(1, n_col+1):\n        for i in range(1, m_row+1):",
        "detail": "DTGRM.eval",
        "documentation": {}
    },
    {
        "label": "edit_score",
        "kind": 2,
        "importPath": "DTGRM.eval",
        "description": "DTGRM.eval",
        "peekOfCode": "def edit_score(recognized, ground_truth, norm=True, bg_class=[\"background\"]):\n    P, _, _ = get_labels_start_end_time(recognized, bg_class)\n    Y, _, _ = get_labels_start_end_time(ground_truth, bg_class)\n    return levenstein(P, Y, norm)\ndef f_score(recognized, ground_truth, overlap, bg_class=[\"background\"]):\n    p_label, p_start, p_end = get_labels_start_end_time(recognized, bg_class)\n    y_label, y_start, y_end = get_labels_start_end_time(ground_truth, bg_class)\n    tp = 0\n    fp = 0\n    hits = np.zeros(len(y_label))",
        "detail": "DTGRM.eval",
        "documentation": {}
    },
    {
        "label": "f_score",
        "kind": 2,
        "importPath": "DTGRM.eval",
        "description": "DTGRM.eval",
        "peekOfCode": "def f_score(recognized, ground_truth, overlap, bg_class=[\"background\"]):\n    p_label, p_start, p_end = get_labels_start_end_time(recognized, bg_class)\n    y_label, y_start, y_end = get_labels_start_end_time(ground_truth, bg_class)\n    tp = 0\n    fp = 0\n    hits = np.zeros(len(y_label))\n    for j in range(len(p_label)):\n        intersection = np.minimum(p_end[j], y_end) - np.maximum(p_start[j], y_start)\n        union = np.maximum(p_end[j], y_end) - np.minimum(p_start[j], y_start)\n        IoU = (1.0*intersection / union)*([p_label[j] == y_label[x] for x in range(len(y_label))])",
        "detail": "DTGRM.eval",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "DTGRM.eval",
        "description": "DTGRM.eval",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--dataset', default=\"gtea\")\n    parser.add_argument('--split', default='1')\n    parser.add_argument('--num_stages', type=str, default='4')\n    parser.add_argument('--num_layers', type=str, default='10')\n    parser.add_argument('--num_f_maps', type=str, default='64')\n    parser.add_argument('--df_size', type=str, default='3')\n    args = parser.parse_args()\n    print(\"Eval::\"+args.dataset+\"_split_\"+args.split+\"---stages_\"+args.num_stages+",
        "detail": "DTGRM.eval",
        "documentation": {}
    },
    {
        "label": "SingleStageModel",
        "kind": 6,
        "importPath": "DTGRM.layers",
        "description": "DTGRM.layers",
        "peekOfCode": "class SingleStageModel(nn.Module):\n    def __init__(self, num_layers, num_f_maps, dim, num_classes):\n        super(SingleStageModel, self).__init__()\n        self.conv_1x1 = nn.Conv1d(dim, num_f_maps, 1)\n        self.layers = nn.ModuleList([copy.deepcopy(DilatedResidualLayer(2 ** i, num_f_maps, num_f_maps)) for i in range(num_layers)])\n        self.conv_out = nn.Conv1d(num_f_maps, num_classes, 1)\n        self.exchange_out = nn.Conv1d(num_f_maps, 2, 1)\n    def forward(self, x, mask, ex_x, ex_label):\n        out = self.conv_1x1(x)\n        for layer in self.layers:",
        "detail": "DTGRM.layers",
        "documentation": {}
    },
    {
        "label": "DilatedResidualLayer",
        "kind": 6,
        "importPath": "DTGRM.layers",
        "description": "DTGRM.layers",
        "peekOfCode": "class DilatedResidualLayer(nn.Module):\n    def __init__(self, dilation, in_channels, out_channels):\n        super(DilatedResidualLayer, self).__init__()\n        self.conv_dilated = nn.Conv1d(in_channels, out_channels, 3, padding=dilation, dilation=dilation)\n        self.conv_1x1 = nn.Conv1d(out_channels, out_channels, 1)\n        self.dropout = nn.Dropout()\n    def forward(self, x, mask):\n        out = F.relu(self.conv_dilated(x))\n        out = self.conv_1x1(out)\n        out = self.dropout(out)",
        "detail": "DTGRM.layers",
        "documentation": {}
    },
    {
        "label": "DRCGraphConvolution",
        "kind": 6,
        "importPath": "DTGRM.layers",
        "description": "DTGRM.layers",
        "peekOfCode": "class DRCGraphConvolution(Module):\n    def __init__(self, in_features, out_features, bias=True, kernel_size=3, dilation=1, padding=1):\n        super(DRCGraphConvolution, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.kernel_size = kernel_size\n        self.dilation = dilation\n        self.padding = padding\n        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n        if bias:",
        "detail": "DTGRM.layers",
        "documentation": {}
    },
    {
        "label": "GCNResidualLayer",
        "kind": 6,
        "importPath": "DTGRM.layers",
        "description": "DTGRM.layers",
        "peekOfCode": "class GCNResidualLayer(nn.Module):\n    def __init__(self, dilation, df_size, in_channels, out_channels):\n        super(GCNResidualLayer, self).__init__()\n        padding = int((dilation*(df_size-1))/2)\n        self.df_size = df_size\n        self.gcn_dilated1 = DRCGraphConvolution(in_channels, out_channels, kernel_size=df_size, dilation=dilation, padding=padding)\n        self.gcn_dilated2 = DRCGraphConvolution(in_channels, out_channels, kernel_size=df_size, dilation=dilation, padding=padding)\n        self.conv_dilated_adj = nn.Conv1d(out_channels, df_size*df_size, df_size, padding=padding, dilation=dilation)\n        self.conv_1x1 = nn.Conv1d(out_channels, out_channels, 1)\n        self.dropout = nn.Dropout()",
        "detail": "DTGRM.layers",
        "documentation": {}
    },
    {
        "label": "GCNStageModel",
        "kind": 6,
        "importPath": "DTGRM.layers",
        "description": "DTGRM.layers",
        "peekOfCode": "class GCNStageModel(nn.Module):\n    def __init__(self, num_layers, num_f_maps, df_size, dim, num_classes):\n        super(GCNStageModel, self).__init__()\n        self.conv_1x1 = nn.Conv1d(dim, num_f_maps, 1)\n        self.gcn_layers = nn.ModuleList([copy.deepcopy(GCNResidualLayer(2 ** i, df_size, num_f_maps, num_f_maps)) for i in range(num_layers)])\n        self.conv_out = nn.Conv1d(num_f_maps, num_classes, 1)\n        self.exchange_out = nn.Conv1d(num_f_maps, 2, 1)\n    def forward(self, x, mask, ex_x, ex_label):\n        out = self.conv_1x1(x)\n        for layer in self.gcn_layers:",
        "detail": "DTGRM.layers",
        "documentation": {}
    },
    {
        "label": "exchange_time",
        "kind": 2,
        "importPath": "DTGRM.layers",
        "description": "DTGRM.layers",
        "peekOfCode": "def exchange_time(x, exchange_rate=0.2):\n    exchange_label = torch.zeros(x.shape[0], x.shape[2], dtype=torch.long).to(x.device)\n    seq_length = (x.shape[2]//2)*2\n    exchange_num = int(seq_length/2*exchange_rate)\n    randn_pair = torch.randperm(x.shape[2])[:seq_length].reshape(2,-1)\n    exchange_pair = randn_pair[:,:exchange_num]\n    exchange_index = torch.arange(start=0, end=x.shape[2])\n    exchange_index[exchange_pair[0]] = exchange_pair[1]\n    exchange_index[exchange_pair[1]] = exchange_pair[0]\n    exchange_x = x[:,:,exchange_index]",
        "detail": "DTGRM.layers",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "DTGRM.main",
        "description": "DTGRM.main",
        "peekOfCode": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nseed = 1538574472\nrandom.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.deterministic = True\nparser = argparse.ArgumentParser()\nparser.add_argument('--action', default='train')\nparser.add_argument('--dataset', default=\"gtea\")\nparser.add_argument('--split', default='1')",
        "detail": "DTGRM.main",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "DTGRM.main",
        "description": "DTGRM.main",
        "peekOfCode": "seed = 1538574472\nrandom.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.deterministic = True\nparser = argparse.ArgumentParser()\nparser.add_argument('--action', default='train')\nparser.add_argument('--dataset', default=\"gtea\")\nparser.add_argument('--split', default='1')\nparser.add_argument('--num_stages', type=str, default='4')",
        "detail": "DTGRM.main",
        "documentation": {}
    },
    {
        "label": "torch.backends.cudnn.deterministic",
        "kind": 5,
        "importPath": "DTGRM.main",
        "description": "DTGRM.main",
        "peekOfCode": "torch.backends.cudnn.deterministic = True\nparser = argparse.ArgumentParser()\nparser.add_argument('--action', default='train')\nparser.add_argument('--dataset', default=\"gtea\")\nparser.add_argument('--split', default='1')\nparser.add_argument('--num_stages', type=str, default='4')\nparser.add_argument('--num_layers', type=str, default='10')\nparser.add_argument('--num_f_maps', type=str, default='64')\nparser.add_argument('--df_size', type=str, default='3')\nargs = parser.parse_args()",
        "detail": "DTGRM.main",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "DTGRM.main",
        "description": "DTGRM.main",
        "peekOfCode": "parser = argparse.ArgumentParser()\nparser.add_argument('--action', default='train')\nparser.add_argument('--dataset', default=\"gtea\")\nparser.add_argument('--split', default='1')\nparser.add_argument('--num_stages', type=str, default='4')\nparser.add_argument('--num_layers', type=str, default='10')\nparser.add_argument('--num_f_maps', type=str, default='64')\nparser.add_argument('--df_size', type=str, default='3')\nargs = parser.parse_args()\nnum_stages = int(args.num_stages)",
        "detail": "DTGRM.main",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "DTGRM.main",
        "description": "DTGRM.main",
        "peekOfCode": "args = parser.parse_args()\nnum_stages = int(args.num_stages)\nnum_layers = int(args.num_layers)\nnum_f_maps = int(args.num_f_maps)\ndf_size = int(args.df_size)\nfeatures_dim = 2048\nbz = 1\nlr = 0.0005\nnum_epochs = 50\nif args.dataset == \"50salads\":",
        "detail": "DTGRM.main",
        "documentation": {}
    },
    {
        "label": "num_stages",
        "kind": 5,
        "importPath": "DTGRM.main",
        "description": "DTGRM.main",
        "peekOfCode": "num_stages = int(args.num_stages)\nnum_layers = int(args.num_layers)\nnum_f_maps = int(args.num_f_maps)\ndf_size = int(args.df_size)\nfeatures_dim = 2048\nbz = 1\nlr = 0.0005\nnum_epochs = 50\nif args.dataset == \"50salads\":\n    sample_rate = 30",
        "detail": "DTGRM.main",
        "documentation": {}
    },
    {
        "label": "num_layers",
        "kind": 5,
        "importPath": "DTGRM.main",
        "description": "DTGRM.main",
        "peekOfCode": "num_layers = int(args.num_layers)\nnum_f_maps = int(args.num_f_maps)\ndf_size = int(args.df_size)\nfeatures_dim = 2048\nbz = 1\nlr = 0.0005\nnum_epochs = 50\nif args.dataset == \"50salads\":\n    sample_rate = 30\nif args.dataset == \"gtea\":",
        "detail": "DTGRM.main",
        "documentation": {}
    },
    {
        "label": "num_f_maps",
        "kind": 5,
        "importPath": "DTGRM.main",
        "description": "DTGRM.main",
        "peekOfCode": "num_f_maps = int(args.num_f_maps)\ndf_size = int(args.df_size)\nfeatures_dim = 2048\nbz = 1\nlr = 0.0005\nnum_epochs = 50\nif args.dataset == \"50salads\":\n    sample_rate = 30\nif args.dataset == \"gtea\":\n    sample_rate = 1",
        "detail": "DTGRM.main",
        "documentation": {}
    },
    {
        "label": "df_size",
        "kind": 5,
        "importPath": "DTGRM.main",
        "description": "DTGRM.main",
        "peekOfCode": "df_size = int(args.df_size)\nfeatures_dim = 2048\nbz = 1\nlr = 0.0005\nnum_epochs = 50\nif args.dataset == \"50salads\":\n    sample_rate = 30\nif args.dataset == \"gtea\":\n    sample_rate = 1\nif args.dataset == \"breakfast\":",
        "detail": "DTGRM.main",
        "documentation": {}
    },
    {
        "label": "features_dim",
        "kind": 5,
        "importPath": "DTGRM.main",
        "description": "DTGRM.main",
        "peekOfCode": "features_dim = 2048\nbz = 1\nlr = 0.0005\nnum_epochs = 50\nif args.dataset == \"50salads\":\n    sample_rate = 30\nif args.dataset == \"gtea\":\n    sample_rate = 1\nif args.dataset == \"breakfast\":\n    sample_rate = 15",
        "detail": "DTGRM.main",
        "documentation": {}
    },
    {
        "label": "bz",
        "kind": 5,
        "importPath": "DTGRM.main",
        "description": "DTGRM.main",
        "peekOfCode": "bz = 1\nlr = 0.0005\nnum_epochs = 50\nif args.dataset == \"50salads\":\n    sample_rate = 30\nif args.dataset == \"gtea\":\n    sample_rate = 1\nif args.dataset == \"breakfast\":\n    sample_rate = 15\nvid_list_file = \"./data/\"+args.dataset+\"/splits/train.split\"+args.split+\".bundle\"",
        "detail": "DTGRM.main",
        "documentation": {}
    },
    {
        "label": "lr",
        "kind": 5,
        "importPath": "DTGRM.main",
        "description": "DTGRM.main",
        "peekOfCode": "lr = 0.0005\nnum_epochs = 50\nif args.dataset == \"50salads\":\n    sample_rate = 30\nif args.dataset == \"gtea\":\n    sample_rate = 1\nif args.dataset == \"breakfast\":\n    sample_rate = 15\nvid_list_file = \"./data/\"+args.dataset+\"/splits/train.split\"+args.split+\".bundle\"\nvid_list_file_tst = \"./data/\"+args.dataset+\"/splits/test.split\"+args.split+\".bundle\"",
        "detail": "DTGRM.main",
        "documentation": {}
    },
    {
        "label": "num_epochs",
        "kind": 5,
        "importPath": "DTGRM.main",
        "description": "DTGRM.main",
        "peekOfCode": "num_epochs = 50\nif args.dataset == \"50salads\":\n    sample_rate = 30\nif args.dataset == \"gtea\":\n    sample_rate = 1\nif args.dataset == \"breakfast\":\n    sample_rate = 15\nvid_list_file = \"./data/\"+args.dataset+\"/splits/train.split\"+args.split+\".bundle\"\nvid_list_file_tst = \"./data/\"+args.dataset+\"/splits/test.split\"+args.split+\".bundle\"\nfeatures_path = \"./data/\"+args.dataset+\"/features/\"",
        "detail": "DTGRM.main",
        "documentation": {}
    },
    {
        "label": "vid_list_file",
        "kind": 5,
        "importPath": "DTGRM.main",
        "description": "DTGRM.main",
        "peekOfCode": "vid_list_file = \"./data/\"+args.dataset+\"/splits/train.split\"+args.split+\".bundle\"\nvid_list_file_tst = \"./data/\"+args.dataset+\"/splits/test.split\"+args.split+\".bundle\"\nfeatures_path = \"./data/\"+args.dataset+\"/features/\"\ngt_path = \"./data/\"+args.dataset+\"/groundTruth/\"\nmapping_file = \"./data/\"+args.dataset+\"/mapping.txt\"\nmodel_dir = \"./models/\"+args.dataset+\"/split_\"+args.split+\"/stages_\"+\\\nargs.num_stages+\"_layers_\"+args.num_layers+\"_fmaps_\"+args.num_f_maps+\"_dfsize_\"+args.df_size\nresults_dir = \"./results/\"+args.dataset+\"/split_\"+args.split+\"/stages_\"+\\\nargs.num_stages+\"_layers_\"+args.num_layers+\"_fmaps_\"+args.num_f_maps+\"_dfsize_\"+args.df_size\nif not os.path.exists(model_dir):",
        "detail": "DTGRM.main",
        "documentation": {}
    },
    {
        "label": "vid_list_file_tst",
        "kind": 5,
        "importPath": "DTGRM.main",
        "description": "DTGRM.main",
        "peekOfCode": "vid_list_file_tst = \"./data/\"+args.dataset+\"/splits/test.split\"+args.split+\".bundle\"\nfeatures_path = \"./data/\"+args.dataset+\"/features/\"\ngt_path = \"./data/\"+args.dataset+\"/groundTruth/\"\nmapping_file = \"./data/\"+args.dataset+\"/mapping.txt\"\nmodel_dir = \"./models/\"+args.dataset+\"/split_\"+args.split+\"/stages_\"+\\\nargs.num_stages+\"_layers_\"+args.num_layers+\"_fmaps_\"+args.num_f_maps+\"_dfsize_\"+args.df_size\nresults_dir = \"./results/\"+args.dataset+\"/split_\"+args.split+\"/stages_\"+\\\nargs.num_stages+\"_layers_\"+args.num_layers+\"_fmaps_\"+args.num_f_maps+\"_dfsize_\"+args.df_size\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)",
        "detail": "DTGRM.main",
        "documentation": {}
    },
    {
        "label": "features_path",
        "kind": 5,
        "importPath": "DTGRM.main",
        "description": "DTGRM.main",
        "peekOfCode": "features_path = \"./data/\"+args.dataset+\"/features/\"\ngt_path = \"./data/\"+args.dataset+\"/groundTruth/\"\nmapping_file = \"./data/\"+args.dataset+\"/mapping.txt\"\nmodel_dir = \"./models/\"+args.dataset+\"/split_\"+args.split+\"/stages_\"+\\\nargs.num_stages+\"_layers_\"+args.num_layers+\"_fmaps_\"+args.num_f_maps+\"_dfsize_\"+args.df_size\nresults_dir = \"./results/\"+args.dataset+\"/split_\"+args.split+\"/stages_\"+\\\nargs.num_stages+\"_layers_\"+args.num_layers+\"_fmaps_\"+args.num_f_maps+\"_dfsize_\"+args.df_size\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\nif not os.path.exists(results_dir):",
        "detail": "DTGRM.main",
        "documentation": {}
    },
    {
        "label": "gt_path",
        "kind": 5,
        "importPath": "DTGRM.main",
        "description": "DTGRM.main",
        "peekOfCode": "gt_path = \"./data/\"+args.dataset+\"/groundTruth/\"\nmapping_file = \"./data/\"+args.dataset+\"/mapping.txt\"\nmodel_dir = \"./models/\"+args.dataset+\"/split_\"+args.split+\"/stages_\"+\\\nargs.num_stages+\"_layers_\"+args.num_layers+\"_fmaps_\"+args.num_f_maps+\"_dfsize_\"+args.df_size\nresults_dir = \"./results/\"+args.dataset+\"/split_\"+args.split+\"/stages_\"+\\\nargs.num_stages+\"_layers_\"+args.num_layers+\"_fmaps_\"+args.num_f_maps+\"_dfsize_\"+args.df_size\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\nif not os.path.exists(results_dir):\n    os.makedirs(results_dir)",
        "detail": "DTGRM.main",
        "documentation": {}
    },
    {
        "label": "mapping_file",
        "kind": 5,
        "importPath": "DTGRM.main",
        "description": "DTGRM.main",
        "peekOfCode": "mapping_file = \"./data/\"+args.dataset+\"/mapping.txt\"\nmodel_dir = \"./models/\"+args.dataset+\"/split_\"+args.split+\"/stages_\"+\\\nargs.num_stages+\"_layers_\"+args.num_layers+\"_fmaps_\"+args.num_f_maps+\"_dfsize_\"+args.df_size\nresults_dir = \"./results/\"+args.dataset+\"/split_\"+args.split+\"/stages_\"+\\\nargs.num_stages+\"_layers_\"+args.num_layers+\"_fmaps_\"+args.num_f_maps+\"_dfsize_\"+args.df_size\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\nif not os.path.exists(results_dir):\n    os.makedirs(results_dir)\nfile_ptr = open(mapping_file, 'r')",
        "detail": "DTGRM.main",
        "documentation": {}
    },
    {
        "label": "model_dir",
        "kind": 5,
        "importPath": "DTGRM.main",
        "description": "DTGRM.main",
        "peekOfCode": "model_dir = \"./models/\"+args.dataset+\"/split_\"+args.split+\"/stages_\"+\\\nargs.num_stages+\"_layers_\"+args.num_layers+\"_fmaps_\"+args.num_f_maps+\"_dfsize_\"+args.df_size\nresults_dir = \"./results/\"+args.dataset+\"/split_\"+args.split+\"/stages_\"+\\\nargs.num_stages+\"_layers_\"+args.num_layers+\"_fmaps_\"+args.num_f_maps+\"_dfsize_\"+args.df_size\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\nif not os.path.exists(results_dir):\n    os.makedirs(results_dir)\nfile_ptr = open(mapping_file, 'r')\nactions = file_ptr.read().split('\\n')[:-1]",
        "detail": "DTGRM.main",
        "documentation": {}
    },
    {
        "label": "results_dir",
        "kind": 5,
        "importPath": "DTGRM.main",
        "description": "DTGRM.main",
        "peekOfCode": "results_dir = \"./results/\"+args.dataset+\"/split_\"+args.split+\"/stages_\"+\\\nargs.num_stages+\"_layers_\"+args.num_layers+\"_fmaps_\"+args.num_f_maps+\"_dfsize_\"+args.df_size\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\nif not os.path.exists(results_dir):\n    os.makedirs(results_dir)\nfile_ptr = open(mapping_file, 'r')\nactions = file_ptr.read().split('\\n')[:-1]\nfile_ptr.close()\nactions_dict = dict()",
        "detail": "DTGRM.main",
        "documentation": {}
    },
    {
        "label": "file_ptr",
        "kind": 5,
        "importPath": "DTGRM.main",
        "description": "DTGRM.main",
        "peekOfCode": "file_ptr = open(mapping_file, 'r')\nactions = file_ptr.read().split('\\n')[:-1]\nfile_ptr.close()\nactions_dict = dict()\nfor a in actions:\n    actions_dict[a.split()[1]] = int(a.split()[0])\nnum_classes = len(actions_dict)\ntrainer = Trainer(num_stages, num_layers, num_f_maps, df_size, features_dim, num_classes, actions_dict)\nif args.action == \"train\":\n    print(\"Train::\"+args.dataset+\"_split_\"+args.split+\"---stages_\"+args.num_stages+",
        "detail": "DTGRM.main",
        "documentation": {}
    },
    {
        "label": "actions",
        "kind": 5,
        "importPath": "DTGRM.main",
        "description": "DTGRM.main",
        "peekOfCode": "actions = file_ptr.read().split('\\n')[:-1]\nfile_ptr.close()\nactions_dict = dict()\nfor a in actions:\n    actions_dict[a.split()[1]] = int(a.split()[0])\nnum_classes = len(actions_dict)\ntrainer = Trainer(num_stages, num_layers, num_f_maps, df_size, features_dim, num_classes, actions_dict)\nif args.action == \"train\":\n    print(\"Train::\"+args.dataset+\"_split_\"+args.split+\"---stages_\"+args.num_stages+\n        \"_layers_\"+args.num_layers+\"_fmaps_\"+args.num_f_maps+\"_dfsize_\"+args.df_size)",
        "detail": "DTGRM.main",
        "documentation": {}
    },
    {
        "label": "actions_dict",
        "kind": 5,
        "importPath": "DTGRM.main",
        "description": "DTGRM.main",
        "peekOfCode": "actions_dict = dict()\nfor a in actions:\n    actions_dict[a.split()[1]] = int(a.split()[0])\nnum_classes = len(actions_dict)\ntrainer = Trainer(num_stages, num_layers, num_f_maps, df_size, features_dim, num_classes, actions_dict)\nif args.action == \"train\":\n    print(\"Train::\"+args.dataset+\"_split_\"+args.split+\"---stages_\"+args.num_stages+\n        \"_layers_\"+args.num_layers+\"_fmaps_\"+args.num_f_maps+\"_dfsize_\"+args.df_size)\n    batch_gen = BatchGenerator(num_classes, actions_dict, gt_path, features_path, sample_rate)\n    batch_gen.read_data(vid_list_file)",
        "detail": "DTGRM.main",
        "documentation": {}
    },
    {
        "label": "num_classes",
        "kind": 5,
        "importPath": "DTGRM.main",
        "description": "DTGRM.main",
        "peekOfCode": "num_classes = len(actions_dict)\ntrainer = Trainer(num_stages, num_layers, num_f_maps, df_size, features_dim, num_classes, actions_dict)\nif args.action == \"train\":\n    print(\"Train::\"+args.dataset+\"_split_\"+args.split+\"---stages_\"+args.num_stages+\n        \"_layers_\"+args.num_layers+\"_fmaps_\"+args.num_f_maps+\"_dfsize_\"+args.df_size)\n    batch_gen = BatchGenerator(num_classes, actions_dict, gt_path, features_path, sample_rate)\n    batch_gen.read_data(vid_list_file)\n    trainer.train(model_dir, batch_gen, num_epochs=num_epochs, batch_size=bz, learning_rate=lr, device=device)\nif args.action == \"predict\":\n    print(\"Predict::\"+args.dataset+\"_split_\"+args.split+\"---stages_\"+args.num_stages+",
        "detail": "DTGRM.main",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "DTGRM.main",
        "description": "DTGRM.main",
        "peekOfCode": "trainer = Trainer(num_stages, num_layers, num_f_maps, df_size, features_dim, num_classes, actions_dict)\nif args.action == \"train\":\n    print(\"Train::\"+args.dataset+\"_split_\"+args.split+\"---stages_\"+args.num_stages+\n        \"_layers_\"+args.num_layers+\"_fmaps_\"+args.num_f_maps+\"_dfsize_\"+args.df_size)\n    batch_gen = BatchGenerator(num_classes, actions_dict, gt_path, features_path, sample_rate)\n    batch_gen.read_data(vid_list_file)\n    trainer.train(model_dir, batch_gen, num_epochs=num_epochs, batch_size=bz, learning_rate=lr, device=device)\nif args.action == \"predict\":\n    print(\"Predict::\"+args.dataset+\"_split_\"+args.split+\"---stages_\"+args.num_stages+\n        \"_layers_\"+args.num_layers+\"_fmaps_\"+args.num_f_maps+\"_dfsize_\"+args.df_size)",
        "detail": "DTGRM.main",
        "documentation": {}
    },
    {
        "label": "MultiStageModel",
        "kind": 6,
        "importPath": "DTGRM.model",
        "description": "DTGRM.model",
        "peekOfCode": "class MultiStageModel(nn.Module):\n    def __init__(self, num_stages, num_layers, num_f_maps, df_size, dim, num_classes, actions_dict):\n        super(MultiStageModel, self).__init__()\n        self.actions_dict = actions_dict\n        self.stage1 = SingleStageModel(num_layers, num_f_maps, dim, num_classes)\n        self.stages = nn.ModuleList([copy.deepcopy(GCNStageModel(num_layers, num_f_maps, df_size, num_classes, num_classes)) \n            for s in range(num_stages-1)])\n    def forward(self, x, mask):\n        exchange_outputs = []\n        exchange_labels = []",
        "detail": "DTGRM.model",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "DTGRM.model",
        "description": "DTGRM.model",
        "peekOfCode": "class Trainer:\n    def __init__(self, num_blocks, num_layers, num_f_maps, df_size, dim, num_classes, actions_dict):\n        self.model = MultiStageModel(num_blocks, num_layers, num_f_maps, df_size, dim, num_classes, actions_dict)\n        self.ce = nn.CrossEntropyLoss(ignore_index=-100)\n        self.mse = nn.MSELoss(reduction='none')\n        self.num_classes = num_classes\n    def train(self, save_dir, batch_gen, num_epochs, batch_size, learning_rate, device):\n        self.model.train()\n        self.model.to(device)\n        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)",
        "detail": "DTGRM.model",
        "documentation": {}
    },
    {
        "label": "add_custom_config",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.config.custom_config",
        "description": "MorphMLP.build.lib.slowfast.config.custom_config",
        "peekOfCode": "def add_custom_config(_C):\n    # Add your own customized configs.\n    pass",
        "detail": "MorphMLP.build.lib.slowfast.config.custom_config",
        "documentation": {}
    },
    {
        "label": "assert_and_infer_cfg",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "def assert_and_infer_cfg(cfg):\n    # BN assertions.\n    if cfg.BN.USE_PRECISE_STATS:\n        assert cfg.BN.NUM_BATCHES_PRECISE >= 0\n    # TRAIN assertions.\n    assert cfg.TRAIN.CHECKPOINT_TYPE in [\"pytorch\", \"caffe2\"]\n    assert cfg.NUM_GPUS == 0 or cfg.TRAIN.BATCH_SIZE % cfg.NUM_GPUS == 0\n    # TEST assertions.\n    assert cfg.TEST.CHECKPOINT_TYPE in [\"pytorch\", \"caffe2\"]\n    assert cfg.NUM_GPUS == 0 or cfg.TEST.BATCH_SIZE % cfg.NUM_GPUS == 0",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "get_cfg",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "def get_cfg():\n    \"\"\"\n    Get a copy of the default config.\n    \"\"\"\n    return _C.clone()",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C = CfgNode()\n# ---------------------------------------------------------------------------- #\n# Batch norm options\n# ---------------------------------------------------------------------------- #\n_C.BN = CfgNode()\n# Precise BN stats.\n_C.BN.USE_PRECISE_STATS = False\n# Number of samples use to compute precise bn.\n_C.BN.NUM_BATCHES_PRECISE = 200\n# Weight decay value that applies on BN.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.BN",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.BN = CfgNode()\n# Precise BN stats.\n_C.BN.USE_PRECISE_STATS = False\n# Number of samples use to compute precise bn.\n_C.BN.NUM_BATCHES_PRECISE = 200\n# Weight decay value that applies on BN.\n_C.BN.WEIGHT_DECAY = 0.0\n# Norm type, options include `batchnorm`, `sub_batchnorm`, `sync_batchnorm`\n_C.BN.NORM_TYPE = \"batchnorm\"\n# Parameter for SubBatchNorm, where it splits the batch dimension into",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.BN.USE_PRECISE_STATS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.BN.USE_PRECISE_STATS = False\n# Number of samples use to compute precise bn.\n_C.BN.NUM_BATCHES_PRECISE = 200\n# Weight decay value that applies on BN.\n_C.BN.WEIGHT_DECAY = 0.0\n# Norm type, options include `batchnorm`, `sub_batchnorm`, `sync_batchnorm`\n_C.BN.NORM_TYPE = \"batchnorm\"\n# Parameter for SubBatchNorm, where it splits the batch dimension into\n# NUM_SPLITS splits, and run BN on each of them separately independently.\n_C.BN.NUM_SPLITS = 1",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.BN.NUM_BATCHES_PRECISE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.BN.NUM_BATCHES_PRECISE = 200\n# Weight decay value that applies on BN.\n_C.BN.WEIGHT_DECAY = 0.0\n# Norm type, options include `batchnorm`, `sub_batchnorm`, `sync_batchnorm`\n_C.BN.NORM_TYPE = \"batchnorm\"\n# Parameter for SubBatchNorm, where it splits the batch dimension into\n# NUM_SPLITS splits, and run BN on each of them separately independently.\n_C.BN.NUM_SPLITS = 1\n# Parameter for NaiveSyncBatchNorm3d, where the stats across `NUM_SYNC_DEVICES`\n# devices will be synchronized.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.BN.WEIGHT_DECAY",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.BN.WEIGHT_DECAY = 0.0\n# Norm type, options include `batchnorm`, `sub_batchnorm`, `sync_batchnorm`\n_C.BN.NORM_TYPE = \"batchnorm\"\n# Parameter for SubBatchNorm, where it splits the batch dimension into\n# NUM_SPLITS splits, and run BN on each of them separately independently.\n_C.BN.NUM_SPLITS = 1\n# Parameter for NaiveSyncBatchNorm3d, where the stats across `NUM_SYNC_DEVICES`\n# devices will be synchronized.\n_C.BN.NUM_SYNC_DEVICES = 1\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.BN.NORM_TYPE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.BN.NORM_TYPE = \"batchnorm\"\n# Parameter for SubBatchNorm, where it splits the batch dimension into\n# NUM_SPLITS splits, and run BN on each of them separately independently.\n_C.BN.NUM_SPLITS = 1\n# Parameter for NaiveSyncBatchNorm3d, where the stats across `NUM_SYNC_DEVICES`\n# devices will be synchronized.\n_C.BN.NUM_SYNC_DEVICES = 1\n# ---------------------------------------------------------------------------- #\n# Training options.\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.BN.NUM_SPLITS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.BN.NUM_SPLITS = 1\n# Parameter for NaiveSyncBatchNorm3d, where the stats across `NUM_SYNC_DEVICES`\n# devices will be synchronized.\n_C.BN.NUM_SYNC_DEVICES = 1\n# ---------------------------------------------------------------------------- #\n# Training options.\n# ---------------------------------------------------------------------------- #\n_C.TRAIN = CfgNode()\n# If True Train the model, else skip training.\n_C.TRAIN.ENABLE = True",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.BN.NUM_SYNC_DEVICES",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.BN.NUM_SYNC_DEVICES = 1\n# ---------------------------------------------------------------------------- #\n# Training options.\n# ---------------------------------------------------------------------------- #\n_C.TRAIN = CfgNode()\n# If True Train the model, else skip training.\n_C.TRAIN.ENABLE = True\n# Dataset.\n_C.TRAIN.DATASET = \"kinetics\"\n# Total mini-batch size.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TRAIN",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TRAIN = CfgNode()\n# If True Train the model, else skip training.\n_C.TRAIN.ENABLE = True\n# Dataset.\n_C.TRAIN.DATASET = \"kinetics\"\n# Total mini-batch size.\n_C.TRAIN.BATCH_SIZE = 64\n# Evaluate model on test data every eval period epochs.\n_C.TRAIN.EVAL_PERIOD = 10\n# Save model checkpoint every checkpoint period epochs.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TRAIN.ENABLE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TRAIN.ENABLE = True\n# Dataset.\n_C.TRAIN.DATASET = \"kinetics\"\n# Total mini-batch size.\n_C.TRAIN.BATCH_SIZE = 64\n# Evaluate model on test data every eval period epochs.\n_C.TRAIN.EVAL_PERIOD = 10\n# Save model checkpoint every checkpoint period epochs.\n_C.TRAIN.CHECKPOINT_PERIOD = 10\n# Resume training from the latest checkpoint in the output directory.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TRAIN.DATASET",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TRAIN.DATASET = \"kinetics\"\n# Total mini-batch size.\n_C.TRAIN.BATCH_SIZE = 64\n# Evaluate model on test data every eval period epochs.\n_C.TRAIN.EVAL_PERIOD = 10\n# Save model checkpoint every checkpoint period epochs.\n_C.TRAIN.CHECKPOINT_PERIOD = 10\n# Resume training from the latest checkpoint in the output directory.\n_C.TRAIN.AUTO_RESUME = True\n# Path to the checkpoint to load the initial weight.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TRAIN.BATCH_SIZE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TRAIN.BATCH_SIZE = 64\n# Evaluate model on test data every eval period epochs.\n_C.TRAIN.EVAL_PERIOD = 10\n# Save model checkpoint every checkpoint period epochs.\n_C.TRAIN.CHECKPOINT_PERIOD = 10\n# Resume training from the latest checkpoint in the output directory.\n_C.TRAIN.AUTO_RESUME = True\n# Path to the checkpoint to load the initial weight.\n_C.TRAIN.CHECKPOINT_FILE_PATH = \"\"\n# Checkpoint types include `caffe2` or `pytorch`.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TRAIN.EVAL_PERIOD",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TRAIN.EVAL_PERIOD = 10\n# Save model checkpoint every checkpoint period epochs.\n_C.TRAIN.CHECKPOINT_PERIOD = 10\n# Resume training from the latest checkpoint in the output directory.\n_C.TRAIN.AUTO_RESUME = True\n# Path to the checkpoint to load the initial weight.\n_C.TRAIN.CHECKPOINT_FILE_PATH = \"\"\n# Checkpoint types include `caffe2` or `pytorch`.\n_C.TRAIN.CHECKPOINT_TYPE = \"pytorch\"\n# If True, perform inflation when loading checkpoint.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TRAIN.CHECKPOINT_PERIOD",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TRAIN.CHECKPOINT_PERIOD = 10\n# Resume training from the latest checkpoint in the output directory.\n_C.TRAIN.AUTO_RESUME = True\n# Path to the checkpoint to load the initial weight.\n_C.TRAIN.CHECKPOINT_FILE_PATH = \"\"\n# Checkpoint types include `caffe2` or `pytorch`.\n_C.TRAIN.CHECKPOINT_TYPE = \"pytorch\"\n# If True, perform inflation when loading checkpoint.\n_C.TRAIN.CHECKPOINT_INFLATE = False\n# If True, reset epochs when loading checkpoint.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TRAIN.AUTO_RESUME",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TRAIN.AUTO_RESUME = True\n# Path to the checkpoint to load the initial weight.\n_C.TRAIN.CHECKPOINT_FILE_PATH = \"\"\n# Checkpoint types include `caffe2` or `pytorch`.\n_C.TRAIN.CHECKPOINT_TYPE = \"pytorch\"\n# If True, perform inflation when loading checkpoint.\n_C.TRAIN.CHECKPOINT_INFLATE = False\n# If True, reset epochs when loading checkpoint.\n_C.TRAIN.CHECKPOINT_EPOCH_RESET = False\n# If set, clear all layer names according to the pattern provided.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TRAIN.CHECKPOINT_FILE_PATH",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TRAIN.CHECKPOINT_FILE_PATH = \"\"\n# Checkpoint types include `caffe2` or `pytorch`.\n_C.TRAIN.CHECKPOINT_TYPE = \"pytorch\"\n# If True, perform inflation when loading checkpoint.\n_C.TRAIN.CHECKPOINT_INFLATE = False\n# If True, reset epochs when loading checkpoint.\n_C.TRAIN.CHECKPOINT_EPOCH_RESET = False\n# If set, clear all layer names according to the pattern provided.\n_C.TRAIN.CHECKPOINT_CLEAR_NAME_PATTERN = ()  # (\"backbone.\",)\n# If True, use FP16 for activations",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TRAIN.CHECKPOINT_TYPE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TRAIN.CHECKPOINT_TYPE = \"pytorch\"\n# If True, perform inflation when loading checkpoint.\n_C.TRAIN.CHECKPOINT_INFLATE = False\n# If True, reset epochs when loading checkpoint.\n_C.TRAIN.CHECKPOINT_EPOCH_RESET = False\n# If set, clear all layer names according to the pattern provided.\n_C.TRAIN.CHECKPOINT_CLEAR_NAME_PATTERN = ()  # (\"backbone.\",)\n# If True, use FP16 for activations\n_C.TRAIN.MIXED_PRECISION = False\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TRAIN.CHECKPOINT_INFLATE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TRAIN.CHECKPOINT_INFLATE = False\n# If True, reset epochs when loading checkpoint.\n_C.TRAIN.CHECKPOINT_EPOCH_RESET = False\n# If set, clear all layer names according to the pattern provided.\n_C.TRAIN.CHECKPOINT_CLEAR_NAME_PATTERN = ()  # (\"backbone.\",)\n# If True, use FP16 for activations\n_C.TRAIN.MIXED_PRECISION = False\n# ---------------------------------------------------------------------------- #\n# Augmentation options.\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TRAIN.CHECKPOINT_EPOCH_RESET",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TRAIN.CHECKPOINT_EPOCH_RESET = False\n# If set, clear all layer names according to the pattern provided.\n_C.TRAIN.CHECKPOINT_CLEAR_NAME_PATTERN = ()  # (\"backbone.\",)\n# If True, use FP16 for activations\n_C.TRAIN.MIXED_PRECISION = False\n# ---------------------------------------------------------------------------- #\n# Augmentation options.\n# ---------------------------------------------------------------------------- #\n_C.AUG = CfgNode()\n# Whether to enable randaug.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TRAIN.CHECKPOINT_CLEAR_NAME_PATTERN",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TRAIN.CHECKPOINT_CLEAR_NAME_PATTERN = ()  # (\"backbone.\",)\n# If True, use FP16 for activations\n_C.TRAIN.MIXED_PRECISION = False\n# ---------------------------------------------------------------------------- #\n# Augmentation options.\n# ---------------------------------------------------------------------------- #\n_C.AUG = CfgNode()\n# Whether to enable randaug.\n_C.AUG.ENABLE = False\n# Number of repeated augmentations to used during training.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TRAIN.MIXED_PRECISION",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TRAIN.MIXED_PRECISION = False\n# ---------------------------------------------------------------------------- #\n# Augmentation options.\n# ---------------------------------------------------------------------------- #\n_C.AUG = CfgNode()\n# Whether to enable randaug.\n_C.AUG.ENABLE = False\n# Number of repeated augmentations to used during training.\n# If this is greater than 1, then the actual batch size is\n# TRAIN.BATCH_SIZE * AUG.NUM_SAMPLE.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AUG",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AUG = CfgNode()\n# Whether to enable randaug.\n_C.AUG.ENABLE = False\n# Number of repeated augmentations to used during training.\n# If this is greater than 1, then the actual batch size is\n# TRAIN.BATCH_SIZE * AUG.NUM_SAMPLE.\n_C.AUG.NUM_SAMPLE = 1\n# Not used if using randaug.\n_C.AUG.COLOR_JITTER = 0.4\n# RandAug parameters.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AUG.ENABLE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AUG.ENABLE = False\n# Number of repeated augmentations to used during training.\n# If this is greater than 1, then the actual batch size is\n# TRAIN.BATCH_SIZE * AUG.NUM_SAMPLE.\n_C.AUG.NUM_SAMPLE = 1\n# Not used if using randaug.\n_C.AUG.COLOR_JITTER = 0.4\n# RandAug parameters.\n_C.AUG.AA_TYPE = \"rand-m9-mstd0.5-inc1\"\n# Interpolation method.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AUG.NUM_SAMPLE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AUG.NUM_SAMPLE = 1\n# Not used if using randaug.\n_C.AUG.COLOR_JITTER = 0.4\n# RandAug parameters.\n_C.AUG.AA_TYPE = \"rand-m9-mstd0.5-inc1\"\n# Interpolation method.\n_C.AUG.INTERPOLATION = \"bicubic\"\n# Probability of random erasing.\n_C.AUG.RE_PROB = 0.25\n# Random erasing mode.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AUG.COLOR_JITTER",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AUG.COLOR_JITTER = 0.4\n# RandAug parameters.\n_C.AUG.AA_TYPE = \"rand-m9-mstd0.5-inc1\"\n# Interpolation method.\n_C.AUG.INTERPOLATION = \"bicubic\"\n# Probability of random erasing.\n_C.AUG.RE_PROB = 0.25\n# Random erasing mode.\n_C.AUG.RE_MODE = \"pixel\"\n# Random erase count.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AUG.AA_TYPE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AUG.AA_TYPE = \"rand-m9-mstd0.5-inc1\"\n# Interpolation method.\n_C.AUG.INTERPOLATION = \"bicubic\"\n# Probability of random erasing.\n_C.AUG.RE_PROB = 0.25\n# Random erasing mode.\n_C.AUG.RE_MODE = \"pixel\"\n# Random erase count.\n_C.AUG.RE_COUNT = 1\n# Do not random erase first (clean) augmentation split.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AUG.INTERPOLATION",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AUG.INTERPOLATION = \"bicubic\"\n# Probability of random erasing.\n_C.AUG.RE_PROB = 0.25\n# Random erasing mode.\n_C.AUG.RE_MODE = \"pixel\"\n# Random erase count.\n_C.AUG.RE_COUNT = 1\n# Do not random erase first (clean) augmentation split.\n_C.AUG.RE_SPLIT = False\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AUG.RE_PROB",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AUG.RE_PROB = 0.25\n# Random erasing mode.\n_C.AUG.RE_MODE = \"pixel\"\n# Random erase count.\n_C.AUG.RE_COUNT = 1\n# Do not random erase first (clean) augmentation split.\n_C.AUG.RE_SPLIT = False\n# ---------------------------------------------------------------------------- #\n# MipUp options.\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AUG.RE_MODE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AUG.RE_MODE = \"pixel\"\n# Random erase count.\n_C.AUG.RE_COUNT = 1\n# Do not random erase first (clean) augmentation split.\n_C.AUG.RE_SPLIT = False\n# ---------------------------------------------------------------------------- #\n# MipUp options.\n# ---------------------------------------------------------------------------- #\n_C.MIXUP = CfgNode()\n# Whether to use mixup.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AUG.RE_COUNT",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AUG.RE_COUNT = 1\n# Do not random erase first (clean) augmentation split.\n_C.AUG.RE_SPLIT = False\n# ---------------------------------------------------------------------------- #\n# MipUp options.\n# ---------------------------------------------------------------------------- #\n_C.MIXUP = CfgNode()\n# Whether to use mixup.\n_C.MIXUP.ENABLE = False\n# Mixup alpha.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AUG.RE_SPLIT",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AUG.RE_SPLIT = False\n# ---------------------------------------------------------------------------- #\n# MipUp options.\n# ---------------------------------------------------------------------------- #\n_C.MIXUP = CfgNode()\n# Whether to use mixup.\n_C.MIXUP.ENABLE = False\n# Mixup alpha.\n_C.MIXUP.ALPHA = 0.8\n# Cutmix alpha.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MIXUP",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MIXUP = CfgNode()\n# Whether to use mixup.\n_C.MIXUP.ENABLE = False\n# Mixup alpha.\n_C.MIXUP.ALPHA = 0.8\n# Cutmix alpha.\n_C.MIXUP.CUTMIX_ALPHA = 1.0\n# Probability of performing mixup or cutmix when either/both is enabled.\n_C.MIXUP.PROB = 1.0\n# Probability of switching to cutmix when both mixup and cutmix enabled.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MIXUP.ENABLE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MIXUP.ENABLE = False\n# Mixup alpha.\n_C.MIXUP.ALPHA = 0.8\n# Cutmix alpha.\n_C.MIXUP.CUTMIX_ALPHA = 1.0\n# Probability of performing mixup or cutmix when either/both is enabled.\n_C.MIXUP.PROB = 1.0\n# Probability of switching to cutmix when both mixup and cutmix enabled.\n_C.MIXUP.SWITCH_PROB = 0.5\n# Label smoothing.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MIXUP.ALPHA",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MIXUP.ALPHA = 0.8\n# Cutmix alpha.\n_C.MIXUP.CUTMIX_ALPHA = 1.0\n# Probability of performing mixup or cutmix when either/both is enabled.\n_C.MIXUP.PROB = 1.0\n# Probability of switching to cutmix when both mixup and cutmix enabled.\n_C.MIXUP.SWITCH_PROB = 0.5\n# Label smoothing.\n_C.MIXUP.LABEL_SMOOTH_VALUE = 0.1\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MIXUP.CUTMIX_ALPHA",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MIXUP.CUTMIX_ALPHA = 1.0\n# Probability of performing mixup or cutmix when either/both is enabled.\n_C.MIXUP.PROB = 1.0\n# Probability of switching to cutmix when both mixup and cutmix enabled.\n_C.MIXUP.SWITCH_PROB = 0.5\n# Label smoothing.\n_C.MIXUP.LABEL_SMOOTH_VALUE = 0.1\n# ---------------------------------------------------------------------------- #\n# Testing options\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MIXUP.PROB",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MIXUP.PROB = 1.0\n# Probability of switching to cutmix when both mixup and cutmix enabled.\n_C.MIXUP.SWITCH_PROB = 0.5\n# Label smoothing.\n_C.MIXUP.LABEL_SMOOTH_VALUE = 0.1\n# ---------------------------------------------------------------------------- #\n# Testing options\n# ---------------------------------------------------------------------------- #\n_C.TEST = CfgNode()\n# If True test the model, else skip the testing.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MIXUP.SWITCH_PROB",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MIXUP.SWITCH_PROB = 0.5\n# Label smoothing.\n_C.MIXUP.LABEL_SMOOTH_VALUE = 0.1\n# ---------------------------------------------------------------------------- #\n# Testing options\n# ---------------------------------------------------------------------------- #\n_C.TEST = CfgNode()\n# If True test the model, else skip the testing.\n_C.TEST.ENABLE = True\n# Dataset for testing.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MIXUP.LABEL_SMOOTH_VALUE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MIXUP.LABEL_SMOOTH_VALUE = 0.1\n# ---------------------------------------------------------------------------- #\n# Testing options\n# ---------------------------------------------------------------------------- #\n_C.TEST = CfgNode()\n# If True test the model, else skip the testing.\n_C.TEST.ENABLE = True\n# Dataset for testing.\n_C.TEST.DATASET = \"kinetics\"\n# Total mini-batch size",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TEST",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TEST = CfgNode()\n# If True test the model, else skip the testing.\n_C.TEST.ENABLE = True\n# Dataset for testing.\n_C.TEST.DATASET = \"kinetics\"\n# Total mini-batch size\n_C.TEST.BATCH_SIZE = 8\n# Path to the checkpoint to load the initial weight.\n_C.TEST.CHECKPOINT_FILE_PATH = \"\"\n# Number of clips to sample from a video uniformly for aggregating the",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TEST.ENABLE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TEST.ENABLE = True\n# Dataset for testing.\n_C.TEST.DATASET = \"kinetics\"\n# Total mini-batch size\n_C.TEST.BATCH_SIZE = 8\n# Path to the checkpoint to load the initial weight.\n_C.TEST.CHECKPOINT_FILE_PATH = \"\"\n# Number of clips to sample from a video uniformly for aggregating the\n# prediction results.\n_C.TEST.NUM_ENSEMBLE_VIEWS = 10",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TEST.DATASET",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TEST.DATASET = \"kinetics\"\n# Total mini-batch size\n_C.TEST.BATCH_SIZE = 8\n# Path to the checkpoint to load the initial weight.\n_C.TEST.CHECKPOINT_FILE_PATH = \"\"\n# Number of clips to sample from a video uniformly for aggregating the\n# prediction results.\n_C.TEST.NUM_ENSEMBLE_VIEWS = 10\n# Number of crops to sample from a frame spatially for aggregating the\n# prediction results.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TEST.BATCH_SIZE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TEST.BATCH_SIZE = 8\n# Path to the checkpoint to load the initial weight.\n_C.TEST.CHECKPOINT_FILE_PATH = \"\"\n# Number of clips to sample from a video uniformly for aggregating the\n# prediction results.\n_C.TEST.NUM_ENSEMBLE_VIEWS = 10\n# Number of crops to sample from a frame spatially for aggregating the\n# prediction results.\n_C.TEST.NUM_SPATIAL_CROPS = 3\n# Checkpoint types include `caffe2` or `pytorch`.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TEST.CHECKPOINT_FILE_PATH",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TEST.CHECKPOINT_FILE_PATH = \"\"\n# Number of clips to sample from a video uniformly for aggregating the\n# prediction results.\n_C.TEST.NUM_ENSEMBLE_VIEWS = 10\n# Number of crops to sample from a frame spatially for aggregating the\n# prediction results.\n_C.TEST.NUM_SPATIAL_CROPS = 3\n# Checkpoint types include `caffe2` or `pytorch`.\n_C.TEST.CHECKPOINT_TYPE = \"pytorch\"\n# Path to saving prediction results file.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TEST.NUM_ENSEMBLE_VIEWS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TEST.NUM_ENSEMBLE_VIEWS = 10\n# Number of crops to sample from a frame spatially for aggregating the\n# prediction results.\n_C.TEST.NUM_SPATIAL_CROPS = 3\n# Checkpoint types include `caffe2` or `pytorch`.\n_C.TEST.CHECKPOINT_TYPE = \"pytorch\"\n# Path to saving prediction results file.\n_C.TEST.SAVE_RESULTS_PATH = \"\"\n# -----------------------------------------------------------------------------\n# ResNet options",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TEST.NUM_SPATIAL_CROPS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TEST.NUM_SPATIAL_CROPS = 3\n# Checkpoint types include `caffe2` or `pytorch`.\n_C.TEST.CHECKPOINT_TYPE = \"pytorch\"\n# Path to saving prediction results file.\n_C.TEST.SAVE_RESULTS_PATH = \"\"\n# -----------------------------------------------------------------------------\n# ResNet options\n# -----------------------------------------------------------------------------\n_C.RESNET = CfgNode()\n# Transformation function.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TEST.CHECKPOINT_TYPE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TEST.CHECKPOINT_TYPE = \"pytorch\"\n# Path to saving prediction results file.\n_C.TEST.SAVE_RESULTS_PATH = \"\"\n# -----------------------------------------------------------------------------\n# ResNet options\n# -----------------------------------------------------------------------------\n_C.RESNET = CfgNode()\n# Transformation function.\n_C.RESNET.TRANS_FUNC = \"bottleneck_transform\"\n# Number of groups. 1 for ResNet, and larger than 1 for ResNeXt).",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TEST.SAVE_RESULTS_PATH",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TEST.SAVE_RESULTS_PATH = \"\"\n# -----------------------------------------------------------------------------\n# ResNet options\n# -----------------------------------------------------------------------------\n_C.RESNET = CfgNode()\n# Transformation function.\n_C.RESNET.TRANS_FUNC = \"bottleneck_transform\"\n# Number of groups. 1 for ResNet, and larger than 1 for ResNeXt).\n_C.RESNET.NUM_GROUPS = 1\n# Width of each group (64 -> ResNet; 4 -> ResNeXt).",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.RESNET",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.RESNET = CfgNode()\n# Transformation function.\n_C.RESNET.TRANS_FUNC = \"bottleneck_transform\"\n# Number of groups. 1 for ResNet, and larger than 1 for ResNeXt).\n_C.RESNET.NUM_GROUPS = 1\n# Width of each group (64 -> ResNet; 4 -> ResNeXt).\n_C.RESNET.WIDTH_PER_GROUP = 64\n# Apply relu in a inplace manner.\n_C.RESNET.INPLACE_RELU = True\n# Apply stride to 1x1 conv.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.RESNET.TRANS_FUNC",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.RESNET.TRANS_FUNC = \"bottleneck_transform\"\n# Number of groups. 1 for ResNet, and larger than 1 for ResNeXt).\n_C.RESNET.NUM_GROUPS = 1\n# Width of each group (64 -> ResNet; 4 -> ResNeXt).\n_C.RESNET.WIDTH_PER_GROUP = 64\n# Apply relu in a inplace manner.\n_C.RESNET.INPLACE_RELU = True\n# Apply stride to 1x1 conv.\n_C.RESNET.STRIDE_1X1 = False\n#  If true, initialize the gamma of the final BN of each block to zero.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.RESNET.NUM_GROUPS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.RESNET.NUM_GROUPS = 1\n# Width of each group (64 -> ResNet; 4 -> ResNeXt).\n_C.RESNET.WIDTH_PER_GROUP = 64\n# Apply relu in a inplace manner.\n_C.RESNET.INPLACE_RELU = True\n# Apply stride to 1x1 conv.\n_C.RESNET.STRIDE_1X1 = False\n#  If true, initialize the gamma of the final BN of each block to zero.\n_C.RESNET.ZERO_INIT_FINAL_BN = False\n# Number of weight layers.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.RESNET.WIDTH_PER_GROUP",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.RESNET.WIDTH_PER_GROUP = 64\n# Apply relu in a inplace manner.\n_C.RESNET.INPLACE_RELU = True\n# Apply stride to 1x1 conv.\n_C.RESNET.STRIDE_1X1 = False\n#  If true, initialize the gamma of the final BN of each block to zero.\n_C.RESNET.ZERO_INIT_FINAL_BN = False\n# Number of weight layers.\n_C.RESNET.DEPTH = 50\n# If the current block has more than NUM_BLOCK_TEMP_KERNEL blocks, use temporal",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.RESNET.INPLACE_RELU",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.RESNET.INPLACE_RELU = True\n# Apply stride to 1x1 conv.\n_C.RESNET.STRIDE_1X1 = False\n#  If true, initialize the gamma of the final BN of each block to zero.\n_C.RESNET.ZERO_INIT_FINAL_BN = False\n# Number of weight layers.\n_C.RESNET.DEPTH = 50\n# If the current block has more than NUM_BLOCK_TEMP_KERNEL blocks, use temporal\n# kernel of 1 for the rest of the blocks.\n_C.RESNET.NUM_BLOCK_TEMP_KERNEL = [[3], [4], [6], [3]]",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.RESNET.STRIDE_1X1",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.RESNET.STRIDE_1X1 = False\n#  If true, initialize the gamma of the final BN of each block to zero.\n_C.RESNET.ZERO_INIT_FINAL_BN = False\n# Number of weight layers.\n_C.RESNET.DEPTH = 50\n# If the current block has more than NUM_BLOCK_TEMP_KERNEL blocks, use temporal\n# kernel of 1 for the rest of the blocks.\n_C.RESNET.NUM_BLOCK_TEMP_KERNEL = [[3], [4], [6], [3]]\n# Size of stride on different res stages.\n_C.RESNET.SPATIAL_STRIDES = [[1], [2], [2], [2]]",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.RESNET.ZERO_INIT_FINAL_BN",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.RESNET.ZERO_INIT_FINAL_BN = False\n# Number of weight layers.\n_C.RESNET.DEPTH = 50\n# If the current block has more than NUM_BLOCK_TEMP_KERNEL blocks, use temporal\n# kernel of 1 for the rest of the blocks.\n_C.RESNET.NUM_BLOCK_TEMP_KERNEL = [[3], [4], [6], [3]]\n# Size of stride on different res stages.\n_C.RESNET.SPATIAL_STRIDES = [[1], [2], [2], [2]]\n# Size of dilation on different res stages.\n_C.RESNET.SPATIAL_DILATIONS = [[1], [1], [1], [1]]",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.RESNET.DEPTH",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.RESNET.DEPTH = 50\n# If the current block has more than NUM_BLOCK_TEMP_KERNEL blocks, use temporal\n# kernel of 1 for the rest of the blocks.\n_C.RESNET.NUM_BLOCK_TEMP_KERNEL = [[3], [4], [6], [3]]\n# Size of stride on different res stages.\n_C.RESNET.SPATIAL_STRIDES = [[1], [2], [2], [2]]\n# Size of dilation on different res stages.\n_C.RESNET.SPATIAL_DILATIONS = [[1], [1], [1], [1]]\n# -----------------------------------------------------------------------------\n# MORPH options",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.RESNET.NUM_BLOCK_TEMP_KERNEL",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.RESNET.NUM_BLOCK_TEMP_KERNEL = [[3], [4], [6], [3]]\n# Size of stride on different res stages.\n_C.RESNET.SPATIAL_STRIDES = [[1], [2], [2], [2]]\n# Size of dilation on different res stages.\n_C.RESNET.SPATIAL_DILATIONS = [[1], [1], [1], [1]]\n# -----------------------------------------------------------------------------\n# MORPH options\n# -----------------------------------------------------------------------------\n_C.MORPH = CfgNode()\n# layers.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.RESNET.SPATIAL_STRIDES",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.RESNET.SPATIAL_STRIDES = [[1], [2], [2], [2]]\n# Size of dilation on different res stages.\n_C.RESNET.SPATIAL_DILATIONS = [[1], [1], [1], [1]]\n# -----------------------------------------------------------------------------\n# MORPH options\n# -----------------------------------------------------------------------------\n_C.MORPH = CfgNode()\n# layers.\n_C.MORPH.LAYERS = [4, 3, 8, 3]\n# whether downsample in the end.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.RESNET.SPATIAL_DILATIONS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.RESNET.SPATIAL_DILATIONS = [[1], [1], [1], [1]]\n# -----------------------------------------------------------------------------\n# MORPH options\n# -----------------------------------------------------------------------------\n_C.MORPH = CfgNode()\n# layers.\n_C.MORPH.LAYERS = [4, 3, 8, 3]\n# whether downsample in the end.\n_C.MORPH.TRANSITIONS = [True, False, False, False]\n# dimesion of segment.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MORPH",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MORPH = CfgNode()\n# layers.\n_C.MORPH.LAYERS = [4, 3, 8, 3]\n# whether downsample in the end.\n_C.MORPH.TRANSITIONS = [True, False, False, False]\n# dimesion of segment.\n_C.MORPH.SEGMENT_DIM = [32, 16, 16, 16]\n# temporal stride.\n_C.MORPH.T_STRIDE = 1\n# mlp ratio of different layer.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MORPH.LAYERS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MORPH.LAYERS = [4, 3, 8, 3]\n# whether downsample in the end.\n_C.MORPH.TRANSITIONS = [True, False, False, False]\n# dimesion of segment.\n_C.MORPH.SEGMENT_DIM = [32, 16, 16, 16]\n# temporal stride.\n_C.MORPH.T_STRIDE = 1\n# mlp ratio of different layer.\n_C.MORPH.MLP_RATIOS = [3, 3, 3, 3]\n# embedding dimensions of different layer.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MORPH.TRANSITIONS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MORPH.TRANSITIONS = [True, False, False, False]\n# dimesion of segment.\n_C.MORPH.SEGMENT_DIM = [32, 16, 16, 16]\n# temporal stride.\n_C.MORPH.T_STRIDE = 1\n# mlp ratio of different layer.\n_C.MORPH.MLP_RATIOS = [3, 3, 3, 3]\n# embedding dimensions of different layer.\n_C.MORPH.EMBED_DIMS = [192, 384, 384, 384]\n# patch size.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MORPH.SEGMENT_DIM",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MORPH.SEGMENT_DIM = [32, 16, 16, 16]\n# temporal stride.\n_C.MORPH.T_STRIDE = 1\n# mlp ratio of different layer.\n_C.MORPH.MLP_RATIOS = [3, 3, 3, 3]\n# embedding dimensions of different layer.\n_C.MORPH.EMBED_DIMS = [192, 384, 384, 384]\n# patch size.\n_C.MORPH.PATCH_SIZE = 7\n# override default qk scale of head_dim ** -0.5 if set.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MORPH.T_STRIDE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MORPH.T_STRIDE = 1\n# mlp ratio of different layer.\n_C.MORPH.MLP_RATIOS = [3, 3, 3, 3]\n# embedding dimensions of different layer.\n_C.MORPH.EMBED_DIMS = [192, 384, 384, 384]\n# patch size.\n_C.MORPH.PATCH_SIZE = 7\n# override default qk scale of head_dim ** -0.5 if set.\n_C.MORPH.QKV_SCALE = None\n# enable bias for qkv if True.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MORPH.MLP_RATIOS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MORPH.MLP_RATIOS = [3, 3, 3, 3]\n# embedding dimensions of different layer.\n_C.MORPH.EMBED_DIMS = [192, 384, 384, 384]\n# patch size.\n_C.MORPH.PATCH_SIZE = 7\n# override default qk scale of head_dim ** -0.5 if set.\n_C.MORPH.QKV_SCALE = None\n# enable bias for qkv if True.\n_C.MORPH.QKV_BIAS = True\n# attention dropout rate.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MORPH.EMBED_DIMS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MORPH.EMBED_DIMS = [192, 384, 384, 384]\n# patch size.\n_C.MORPH.PATCH_SIZE = 7\n# override default qk scale of head_dim ** -0.5 if set.\n_C.MORPH.QKV_SCALE = None\n# enable bias for qkv if True.\n_C.MORPH.QKV_BIAS = True\n# attention dropout rate.\n_C.MORPH.ATTENTION_DROPOUT_RATE = 0\n# stochastic depth rate.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MORPH.PATCH_SIZE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MORPH.PATCH_SIZE = 7\n# override default qk scale of head_dim ** -0.5 if set.\n_C.MORPH.QKV_SCALE = None\n# enable bias for qkv if True.\n_C.MORPH.QKV_BIAS = True\n# attention dropout rate.\n_C.MORPH.ATTENTION_DROPOUT_RATE = 0\n# stochastic depth rate.\n_C.MORPH.DROP_DEPTH_RATE = 0.1\n# pretrained path",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MORPH.QKV_SCALE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MORPH.QKV_SCALE = None\n# enable bias for qkv if True.\n_C.MORPH.QKV_BIAS = True\n# attention dropout rate.\n_C.MORPH.ATTENTION_DROPOUT_RATE = 0\n# stochastic depth rate.\n_C.MORPH.DROP_DEPTH_RATE = 0.1\n# pretrained path\n_C.MORPH.PRETRAIN_PATH = None\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MORPH.QKV_BIAS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MORPH.QKV_BIAS = True\n# attention dropout rate.\n_C.MORPH.ATTENTION_DROPOUT_RATE = 0\n# stochastic depth rate.\n_C.MORPH.DROP_DEPTH_RATE = 0.1\n# pretrained path\n_C.MORPH.PRETRAIN_PATH = None\n# ---------------------------------------------------------------------------- #\n# X3D  options\n# See https://arxiv.org/abs/2004.04730 for details about X3D Networks.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MORPH.ATTENTION_DROPOUT_RATE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MORPH.ATTENTION_DROPOUT_RATE = 0\n# stochastic depth rate.\n_C.MORPH.DROP_DEPTH_RATE = 0.1\n# pretrained path\n_C.MORPH.PRETRAIN_PATH = None\n# ---------------------------------------------------------------------------- #\n# X3D  options\n# See https://arxiv.org/abs/2004.04730 for details about X3D Networks.\n# ---------------------------------------------------------------------------- #\n_C.X3D = CfgNode()",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MORPH.DROP_DEPTH_RATE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MORPH.DROP_DEPTH_RATE = 0.1\n# pretrained path\n_C.MORPH.PRETRAIN_PATH = None\n# ---------------------------------------------------------------------------- #\n# X3D  options\n# See https://arxiv.org/abs/2004.04730 for details about X3D Networks.\n# ---------------------------------------------------------------------------- #\n_C.X3D = CfgNode()\n# Width expansion factor.\n_C.X3D.WIDTH_FACTOR = 1.0",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MORPH.PRETRAIN_PATH",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MORPH.PRETRAIN_PATH = None\n# ---------------------------------------------------------------------------- #\n# X3D  options\n# See https://arxiv.org/abs/2004.04730 for details about X3D Networks.\n# ---------------------------------------------------------------------------- #\n_C.X3D = CfgNode()\n# Width expansion factor.\n_C.X3D.WIDTH_FACTOR = 1.0\n# Depth expansion factor.\n_C.X3D.DEPTH_FACTOR = 1.0",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.X3D",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.X3D = CfgNode()\n# Width expansion factor.\n_C.X3D.WIDTH_FACTOR = 1.0\n# Depth expansion factor.\n_C.X3D.DEPTH_FACTOR = 1.0\n# Bottleneck expansion factor for the 3x3x3 conv.\n_C.X3D.BOTTLENECK_FACTOR = 1.0  #\n# Dimensions of the last linear layer before classificaiton.\n_C.X3D.DIM_C5 = 2048\n# Dimensions of the first 3x3 conv layer.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.X3D.WIDTH_FACTOR",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.X3D.WIDTH_FACTOR = 1.0\n# Depth expansion factor.\n_C.X3D.DEPTH_FACTOR = 1.0\n# Bottleneck expansion factor for the 3x3x3 conv.\n_C.X3D.BOTTLENECK_FACTOR = 1.0  #\n# Dimensions of the last linear layer before classificaiton.\n_C.X3D.DIM_C5 = 2048\n# Dimensions of the first 3x3 conv layer.\n_C.X3D.DIM_C1 = 12\n# Whether to scale the width of Res2, default is false.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.X3D.DEPTH_FACTOR",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.X3D.DEPTH_FACTOR = 1.0\n# Bottleneck expansion factor for the 3x3x3 conv.\n_C.X3D.BOTTLENECK_FACTOR = 1.0  #\n# Dimensions of the last linear layer before classificaiton.\n_C.X3D.DIM_C5 = 2048\n# Dimensions of the first 3x3 conv layer.\n_C.X3D.DIM_C1 = 12\n# Whether to scale the width of Res2, default is false.\n_C.X3D.SCALE_RES2 = False\n# Whether to use a BatchNorm (BN) layer before the classifier, default is false.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.X3D.BOTTLENECK_FACTOR",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.X3D.BOTTLENECK_FACTOR = 1.0  #\n# Dimensions of the last linear layer before classificaiton.\n_C.X3D.DIM_C5 = 2048\n# Dimensions of the first 3x3 conv layer.\n_C.X3D.DIM_C1 = 12\n# Whether to scale the width of Res2, default is false.\n_C.X3D.SCALE_RES2 = False\n# Whether to use a BatchNorm (BN) layer before the classifier, default is false.\n_C.X3D.BN_LIN5 = False\n# Whether to use channelwise (=depthwise) convolution in the center (3x3x3)",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.X3D.DIM_C5",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.X3D.DIM_C5 = 2048\n# Dimensions of the first 3x3 conv layer.\n_C.X3D.DIM_C1 = 12\n# Whether to scale the width of Res2, default is false.\n_C.X3D.SCALE_RES2 = False\n# Whether to use a BatchNorm (BN) layer before the classifier, default is false.\n_C.X3D.BN_LIN5 = False\n# Whether to use channelwise (=depthwise) convolution in the center (3x3x3)\n# convolution operation of the residual blocks.\n_C.X3D.CHANNELWISE_3x3x3 = True",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.X3D.DIM_C1",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.X3D.DIM_C1 = 12\n# Whether to scale the width of Res2, default is false.\n_C.X3D.SCALE_RES2 = False\n# Whether to use a BatchNorm (BN) layer before the classifier, default is false.\n_C.X3D.BN_LIN5 = False\n# Whether to use channelwise (=depthwise) convolution in the center (3x3x3)\n# convolution operation of the residual blocks.\n_C.X3D.CHANNELWISE_3x3x3 = True\n# -----------------------------------------------------------------------------\n# Nonlocal options",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.X3D.SCALE_RES2",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.X3D.SCALE_RES2 = False\n# Whether to use a BatchNorm (BN) layer before the classifier, default is false.\n_C.X3D.BN_LIN5 = False\n# Whether to use channelwise (=depthwise) convolution in the center (3x3x3)\n# convolution operation of the residual blocks.\n_C.X3D.CHANNELWISE_3x3x3 = True\n# -----------------------------------------------------------------------------\n# Nonlocal options\n# -----------------------------------------------------------------------------\n_C.NONLOCAL = CfgNode()",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.X3D.BN_LIN5",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.X3D.BN_LIN5 = False\n# Whether to use channelwise (=depthwise) convolution in the center (3x3x3)\n# convolution operation of the residual blocks.\n_C.X3D.CHANNELWISE_3x3x3 = True\n# -----------------------------------------------------------------------------\n# Nonlocal options\n# -----------------------------------------------------------------------------\n_C.NONLOCAL = CfgNode()\n# Index of each stage and block to add nonlocal layers.\n_C.NONLOCAL.LOCATION = [[[]], [[]], [[]], [[]]]",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.X3D.CHANNELWISE_3x3x3",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.X3D.CHANNELWISE_3x3x3 = True\n# -----------------------------------------------------------------------------\n# Nonlocal options\n# -----------------------------------------------------------------------------\n_C.NONLOCAL = CfgNode()\n# Index of each stage and block to add nonlocal layers.\n_C.NONLOCAL.LOCATION = [[[]], [[]], [[]], [[]]]\n# Number of group for nonlocal for each stage.\n_C.NONLOCAL.GROUP = [[1], [1], [1], [1]]\n# Instatiation to use for non-local layer.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.NONLOCAL",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.NONLOCAL = CfgNode()\n# Index of each stage and block to add nonlocal layers.\n_C.NONLOCAL.LOCATION = [[[]], [[]], [[]], [[]]]\n# Number of group for nonlocal for each stage.\n_C.NONLOCAL.GROUP = [[1], [1], [1], [1]]\n# Instatiation to use for non-local layer.\n_C.NONLOCAL.INSTANTIATION = \"dot_product\"\n# Size of pooling layers used in Non-Local.\n_C.NONLOCAL.POOL = [\n    # Res2",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.NONLOCAL.LOCATION",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.NONLOCAL.LOCATION = [[[]], [[]], [[]], [[]]]\n# Number of group for nonlocal for each stage.\n_C.NONLOCAL.GROUP = [[1], [1], [1], [1]]\n# Instatiation to use for non-local layer.\n_C.NONLOCAL.INSTANTIATION = \"dot_product\"\n# Size of pooling layers used in Non-Local.\n_C.NONLOCAL.POOL = [\n    # Res2\n    [[1, 2, 2], [1, 2, 2]],\n    # Res3",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.NONLOCAL.GROUP",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.NONLOCAL.GROUP = [[1], [1], [1], [1]]\n# Instatiation to use for non-local layer.\n_C.NONLOCAL.INSTANTIATION = \"dot_product\"\n# Size of pooling layers used in Non-Local.\n_C.NONLOCAL.POOL = [\n    # Res2\n    [[1, 2, 2], [1, 2, 2]],\n    # Res3\n    [[1, 2, 2], [1, 2, 2]],\n    # Res4",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.NONLOCAL.INSTANTIATION",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.NONLOCAL.INSTANTIATION = \"dot_product\"\n# Size of pooling layers used in Non-Local.\n_C.NONLOCAL.POOL = [\n    # Res2\n    [[1, 2, 2], [1, 2, 2]],\n    # Res3\n    [[1, 2, 2], [1, 2, 2]],\n    # Res4\n    [[1, 2, 2], [1, 2, 2]],\n    # Res5",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.NONLOCAL.POOL",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.NONLOCAL.POOL = [\n    # Res2\n    [[1, 2, 2], [1, 2, 2]],\n    # Res3\n    [[1, 2, 2], [1, 2, 2]],\n    # Res4\n    [[1, 2, 2], [1, 2, 2]],\n    # Res5\n    [[1, 2, 2], [1, 2, 2]],\n]",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MODEL",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MODEL = CfgNode()\n# Model architecture.\n_C.MODEL.ARCH = \"slowfast\"\n_C.MODEL.USE_CHECKPOINT=True\n_C.MODEL.CHECKPOINT_NUM=2\n# Model name\n_C.MODEL.MODEL_NAME = \"SlowFast\"\n# The number of classes to predict for the model.\n_C.MODEL.NUM_CLASSES = 400\n# Loss function.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MODEL.ARCH",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MODEL.ARCH = \"slowfast\"\n_C.MODEL.USE_CHECKPOINT=True\n_C.MODEL.CHECKPOINT_NUM=2\n# Model name\n_C.MODEL.MODEL_NAME = \"SlowFast\"\n# The number of classes to predict for the model.\n_C.MODEL.NUM_CLASSES = 400\n# Loss function.\n_C.MODEL.LOSS_FUNC = \"cross_entropy\"\n# Model architectures that has one single pathway.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MODEL.MODEL_NAME",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MODEL.MODEL_NAME = \"SlowFast\"\n# The number of classes to predict for the model.\n_C.MODEL.NUM_CLASSES = 400\n# Loss function.\n_C.MODEL.LOSS_FUNC = \"cross_entropy\"\n# Model architectures that has one single pathway.\n_C.MODEL.SINGLE_PATHWAY_ARCH = [\"2d\", \"c2d\", \"i3d\", \"slow\", \"x3d\", \"mvit\", \"morph\"]\n# Model architectures that has multiple pathways.\n_C.MODEL.MULTI_PATHWAY_ARCH = [\"slowfast\"]\n# Dropout rate before final projection in the backbone.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MODEL.NUM_CLASSES",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MODEL.NUM_CLASSES = 400\n# Loss function.\n_C.MODEL.LOSS_FUNC = \"cross_entropy\"\n# Model architectures that has one single pathway.\n_C.MODEL.SINGLE_PATHWAY_ARCH = [\"2d\", \"c2d\", \"i3d\", \"slow\", \"x3d\", \"mvit\", \"morph\"]\n# Model architectures that has multiple pathways.\n_C.MODEL.MULTI_PATHWAY_ARCH = [\"slowfast\"]\n# Dropout rate before final projection in the backbone.\n_C.MODEL.DROPOUT_RATE = 0.5\n# Randomly drop rate for Res-blocks, linearly increase from res2 to res5",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MODEL.LOSS_FUNC",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MODEL.LOSS_FUNC = \"cross_entropy\"\n# Model architectures that has one single pathway.\n_C.MODEL.SINGLE_PATHWAY_ARCH = [\"2d\", \"c2d\", \"i3d\", \"slow\", \"x3d\", \"mvit\", \"morph\"]\n# Model architectures that has multiple pathways.\n_C.MODEL.MULTI_PATHWAY_ARCH = [\"slowfast\"]\n# Dropout rate before final projection in the backbone.\n_C.MODEL.DROPOUT_RATE = 0.5\n# Randomly drop rate for Res-blocks, linearly increase from res2 to res5\n_C.MODEL.DROPCONNECT_RATE = 0.0\n# The std to initialize the fc layer(s).",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MODEL.SINGLE_PATHWAY_ARCH",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MODEL.SINGLE_PATHWAY_ARCH = [\"2d\", \"c2d\", \"i3d\", \"slow\", \"x3d\", \"mvit\", \"morph\"]\n# Model architectures that has multiple pathways.\n_C.MODEL.MULTI_PATHWAY_ARCH = [\"slowfast\"]\n# Dropout rate before final projection in the backbone.\n_C.MODEL.DROPOUT_RATE = 0.5\n# Randomly drop rate for Res-blocks, linearly increase from res2 to res5\n_C.MODEL.DROPCONNECT_RATE = 0.0\n# The std to initialize the fc layer(s).\n_C.MODEL.FC_INIT_STD = 0.01\n# Activation layer for the output head.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MODEL.MULTI_PATHWAY_ARCH",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MODEL.MULTI_PATHWAY_ARCH = [\"slowfast\"]\n# Dropout rate before final projection in the backbone.\n_C.MODEL.DROPOUT_RATE = 0.5\n# Randomly drop rate for Res-blocks, linearly increase from res2 to res5\n_C.MODEL.DROPCONNECT_RATE = 0.0\n# The std to initialize the fc layer(s).\n_C.MODEL.FC_INIT_STD = 0.01\n# Activation layer for the output head.\n_C.MODEL.HEAD_ACT = \"softmax\"\n# -----------------------------------------------------------------------------",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MODEL.DROPOUT_RATE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MODEL.DROPOUT_RATE = 0.5\n# Randomly drop rate for Res-blocks, linearly increase from res2 to res5\n_C.MODEL.DROPCONNECT_RATE = 0.0\n# The std to initialize the fc layer(s).\n_C.MODEL.FC_INIT_STD = 0.01\n# Activation layer for the output head.\n_C.MODEL.HEAD_ACT = \"softmax\"\n# -----------------------------------------------------------------------------\n# MViT options\n# -----------------------------------------------------------------------------",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MODEL.DROPCONNECT_RATE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MODEL.DROPCONNECT_RATE = 0.0\n# The std to initialize the fc layer(s).\n_C.MODEL.FC_INIT_STD = 0.01\n# Activation layer for the output head.\n_C.MODEL.HEAD_ACT = \"softmax\"\n# -----------------------------------------------------------------------------\n# MViT options\n# -----------------------------------------------------------------------------\n_C.MVIT = CfgNode()\n# Options include `conv`, `max`.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MODEL.FC_INIT_STD",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MODEL.FC_INIT_STD = 0.01\n# Activation layer for the output head.\n_C.MODEL.HEAD_ACT = \"softmax\"\n# -----------------------------------------------------------------------------\n# MViT options\n# -----------------------------------------------------------------------------\n_C.MVIT = CfgNode()\n# Options include `conv`, `max`.\n_C.MVIT.MODE = \"conv\"\n# If True, perform pool before projection in attention.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MODEL.HEAD_ACT",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MODEL.HEAD_ACT = \"softmax\"\n# -----------------------------------------------------------------------------\n# MViT options\n# -----------------------------------------------------------------------------\n_C.MVIT = CfgNode()\n# Options include `conv`, `max`.\n_C.MVIT.MODE = \"conv\"\n# If True, perform pool before projection in attention.\n_C.MVIT.POOL_FIRST = False\n# If True, use cls embed in the network, otherwise don't use cls_embed in transformer.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT = CfgNode()\n# Options include `conv`, `max`.\n_C.MVIT.MODE = \"conv\"\n# If True, perform pool before projection in attention.\n_C.MVIT.POOL_FIRST = False\n# If True, use cls embed in the network, otherwise don't use cls_embed in transformer.\n_C.MVIT.CLS_EMBED_ON = True\n# Kernel size for patchtification.\n_C.MVIT.PATCH_KERNEL = [3, 7, 7]\n# Stride size for patchtification.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.MODE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.MODE = \"conv\"\n# If True, perform pool before projection in attention.\n_C.MVIT.POOL_FIRST = False\n# If True, use cls embed in the network, otherwise don't use cls_embed in transformer.\n_C.MVIT.CLS_EMBED_ON = True\n# Kernel size for patchtification.\n_C.MVIT.PATCH_KERNEL = [3, 7, 7]\n# Stride size for patchtification.\n_C.MVIT.PATCH_STRIDE = [2, 4, 4]\n# Padding size for patchtification.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.POOL_FIRST",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.POOL_FIRST = False\n# If True, use cls embed in the network, otherwise don't use cls_embed in transformer.\n_C.MVIT.CLS_EMBED_ON = True\n# Kernel size for patchtification.\n_C.MVIT.PATCH_KERNEL = [3, 7, 7]\n# Stride size for patchtification.\n_C.MVIT.PATCH_STRIDE = [2, 4, 4]\n# Padding size for patchtification.\n_C.MVIT.PATCH_PADDING = [2, 4, 4]\n# If True, use 2d patch, otherwise use 3d patch.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.CLS_EMBED_ON",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.CLS_EMBED_ON = True\n# Kernel size for patchtification.\n_C.MVIT.PATCH_KERNEL = [3, 7, 7]\n# Stride size for patchtification.\n_C.MVIT.PATCH_STRIDE = [2, 4, 4]\n# Padding size for patchtification.\n_C.MVIT.PATCH_PADDING = [2, 4, 4]\n# If True, use 2d patch, otherwise use 3d patch.\n_C.MVIT.PATCH_2D = False\n# Base embedding dimension for the transformer.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.PATCH_KERNEL",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.PATCH_KERNEL = [3, 7, 7]\n# Stride size for patchtification.\n_C.MVIT.PATCH_STRIDE = [2, 4, 4]\n# Padding size for patchtification.\n_C.MVIT.PATCH_PADDING = [2, 4, 4]\n# If True, use 2d patch, otherwise use 3d patch.\n_C.MVIT.PATCH_2D = False\n# Base embedding dimension for the transformer.\n_C.MVIT.EMBED_DIM = 96\n# Base num of heads for the transformer.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.PATCH_STRIDE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.PATCH_STRIDE = [2, 4, 4]\n# Padding size for patchtification.\n_C.MVIT.PATCH_PADDING = [2, 4, 4]\n# If True, use 2d patch, otherwise use 3d patch.\n_C.MVIT.PATCH_2D = False\n# Base embedding dimension for the transformer.\n_C.MVIT.EMBED_DIM = 96\n# Base num of heads for the transformer.\n_C.MVIT.NUM_HEADS = 1\n# Dimension reduction ratio for the MLP layers.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.PATCH_PADDING",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.PATCH_PADDING = [2, 4, 4]\n# If True, use 2d patch, otherwise use 3d patch.\n_C.MVIT.PATCH_2D = False\n# Base embedding dimension for the transformer.\n_C.MVIT.EMBED_DIM = 96\n# Base num of heads for the transformer.\n_C.MVIT.NUM_HEADS = 1\n# Dimension reduction ratio for the MLP layers.\n_C.MVIT.MLP_RATIO = 4.0\n# If use, use bias term in attention fc layers.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.PATCH_2D",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.PATCH_2D = False\n# Base embedding dimension for the transformer.\n_C.MVIT.EMBED_DIM = 96\n# Base num of heads for the transformer.\n_C.MVIT.NUM_HEADS = 1\n# Dimension reduction ratio for the MLP layers.\n_C.MVIT.MLP_RATIO = 4.0\n# If use, use bias term in attention fc layers.\n_C.MVIT.QKV_BIAS = True\n# Drop path rate for the tranfomer.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.EMBED_DIM",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.EMBED_DIM = 96\n# Base num of heads for the transformer.\n_C.MVIT.NUM_HEADS = 1\n# Dimension reduction ratio for the MLP layers.\n_C.MVIT.MLP_RATIO = 4.0\n# If use, use bias term in attention fc layers.\n_C.MVIT.QKV_BIAS = True\n# Drop path rate for the tranfomer.\n_C.MVIT.DROPPATH_RATE = 0.1\n# Depth of the transformer.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.NUM_HEADS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.NUM_HEADS = 1\n# Dimension reduction ratio for the MLP layers.\n_C.MVIT.MLP_RATIO = 4.0\n# If use, use bias term in attention fc layers.\n_C.MVIT.QKV_BIAS = True\n# Drop path rate for the tranfomer.\n_C.MVIT.DROPPATH_RATE = 0.1\n# Depth of the transformer.\n_C.MVIT.DEPTH = 16\n# Normalization layer for the transformer. Only layernorm is supported now.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.MLP_RATIO",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.MLP_RATIO = 4.0\n# If use, use bias term in attention fc layers.\n_C.MVIT.QKV_BIAS = True\n# Drop path rate for the tranfomer.\n_C.MVIT.DROPPATH_RATE = 0.1\n# Depth of the transformer.\n_C.MVIT.DEPTH = 16\n# Normalization layer for the transformer. Only layernorm is supported now.\n_C.MVIT.NORM = \"layernorm\"\n# Dimension multiplication at layer i. If 2.0 is used, then the next block will increase",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.QKV_BIAS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.QKV_BIAS = True\n# Drop path rate for the tranfomer.\n_C.MVIT.DROPPATH_RATE = 0.1\n# Depth of the transformer.\n_C.MVIT.DEPTH = 16\n# Normalization layer for the transformer. Only layernorm is supported now.\n_C.MVIT.NORM = \"layernorm\"\n# Dimension multiplication at layer i. If 2.0 is used, then the next block will increase\n# the dimension by 2 times. Format: [depth_i: mul_dim_ratio]\n_C.MVIT.DIM_MUL = []",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.DROPPATH_RATE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.DROPPATH_RATE = 0.1\n# Depth of the transformer.\n_C.MVIT.DEPTH = 16\n# Normalization layer for the transformer. Only layernorm is supported now.\n_C.MVIT.NORM = \"layernorm\"\n# Dimension multiplication at layer i. If 2.0 is used, then the next block will increase\n# the dimension by 2 times. Format: [depth_i: mul_dim_ratio]\n_C.MVIT.DIM_MUL = []\n# Head number multiplication at layer i. If 2.0 is used, then the next block will\n# increase the number of heads by 2 times. Format: [depth_i: head_mul_ratio]",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.DEPTH",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.DEPTH = 16\n# Normalization layer for the transformer. Only layernorm is supported now.\n_C.MVIT.NORM = \"layernorm\"\n# Dimension multiplication at layer i. If 2.0 is used, then the next block will increase\n# the dimension by 2 times. Format: [depth_i: mul_dim_ratio]\n_C.MVIT.DIM_MUL = []\n# Head number multiplication at layer i. If 2.0 is used, then the next block will\n# increase the number of heads by 2 times. Format: [depth_i: head_mul_ratio]\n_C.MVIT.HEAD_MUL = []\n# Stride size for the Pool KV at layer i.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.NORM",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.NORM = \"layernorm\"\n# Dimension multiplication at layer i. If 2.0 is used, then the next block will increase\n# the dimension by 2 times. Format: [depth_i: mul_dim_ratio]\n_C.MVIT.DIM_MUL = []\n# Head number multiplication at layer i. If 2.0 is used, then the next block will\n# increase the number of heads by 2 times. Format: [depth_i: head_mul_ratio]\n_C.MVIT.HEAD_MUL = []\n# Stride size for the Pool KV at layer i.\n# Format: [[i, stride_t_i, stride_h_i, stride_w_i], ...,]\n_C.MVIT.POOL_KV_STRIDE = None",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.DIM_MUL",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.DIM_MUL = []\n# Head number multiplication at layer i. If 2.0 is used, then the next block will\n# increase the number of heads by 2 times. Format: [depth_i: head_mul_ratio]\n_C.MVIT.HEAD_MUL = []\n# Stride size for the Pool KV at layer i.\n# Format: [[i, stride_t_i, stride_h_i, stride_w_i], ...,]\n_C.MVIT.POOL_KV_STRIDE = None\n# Initial stride size for KV at layer 1. The stride size will be further reduced with\n# the raio of MVIT.DIM_MUL. If will overwrite MVIT.POOL_KV_STRIDE if not None.\n_C.MVIT.POOL_KV_STRIDE_ADAPTIVE = None",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.HEAD_MUL",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.HEAD_MUL = []\n# Stride size for the Pool KV at layer i.\n# Format: [[i, stride_t_i, stride_h_i, stride_w_i], ...,]\n_C.MVIT.POOL_KV_STRIDE = None\n# Initial stride size for KV at layer 1. The stride size will be further reduced with\n# the raio of MVIT.DIM_MUL. If will overwrite MVIT.POOL_KV_STRIDE if not None.\n_C.MVIT.POOL_KV_STRIDE_ADAPTIVE = None\n# Stride size for the Pool Q at layer i.\n# Format: [[i, stride_t_i, stride_h_i, stride_w_i], ...,]\n_C.MVIT.POOL_Q_STRIDE = []",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.POOL_KV_STRIDE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.POOL_KV_STRIDE = None\n# Initial stride size for KV at layer 1. The stride size will be further reduced with\n# the raio of MVIT.DIM_MUL. If will overwrite MVIT.POOL_KV_STRIDE if not None.\n_C.MVIT.POOL_KV_STRIDE_ADAPTIVE = None\n# Stride size for the Pool Q at layer i.\n# Format: [[i, stride_t_i, stride_h_i, stride_w_i], ...,]\n_C.MVIT.POOL_Q_STRIDE = []\n# If not None, overwrite the KV_KERNEL and Q_KERNEL size with POOL_KVQ_CONV_SIZ.\n# Otherwise the kernel_size is [s + 1 if s > 1 else s for s in stride_size].\n_C.MVIT.POOL_KVQ_KERNEL = None",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.POOL_KV_STRIDE_ADAPTIVE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.POOL_KV_STRIDE_ADAPTIVE = None\n# Stride size for the Pool Q at layer i.\n# Format: [[i, stride_t_i, stride_h_i, stride_w_i], ...,]\n_C.MVIT.POOL_Q_STRIDE = []\n# If not None, overwrite the KV_KERNEL and Q_KERNEL size with POOL_KVQ_CONV_SIZ.\n# Otherwise the kernel_size is [s + 1 if s > 1 else s for s in stride_size].\n_C.MVIT.POOL_KVQ_KERNEL = None\n# If True, perform no decay on positional embedding and cls embedding.\n_C.MVIT.ZERO_DECAY_POS_CLS = True\n# If True, use norm after stem.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.POOL_Q_STRIDE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.POOL_Q_STRIDE = []\n# If not None, overwrite the KV_KERNEL and Q_KERNEL size with POOL_KVQ_CONV_SIZ.\n# Otherwise the kernel_size is [s + 1 if s > 1 else s for s in stride_size].\n_C.MVIT.POOL_KVQ_KERNEL = None\n# If True, perform no decay on positional embedding and cls embedding.\n_C.MVIT.ZERO_DECAY_POS_CLS = True\n# If True, use norm after stem.\n_C.MVIT.NORM_STEM = False\n# If True, perform separate positional embedding.\n_C.MVIT.SEP_POS_EMBED = False",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.POOL_KVQ_KERNEL",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.POOL_KVQ_KERNEL = None\n# If True, perform no decay on positional embedding and cls embedding.\n_C.MVIT.ZERO_DECAY_POS_CLS = True\n# If True, use norm after stem.\n_C.MVIT.NORM_STEM = False\n# If True, perform separate positional embedding.\n_C.MVIT.SEP_POS_EMBED = False\n# Dropout rate for the MViT backbone.\n_C.MVIT.DROPOUT_RATE = 0.0\n# -----------------------------------------------------------------------------",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.ZERO_DECAY_POS_CLS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.ZERO_DECAY_POS_CLS = True\n# If True, use norm after stem.\n_C.MVIT.NORM_STEM = False\n# If True, perform separate positional embedding.\n_C.MVIT.SEP_POS_EMBED = False\n# Dropout rate for the MViT backbone.\n_C.MVIT.DROPOUT_RATE = 0.0\n# -----------------------------------------------------------------------------\n# SlowFast options\n# -----------------------------------------------------------------------------",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.NORM_STEM",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.NORM_STEM = False\n# If True, perform separate positional embedding.\n_C.MVIT.SEP_POS_EMBED = False\n# Dropout rate for the MViT backbone.\n_C.MVIT.DROPOUT_RATE = 0.0\n# -----------------------------------------------------------------------------\n# SlowFast options\n# -----------------------------------------------------------------------------\n_C.SLOWFAST = CfgNode()\n# Corresponds to the inverse of the channel reduction ratio, $\\beta$ between",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.SEP_POS_EMBED",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.SEP_POS_EMBED = False\n# Dropout rate for the MViT backbone.\n_C.MVIT.DROPOUT_RATE = 0.0\n# -----------------------------------------------------------------------------\n# SlowFast options\n# -----------------------------------------------------------------------------\n_C.SLOWFAST = CfgNode()\n# Corresponds to the inverse of the channel reduction ratio, $\\beta$ between\n# the Slow and Fast pathways.\n_C.SLOWFAST.BETA_INV = 8",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.DROPOUT_RATE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.DROPOUT_RATE = 0.0\n# -----------------------------------------------------------------------------\n# SlowFast options\n# -----------------------------------------------------------------------------\n_C.SLOWFAST = CfgNode()\n# Corresponds to the inverse of the channel reduction ratio, $\\beta$ between\n# the Slow and Fast pathways.\n_C.SLOWFAST.BETA_INV = 8\n# Corresponds to the frame rate reduction ratio, $\\alpha$ between the Slow and\n# Fast pathways.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SLOWFAST",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SLOWFAST = CfgNode()\n# Corresponds to the inverse of the channel reduction ratio, $\\beta$ between\n# the Slow and Fast pathways.\n_C.SLOWFAST.BETA_INV = 8\n# Corresponds to the frame rate reduction ratio, $\\alpha$ between the Slow and\n# Fast pathways.\n_C.SLOWFAST.ALPHA = 8\n# Ratio of channel dimensions between the Slow and Fast pathways.\n_C.SLOWFAST.FUSION_CONV_CHANNEL_RATIO = 2\n# Kernel dimension used for fusing information from Fast pathway to Slow",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SLOWFAST.BETA_INV",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SLOWFAST.BETA_INV = 8\n# Corresponds to the frame rate reduction ratio, $\\alpha$ between the Slow and\n# Fast pathways.\n_C.SLOWFAST.ALPHA = 8\n# Ratio of channel dimensions between the Slow and Fast pathways.\n_C.SLOWFAST.FUSION_CONV_CHANNEL_RATIO = 2\n# Kernel dimension used for fusing information from Fast pathway to Slow\n# pathway.\n_C.SLOWFAST.FUSION_KERNEL_SZ = 5\n# -----------------------------------------------------------------------------",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SLOWFAST.ALPHA",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SLOWFAST.ALPHA = 8\n# Ratio of channel dimensions between the Slow and Fast pathways.\n_C.SLOWFAST.FUSION_CONV_CHANNEL_RATIO = 2\n# Kernel dimension used for fusing information from Fast pathway to Slow\n# pathway.\n_C.SLOWFAST.FUSION_KERNEL_SZ = 5\n# -----------------------------------------------------------------------------\n# Data options\n# -----------------------------------------------------------------------------\n_C.DATA = CfgNode()",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SLOWFAST.FUSION_CONV_CHANNEL_RATIO",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SLOWFAST.FUSION_CONV_CHANNEL_RATIO = 2\n# Kernel dimension used for fusing information from Fast pathway to Slow\n# pathway.\n_C.SLOWFAST.FUSION_KERNEL_SZ = 5\n# -----------------------------------------------------------------------------\n# Data options\n# -----------------------------------------------------------------------------\n_C.DATA = CfgNode()\n_C.DATA.LABEL_PATH_TEMPLATE=\"somesomev1_rgb_{}_split.txt\"\n_C.DATA.IMAGE_TEMPLATE=\"{:05d}.jpg\"",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SLOWFAST.FUSION_KERNEL_SZ",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SLOWFAST.FUSION_KERNEL_SZ = 5\n# -----------------------------------------------------------------------------\n# Data options\n# -----------------------------------------------------------------------------\n_C.DATA = CfgNode()\n_C.DATA.LABEL_PATH_TEMPLATE=\"somesomev1_rgb_{}_split.txt\"\n_C.DATA.IMAGE_TEMPLATE=\"{:05d}.jpg\"\n# The path to the data directory.\n_C.DATA.PATH_TO_DATA_DIR = \"/mnt/bd/jh-backbone/UniFormer/video_classification/data_list/sthv1\"\n# The separator used between path and label.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA = CfgNode()\n_C.DATA.LABEL_PATH_TEMPLATE=\"somesomev1_rgb_{}_split.txt\"\n_C.DATA.IMAGE_TEMPLATE=\"{:05d}.jpg\"\n# The path to the data directory.\n_C.DATA.PATH_TO_DATA_DIR = \"/mnt/bd/jh-backbone/UniFormer/video_classification/data_list/sthv1\"\n# The separator used between path and label.\n_C.DATA.PATH_LABEL_SEPARATOR = \" \"\n# Video path prefix if any.\n_C.DATA.PATH_PREFIX = \"\"\n# The number of frames of the input clip.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.PATH_TO_DATA_DIR",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.PATH_TO_DATA_DIR = \"/mnt/bd/jh-backbone/UniFormer/video_classification/data_list/sthv1\"\n# The separator used between path and label.\n_C.DATA.PATH_LABEL_SEPARATOR = \" \"\n# Video path prefix if any.\n_C.DATA.PATH_PREFIX = \"\"\n# The number of frames of the input clip.\n_C.DATA.NUM_FRAMES = 8\n# The video sampling rate of the input clip.\n_C.DATA.SAMPLING_RATE = 8\n# Eigenvalues for PCA jittering. Note PCA is RGB based.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.PATH_LABEL_SEPARATOR",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.PATH_LABEL_SEPARATOR = \" \"\n# Video path prefix if any.\n_C.DATA.PATH_PREFIX = \"\"\n# The number of frames of the input clip.\n_C.DATA.NUM_FRAMES = 8\n# The video sampling rate of the input clip.\n_C.DATA.SAMPLING_RATE = 8\n# Eigenvalues for PCA jittering. Note PCA is RGB based.\n_C.DATA.TRAIN_PCA_EIGVAL = [0.225, 0.224, 0.229]\n# Eigenvectors for PCA jittering.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.PATH_PREFIX",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.PATH_PREFIX = \"\"\n# The number of frames of the input clip.\n_C.DATA.NUM_FRAMES = 8\n# The video sampling rate of the input clip.\n_C.DATA.SAMPLING_RATE = 8\n# Eigenvalues for PCA jittering. Note PCA is RGB based.\n_C.DATA.TRAIN_PCA_EIGVAL = [0.225, 0.224, 0.229]\n# Eigenvectors for PCA jittering.\n_C.DATA.TRAIN_PCA_EIGVEC = [\n    [-0.5675, 0.7192, 0.4009],",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.NUM_FRAMES",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.NUM_FRAMES = 8\n# The video sampling rate of the input clip.\n_C.DATA.SAMPLING_RATE = 8\n# Eigenvalues for PCA jittering. Note PCA is RGB based.\n_C.DATA.TRAIN_PCA_EIGVAL = [0.225, 0.224, 0.229]\n# Eigenvectors for PCA jittering.\n_C.DATA.TRAIN_PCA_EIGVEC = [\n    [-0.5675, 0.7192, 0.4009],\n    [-0.5808, -0.0045, -0.8140],\n    [-0.5836, -0.6948, 0.4203],",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.SAMPLING_RATE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.SAMPLING_RATE = 8\n# Eigenvalues for PCA jittering. Note PCA is RGB based.\n_C.DATA.TRAIN_PCA_EIGVAL = [0.225, 0.224, 0.229]\n# Eigenvectors for PCA jittering.\n_C.DATA.TRAIN_PCA_EIGVEC = [\n    [-0.5675, 0.7192, 0.4009],\n    [-0.5808, -0.0045, -0.8140],\n    [-0.5836, -0.6948, 0.4203],\n]\n# If a imdb have been dumpped to a local file with the following format:",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.TRAIN_PCA_EIGVAL",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.TRAIN_PCA_EIGVAL = [0.225, 0.224, 0.229]\n# Eigenvectors for PCA jittering.\n_C.DATA.TRAIN_PCA_EIGVEC = [\n    [-0.5675, 0.7192, 0.4009],\n    [-0.5808, -0.0045, -0.8140],\n    [-0.5836, -0.6948, 0.4203],\n]\n# If a imdb have been dumpped to a local file with the following format:\n# `{\"im_path\": im_path, \"class\": cont_id}`\n# then we can skip the construction of imdb and load it from the local file.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.TRAIN_PCA_EIGVEC",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.TRAIN_PCA_EIGVEC = [\n    [-0.5675, 0.7192, 0.4009],\n    [-0.5808, -0.0045, -0.8140],\n    [-0.5836, -0.6948, 0.4203],\n]\n# If a imdb have been dumpped to a local file with the following format:\n# `{\"im_path\": im_path, \"class\": cont_id}`\n# then we can skip the construction of imdb and load it from the local file.\n_C.DATA.PATH_TO_PRELOAD_IMDB = \"\"\n# The mean value of the video raw pixels across the R G B channels.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.PATH_TO_PRELOAD_IMDB",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.PATH_TO_PRELOAD_IMDB = \"\"\n# The mean value of the video raw pixels across the R G B channels.\n_C.DATA.MEAN = [0.45, 0.45, 0.45]\n# List of input frame channel dimensions.\n_C.DATA.INPUT_CHANNEL_NUM = [3, 3]\n# The std value of the video raw pixels across the R G B channels.\n_C.DATA.STD = [0.225, 0.225, 0.225]\n# The spatial augmentation jitter scales for training.\n_C.DATA.TRAIN_JITTER_SCALES = [256, 320]\n# The relative scale range of Inception-style area based random resizing augmentation.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.MEAN",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.MEAN = [0.45, 0.45, 0.45]\n# List of input frame channel dimensions.\n_C.DATA.INPUT_CHANNEL_NUM = [3, 3]\n# The std value of the video raw pixels across the R G B channels.\n_C.DATA.STD = [0.225, 0.225, 0.225]\n# The spatial augmentation jitter scales for training.\n_C.DATA.TRAIN_JITTER_SCALES = [256, 320]\n# The relative scale range of Inception-style area based random resizing augmentation.\n# If this is provided, DATA.TRAIN_JITTER_SCALES above is ignored.\n_C.DATA.TRAIN_JITTER_SCALES_RELATIVE = []",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.INPUT_CHANNEL_NUM",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.INPUT_CHANNEL_NUM = [3, 3]\n# The std value of the video raw pixels across the R G B channels.\n_C.DATA.STD = [0.225, 0.225, 0.225]\n# The spatial augmentation jitter scales for training.\n_C.DATA.TRAIN_JITTER_SCALES = [256, 320]\n# The relative scale range of Inception-style area based random resizing augmentation.\n# If this is provided, DATA.TRAIN_JITTER_SCALES above is ignored.\n_C.DATA.TRAIN_JITTER_SCALES_RELATIVE = []\n# The relative aspect ratio range of Inception-style area based random resizing\n# augmentation.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.STD",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.STD = [0.225, 0.225, 0.225]\n# The spatial augmentation jitter scales for training.\n_C.DATA.TRAIN_JITTER_SCALES = [256, 320]\n# The relative scale range of Inception-style area based random resizing augmentation.\n# If this is provided, DATA.TRAIN_JITTER_SCALES above is ignored.\n_C.DATA.TRAIN_JITTER_SCALES_RELATIVE = []\n# The relative aspect ratio range of Inception-style area based random resizing\n# augmentation.\n_C.DATA.TRAIN_JITTER_ASPECT_RELATIVE = []\n# If True, perform stride length uniform temporal sampling.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.TRAIN_JITTER_SCALES",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.TRAIN_JITTER_SCALES = [256, 320]\n# The relative scale range of Inception-style area based random resizing augmentation.\n# If this is provided, DATA.TRAIN_JITTER_SCALES above is ignored.\n_C.DATA.TRAIN_JITTER_SCALES_RELATIVE = []\n# The relative aspect ratio range of Inception-style area based random resizing\n# augmentation.\n_C.DATA.TRAIN_JITTER_ASPECT_RELATIVE = []\n# If True, perform stride length uniform temporal sampling.\n_C.DATA.USE_OFFSET_SAMPLING = False\n# Whether to apply motion shift for augmentation.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.TRAIN_JITTER_SCALES_RELATIVE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.TRAIN_JITTER_SCALES_RELATIVE = []\n# The relative aspect ratio range of Inception-style area based random resizing\n# augmentation.\n_C.DATA.TRAIN_JITTER_ASPECT_RELATIVE = []\n# If True, perform stride length uniform temporal sampling.\n_C.DATA.USE_OFFSET_SAMPLING = False\n# Whether to apply motion shift for augmentation.\n_C.DATA.TRAIN_JITTER_MOTION_SHIFT = False\n# The spatial crop size for training.\n_C.DATA.TRAIN_CROP_SIZE = 224",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.TRAIN_JITTER_ASPECT_RELATIVE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.TRAIN_JITTER_ASPECT_RELATIVE = []\n# If True, perform stride length uniform temporal sampling.\n_C.DATA.USE_OFFSET_SAMPLING = False\n# Whether to apply motion shift for augmentation.\n_C.DATA.TRAIN_JITTER_MOTION_SHIFT = False\n# The spatial crop size for training.\n_C.DATA.TRAIN_CROP_SIZE = 224\n# The spatial crop size for testing.\n_C.DATA.TEST_CROP_SIZE = 256\n# Input videos may has different fps, convert it to the target video fps before",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.USE_OFFSET_SAMPLING",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.USE_OFFSET_SAMPLING = False\n# Whether to apply motion shift for augmentation.\n_C.DATA.TRAIN_JITTER_MOTION_SHIFT = False\n# The spatial crop size for training.\n_C.DATA.TRAIN_CROP_SIZE = 224\n# The spatial crop size for testing.\n_C.DATA.TEST_CROP_SIZE = 256\n# Input videos may has different fps, convert it to the target video fps before\n# frame sampling.\n_C.DATA.TARGET_FPS = 30",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.TRAIN_JITTER_MOTION_SHIFT",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.TRAIN_JITTER_MOTION_SHIFT = False\n# The spatial crop size for training.\n_C.DATA.TRAIN_CROP_SIZE = 224\n# The spatial crop size for testing.\n_C.DATA.TEST_CROP_SIZE = 256\n# Input videos may has different fps, convert it to the target video fps before\n# frame sampling.\n_C.DATA.TARGET_FPS = 30\n# Decoding backend, options include `pyav` or `torchvision`\n_C.DATA.DECODING_BACKEND = \"pyav\"",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.TRAIN_CROP_SIZE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.TRAIN_CROP_SIZE = 224\n# The spatial crop size for testing.\n_C.DATA.TEST_CROP_SIZE = 256\n# Input videos may has different fps, convert it to the target video fps before\n# frame sampling.\n_C.DATA.TARGET_FPS = 30\n# Decoding backend, options include `pyav` or `torchvision`\n_C.DATA.DECODING_BACKEND = \"pyav\"\n# if True, sample uniformly in [1 / max_scale, 1 / min_scale] and take a\n# reciprocal to get the scale. If False, take a uniform sample from",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.TEST_CROP_SIZE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.TEST_CROP_SIZE = 256\n# Input videos may has different fps, convert it to the target video fps before\n# frame sampling.\n_C.DATA.TARGET_FPS = 30\n# Decoding backend, options include `pyav` or `torchvision`\n_C.DATA.DECODING_BACKEND = \"pyav\"\n# if True, sample uniformly in [1 / max_scale, 1 / min_scale] and take a\n# reciprocal to get the scale. If False, take a uniform sample from\n# [min_scale, max_scale].\n_C.DATA.INV_UNIFORM_SAMPLE = False",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.TARGET_FPS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.TARGET_FPS = 30\n# Decoding backend, options include `pyav` or `torchvision`\n_C.DATA.DECODING_BACKEND = \"pyav\"\n# if True, sample uniformly in [1 / max_scale, 1 / min_scale] and take a\n# reciprocal to get the scale. If False, take a uniform sample from\n# [min_scale, max_scale].\n_C.DATA.INV_UNIFORM_SAMPLE = False\n# If True, perform random horizontal flip on the video frames during training.\n_C.DATA.RANDOM_FLIP = True\n# If True, calculdate the map as metric.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.DECODING_BACKEND",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.DECODING_BACKEND = \"pyav\"\n# if True, sample uniformly in [1 / max_scale, 1 / min_scale] and take a\n# reciprocal to get the scale. If False, take a uniform sample from\n# [min_scale, max_scale].\n_C.DATA.INV_UNIFORM_SAMPLE = False\n# If True, perform random horizontal flip on the video frames during training.\n_C.DATA.RANDOM_FLIP = True\n# If True, calculdate the map as metric.\n_C.DATA.MULTI_LABEL = False\n# Method to perform the ensemble, options include \"sum\" and \"max\".",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.INV_UNIFORM_SAMPLE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.INV_UNIFORM_SAMPLE = False\n# If True, perform random horizontal flip on the video frames during training.\n_C.DATA.RANDOM_FLIP = True\n# If True, calculdate the map as metric.\n_C.DATA.MULTI_LABEL = False\n# Method to perform the ensemble, options include \"sum\" and \"max\".\n_C.DATA.ENSEMBLE_METHOD = \"sum\"\n# If True, revert the default input channel (RBG <-> BGR).\n_C.DATA.REVERSE_INPUT_CHANNEL = False\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.RANDOM_FLIP",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.RANDOM_FLIP = True\n# If True, calculdate the map as metric.\n_C.DATA.MULTI_LABEL = False\n# Method to perform the ensemble, options include \"sum\" and \"max\".\n_C.DATA.ENSEMBLE_METHOD = \"sum\"\n# If True, revert the default input channel (RBG <-> BGR).\n_C.DATA.REVERSE_INPUT_CHANNEL = False\n# ---------------------------------------------------------------------------- #\n# Optimizer options\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.MULTI_LABEL",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.MULTI_LABEL = False\n# Method to perform the ensemble, options include \"sum\" and \"max\".\n_C.DATA.ENSEMBLE_METHOD = \"sum\"\n# If True, revert the default input channel (RBG <-> BGR).\n_C.DATA.REVERSE_INPUT_CHANNEL = False\n# ---------------------------------------------------------------------------- #\n# Optimizer options\n# ---------------------------------------------------------------------------- #\n_C.SOLVER = CfgNode()\n# Base learning rate.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.ENSEMBLE_METHOD",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.ENSEMBLE_METHOD = \"sum\"\n# If True, revert the default input channel (RBG <-> BGR).\n_C.DATA.REVERSE_INPUT_CHANNEL = False\n# ---------------------------------------------------------------------------- #\n# Optimizer options\n# ---------------------------------------------------------------------------- #\n_C.SOLVER = CfgNode()\n# Base learning rate.\n_C.SOLVER.BASE_LR = 0.1\n# Learning rate policy (see utils/lr_policy.py for options and examples).",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.REVERSE_INPUT_CHANNEL",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.REVERSE_INPUT_CHANNEL = False\n# ---------------------------------------------------------------------------- #\n# Optimizer options\n# ---------------------------------------------------------------------------- #\n_C.SOLVER = CfgNode()\n# Base learning rate.\n_C.SOLVER.BASE_LR = 0.1\n# Learning rate policy (see utils/lr_policy.py for options and examples).\n_C.SOLVER.LR_POLICY = \"cosine\"\n# Final learning rates for 'cosine' policy.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER = CfgNode()\n# Base learning rate.\n_C.SOLVER.BASE_LR = 0.1\n# Learning rate policy (see utils/lr_policy.py for options and examples).\n_C.SOLVER.LR_POLICY = \"cosine\"\n# Final learning rates for 'cosine' policy.\n_C.SOLVER.COSINE_END_LR = 0.0\n# Exponential decay factor.\n_C.SOLVER.GAMMA = 0.1\n# Step size for 'exp' and 'cos' policies (in epochs).",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.BASE_LR",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.BASE_LR = 0.1\n# Learning rate policy (see utils/lr_policy.py for options and examples).\n_C.SOLVER.LR_POLICY = \"cosine\"\n# Final learning rates for 'cosine' policy.\n_C.SOLVER.COSINE_END_LR = 0.0\n# Exponential decay factor.\n_C.SOLVER.GAMMA = 0.1\n# Step size for 'exp' and 'cos' policies (in epochs).\n_C.SOLVER.STEP_SIZE = 1\n# Steps for 'steps_' policies (in epochs).",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.LR_POLICY",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.LR_POLICY = \"cosine\"\n# Final learning rates for 'cosine' policy.\n_C.SOLVER.COSINE_END_LR = 0.0\n# Exponential decay factor.\n_C.SOLVER.GAMMA = 0.1\n# Step size for 'exp' and 'cos' policies (in epochs).\n_C.SOLVER.STEP_SIZE = 1\n# Steps for 'steps_' policies (in epochs).\n_C.SOLVER.STEPS = []\n# Learning rates for 'steps_' policies.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.COSINE_END_LR",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.COSINE_END_LR = 0.0\n# Exponential decay factor.\n_C.SOLVER.GAMMA = 0.1\n# Step size for 'exp' and 'cos' policies (in epochs).\n_C.SOLVER.STEP_SIZE = 1\n# Steps for 'steps_' policies (in epochs).\n_C.SOLVER.STEPS = []\n# Learning rates for 'steps_' policies.\n_C.SOLVER.LRS = []\n# Maximal number of epochs.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.GAMMA",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.GAMMA = 0.1\n# Step size for 'exp' and 'cos' policies (in epochs).\n_C.SOLVER.STEP_SIZE = 1\n# Steps for 'steps_' policies (in epochs).\n_C.SOLVER.STEPS = []\n# Learning rates for 'steps_' policies.\n_C.SOLVER.LRS = []\n# Maximal number of epochs.\n_C.SOLVER.MAX_EPOCH = 300\n# Momentum.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.STEP_SIZE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.STEP_SIZE = 1\n# Steps for 'steps_' policies (in epochs).\n_C.SOLVER.STEPS = []\n# Learning rates for 'steps_' policies.\n_C.SOLVER.LRS = []\n# Maximal number of epochs.\n_C.SOLVER.MAX_EPOCH = 300\n# Momentum.\n_C.SOLVER.MOMENTUM = 0.9\n# Momentum dampening.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.STEPS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.STEPS = []\n# Learning rates for 'steps_' policies.\n_C.SOLVER.LRS = []\n# Maximal number of epochs.\n_C.SOLVER.MAX_EPOCH = 300\n# Momentum.\n_C.SOLVER.MOMENTUM = 0.9\n# Momentum dampening.\n_C.SOLVER.DAMPENING = 0.0\n# Nesterov momentum.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.LRS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.LRS = []\n# Maximal number of epochs.\n_C.SOLVER.MAX_EPOCH = 300\n# Momentum.\n_C.SOLVER.MOMENTUM = 0.9\n# Momentum dampening.\n_C.SOLVER.DAMPENING = 0.0\n# Nesterov momentum.\n_C.SOLVER.NESTEROV = True\n# L2 regularization.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.MAX_EPOCH",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.MAX_EPOCH = 300\n# Momentum.\n_C.SOLVER.MOMENTUM = 0.9\n# Momentum dampening.\n_C.SOLVER.DAMPENING = 0.0\n# Nesterov momentum.\n_C.SOLVER.NESTEROV = True\n# L2 regularization.\n_C.SOLVER.WEIGHT_DECAY = 1e-4\n# Start the warm up from SOLVER.BASE_LR * SOLVER.WARMUP_FACTOR.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.MOMENTUM",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.MOMENTUM = 0.9\n# Momentum dampening.\n_C.SOLVER.DAMPENING = 0.0\n# Nesterov momentum.\n_C.SOLVER.NESTEROV = True\n# L2 regularization.\n_C.SOLVER.WEIGHT_DECAY = 1e-4\n# Start the warm up from SOLVER.BASE_LR * SOLVER.WARMUP_FACTOR.\n_C.SOLVER.WARMUP_FACTOR = 0.1\n# Gradually warm up the SOLVER.BASE_LR over this number of epochs.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.DAMPENING",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.DAMPENING = 0.0\n# Nesterov momentum.\n_C.SOLVER.NESTEROV = True\n# L2 regularization.\n_C.SOLVER.WEIGHT_DECAY = 1e-4\n# Start the warm up from SOLVER.BASE_LR * SOLVER.WARMUP_FACTOR.\n_C.SOLVER.WARMUP_FACTOR = 0.1\n# Gradually warm up the SOLVER.BASE_LR over this number of epochs.\n_C.SOLVER.WARMUP_EPOCHS = 0.0\n# The start learning rate of the warm up.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.NESTEROV",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.NESTEROV = True\n# L2 regularization.\n_C.SOLVER.WEIGHT_DECAY = 1e-4\n# Start the warm up from SOLVER.BASE_LR * SOLVER.WARMUP_FACTOR.\n_C.SOLVER.WARMUP_FACTOR = 0.1\n# Gradually warm up the SOLVER.BASE_LR over this number of epochs.\n_C.SOLVER.WARMUP_EPOCHS = 0.0\n# The start learning rate of the warm up.\n_C.SOLVER.WARMUP_START_LR = 0.01\n# Optimization method.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.WEIGHT_DECAY",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.WEIGHT_DECAY = 1e-4\n# Start the warm up from SOLVER.BASE_LR * SOLVER.WARMUP_FACTOR.\n_C.SOLVER.WARMUP_FACTOR = 0.1\n# Gradually warm up the SOLVER.BASE_LR over this number of epochs.\n_C.SOLVER.WARMUP_EPOCHS = 0.0\n# The start learning rate of the warm up.\n_C.SOLVER.WARMUP_START_LR = 0.01\n# Optimization method.\n_C.SOLVER.OPTIMIZING_METHOD = \"sgd\"\n# Base learning rate is linearly scaled with NUM_SHARDS.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.WARMUP_FACTOR",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.WARMUP_FACTOR = 0.1\n# Gradually warm up the SOLVER.BASE_LR over this number of epochs.\n_C.SOLVER.WARMUP_EPOCHS = 0.0\n# The start learning rate of the warm up.\n_C.SOLVER.WARMUP_START_LR = 0.01\n# Optimization method.\n_C.SOLVER.OPTIMIZING_METHOD = \"sgd\"\n# Base learning rate is linearly scaled with NUM_SHARDS.\n_C.SOLVER.BASE_LR_SCALE_NUM_SHARDS = False\n# If True, start from the peak cosine learning rate after warm up.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.WARMUP_EPOCHS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.WARMUP_EPOCHS = 0.0\n# The start learning rate of the warm up.\n_C.SOLVER.WARMUP_START_LR = 0.01\n# Optimization method.\n_C.SOLVER.OPTIMIZING_METHOD = \"sgd\"\n# Base learning rate is linearly scaled with NUM_SHARDS.\n_C.SOLVER.BASE_LR_SCALE_NUM_SHARDS = False\n# If True, start from the peak cosine learning rate after warm up.\n_C.SOLVER.COSINE_AFTER_WARMUP = False\n# If True, perform no weight decay on parameter with one dimension (bias term, etc).",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.WARMUP_START_LR",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.WARMUP_START_LR = 0.01\n# Optimization method.\n_C.SOLVER.OPTIMIZING_METHOD = \"sgd\"\n# Base learning rate is linearly scaled with NUM_SHARDS.\n_C.SOLVER.BASE_LR_SCALE_NUM_SHARDS = False\n# If True, start from the peak cosine learning rate after warm up.\n_C.SOLVER.COSINE_AFTER_WARMUP = False\n# If True, perform no weight decay on parameter with one dimension (bias term, etc).\n_C.SOLVER.ZERO_WD_1D_PARAM = False\n# Clip gradient at this value before optimizer update",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.OPTIMIZING_METHOD",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.OPTIMIZING_METHOD = \"sgd\"\n# Base learning rate is linearly scaled with NUM_SHARDS.\n_C.SOLVER.BASE_LR_SCALE_NUM_SHARDS = False\n# If True, start from the peak cosine learning rate after warm up.\n_C.SOLVER.COSINE_AFTER_WARMUP = False\n# If True, perform no weight decay on parameter with one dimension (bias term, etc).\n_C.SOLVER.ZERO_WD_1D_PARAM = False\n# Clip gradient at this value before optimizer update\n_C.SOLVER.CLIP_GRAD_VAL = None\n# Clip gradient at this norm before optimizer update",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.BASE_LR_SCALE_NUM_SHARDS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.BASE_LR_SCALE_NUM_SHARDS = False\n# If True, start from the peak cosine learning rate after warm up.\n_C.SOLVER.COSINE_AFTER_WARMUP = False\n# If True, perform no weight decay on parameter with one dimension (bias term, etc).\n_C.SOLVER.ZERO_WD_1D_PARAM = False\n# Clip gradient at this value before optimizer update\n_C.SOLVER.CLIP_GRAD_VAL = None\n# Clip gradient at this norm before optimizer update\n_C.SOLVER.CLIP_GRAD_L2NORM = None\n_C.SOLVER.CLIP_GRADIENT = 20",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.COSINE_AFTER_WARMUP",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.COSINE_AFTER_WARMUP = False\n# If True, perform no weight decay on parameter with one dimension (bias term, etc).\n_C.SOLVER.ZERO_WD_1D_PARAM = False\n# Clip gradient at this value before optimizer update\n_C.SOLVER.CLIP_GRAD_VAL = None\n# Clip gradient at this norm before optimizer update\n_C.SOLVER.CLIP_GRAD_L2NORM = None\n_C.SOLVER.CLIP_GRADIENT = 20\n# ---------------------------------------------------------------------------- #\n# Misc options",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.ZERO_WD_1D_PARAM",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.ZERO_WD_1D_PARAM = False\n# Clip gradient at this value before optimizer update\n_C.SOLVER.CLIP_GRAD_VAL = None\n# Clip gradient at this norm before optimizer update\n_C.SOLVER.CLIP_GRAD_L2NORM = None\n_C.SOLVER.CLIP_GRADIENT = 20\n# ---------------------------------------------------------------------------- #\n# Misc options\n# ---------------------------------------------------------------------------- #\n# Number of GPUs to use (applies to both training and testing).",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.CLIP_GRAD_VAL",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.CLIP_GRAD_VAL = None\n# Clip gradient at this norm before optimizer update\n_C.SOLVER.CLIP_GRAD_L2NORM = None\n_C.SOLVER.CLIP_GRADIENT = 20\n# ---------------------------------------------------------------------------- #\n# Misc options\n# ---------------------------------------------------------------------------- #\n# Number of GPUs to use (applies to both training and testing).\n_C.NUM_GPUS = 1\n# Number of machine to use for the job.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.CLIP_GRAD_L2NORM",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.CLIP_GRAD_L2NORM = None\n_C.SOLVER.CLIP_GRADIENT = 20\n# ---------------------------------------------------------------------------- #\n# Misc options\n# ---------------------------------------------------------------------------- #\n# Number of GPUs to use (applies to both training and testing).\n_C.NUM_GPUS = 1\n# Number of machine to use for the job.\n_C.NUM_SHARDS = 1\n# The index of the current machine.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.CLIP_GRADIENT",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.CLIP_GRADIENT = 20\n# ---------------------------------------------------------------------------- #\n# Misc options\n# ---------------------------------------------------------------------------- #\n# Number of GPUs to use (applies to both training and testing).\n_C.NUM_GPUS = 1\n# Number of machine to use for the job.\n_C.NUM_SHARDS = 1\n# The index of the current machine.\n_C.SHARD_ID = 0",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.NUM_GPUS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.NUM_GPUS = 1\n# Number of machine to use for the job.\n_C.NUM_SHARDS = 1\n# The index of the current machine.\n_C.SHARD_ID = 0\n# Output basedir.\n_C.OUTPUT_DIR = \"./tmp\"\n# Note that non-determinism may still be present due to non-deterministic\n# operator implementations in GPU operator libraries.\n_C.RNG_SEED = 1",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.NUM_SHARDS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.NUM_SHARDS = 1\n# The index of the current machine.\n_C.SHARD_ID = 0\n# Output basedir.\n_C.OUTPUT_DIR = \"./tmp\"\n# Note that non-determinism may still be present due to non-deterministic\n# operator implementations in GPU operator libraries.\n_C.RNG_SEED = 1\n# Log period in iters.\n_C.LOG_PERIOD = 10",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SHARD_ID",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.SHARD_ID = 0\n# Output basedir.\n_C.OUTPUT_DIR = \"./tmp\"\n# Note that non-determinism may still be present due to non-deterministic\n# operator implementations in GPU operator libraries.\n_C.RNG_SEED = 1\n# Log period in iters.\n_C.LOG_PERIOD = 10\n# If True, log the model info.\n_C.LOG_MODEL_INFO = True",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.OUTPUT_DIR",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.OUTPUT_DIR = \"./tmp\"\n# Note that non-determinism may still be present due to non-deterministic\n# operator implementations in GPU operator libraries.\n_C.RNG_SEED = 1\n# Log period in iters.\n_C.LOG_PERIOD = 10\n# If True, log the model info.\n_C.LOG_MODEL_INFO = True\n# Distributed backend.\n_C.DIST_BACKEND = \"nccl\"",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.RNG_SEED",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.RNG_SEED = 1\n# Log period in iters.\n_C.LOG_PERIOD = 10\n# If True, log the model info.\n_C.LOG_MODEL_INFO = True\n# Distributed backend.\n_C.DIST_BACKEND = \"nccl\"\n# ---------------------------------------------------------------------------- #\n# Benchmark options\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.LOG_PERIOD",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.LOG_PERIOD = 10\n# If True, log the model info.\n_C.LOG_MODEL_INFO = True\n# Distributed backend.\n_C.DIST_BACKEND = \"nccl\"\n# ---------------------------------------------------------------------------- #\n# Benchmark options\n# ---------------------------------------------------------------------------- #\n_C.BENCHMARK = CfgNode()\n# Number of epochs for data loading benchmark.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.LOG_MODEL_INFO",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.LOG_MODEL_INFO = True\n# Distributed backend.\n_C.DIST_BACKEND = \"nccl\"\n# ---------------------------------------------------------------------------- #\n# Benchmark options\n# ---------------------------------------------------------------------------- #\n_C.BENCHMARK = CfgNode()\n# Number of epochs for data loading benchmark.\n_C.BENCHMARK.NUM_EPOCHS = 5\n# Log period in iters for data loading benchmark.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DIST_BACKEND",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DIST_BACKEND = \"nccl\"\n# ---------------------------------------------------------------------------- #\n# Benchmark options\n# ---------------------------------------------------------------------------- #\n_C.BENCHMARK = CfgNode()\n# Number of epochs for data loading benchmark.\n_C.BENCHMARK.NUM_EPOCHS = 5\n# Log period in iters for data loading benchmark.\n_C.BENCHMARK.LOG_PERIOD = 100\n# If True, shuffle dataloader for epoch during benchmark.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.BENCHMARK",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.BENCHMARK = CfgNode()\n# Number of epochs for data loading benchmark.\n_C.BENCHMARK.NUM_EPOCHS = 5\n# Log period in iters for data loading benchmark.\n_C.BENCHMARK.LOG_PERIOD = 100\n# If True, shuffle dataloader for epoch during benchmark.\n_C.BENCHMARK.SHUFFLE = False\n# ---------------------------------------------------------------------------- #\n# Common train/test data loader options\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.BENCHMARK.NUM_EPOCHS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.BENCHMARK.NUM_EPOCHS = 5\n# Log period in iters for data loading benchmark.\n_C.BENCHMARK.LOG_PERIOD = 100\n# If True, shuffle dataloader for epoch during benchmark.\n_C.BENCHMARK.SHUFFLE = False\n# ---------------------------------------------------------------------------- #\n# Common train/test data loader options\n# ---------------------------------------------------------------------------- #\n_C.DATA_LOADER = CfgNode()\n# Number of data loader workers per training process.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.BENCHMARK.LOG_PERIOD",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.BENCHMARK.LOG_PERIOD = 100\n# If True, shuffle dataloader for epoch during benchmark.\n_C.BENCHMARK.SHUFFLE = False\n# ---------------------------------------------------------------------------- #\n# Common train/test data loader options\n# ---------------------------------------------------------------------------- #\n_C.DATA_LOADER = CfgNode()\n# Number of data loader workers per training process.\n_C.DATA_LOADER.NUM_WORKERS = 8\n# Load data to pinned host memory.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.BENCHMARK.SHUFFLE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.BENCHMARK.SHUFFLE = False\n# ---------------------------------------------------------------------------- #\n# Common train/test data loader options\n# ---------------------------------------------------------------------------- #\n_C.DATA_LOADER = CfgNode()\n# Number of data loader workers per training process.\n_C.DATA_LOADER.NUM_WORKERS = 8\n# Load data to pinned host memory.\n_C.DATA_LOADER.PIN_MEMORY = True\n# Enable multi thread decoding.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA_LOADER",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA_LOADER = CfgNode()\n# Number of data loader workers per training process.\n_C.DATA_LOADER.NUM_WORKERS = 8\n# Load data to pinned host memory.\n_C.DATA_LOADER.PIN_MEMORY = True\n# Enable multi thread decoding.\n_C.DATA_LOADER.ENABLE_MULTI_THREAD_DECODE = False\n# ---------------------------------------------------------------------------- #\n# Detection options.\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA_LOADER.NUM_WORKERS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA_LOADER.NUM_WORKERS = 8\n# Load data to pinned host memory.\n_C.DATA_LOADER.PIN_MEMORY = True\n# Enable multi thread decoding.\n_C.DATA_LOADER.ENABLE_MULTI_THREAD_DECODE = False\n# ---------------------------------------------------------------------------- #\n# Detection options.\n# ---------------------------------------------------------------------------- #\n_C.DETECTION = CfgNode()\n# Whether enable video detection.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA_LOADER.PIN_MEMORY",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA_LOADER.PIN_MEMORY = True\n# Enable multi thread decoding.\n_C.DATA_LOADER.ENABLE_MULTI_THREAD_DECODE = False\n# ---------------------------------------------------------------------------- #\n# Detection options.\n# ---------------------------------------------------------------------------- #\n_C.DETECTION = CfgNode()\n# Whether enable video detection.\n_C.DETECTION.ENABLE = False\n# Aligned version of RoI. More details can be found at slowfast/models/head_helper.py",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA_LOADER.ENABLE_MULTI_THREAD_DECODE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DATA_LOADER.ENABLE_MULTI_THREAD_DECODE = False\n# ---------------------------------------------------------------------------- #\n# Detection options.\n# ---------------------------------------------------------------------------- #\n_C.DETECTION = CfgNode()\n# Whether enable video detection.\n_C.DETECTION.ENABLE = False\n# Aligned version of RoI. More details can be found at slowfast/models/head_helper.py\n_C.DETECTION.ALIGNED = True\n# Spatial scale factor.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DETECTION",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DETECTION = CfgNode()\n# Whether enable video detection.\n_C.DETECTION.ENABLE = False\n# Aligned version of RoI. More details can be found at slowfast/models/head_helper.py\n_C.DETECTION.ALIGNED = True\n# Spatial scale factor.\n_C.DETECTION.SPATIAL_SCALE_FACTOR = 16\n# RoI tranformation resolution.\n_C.DETECTION.ROI_XFORM_RESOLUTION = 7\n# -----------------------------------------------------------------------------",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DETECTION.ENABLE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DETECTION.ENABLE = False\n# Aligned version of RoI. More details can be found at slowfast/models/head_helper.py\n_C.DETECTION.ALIGNED = True\n# Spatial scale factor.\n_C.DETECTION.SPATIAL_SCALE_FACTOR = 16\n# RoI tranformation resolution.\n_C.DETECTION.ROI_XFORM_RESOLUTION = 7\n# -----------------------------------------------------------------------------\n# AVA Dataset options\n# -----------------------------------------------------------------------------",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DETECTION.ALIGNED",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DETECTION.ALIGNED = True\n# Spatial scale factor.\n_C.DETECTION.SPATIAL_SCALE_FACTOR = 16\n# RoI tranformation resolution.\n_C.DETECTION.ROI_XFORM_RESOLUTION = 7\n# -----------------------------------------------------------------------------\n# AVA Dataset options\n# -----------------------------------------------------------------------------\n_C.AVA = CfgNode()\n# Directory path of frames.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DETECTION.SPATIAL_SCALE_FACTOR",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DETECTION.SPATIAL_SCALE_FACTOR = 16\n# RoI tranformation resolution.\n_C.DETECTION.ROI_XFORM_RESOLUTION = 7\n# -----------------------------------------------------------------------------\n# AVA Dataset options\n# -----------------------------------------------------------------------------\n_C.AVA = CfgNode()\n# Directory path of frames.\n_C.AVA.FRAME_DIR = \"/mnt/fair-flash3-east/ava_trainval_frames.img/\"\n# Directory path for files of frame lists.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DETECTION.ROI_XFORM_RESOLUTION",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DETECTION.ROI_XFORM_RESOLUTION = 7\n# -----------------------------------------------------------------------------\n# AVA Dataset options\n# -----------------------------------------------------------------------------\n_C.AVA = CfgNode()\n# Directory path of frames.\n_C.AVA.FRAME_DIR = \"/mnt/fair-flash3-east/ava_trainval_frames.img/\"\n# Directory path for files of frame lists.\n_C.AVA.FRAME_LIST_DIR = (\n    \"/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/\"",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AVA = CfgNode()\n# Directory path of frames.\n_C.AVA.FRAME_DIR = \"/mnt/fair-flash3-east/ava_trainval_frames.img/\"\n# Directory path for files of frame lists.\n_C.AVA.FRAME_LIST_DIR = (\n    \"/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/\"\n)\n# Directory path for annotation files.\n_C.AVA.ANNOTATION_DIR = (\n    \"/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/\"",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.FRAME_DIR",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.FRAME_DIR = \"/mnt/fair-flash3-east/ava_trainval_frames.img/\"\n# Directory path for files of frame lists.\n_C.AVA.FRAME_LIST_DIR = (\n    \"/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/\"\n)\n# Directory path for annotation files.\n_C.AVA.ANNOTATION_DIR = (\n    \"/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/\"\n)\n# Filenames of training samples list files.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.FRAME_LIST_DIR",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.FRAME_LIST_DIR = (\n    \"/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/\"\n)\n# Directory path for annotation files.\n_C.AVA.ANNOTATION_DIR = (\n    \"/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/\"\n)\n# Filenames of training samples list files.\n_C.AVA.TRAIN_LISTS = [\"train.csv\"]\n# Filenames of test samples list files.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.ANNOTATION_DIR",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.ANNOTATION_DIR = (\n    \"/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/\"\n)\n# Filenames of training samples list files.\n_C.AVA.TRAIN_LISTS = [\"train.csv\"]\n# Filenames of test samples list files.\n_C.AVA.TEST_LISTS = [\"val.csv\"]\n# Filenames of box list files for training. Note that we assume files which\n# contains predicted boxes will have a suffix \"predicted_boxes\" in the\n# filename.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.TRAIN_LISTS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.TRAIN_LISTS = [\"train.csv\"]\n# Filenames of test samples list files.\n_C.AVA.TEST_LISTS = [\"val.csv\"]\n# Filenames of box list files for training. Note that we assume files which\n# contains predicted boxes will have a suffix \"predicted_boxes\" in the\n# filename.\n_C.AVA.TRAIN_GT_BOX_LISTS = [\"ava_train_v2.2.csv\"]\n_C.AVA.TRAIN_PREDICT_BOX_LISTS = []\n# Filenames of box list files for test.\n_C.AVA.TEST_PREDICT_BOX_LISTS = [\"ava_val_predicted_boxes.csv\"]",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.TEST_LISTS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.TEST_LISTS = [\"val.csv\"]\n# Filenames of box list files for training. Note that we assume files which\n# contains predicted boxes will have a suffix \"predicted_boxes\" in the\n# filename.\n_C.AVA.TRAIN_GT_BOX_LISTS = [\"ava_train_v2.2.csv\"]\n_C.AVA.TRAIN_PREDICT_BOX_LISTS = []\n# Filenames of box list files for test.\n_C.AVA.TEST_PREDICT_BOX_LISTS = [\"ava_val_predicted_boxes.csv\"]\n# This option controls the score threshold for the predicted boxes to use.\n_C.AVA.DETECTION_SCORE_THRESH = 0.9",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.TRAIN_GT_BOX_LISTS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.TRAIN_GT_BOX_LISTS = [\"ava_train_v2.2.csv\"]\n_C.AVA.TRAIN_PREDICT_BOX_LISTS = []\n# Filenames of box list files for test.\n_C.AVA.TEST_PREDICT_BOX_LISTS = [\"ava_val_predicted_boxes.csv\"]\n# This option controls the score threshold for the predicted boxes to use.\n_C.AVA.DETECTION_SCORE_THRESH = 0.9\n# If use BGR as the format of input frames.\n_C.AVA.BGR = False\n# Training augmentation parameters\n# Whether to use color augmentation method.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.TRAIN_PREDICT_BOX_LISTS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.TRAIN_PREDICT_BOX_LISTS = []\n# Filenames of box list files for test.\n_C.AVA.TEST_PREDICT_BOX_LISTS = [\"ava_val_predicted_boxes.csv\"]\n# This option controls the score threshold for the predicted boxes to use.\n_C.AVA.DETECTION_SCORE_THRESH = 0.9\n# If use BGR as the format of input frames.\n_C.AVA.BGR = False\n# Training augmentation parameters\n# Whether to use color augmentation method.\n_C.AVA.TRAIN_USE_COLOR_AUGMENTATION = False",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.TEST_PREDICT_BOX_LISTS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.TEST_PREDICT_BOX_LISTS = [\"ava_val_predicted_boxes.csv\"]\n# This option controls the score threshold for the predicted boxes to use.\n_C.AVA.DETECTION_SCORE_THRESH = 0.9\n# If use BGR as the format of input frames.\n_C.AVA.BGR = False\n# Training augmentation parameters\n# Whether to use color augmentation method.\n_C.AVA.TRAIN_USE_COLOR_AUGMENTATION = False\n# Whether to only use PCA jitter augmentation when using color augmentation\n# method (otherwise combine with color jitter method).",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.DETECTION_SCORE_THRESH",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.DETECTION_SCORE_THRESH = 0.9\n# If use BGR as the format of input frames.\n_C.AVA.BGR = False\n# Training augmentation parameters\n# Whether to use color augmentation method.\n_C.AVA.TRAIN_USE_COLOR_AUGMENTATION = False\n# Whether to only use PCA jitter augmentation when using color augmentation\n# method (otherwise combine with color jitter method).\n_C.AVA.TRAIN_PCA_JITTER_ONLY = True\n# Whether to do horizontal flipping during test.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.BGR",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.BGR = False\n# Training augmentation parameters\n# Whether to use color augmentation method.\n_C.AVA.TRAIN_USE_COLOR_AUGMENTATION = False\n# Whether to only use PCA jitter augmentation when using color augmentation\n# method (otherwise combine with color jitter method).\n_C.AVA.TRAIN_PCA_JITTER_ONLY = True\n# Whether to do horizontal flipping during test.\n_C.AVA.TEST_FORCE_FLIP = False\n# Whether to use full test set for validation split.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.TRAIN_USE_COLOR_AUGMENTATION",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.TRAIN_USE_COLOR_AUGMENTATION = False\n# Whether to only use PCA jitter augmentation when using color augmentation\n# method (otherwise combine with color jitter method).\n_C.AVA.TRAIN_PCA_JITTER_ONLY = True\n# Whether to do horizontal flipping during test.\n_C.AVA.TEST_FORCE_FLIP = False\n# Whether to use full test set for validation split.\n_C.AVA.FULL_TEST_ON_VAL = False\n# The name of the file to the ava label map.\n_C.AVA.LABEL_MAP_FILE = \"ava_action_list_v2.2_for_activitynet_2019.pbtxt\"",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.TRAIN_PCA_JITTER_ONLY",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.TRAIN_PCA_JITTER_ONLY = True\n# Whether to do horizontal flipping during test.\n_C.AVA.TEST_FORCE_FLIP = False\n# Whether to use full test set for validation split.\n_C.AVA.FULL_TEST_ON_VAL = False\n# The name of the file to the ava label map.\n_C.AVA.LABEL_MAP_FILE = \"ava_action_list_v2.2_for_activitynet_2019.pbtxt\"\n# The name of the file to the ava exclusion.\n_C.AVA.EXCLUSION_FILE = \"ava_val_excluded_timestamps_v2.2.csv\"\n# The name of the file to the ava groundtruth.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.TEST_FORCE_FLIP",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.TEST_FORCE_FLIP = False\n# Whether to use full test set for validation split.\n_C.AVA.FULL_TEST_ON_VAL = False\n# The name of the file to the ava label map.\n_C.AVA.LABEL_MAP_FILE = \"ava_action_list_v2.2_for_activitynet_2019.pbtxt\"\n# The name of the file to the ava exclusion.\n_C.AVA.EXCLUSION_FILE = \"ava_val_excluded_timestamps_v2.2.csv\"\n# The name of the file to the ava groundtruth.\n_C.AVA.GROUNDTRUTH_FILE = \"ava_val_v2.2.csv\"\n# Backend to process image, includes `pytorch` and `cv2`.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.FULL_TEST_ON_VAL",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.FULL_TEST_ON_VAL = False\n# The name of the file to the ava label map.\n_C.AVA.LABEL_MAP_FILE = \"ava_action_list_v2.2_for_activitynet_2019.pbtxt\"\n# The name of the file to the ava exclusion.\n_C.AVA.EXCLUSION_FILE = \"ava_val_excluded_timestamps_v2.2.csv\"\n# The name of the file to the ava groundtruth.\n_C.AVA.GROUNDTRUTH_FILE = \"ava_val_v2.2.csv\"\n# Backend to process image, includes `pytorch` and `cv2`.\n_C.AVA.IMG_PROC_BACKEND = \"cv2\"\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.LABEL_MAP_FILE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.LABEL_MAP_FILE = \"ava_action_list_v2.2_for_activitynet_2019.pbtxt\"\n# The name of the file to the ava exclusion.\n_C.AVA.EXCLUSION_FILE = \"ava_val_excluded_timestamps_v2.2.csv\"\n# The name of the file to the ava groundtruth.\n_C.AVA.GROUNDTRUTH_FILE = \"ava_val_v2.2.csv\"\n# Backend to process image, includes `pytorch` and `cv2`.\n_C.AVA.IMG_PROC_BACKEND = \"cv2\"\n# ---------------------------------------------------------------------------- #\n# Multigrid training options\n# See https://arxiv.org/abs/1912.00998 for details about multigrid training.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.EXCLUSION_FILE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.EXCLUSION_FILE = \"ava_val_excluded_timestamps_v2.2.csv\"\n# The name of the file to the ava groundtruth.\n_C.AVA.GROUNDTRUTH_FILE = \"ava_val_v2.2.csv\"\n# Backend to process image, includes `pytorch` and `cv2`.\n_C.AVA.IMG_PROC_BACKEND = \"cv2\"\n# ---------------------------------------------------------------------------- #\n# Multigrid training options\n# See https://arxiv.org/abs/1912.00998 for details about multigrid training.\n# ---------------------------------------------------------------------------- #\n_C.MULTIGRID = CfgNode()",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.GROUNDTRUTH_FILE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.GROUNDTRUTH_FILE = \"ava_val_v2.2.csv\"\n# Backend to process image, includes `pytorch` and `cv2`.\n_C.AVA.IMG_PROC_BACKEND = \"cv2\"\n# ---------------------------------------------------------------------------- #\n# Multigrid training options\n# See https://arxiv.org/abs/1912.00998 for details about multigrid training.\n# ---------------------------------------------------------------------------- #\n_C.MULTIGRID = CfgNode()\n# Multigrid training allows us to train for more epochs with fewer iterations.\n# This hyperparameter specifies how many times more epochs to train.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.IMG_PROC_BACKEND",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.IMG_PROC_BACKEND = \"cv2\"\n# ---------------------------------------------------------------------------- #\n# Multigrid training options\n# See https://arxiv.org/abs/1912.00998 for details about multigrid training.\n# ---------------------------------------------------------------------------- #\n_C.MULTIGRID = CfgNode()\n# Multigrid training allows us to train for more epochs with fewer iterations.\n# This hyperparameter specifies how many times more epochs to train.\n# The default setting in paper trains for 1.5x more epochs than baseline.\n_C.MULTIGRID.EPOCH_FACTOR = 1.5",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MULTIGRID",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MULTIGRID = CfgNode()\n# Multigrid training allows us to train for more epochs with fewer iterations.\n# This hyperparameter specifies how many times more epochs to train.\n# The default setting in paper trains for 1.5x more epochs than baseline.\n_C.MULTIGRID.EPOCH_FACTOR = 1.5\n# Enable short cycles.\n_C.MULTIGRID.SHORT_CYCLE = False\n# Short cycle additional spatial dimensions relative to the default crop size.\n_C.MULTIGRID.SHORT_CYCLE_FACTORS = [0.5, 0.5 ** 0.5]\n_C.MULTIGRID.LONG_CYCLE = False",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MULTIGRID.EPOCH_FACTOR",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MULTIGRID.EPOCH_FACTOR = 1.5\n# Enable short cycles.\n_C.MULTIGRID.SHORT_CYCLE = False\n# Short cycle additional spatial dimensions relative to the default crop size.\n_C.MULTIGRID.SHORT_CYCLE_FACTORS = [0.5, 0.5 ** 0.5]\n_C.MULTIGRID.LONG_CYCLE = False\n# (Temporal, Spatial) dimensions relative to the default shape.\n_C.MULTIGRID.LONG_CYCLE_FACTORS = [\n    (0.25, 0.5 ** 0.5),\n    (0.5, 0.5 ** 0.5),",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MULTIGRID.SHORT_CYCLE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MULTIGRID.SHORT_CYCLE = False\n# Short cycle additional spatial dimensions relative to the default crop size.\n_C.MULTIGRID.SHORT_CYCLE_FACTORS = [0.5, 0.5 ** 0.5]\n_C.MULTIGRID.LONG_CYCLE = False\n# (Temporal, Spatial) dimensions relative to the default shape.\n_C.MULTIGRID.LONG_CYCLE_FACTORS = [\n    (0.25, 0.5 ** 0.5),\n    (0.5, 0.5 ** 0.5),\n    (0.5, 1),\n    (1, 1),",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MULTIGRID.SHORT_CYCLE_FACTORS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MULTIGRID.SHORT_CYCLE_FACTORS = [0.5, 0.5 ** 0.5]\n_C.MULTIGRID.LONG_CYCLE = False\n# (Temporal, Spatial) dimensions relative to the default shape.\n_C.MULTIGRID.LONG_CYCLE_FACTORS = [\n    (0.25, 0.5 ** 0.5),\n    (0.5, 0.5 ** 0.5),\n    (0.5, 1),\n    (1, 1),\n]\n# While a standard BN computes stats across all examples in a GPU,",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MULTIGRID.LONG_CYCLE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MULTIGRID.LONG_CYCLE = False\n# (Temporal, Spatial) dimensions relative to the default shape.\n_C.MULTIGRID.LONG_CYCLE_FACTORS = [\n    (0.25, 0.5 ** 0.5),\n    (0.5, 0.5 ** 0.5),\n    (0.5, 1),\n    (1, 1),\n]\n# While a standard BN computes stats across all examples in a GPU,\n# for multigrid training we fix the number of clips to compute BN stats on.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MULTIGRID.LONG_CYCLE_FACTORS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MULTIGRID.LONG_CYCLE_FACTORS = [\n    (0.25, 0.5 ** 0.5),\n    (0.5, 0.5 ** 0.5),\n    (0.5, 1),\n    (1, 1),\n]\n# While a standard BN computes stats across all examples in a GPU,\n# for multigrid training we fix the number of clips to compute BN stats on.\n# See https://arxiv.org/abs/1912.00998 for details.\n_C.MULTIGRID.BN_BASE_SIZE = 8",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MULTIGRID.BN_BASE_SIZE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MULTIGRID.BN_BASE_SIZE = 8\n# Multigrid training epochs are not proportional to actual training time or\n# computations, so _C.TRAIN.EVAL_PERIOD leads to too frequent or rare\n# evaluation. We use a multigrid-specific rule to determine when to evaluate:\n# This hyperparameter defines how many times to evaluate a model per long\n# cycle shape.\n_C.MULTIGRID.EVAL_FREQ = 3\n# No need to specify; Set automatically and used as global variables.\n_C.MULTIGRID.LONG_CYCLE_SAMPLING_RATE = 0\n_C.MULTIGRID.DEFAULT_B = 0",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MULTIGRID.EVAL_FREQ",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MULTIGRID.EVAL_FREQ = 3\n# No need to specify; Set automatically and used as global variables.\n_C.MULTIGRID.LONG_CYCLE_SAMPLING_RATE = 0\n_C.MULTIGRID.DEFAULT_B = 0\n_C.MULTIGRID.DEFAULT_T = 0\n_C.MULTIGRID.DEFAULT_S = 0\n# -----------------------------------------------------------------------------\n# Tensorboard Visualization Options\n# -----------------------------------------------------------------------------\n_C.TENSORBOARD = CfgNode()",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MULTIGRID.LONG_CYCLE_SAMPLING_RATE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MULTIGRID.LONG_CYCLE_SAMPLING_RATE = 0\n_C.MULTIGRID.DEFAULT_B = 0\n_C.MULTIGRID.DEFAULT_T = 0\n_C.MULTIGRID.DEFAULT_S = 0\n# -----------------------------------------------------------------------------\n# Tensorboard Visualization Options\n# -----------------------------------------------------------------------------\n_C.TENSORBOARD = CfgNode()\n# Log to summary writer, this will automatically.\n# log loss, lr and metrics during train/eval.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MULTIGRID.DEFAULT_B",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MULTIGRID.DEFAULT_B = 0\n_C.MULTIGRID.DEFAULT_T = 0\n_C.MULTIGRID.DEFAULT_S = 0\n# -----------------------------------------------------------------------------\n# Tensorboard Visualization Options\n# -----------------------------------------------------------------------------\n_C.TENSORBOARD = CfgNode()\n# Log to summary writer, this will automatically.\n# log loss, lr and metrics during train/eval.\n_C.TENSORBOARD.ENABLE = False",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MULTIGRID.DEFAULT_T",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MULTIGRID.DEFAULT_T = 0\n_C.MULTIGRID.DEFAULT_S = 0\n# -----------------------------------------------------------------------------\n# Tensorboard Visualization Options\n# -----------------------------------------------------------------------------\n_C.TENSORBOARD = CfgNode()\n# Log to summary writer, this will automatically.\n# log loss, lr and metrics during train/eval.\n_C.TENSORBOARD.ENABLE = False\n# Provide path to prediction results for visualization.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MULTIGRID.DEFAULT_S",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.MULTIGRID.DEFAULT_S = 0\n# -----------------------------------------------------------------------------\n# Tensorboard Visualization Options\n# -----------------------------------------------------------------------------\n_C.TENSORBOARD = CfgNode()\n# Log to summary writer, this will automatically.\n# log loss, lr and metrics during train/eval.\n_C.TENSORBOARD.ENABLE = False\n# Provide path to prediction results for visualization.\n# This is a pickle file of [prediction_tensor, label_tensor]",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD = CfgNode()\n# Log to summary writer, this will automatically.\n# log loss, lr and metrics during train/eval.\n_C.TENSORBOARD.ENABLE = False\n# Provide path to prediction results for visualization.\n# This is a pickle file of [prediction_tensor, label_tensor]\n_C.TENSORBOARD.PREDICTIONS_PATH = \"\"\n# Path to directory for tensorboard logs.\n# Default to to cfg.OUTPUT_DIR/runs-{cfg.TRAIN.DATASET}.\n_C.TENSORBOARD.LOG_DIR = \"\"",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.ENABLE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.ENABLE = False\n# Provide path to prediction results for visualization.\n# This is a pickle file of [prediction_tensor, label_tensor]\n_C.TENSORBOARD.PREDICTIONS_PATH = \"\"\n# Path to directory for tensorboard logs.\n# Default to to cfg.OUTPUT_DIR/runs-{cfg.TRAIN.DATASET}.\n_C.TENSORBOARD.LOG_DIR = \"\"\n# Path to a json file providing class_name - id mapping\n# in the format {\"class_name1\": id1, \"class_name2\": id2, ...}.\n# This file must be provided to enable plotting confusion matrix",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.PREDICTIONS_PATH",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.PREDICTIONS_PATH = \"\"\n# Path to directory for tensorboard logs.\n# Default to to cfg.OUTPUT_DIR/runs-{cfg.TRAIN.DATASET}.\n_C.TENSORBOARD.LOG_DIR = \"\"\n# Path to a json file providing class_name - id mapping\n# in the format {\"class_name1\": id1, \"class_name2\": id2, ...}.\n# This file must be provided to enable plotting confusion matrix\n# by a subset or parent categories.\n_C.TENSORBOARD.CLASS_NAMES_PATH = \"\"\n# Path to a json file for categories -> classes mapping",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.LOG_DIR",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.LOG_DIR = \"\"\n# Path to a json file providing class_name - id mapping\n# in the format {\"class_name1\": id1, \"class_name2\": id2, ...}.\n# This file must be provided to enable plotting confusion matrix\n# by a subset or parent categories.\n_C.TENSORBOARD.CLASS_NAMES_PATH = \"\"\n# Path to a json file for categories -> classes mapping\n# in the format {\"parent_class\": [\"child_class1\", \"child_class2\",...], ...}.\n_C.TENSORBOARD.CATEGORIES_PATH = \"\"\n# Config for confusion matrices visualization.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.CLASS_NAMES_PATH",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.CLASS_NAMES_PATH = \"\"\n# Path to a json file for categories -> classes mapping\n# in the format {\"parent_class\": [\"child_class1\", \"child_class2\",...], ...}.\n_C.TENSORBOARD.CATEGORIES_PATH = \"\"\n# Config for confusion matrices visualization.\n_C.TENSORBOARD.CONFUSION_MATRIX = CfgNode()\n# Visualize confusion matrix.\n_C.TENSORBOARD.CONFUSION_MATRIX.ENABLE = False\n# Figure size of the confusion matrices plotted.\n_C.TENSORBOARD.CONFUSION_MATRIX.FIGSIZE = [8, 8]",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.CATEGORIES_PATH",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.CATEGORIES_PATH = \"\"\n# Config for confusion matrices visualization.\n_C.TENSORBOARD.CONFUSION_MATRIX = CfgNode()\n# Visualize confusion matrix.\n_C.TENSORBOARD.CONFUSION_MATRIX.ENABLE = False\n# Figure size of the confusion matrices plotted.\n_C.TENSORBOARD.CONFUSION_MATRIX.FIGSIZE = [8, 8]\n# Path to a subset of categories to visualize.\n# File contains class names separated by newline characters.\n_C.TENSORBOARD.CONFUSION_MATRIX.SUBSET_PATH = \"\"",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.CONFUSION_MATRIX",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.CONFUSION_MATRIX = CfgNode()\n# Visualize confusion matrix.\n_C.TENSORBOARD.CONFUSION_MATRIX.ENABLE = False\n# Figure size of the confusion matrices plotted.\n_C.TENSORBOARD.CONFUSION_MATRIX.FIGSIZE = [8, 8]\n# Path to a subset of categories to visualize.\n# File contains class names separated by newline characters.\n_C.TENSORBOARD.CONFUSION_MATRIX.SUBSET_PATH = \"\"\n# Config for histogram visualization.\n_C.TENSORBOARD.HISTOGRAM = CfgNode()",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.CONFUSION_MATRIX.ENABLE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.CONFUSION_MATRIX.ENABLE = False\n# Figure size of the confusion matrices plotted.\n_C.TENSORBOARD.CONFUSION_MATRIX.FIGSIZE = [8, 8]\n# Path to a subset of categories to visualize.\n# File contains class names separated by newline characters.\n_C.TENSORBOARD.CONFUSION_MATRIX.SUBSET_PATH = \"\"\n# Config for histogram visualization.\n_C.TENSORBOARD.HISTOGRAM = CfgNode()\n# Visualize histograms.\n_C.TENSORBOARD.HISTOGRAM.ENABLE = False",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.CONFUSION_MATRIX.FIGSIZE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.CONFUSION_MATRIX.FIGSIZE = [8, 8]\n# Path to a subset of categories to visualize.\n# File contains class names separated by newline characters.\n_C.TENSORBOARD.CONFUSION_MATRIX.SUBSET_PATH = \"\"\n# Config for histogram visualization.\n_C.TENSORBOARD.HISTOGRAM = CfgNode()\n# Visualize histograms.\n_C.TENSORBOARD.HISTOGRAM.ENABLE = False\n# Path to a subset of classes to plot histograms.\n# Class names must be separated by newline characters.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.CONFUSION_MATRIX.SUBSET_PATH",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.CONFUSION_MATRIX.SUBSET_PATH = \"\"\n# Config for histogram visualization.\n_C.TENSORBOARD.HISTOGRAM = CfgNode()\n# Visualize histograms.\n_C.TENSORBOARD.HISTOGRAM.ENABLE = False\n# Path to a subset of classes to plot histograms.\n# Class names must be separated by newline characters.\n_C.TENSORBOARD.HISTOGRAM.SUBSET_PATH = \"\"\n# Visualize top-k most predicted classes on histograms for each\n# chosen true label.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.HISTOGRAM",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.HISTOGRAM = CfgNode()\n# Visualize histograms.\n_C.TENSORBOARD.HISTOGRAM.ENABLE = False\n# Path to a subset of classes to plot histograms.\n# Class names must be separated by newline characters.\n_C.TENSORBOARD.HISTOGRAM.SUBSET_PATH = \"\"\n# Visualize top-k most predicted classes on histograms for each\n# chosen true label.\n_C.TENSORBOARD.HISTOGRAM.TOPK = 10\n# Figure size of the histograms plotted.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.HISTOGRAM.ENABLE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.HISTOGRAM.ENABLE = False\n# Path to a subset of classes to plot histograms.\n# Class names must be separated by newline characters.\n_C.TENSORBOARD.HISTOGRAM.SUBSET_PATH = \"\"\n# Visualize top-k most predicted classes on histograms for each\n# chosen true label.\n_C.TENSORBOARD.HISTOGRAM.TOPK = 10\n# Figure size of the histograms plotted.\n_C.TENSORBOARD.HISTOGRAM.FIGSIZE = [8, 8]\n# Config for layers' weights and activations visualization.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.HISTOGRAM.SUBSET_PATH",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.HISTOGRAM.SUBSET_PATH = \"\"\n# Visualize top-k most predicted classes on histograms for each\n# chosen true label.\n_C.TENSORBOARD.HISTOGRAM.TOPK = 10\n# Figure size of the histograms plotted.\n_C.TENSORBOARD.HISTOGRAM.FIGSIZE = [8, 8]\n# Config for layers' weights and activations visualization.\n# _C.TENSORBOARD.ENABLE must be True.\n_C.TENSORBOARD.MODEL_VIS = CfgNode()\n# If False, skip model visualization.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.HISTOGRAM.TOPK",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.HISTOGRAM.TOPK = 10\n# Figure size of the histograms plotted.\n_C.TENSORBOARD.HISTOGRAM.FIGSIZE = [8, 8]\n# Config for layers' weights and activations visualization.\n# _C.TENSORBOARD.ENABLE must be True.\n_C.TENSORBOARD.MODEL_VIS = CfgNode()\n# If False, skip model visualization.\n_C.TENSORBOARD.MODEL_VIS.ENABLE = False\n# If False, skip visualizing model weights.\n_C.TENSORBOARD.MODEL_VIS.MODEL_WEIGHTS = False",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.HISTOGRAM.FIGSIZE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.HISTOGRAM.FIGSIZE = [8, 8]\n# Config for layers' weights and activations visualization.\n# _C.TENSORBOARD.ENABLE must be True.\n_C.TENSORBOARD.MODEL_VIS = CfgNode()\n# If False, skip model visualization.\n_C.TENSORBOARD.MODEL_VIS.ENABLE = False\n# If False, skip visualizing model weights.\n_C.TENSORBOARD.MODEL_VIS.MODEL_WEIGHTS = False\n# If False, skip visualizing model activations.\n_C.TENSORBOARD.MODEL_VIS.ACTIVATIONS = False",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.MODEL_VIS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.MODEL_VIS = CfgNode()\n# If False, skip model visualization.\n_C.TENSORBOARD.MODEL_VIS.ENABLE = False\n# If False, skip visualizing model weights.\n_C.TENSORBOARD.MODEL_VIS.MODEL_WEIGHTS = False\n# If False, skip visualizing model activations.\n_C.TENSORBOARD.MODEL_VIS.ACTIVATIONS = False\n# If False, skip visualizing input videos.\n_C.TENSORBOARD.MODEL_VIS.INPUT_VIDEO = False\n# List of strings containing data about layer names and their indexing to",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.MODEL_VIS.ENABLE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.MODEL_VIS.ENABLE = False\n# If False, skip visualizing model weights.\n_C.TENSORBOARD.MODEL_VIS.MODEL_WEIGHTS = False\n# If False, skip visualizing model activations.\n_C.TENSORBOARD.MODEL_VIS.ACTIVATIONS = False\n# If False, skip visualizing input videos.\n_C.TENSORBOARD.MODEL_VIS.INPUT_VIDEO = False\n# List of strings containing data about layer names and their indexing to\n# visualize weights and activations for. The indexing is meant for\n# choosing a subset of activations outputed by a layer for visualization.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.MODEL_VIS.MODEL_WEIGHTS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.MODEL_VIS.MODEL_WEIGHTS = False\n# If False, skip visualizing model activations.\n_C.TENSORBOARD.MODEL_VIS.ACTIVATIONS = False\n# If False, skip visualizing input videos.\n_C.TENSORBOARD.MODEL_VIS.INPUT_VIDEO = False\n# List of strings containing data about layer names and their indexing to\n# visualize weights and activations for. The indexing is meant for\n# choosing a subset of activations outputed by a layer for visualization.\n# If indexing is not specified, visualize all activations outputed by the layer.\n# For each string, layer name and indexing is separated by whitespaces.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.MODEL_VIS.ACTIVATIONS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.MODEL_VIS.ACTIVATIONS = False\n# If False, skip visualizing input videos.\n_C.TENSORBOARD.MODEL_VIS.INPUT_VIDEO = False\n# List of strings containing data about layer names and their indexing to\n# visualize weights and activations for. The indexing is meant for\n# choosing a subset of activations outputed by a layer for visualization.\n# If indexing is not specified, visualize all activations outputed by the layer.\n# For each string, layer name and indexing is separated by whitespaces.\n# e.g.: [layer1 1,2;1,2, layer2, layer3 150,151;3,4]; this means for each array `arr`\n# along the batch dimension in `layer1`, we take arr[[1, 2], [1, 2]]",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.MODEL_VIS.INPUT_VIDEO",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.MODEL_VIS.INPUT_VIDEO = False\n# List of strings containing data about layer names and their indexing to\n# visualize weights and activations for. The indexing is meant for\n# choosing a subset of activations outputed by a layer for visualization.\n# If indexing is not specified, visualize all activations outputed by the layer.\n# For each string, layer name and indexing is separated by whitespaces.\n# e.g.: [layer1 1,2;1,2, layer2, layer3 150,151;3,4]; this means for each array `arr`\n# along the batch dimension in `layer1`, we take arr[[1, 2], [1, 2]]\n_C.TENSORBOARD.MODEL_VIS.LAYER_LIST = []\n# Top-k predictions to plot on videos",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.MODEL_VIS.LAYER_LIST",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.MODEL_VIS.LAYER_LIST = []\n# Top-k predictions to plot on videos\n_C.TENSORBOARD.MODEL_VIS.TOPK_PREDS = 1\n# Colormap to for text boxes and bounding boxes colors\n_C.TENSORBOARD.MODEL_VIS.COLORMAP = \"Pastel2\"\n# Config for visualization video inputs with Grad-CAM.\n# _C.TENSORBOARD.ENABLE must be True.\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM = CfgNode()\n# Whether to run visualization using Grad-CAM technique.\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.ENABLE = True",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.MODEL_VIS.TOPK_PREDS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.MODEL_VIS.TOPK_PREDS = 1\n# Colormap to for text boxes and bounding boxes colors\n_C.TENSORBOARD.MODEL_VIS.COLORMAP = \"Pastel2\"\n# Config for visualization video inputs with Grad-CAM.\n# _C.TENSORBOARD.ENABLE must be True.\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM = CfgNode()\n# Whether to run visualization using Grad-CAM technique.\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.ENABLE = True\n# CNN layers to use for Grad-CAM. The number of layers must be equal to\n# number of pathway(s).",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.MODEL_VIS.COLORMAP",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.MODEL_VIS.COLORMAP = \"Pastel2\"\n# Config for visualization video inputs with Grad-CAM.\n# _C.TENSORBOARD.ENABLE must be True.\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM = CfgNode()\n# Whether to run visualization using Grad-CAM technique.\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.ENABLE = True\n# CNN layers to use for Grad-CAM. The number of layers must be equal to\n# number of pathway(s).\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.LAYER_LIST = []\n# If True, visualize Grad-CAM using true labels for each instances.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.MODEL_VIS.GRAD_CAM",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.MODEL_VIS.GRAD_CAM = CfgNode()\n# Whether to run visualization using Grad-CAM technique.\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.ENABLE = True\n# CNN layers to use for Grad-CAM. The number of layers must be equal to\n# number of pathway(s).\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.LAYER_LIST = []\n# If True, visualize Grad-CAM using true labels for each instances.\n# If False, use the highest predicted class.\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.USE_TRUE_LABEL = False\n# Colormap to for text boxes and bounding boxes colors",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.ENABLE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.ENABLE = True\n# CNN layers to use for Grad-CAM. The number of layers must be equal to\n# number of pathway(s).\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.LAYER_LIST = []\n# If True, visualize Grad-CAM using true labels for each instances.\n# If False, use the highest predicted class.\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.USE_TRUE_LABEL = False\n# Colormap to for text boxes and bounding boxes colors\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.COLORMAP = \"viridis\"\n# Config for visualization for wrong prediction visualization.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.LAYER_LIST",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.LAYER_LIST = []\n# If True, visualize Grad-CAM using true labels for each instances.\n# If False, use the highest predicted class.\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.USE_TRUE_LABEL = False\n# Colormap to for text boxes and bounding boxes colors\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.COLORMAP = \"viridis\"\n# Config for visualization for wrong prediction visualization.\n# _C.TENSORBOARD.ENABLE must be True.\n_C.TENSORBOARD.WRONG_PRED_VIS = CfgNode()\n_C.TENSORBOARD.WRONG_PRED_VIS.ENABLE = False",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.USE_TRUE_LABEL",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.USE_TRUE_LABEL = False\n# Colormap to for text boxes and bounding boxes colors\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.COLORMAP = \"viridis\"\n# Config for visualization for wrong prediction visualization.\n# _C.TENSORBOARD.ENABLE must be True.\n_C.TENSORBOARD.WRONG_PRED_VIS = CfgNode()\n_C.TENSORBOARD.WRONG_PRED_VIS.ENABLE = False\n# Folder tag to origanize model eval videos under.\n_C.TENSORBOARD.WRONG_PRED_VIS.TAG = \"Incorrectly classified videos.\"\n# Subset of labels to visualize. Only wrong predictions with true labels",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.COLORMAP",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.COLORMAP = \"viridis\"\n# Config for visualization for wrong prediction visualization.\n# _C.TENSORBOARD.ENABLE must be True.\n_C.TENSORBOARD.WRONG_PRED_VIS = CfgNode()\n_C.TENSORBOARD.WRONG_PRED_VIS.ENABLE = False\n# Folder tag to origanize model eval videos under.\n_C.TENSORBOARD.WRONG_PRED_VIS.TAG = \"Incorrectly classified videos.\"\n# Subset of labels to visualize. Only wrong predictions with true labels\n# within this subset is visualized.\n_C.TENSORBOARD.WRONG_PRED_VIS.SUBSET_PATH = \"\"",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.WRONG_PRED_VIS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.WRONG_PRED_VIS = CfgNode()\n_C.TENSORBOARD.WRONG_PRED_VIS.ENABLE = False\n# Folder tag to origanize model eval videos under.\n_C.TENSORBOARD.WRONG_PRED_VIS.TAG = \"Incorrectly classified videos.\"\n# Subset of labels to visualize. Only wrong predictions with true labels\n# within this subset is visualized.\n_C.TENSORBOARD.WRONG_PRED_VIS.SUBSET_PATH = \"\"\n# ---------------------------------------------------------------------------- #\n# Demo options\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.WRONG_PRED_VIS.ENABLE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.WRONG_PRED_VIS.ENABLE = False\n# Folder tag to origanize model eval videos under.\n_C.TENSORBOARD.WRONG_PRED_VIS.TAG = \"Incorrectly classified videos.\"\n# Subset of labels to visualize. Only wrong predictions with true labels\n# within this subset is visualized.\n_C.TENSORBOARD.WRONG_PRED_VIS.SUBSET_PATH = \"\"\n# ---------------------------------------------------------------------------- #\n# Demo options\n# ---------------------------------------------------------------------------- #\n_C.DEMO = CfgNode()",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.WRONG_PRED_VIS.TAG",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.WRONG_PRED_VIS.TAG = \"Incorrectly classified videos.\"\n# Subset of labels to visualize. Only wrong predictions with true labels\n# within this subset is visualized.\n_C.TENSORBOARD.WRONG_PRED_VIS.SUBSET_PATH = \"\"\n# ---------------------------------------------------------------------------- #\n# Demo options\n# ---------------------------------------------------------------------------- #\n_C.DEMO = CfgNode()\n# Run model in DEMO mode.\n_C.DEMO.ENABLE = False",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.WRONG_PRED_VIS.SUBSET_PATH",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.WRONG_PRED_VIS.SUBSET_PATH = \"\"\n# ---------------------------------------------------------------------------- #\n# Demo options\n# ---------------------------------------------------------------------------- #\n_C.DEMO = CfgNode()\n# Run model in DEMO mode.\n_C.DEMO.ENABLE = False\n# Path to a json file providing class_name - id mapping\n# in the format {\"class_name1\": id1, \"class_name2\": id2, ...}.\n_C.DEMO.LABEL_FILE_PATH = \"\"",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO = CfgNode()\n# Run model in DEMO mode.\n_C.DEMO.ENABLE = False\n# Path to a json file providing class_name - id mapping\n# in the format {\"class_name1\": id1, \"class_name2\": id2, ...}.\n_C.DEMO.LABEL_FILE_PATH = \"\"\n# Specify a camera device as input. This will be prioritized\n# over input video if set.\n# If -1, use input video instead.\n_C.DEMO.WEBCAM = -1",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.ENABLE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.ENABLE = False\n# Path to a json file providing class_name - id mapping\n# in the format {\"class_name1\": id1, \"class_name2\": id2, ...}.\n_C.DEMO.LABEL_FILE_PATH = \"\"\n# Specify a camera device as input. This will be prioritized\n# over input video if set.\n# If -1, use input video instead.\n_C.DEMO.WEBCAM = -1\n# Path to input video for demo.\n_C.DEMO.INPUT_VIDEO = \"\"",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.LABEL_FILE_PATH",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.LABEL_FILE_PATH = \"\"\n# Specify a camera device as input. This will be prioritized\n# over input video if set.\n# If -1, use input video instead.\n_C.DEMO.WEBCAM = -1\n# Path to input video for demo.\n_C.DEMO.INPUT_VIDEO = \"\"\n# Custom width for reading input video data.\n_C.DEMO.DISPLAY_WIDTH = 0\n# Custom height for reading input video data.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.WEBCAM",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.WEBCAM = -1\n# Path to input video for demo.\n_C.DEMO.INPUT_VIDEO = \"\"\n# Custom width for reading input video data.\n_C.DEMO.DISPLAY_WIDTH = 0\n# Custom height for reading input video data.\n_C.DEMO.DISPLAY_HEIGHT = 0\n# Path to Detectron2 object detection model configuration,\n# only used for detection tasks.\n_C.DEMO.DETECTRON2_CFG = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.INPUT_VIDEO",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.INPUT_VIDEO = \"\"\n# Custom width for reading input video data.\n_C.DEMO.DISPLAY_WIDTH = 0\n# Custom height for reading input video data.\n_C.DEMO.DISPLAY_HEIGHT = 0\n# Path to Detectron2 object detection model configuration,\n# only used for detection tasks.\n_C.DEMO.DETECTRON2_CFG = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n# Path to Detectron2 object detection model pre-trained weights.\n_C.DEMO.DETECTRON2_WEIGHTS = \"detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\"",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.DISPLAY_WIDTH",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.DISPLAY_WIDTH = 0\n# Custom height for reading input video data.\n_C.DEMO.DISPLAY_HEIGHT = 0\n# Path to Detectron2 object detection model configuration,\n# only used for detection tasks.\n_C.DEMO.DETECTRON2_CFG = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n# Path to Detectron2 object detection model pre-trained weights.\n_C.DEMO.DETECTRON2_WEIGHTS = \"detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\"\n# Threshold for choosing predicted bounding boxes by Detectron2.\n_C.DEMO.DETECTRON2_THRESH = 0.9",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.DISPLAY_HEIGHT",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.DISPLAY_HEIGHT = 0\n# Path to Detectron2 object detection model configuration,\n# only used for detection tasks.\n_C.DEMO.DETECTRON2_CFG = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n# Path to Detectron2 object detection model pre-trained weights.\n_C.DEMO.DETECTRON2_WEIGHTS = \"detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\"\n# Threshold for choosing predicted bounding boxes by Detectron2.\n_C.DEMO.DETECTRON2_THRESH = 0.9\n# Number of overlapping frames between 2 consecutive clips.\n# Increase this number for more frequent action predictions.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.DETECTRON2_CFG",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.DETECTRON2_CFG = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n# Path to Detectron2 object detection model pre-trained weights.\n_C.DEMO.DETECTRON2_WEIGHTS = \"detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\"\n# Threshold for choosing predicted bounding boxes by Detectron2.\n_C.DEMO.DETECTRON2_THRESH = 0.9\n# Number of overlapping frames between 2 consecutive clips.\n# Increase this number for more frequent action predictions.\n# The number of overlapping frames cannot be larger than\n# half of the sequence length `cfg.DATA.NUM_FRAMES * cfg.DATA.SAMPLING_RATE`\n_C.DEMO.BUFFER_SIZE = 0",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.DETECTRON2_WEIGHTS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.DETECTRON2_WEIGHTS = \"detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\"\n# Threshold for choosing predicted bounding boxes by Detectron2.\n_C.DEMO.DETECTRON2_THRESH = 0.9\n# Number of overlapping frames between 2 consecutive clips.\n# Increase this number for more frequent action predictions.\n# The number of overlapping frames cannot be larger than\n# half of the sequence length `cfg.DATA.NUM_FRAMES * cfg.DATA.SAMPLING_RATE`\n_C.DEMO.BUFFER_SIZE = 0\n# If specified, the visualized outputs will be written this a video file of\n# this path. Otherwise, the visualized outputs will be displayed in a window.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.DETECTRON2_THRESH",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.DETECTRON2_THRESH = 0.9\n# Number of overlapping frames between 2 consecutive clips.\n# Increase this number for more frequent action predictions.\n# The number of overlapping frames cannot be larger than\n# half of the sequence length `cfg.DATA.NUM_FRAMES * cfg.DATA.SAMPLING_RATE`\n_C.DEMO.BUFFER_SIZE = 0\n# If specified, the visualized outputs will be written this a video file of\n# this path. Otherwise, the visualized outputs will be displayed in a window.\n_C.DEMO.OUTPUT_FILE = \"\"\n# Frames per second rate for writing to output video file.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.BUFFER_SIZE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.BUFFER_SIZE = 0\n# If specified, the visualized outputs will be written this a video file of\n# this path. Otherwise, the visualized outputs will be displayed in a window.\n_C.DEMO.OUTPUT_FILE = \"\"\n# Frames per second rate for writing to output video file.\n# If not set (-1), use fps rate from input file.\n_C.DEMO.OUTPUT_FPS = -1\n# Input format from demo video reader (\"RGB\" or \"BGR\").\n_C.DEMO.INPUT_FORMAT = \"BGR\"\n# Draw visualization frames in [keyframe_idx - CLIP_VIS_SIZE, keyframe_idx + CLIP_VIS_SIZE] inclusively.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.OUTPUT_FILE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.OUTPUT_FILE = \"\"\n# Frames per second rate for writing to output video file.\n# If not set (-1), use fps rate from input file.\n_C.DEMO.OUTPUT_FPS = -1\n# Input format from demo video reader (\"RGB\" or \"BGR\").\n_C.DEMO.INPUT_FORMAT = \"BGR\"\n# Draw visualization frames in [keyframe_idx - CLIP_VIS_SIZE, keyframe_idx + CLIP_VIS_SIZE] inclusively.\n_C.DEMO.CLIP_VIS_SIZE = 10\n# Number of processes to run video visualizer.\n_C.DEMO.NUM_VIS_INSTANCES = 2",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.OUTPUT_FPS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.OUTPUT_FPS = -1\n# Input format from demo video reader (\"RGB\" or \"BGR\").\n_C.DEMO.INPUT_FORMAT = \"BGR\"\n# Draw visualization frames in [keyframe_idx - CLIP_VIS_SIZE, keyframe_idx + CLIP_VIS_SIZE] inclusively.\n_C.DEMO.CLIP_VIS_SIZE = 10\n# Number of processes to run video visualizer.\n_C.DEMO.NUM_VIS_INSTANCES = 2\n# Path to pre-computed predicted boxes\n_C.DEMO.PREDS_BOXES = \"\"\n# Whether to run in with multi-threaded video reader.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.INPUT_FORMAT",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.INPUT_FORMAT = \"BGR\"\n# Draw visualization frames in [keyframe_idx - CLIP_VIS_SIZE, keyframe_idx + CLIP_VIS_SIZE] inclusively.\n_C.DEMO.CLIP_VIS_SIZE = 10\n# Number of processes to run video visualizer.\n_C.DEMO.NUM_VIS_INSTANCES = 2\n# Path to pre-computed predicted boxes\n_C.DEMO.PREDS_BOXES = \"\"\n# Whether to run in with multi-threaded video reader.\n_C.DEMO.THREAD_ENABLE = False\n# Take one clip for every `DEMO.NUM_CLIPS_SKIP` + 1 for prediction and visualization.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.CLIP_VIS_SIZE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.CLIP_VIS_SIZE = 10\n# Number of processes to run video visualizer.\n_C.DEMO.NUM_VIS_INSTANCES = 2\n# Path to pre-computed predicted boxes\n_C.DEMO.PREDS_BOXES = \"\"\n# Whether to run in with multi-threaded video reader.\n_C.DEMO.THREAD_ENABLE = False\n# Take one clip for every `DEMO.NUM_CLIPS_SKIP` + 1 for prediction and visualization.\n# This is used for fast demo speed by reducing the prediction/visualiztion frequency.\n# If -1, take the most recent read clip for visualization. This mode is only supported",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.NUM_VIS_INSTANCES",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.NUM_VIS_INSTANCES = 2\n# Path to pre-computed predicted boxes\n_C.DEMO.PREDS_BOXES = \"\"\n# Whether to run in with multi-threaded video reader.\n_C.DEMO.THREAD_ENABLE = False\n# Take one clip for every `DEMO.NUM_CLIPS_SKIP` + 1 for prediction and visualization.\n# This is used for fast demo speed by reducing the prediction/visualiztion frequency.\n# If -1, take the most recent read clip for visualization. This mode is only supported\n# if `DEMO.THREAD_ENABLE` is set to True.\n_C.DEMO.NUM_CLIPS_SKIP = 0",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.PREDS_BOXES",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.PREDS_BOXES = \"\"\n# Whether to run in with multi-threaded video reader.\n_C.DEMO.THREAD_ENABLE = False\n# Take one clip for every `DEMO.NUM_CLIPS_SKIP` + 1 for prediction and visualization.\n# This is used for fast demo speed by reducing the prediction/visualiztion frequency.\n# If -1, take the most recent read clip for visualization. This mode is only supported\n# if `DEMO.THREAD_ENABLE` is set to True.\n_C.DEMO.NUM_CLIPS_SKIP = 0\n# Path to ground-truth boxes and labels (optional)\n_C.DEMO.GT_BOXES = \"\"",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.THREAD_ENABLE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.THREAD_ENABLE = False\n# Take one clip for every `DEMO.NUM_CLIPS_SKIP` + 1 for prediction and visualization.\n# This is used for fast demo speed by reducing the prediction/visualiztion frequency.\n# If -1, take the most recent read clip for visualization. This mode is only supported\n# if `DEMO.THREAD_ENABLE` is set to True.\n_C.DEMO.NUM_CLIPS_SKIP = 0\n# Path to ground-truth boxes and labels (optional)\n_C.DEMO.GT_BOXES = \"\"\n# The starting second of the video w.r.t bounding boxes file.\n_C.DEMO.STARTING_SECOND = 900",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.NUM_CLIPS_SKIP",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.NUM_CLIPS_SKIP = 0\n# Path to ground-truth boxes and labels (optional)\n_C.DEMO.GT_BOXES = \"\"\n# The starting second of the video w.r.t bounding boxes file.\n_C.DEMO.STARTING_SECOND = 900\n# Frames per second of the input video/folder of images.\n_C.DEMO.FPS = 30\n# Visualize with top-k predictions or predictions above certain threshold(s).\n# Option: {\"thres\", \"top-k\"}\n_C.DEMO.VIS_MODE = \"thres\"",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.GT_BOXES",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.GT_BOXES = \"\"\n# The starting second of the video w.r.t bounding boxes file.\n_C.DEMO.STARTING_SECOND = 900\n# Frames per second of the input video/folder of images.\n_C.DEMO.FPS = 30\n# Visualize with top-k predictions or predictions above certain threshold(s).\n# Option: {\"thres\", \"top-k\"}\n_C.DEMO.VIS_MODE = \"thres\"\n# Threshold for common class names.\n_C.DEMO.COMMON_CLASS_THRES = 0.7",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.STARTING_SECOND",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.STARTING_SECOND = 900\n# Frames per second of the input video/folder of images.\n_C.DEMO.FPS = 30\n# Visualize with top-k predictions or predictions above certain threshold(s).\n# Option: {\"thres\", \"top-k\"}\n_C.DEMO.VIS_MODE = \"thres\"\n# Threshold for common class names.\n_C.DEMO.COMMON_CLASS_THRES = 0.7\n# Theshold for uncommon class names. This will not be\n# used if `_C.DEMO.COMMON_CLASS_NAMES` is empty.",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.FPS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.FPS = 30\n# Visualize with top-k predictions or predictions above certain threshold(s).\n# Option: {\"thres\", \"top-k\"}\n_C.DEMO.VIS_MODE = \"thres\"\n# Threshold for common class names.\n_C.DEMO.COMMON_CLASS_THRES = 0.7\n# Theshold for uncommon class names. This will not be\n# used if `_C.DEMO.COMMON_CLASS_NAMES` is empty.\n_C.DEMO.UNCOMMON_CLASS_THRES = 0.3\n# This is chosen based on distribution of examples in",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.VIS_MODE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.VIS_MODE = \"thres\"\n# Threshold for common class names.\n_C.DEMO.COMMON_CLASS_THRES = 0.7\n# Theshold for uncommon class names. This will not be\n# used if `_C.DEMO.COMMON_CLASS_NAMES` is empty.\n_C.DEMO.UNCOMMON_CLASS_THRES = 0.3\n# This is chosen based on distribution of examples in\n# each classes in AVA dataset.\n_C.DEMO.COMMON_CLASS_NAMES = [\n    \"watch (a person)\",",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.COMMON_CLASS_THRES",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.COMMON_CLASS_THRES = 0.7\n# Theshold for uncommon class names. This will not be\n# used if `_C.DEMO.COMMON_CLASS_NAMES` is empty.\n_C.DEMO.UNCOMMON_CLASS_THRES = 0.3\n# This is chosen based on distribution of examples in\n# each classes in AVA dataset.\n_C.DEMO.COMMON_CLASS_NAMES = [\n    \"watch (a person)\",\n    \"talk to (e.g., self, a person, a group)\",\n    \"listen to (a person)\",",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.UNCOMMON_CLASS_THRES",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.UNCOMMON_CLASS_THRES = 0.3\n# This is chosen based on distribution of examples in\n# each classes in AVA dataset.\n_C.DEMO.COMMON_CLASS_NAMES = [\n    \"watch (a person)\",\n    \"talk to (e.g., self, a person, a group)\",\n    \"listen to (a person)\",\n    \"touch (an object)\",\n    \"carry/hold (an object)\",\n    \"walk\",",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.COMMON_CLASS_NAMES",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.COMMON_CLASS_NAMES = [\n    \"watch (a person)\",\n    \"talk to (e.g., self, a person, a group)\",\n    \"listen to (a person)\",\n    \"touch (an object)\",\n    \"carry/hold (an object)\",\n    \"walk\",\n    \"sit\",\n    \"lie/sleep\",\n    \"bend/bow (at the waist)\",",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.SLOWMO",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.config.defaults",
        "description": "MorphMLP.build.lib.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.SLOWMO = 1\n# Add custom config with default values.\ncustom_config.add_custom_config(_C)\ndef assert_and_infer_cfg(cfg):\n    # BN assertions.\n    if cfg.BN.USE_PRECISE_STATS:\n        assert cfg.BN.NUM_BATCHES_PRECISE >= 0\n    # TRAIN assertions.\n    assert cfg.TRAIN.CHECKPOINT_TYPE in [\"pytorch\", \"caffe2\"]\n    assert cfg.NUM_GPUS == 0 or cfg.TRAIN.BATCH_SIZE % cfg.NUM_GPUS == 0",
        "detail": "MorphMLP.build.lib.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "Ava",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.ava_dataset",
        "description": "MorphMLP.build.lib.slowfast.datasets.ava_dataset",
        "peekOfCode": "class Ava(torch.utils.data.Dataset):\n    \"\"\"\n    AVA Dataset\n    \"\"\"\n    def __init__(self, cfg, split):\n        self.cfg = cfg\n        self._split = split\n        self._sample_rate = cfg.DATA.SAMPLING_RATE\n        self._video_length = cfg.DATA.NUM_FRAMES\n        self._seq_len = self._video_length * self._sample_rate",
        "detail": "MorphMLP.build.lib.slowfast.datasets.ava_dataset",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.ava_dataset",
        "description": "MorphMLP.build.lib.slowfast.datasets.ava_dataset",
        "peekOfCode": "logger = logging.getLogger(__name__)\n@DATASET_REGISTRY.register()\nclass Ava(torch.utils.data.Dataset):\n    \"\"\"\n    AVA Dataset\n    \"\"\"\n    def __init__(self, cfg, split):\n        self.cfg = cfg\n        self._split = split\n        self._sample_rate = cfg.DATA.SAMPLING_RATE",
        "detail": "MorphMLP.build.lib.slowfast.datasets.ava_dataset",
        "documentation": {}
    },
    {
        "label": "load_image_lists",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.ava_helper",
        "description": "MorphMLP.build.lib.slowfast.datasets.ava_helper",
        "peekOfCode": "def load_image_lists(cfg, is_train):\n    \"\"\"\n    Loading image paths from corresponding files.\n    Args:\n        cfg (CfgNode): config.\n        is_train (bool): if it is training dataset or not.\n    Returns:\n        image_paths (list[list]): a list of items. Each item (also a list)\n            corresponds to one video and contains the paths of images for\n            this video.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.ava_helper",
        "documentation": {}
    },
    {
        "label": "load_boxes_and_labels",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.ava_helper",
        "description": "MorphMLP.build.lib.slowfast.datasets.ava_helper",
        "peekOfCode": "def load_boxes_and_labels(cfg, mode):\n    \"\"\"\n    Loading boxes and labels from csv files.\n    Args:\n        cfg (CfgNode): config.\n        mode (str): 'train', 'val', or 'test' mode.\n    Returns:\n        all_boxes (dict): a dict which maps from `video_name` and\n            `frame_sec` to a list of `box`. Each `box` is a\n            [`box_coord`, `box_labels`] where `box_coord` is the",
        "detail": "MorphMLP.build.lib.slowfast.datasets.ava_helper",
        "documentation": {}
    },
    {
        "label": "get_keyframe_data",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.ava_helper",
        "description": "MorphMLP.build.lib.slowfast.datasets.ava_helper",
        "peekOfCode": "def get_keyframe_data(boxes_and_labels):\n    \"\"\"\n    Getting keyframe indices, boxes and labels in the dataset.\n    Args:\n        boxes_and_labels (list[dict]): a list which maps from video_idx to a dict.\n            Each dict `frame_sec` to a list of boxes and corresponding labels.\n    Returns:\n        keyframe_indices (list): a list of indices of the keyframes.\n        keyframe_boxes_and_labels (list[list[list]]): a list of list which maps from\n            video_idx and sec_idx to a list of boxes and corresponding labels.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.ava_helper",
        "documentation": {}
    },
    {
        "label": "get_num_boxes_used",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.ava_helper",
        "description": "MorphMLP.build.lib.slowfast.datasets.ava_helper",
        "peekOfCode": "def get_num_boxes_used(keyframe_indices, keyframe_boxes_and_labels):\n    \"\"\"\n    Get total number of used boxes.\n    Args:\n        keyframe_indices (list): a list of indices of the keyframes.\n        keyframe_boxes_and_labels (list[list[list]]): a list of list which maps from\n            video_idx and sec_idx to a list of boxes and corresponding labels.\n    Returns:\n        count (int): total number of used boxes.\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.datasets.ava_helper",
        "documentation": {}
    },
    {
        "label": "parse_bboxes_file",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.ava_helper",
        "description": "MorphMLP.build.lib.slowfast.datasets.ava_helper",
        "peekOfCode": "def parse_bboxes_file(\n    ann_filenames, ann_is_gt_box, detect_thresh, boxes_sample_rate=1\n):\n    \"\"\"\n    Parse AVA bounding boxes files.\n    Args:\n        ann_filenames (list of str(s)): a list of AVA bounding boxes annotation files.\n        ann_is_gt_box (list of bools): a list of boolean to indicate whether the corresponding\n            ann_file is ground-truth. `ann_is_gt_box[i]` correspond to `ann_filenames[i]`.\n        detect_thresh (float): threshold for accepting predicted boxes, range [0, 1].",
        "detail": "MorphMLP.build.lib.slowfast.datasets.ava_helper",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.ava_helper",
        "description": "MorphMLP.build.lib.slowfast.datasets.ava_helper",
        "peekOfCode": "logger = logging.getLogger(__name__)\nFPS = 30\nAVA_VALID_FRAMES = range(902, 1799)\ndef load_image_lists(cfg, is_train):\n    \"\"\"\n    Loading image paths from corresponding files.\n    Args:\n        cfg (CfgNode): config.\n        is_train (bool): if it is training dataset or not.\n    Returns:",
        "detail": "MorphMLP.build.lib.slowfast.datasets.ava_helper",
        "documentation": {}
    },
    {
        "label": "FPS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.ava_helper",
        "description": "MorphMLP.build.lib.slowfast.datasets.ava_helper",
        "peekOfCode": "FPS = 30\nAVA_VALID_FRAMES = range(902, 1799)\ndef load_image_lists(cfg, is_train):\n    \"\"\"\n    Loading image paths from corresponding files.\n    Args:\n        cfg (CfgNode): config.\n        is_train (bool): if it is training dataset or not.\n    Returns:\n        image_paths (list[list]): a list of items. Each item (also a list)",
        "detail": "MorphMLP.build.lib.slowfast.datasets.ava_helper",
        "documentation": {}
    },
    {
        "label": "AVA_VALID_FRAMES",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.ava_helper",
        "description": "MorphMLP.build.lib.slowfast.datasets.ava_helper",
        "peekOfCode": "AVA_VALID_FRAMES = range(902, 1799)\ndef load_image_lists(cfg, is_train):\n    \"\"\"\n    Loading image paths from corresponding files.\n    Args:\n        cfg (CfgNode): config.\n        is_train (bool): if it is training dataset or not.\n    Returns:\n        image_paths (list[list]): a list of items. Each item (also a list)\n            corresponds to one video and contains the paths of images for",
        "detail": "MorphMLP.build.lib.slowfast.datasets.ava_helper",
        "documentation": {}
    },
    {
        "label": "build_dataset",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.build",
        "description": "MorphMLP.build.lib.slowfast.datasets.build",
        "peekOfCode": "def build_dataset(dataset_name, cfg, split):\n    \"\"\"\n    Build a dataset, defined by `dataset_name`.\n    Args:\n        dataset_name (str): the name of the dataset to be constructed.\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        split (str): the split of the data loader. Options include `train`,\n            `val`, and `test`.\n    Returns:",
        "detail": "MorphMLP.build.lib.slowfast.datasets.build",
        "documentation": {}
    },
    {
        "label": "DATASET_REGISTRY",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.build",
        "description": "MorphMLP.build.lib.slowfast.datasets.build",
        "peekOfCode": "DATASET_REGISTRY = Registry(\"DATASET\")\nDATASET_REGISTRY.__doc__ = \"\"\"\nRegistry for dataset.\nThe registered object will be called with `obj(cfg, split)`.\nThe call should return a `torch.utils.data.Dataset` object.\n\"\"\"\ndef build_dataset(dataset_name, cfg, split):\n    \"\"\"\n    Build a dataset, defined by `dataset_name`.\n    Args:",
        "detail": "MorphMLP.build.lib.slowfast.datasets.build",
        "documentation": {}
    },
    {
        "label": "DATASET_REGISTRY.__doc__",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.build",
        "description": "MorphMLP.build.lib.slowfast.datasets.build",
        "peekOfCode": "DATASET_REGISTRY.__doc__ = \"\"\"\nRegistry for dataset.\nThe registered object will be called with `obj(cfg, split)`.\nThe call should return a `torch.utils.data.Dataset` object.\n\"\"\"\ndef build_dataset(dataset_name, cfg, split):\n    \"\"\"\n    Build a dataset, defined by `dataset_name`.\n    Args:\n        dataset_name (str): the name of the dataset to be constructed.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.build",
        "documentation": {}
    },
    {
        "label": "Charades",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.charades",
        "description": "MorphMLP.build.lib.slowfast.datasets.charades",
        "peekOfCode": "class Charades(torch.utils.data.Dataset):\n    \"\"\"\n    Charades video loader. Construct the Charades video loader, then sample\n    clips from the videos. For training and validation, a single clip is randomly\n    sampled from every video with random cropping, scaling, and flipping. For\n    testing, multiple clips are uniformaly sampled from every video with uniform\n    cropping. For uniform cropping, we take the left, center, and right crop if\n    the width is larger than height, or take top, center, and bottom crop if the\n    height is larger than the width.\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.datasets.charades",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.charades",
        "description": "MorphMLP.build.lib.slowfast.datasets.charades",
        "peekOfCode": "logger = logging.get_logger(__name__)\n@DATASET_REGISTRY.register()\nclass Charades(torch.utils.data.Dataset):\n    \"\"\"\n    Charades video loader. Construct the Charades video loader, then sample\n    clips from the videos. For training and validation, a single clip is randomly\n    sampled from every video with random cropping, scaling, and flipping. For\n    testing, multiple clips are uniformaly sampled from every video with uniform\n    cropping. For uniform cropping, we take the left, center, and right crop if\n    the width is larger than height, or take top, center, and bottom crop if the",
        "detail": "MorphMLP.build.lib.slowfast.datasets.charades",
        "documentation": {}
    },
    {
        "label": "clip_boxes_to_image",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def clip_boxes_to_image(boxes, height, width):\n    \"\"\"\n    Clip the boxes with the height and width of the image size.\n    Args:\n        boxes (ndarray): bounding boxes to peform crop. The dimension is\n        `num boxes` x 4.\n        height (int): the height of the image.\n        width (int): the width of the image.\n    Returns:\n        boxes (ndarray): cropped bounding boxes.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "random_short_side_scale_jitter_list",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def random_short_side_scale_jitter_list(images, min_size, max_size, boxes=None):\n    \"\"\"\n    Perform a spatial short scale jittering on the given images and\n    corresponding boxes.\n    Args:\n        images (list): list of images to perform scale jitter. Dimension is\n            `height` x `width` x `channel`.\n        min_size (int): the minimal size to scale the frames.\n        max_size (int): the maximal size to scale the frames.\n        boxes (list): optional. Corresponding boxes to images. Dimension is",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "scale",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def scale(size, image):\n    \"\"\"\n    Scale the short side of the image to size.\n    Args:\n        size (int): size to scale the image.\n        image (array): image to perform short side scale. Dimension is\n            `height` x `width` x `channel`.\n    Returns:\n        (ndarray): the scaled image with dimension of\n            `height` x `width` x `channel`.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "scale_boxes",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def scale_boxes(size, boxes, height, width):\n    \"\"\"\n    Scale the short side of the box to size.\n    Args:\n        size (int): size to scale the image.\n        boxes (ndarray): bounding boxes to peform scale. The dimension is\n        `num boxes` x 4.\n        height (int): the height of the image.\n        width (int): the width of the image.\n    Returns:",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "horizontal_flip_list",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def horizontal_flip_list(prob, images, order=\"CHW\", boxes=None):\n    \"\"\"\n    Horizontally flip the list of image and optional boxes.\n    Args:\n        prob (float): probability to flip.\n        image (list): ilist of images to perform short side scale. Dimension is\n            `height` x `width` x `channel` or `channel` x `height` x `width`.\n        order (str): order of the `height`, `channel` and `width`.\n        boxes (list): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "spatial_shift_crop_list",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def spatial_shift_crop_list(size, images, spatial_shift_pos, boxes=None):\n    \"\"\"\n    Perform left, center, or right crop of the given list of images.\n    Args:\n        size (int): size to crop.\n        image (list): ilist of images to perform short side scale. Dimension is\n            `height` x `width` x `channel` or `channel` x `height` x `width`.\n        spatial_shift_pos (int): option includes 0 (left), 1 (middle), and\n            2 (right) crop.\n        boxes (list): optional. Corresponding boxes to images.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "CHW2HWC",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def CHW2HWC(image):\n    \"\"\"\n    Transpose the dimension from `channel` x `height` x `width` to\n        `height` x `width` x `channel`.\n    Args:\n        image (array): image to transpose.\n    Returns\n        (array): transposed image.\n    \"\"\"\n    return image.transpose([1, 2, 0])",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "HWC2CHW",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def HWC2CHW(image):\n    \"\"\"\n    Transpose the dimension from `height` x `width` x `channel` to\n        `channel` x `height` x `width`.\n    Args:\n        image (array): image to transpose.\n    Returns\n        (array): transposed image.\n    \"\"\"\n    return image.transpose([2, 0, 1])",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "color_jitter_list",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def color_jitter_list(\n    images, img_brightness=0, img_contrast=0, img_saturation=0\n):\n    \"\"\"\n    Perform color jitter on the list of images.\n    Args:\n        images (list): list of images to perform color jitter.\n        img_brightness (float): jitter ratio for brightness.\n        img_contrast (float): jitter ratio for contrast.\n        img_saturation (float): jitter ratio for saturation.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "lighting_list",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def lighting_list(imgs, alphastd, eigval, eigvec, alpha=None):\n    \"\"\"\n    Perform AlexNet-style PCA jitter on the given list of images.\n    Args:\n        images (list): list of images to perform lighting jitter.\n        alphastd (float): jitter ratio for PCA jitter.\n        eigval (list): eigenvalues for PCA jitter.\n        eigvec (list[list]): eigenvectors for PCA jitter.\n    Returns:\n        out_images (list): the list of jittered images.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "color_normalization",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def color_normalization(image, mean, stddev):\n    \"\"\"\n    Perform color normalization on the image with the given mean and stddev.\n    Args:\n        image (array): image to perform color normalization.\n        mean (float): mean value to subtract.\n        stddev (float): stddev to devide.\n    \"\"\"\n    # Input image should in format of CHW\n    assert len(mean) == image.shape[0], \"channel mean not computed properly\"",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "pad_image",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def pad_image(image, pad_size, order=\"CHW\"):\n    \"\"\"\n    Pad the given image with the size of pad_size.\n    Args:\n        image (array): image to pad.\n        pad_size (int): size to pad.\n        order (str): order of the `height`, `channel` and `width`.\n    Returns:\n        img (array): padded image.\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "horizontal_flip",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def horizontal_flip(prob, image, order=\"CHW\"):\n    \"\"\"\n    Horizontally flip the image.\n    Args:\n        prob (float): probability to flip.\n        image (array): image to pad.\n        order (str): order of the `height`, `channel` and `width`.\n    Returns:\n        img (array): flipped image.\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "flip_boxes",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def flip_boxes(boxes, im_width):\n    \"\"\"\n    Horizontally flip the boxes.\n    Args:\n        boxes (array): box to flip.\n        im_width (int): width of the image.\n    Returns:\n        boxes_flipped (array): flipped box.\n    \"\"\"\n    boxes_flipped = boxes.copy()",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "crop_boxes",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def crop_boxes(boxes, x_offset, y_offset):\n    \"\"\"\n    Crop the boxes given the offsets.\n    Args:\n        boxes (array): boxes to crop.\n        x_offset (int): offset on x.\n        y_offset (int): offset on y.\n    \"\"\"\n    boxes[:, [0, 2]] = boxes[:, [0, 2]] - x_offset\n    boxes[:, [1, 3]] = boxes[:, [1, 3]] - y_offset",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "random_crop_list",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def random_crop_list(images, size, pad_size=0, order=\"CHW\", boxes=None):\n    \"\"\"\n    Perform random crop on a list of images.\n    Args:\n        images (list): list of images to perform random crop.\n        size (int): size to crop.\n        pad_size (int): padding size.\n        order (str): order of the `height`, `channel` and `width`.\n        boxes (list): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "center_crop",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def center_crop(size, image):\n    \"\"\"\n    Perform center crop on input images.\n    Args:\n        size (int): size of the cropped height and width.\n        image (array): the image to perform center crop.\n    \"\"\"\n    height = image.shape[0]\n    width = image.shape[1]\n    y_offset = int(math.ceil((height - size) / 2))",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "random_scale_jitter",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def random_scale_jitter(image, min_size, max_size):\n    \"\"\"\n    Perform ResNet style random scale jittering: randomly select the scale from\n        [1/max_size, 1/min_size].\n    Args:\n        image (array): image to perform random scale.\n        min_size (int): min size to scale.\n        max_size (int) max size to scale.\n    Returns:\n        image (array): scaled image.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "random_scale_jitter_list",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def random_scale_jitter_list(images, min_size, max_size):\n    \"\"\"\n    Perform ResNet style random scale jittering on a list of image: randomly\n        select the scale from [1/max_size, 1/min_size]. Note that all the image\n        will share the same scale.\n    Args:\n        images (list): list of images to perform random scale.\n        min_size (int): min size to scale.\n        max_size (int) max size to scale.\n    Returns:",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "random_sized_crop",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def random_sized_crop(image, size, area_frac=0.08):\n    \"\"\"\n    Perform random sized cropping on the given image. Random crop with size\n        8% - 100% image area and aspect ratio in [3/4, 4/3].\n    Args:\n        image (array): image to crop.\n        size (int): size to crop.\n        area_frac (float): area of fraction.\n    Returns:\n        (array): cropped image.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "lighting",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def lighting(img, alphastd, eigval, eigvec):\n    \"\"\"\n    Perform AlexNet-style PCA jitter on the given image.\n    Args:\n        image (array): list of images to perform lighting jitter.\n        alphastd (float): jitter ratio for PCA jitter.\n        eigval (array): eigenvalues for PCA jitter.\n        eigvec (list): eigenvectors for PCA jitter.\n    Returns:\n        img (tensor): the jittered image.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "random_sized_crop_list",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def random_sized_crop_list(images, size, crop_area_fraction=0.08):\n    \"\"\"\n    Perform random sized cropping on the given list of images. Random crop with\n        size 8% - 100% image area and aspect ratio in [3/4, 4/3].\n    Args:\n        images (list): image to crop.\n        size (int): size to crop.\n        area_frac (float): area of fraction.\n    Returns:\n        (list): list of cropped image.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "blend",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def blend(image1, image2, alpha):\n    return image1 * alpha + image2 * (1 - alpha)\ndef grayscale(image):\n    \"\"\"\n    Convert the image to gray scale.\n    Args:\n        image (tensor): image to convert to gray scale. Dimension is\n            `channel` x `height` x `width`.\n    Returns:\n        img_gray (tensor): image in gray scale.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "grayscale",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def grayscale(image):\n    \"\"\"\n    Convert the image to gray scale.\n    Args:\n        image (tensor): image to convert to gray scale. Dimension is\n            `channel` x `height` x `width`.\n    Returns:\n        img_gray (tensor): image in gray scale.\n    \"\"\"\n    # R -> 0.299, G -> 0.587, B -> 0.114.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "saturation",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def saturation(var, image):\n    \"\"\"\n    Perform color saturation on the given image.\n    Args:\n        var (float): variance.\n        image (array): image to perform color saturation.\n    Returns:\n        (array): image that performed color saturation.\n    \"\"\"\n    img_gray = grayscale(image)",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "brightness",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def brightness(var, image):\n    \"\"\"\n    Perform color brightness on the given image.\n    Args:\n        var (float): variance.\n        image (array): image to perform color brightness.\n    Returns:\n        (array): image that performed color brightness.\n    \"\"\"\n    img_bright = np.zeros(image.shape).astype(image.dtype)",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "contrast",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def contrast(var, image):\n    \"\"\"\n    Perform color contrast on the given image.\n    Args:\n        var (float): variance.\n        image (array): image to perform color contrast.\n    Returns:\n        (array): image that performed color contrast.\n    \"\"\"\n    img_gray = grayscale(image)",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "saturation_list",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def saturation_list(var, images):\n    \"\"\"\n    Perform color saturation on the list of given images.\n    Args:\n        var (float): variance.\n        images (list): list of images to perform color saturation.\n    Returns:\n        (list): list of images that performed color saturation.\n    \"\"\"\n    alpha = 1.0 + np.random.uniform(-var, var)",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "brightness_list",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def brightness_list(var, images):\n    \"\"\"\n    Perform color brightness on the given list of images.\n    Args:\n        var (float): variance.\n        images (list): list of images to perform color brightness.\n    Returns:\n        (array): list of images that performed color brightness.\n    \"\"\"\n    alpha = 1.0 + np.random.uniform(-var, var)",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "contrast_list",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def contrast_list(var, images):\n    \"\"\"\n    Perform color contrast on the given list of images.\n    Args:\n        var (float): variance.\n        images (list): list of images to perform color contrast.\n    Returns:\n        (array): image that performed color contrast.\n    \"\"\"\n    alpha = 1.0 + np.random.uniform(-var, var)",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "color_jitter",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def color_jitter(image, img_brightness=0, img_contrast=0, img_saturation=0):\n    \"\"\"\n    Perform color jitter on the given image.\n    Args:\n        image (array): image to perform color jitter.\n        img_brightness (float): jitter ratio for brightness.\n        img_contrast (float): jitter ratio for contrast.\n        img_saturation (float): jitter ratio for saturation.\n    Returns:\n        image (array): the jittered image.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "revert_scaled_boxes",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "peekOfCode": "def revert_scaled_boxes(size, boxes, img_height, img_width):\n    \"\"\"\n    Revert scaled input boxes to match the original image size.\n    Args:\n        size (int): size of the cropped image.\n        boxes (array): shape (num_boxes, 4).\n        img_height (int): height of original image.\n        img_width (int): width of original image.\n    Returns:\n        reverted_boxes (array): boxes scaled back to the original image size.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "temporal_sampling",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.decoder",
        "description": "MorphMLP.build.lib.slowfast.datasets.decoder",
        "peekOfCode": "def temporal_sampling(frames, start_idx, end_idx, num_samples):\n    \"\"\"\n    Given the start and end frame index, sample num_samples frames between\n    the start and end with equal interval.\n    Args:\n        frames (tensor): a tensor of video frames, dimension is\n            `num video frames` x `channel` x `height` x `width`.\n        start_idx (int): the index of the start frame.\n        end_idx (int): the index of the end frame.\n        num_samples (int): number of frames to sample.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.decoder",
        "documentation": {}
    },
    {
        "label": "get_start_end_idx",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.decoder",
        "description": "MorphMLP.build.lib.slowfast.datasets.decoder",
        "peekOfCode": "def get_start_end_idx(\n    video_size, clip_size, clip_idx, num_clips, use_offset=False\n):\n    \"\"\"\n    Sample a clip of size clip_size from a video of size video_size and\n    return the indices of the first and last frame of the clip. If clip_idx is\n    -1, the clip is randomly sampled, otherwise uniformly split the video to\n    num_clips clips, and select the start and end index of clip_idx-th video\n    clip.\n    Args:",
        "detail": "MorphMLP.build.lib.slowfast.datasets.decoder",
        "documentation": {}
    },
    {
        "label": "pyav_decode_stream",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.decoder",
        "description": "MorphMLP.build.lib.slowfast.datasets.decoder",
        "peekOfCode": "def pyav_decode_stream(\n    container, start_pts, end_pts, stream, stream_name, buffer_size=0\n):\n    \"\"\"\n    Decode the video with PyAV decoder.\n    Args:\n        container (container): PyAV container.\n        start_pts (int): the starting Presentation TimeStamp to fetch the\n            video frames.\n        end_pts (int): the ending Presentation TimeStamp of the decoded frames.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.decoder",
        "documentation": {}
    },
    {
        "label": "torchvision_decode",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.decoder",
        "description": "MorphMLP.build.lib.slowfast.datasets.decoder",
        "peekOfCode": "def torchvision_decode(\n    video_handle,\n    sampling_rate,\n    num_frames,\n    clip_idx,\n    video_meta,\n    num_clips=10,\n    target_fps=30,\n    modalities=(\"visual\",),\n    max_spatial_scale=0,",
        "detail": "MorphMLP.build.lib.slowfast.datasets.decoder",
        "documentation": {}
    },
    {
        "label": "pyav_decode",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.decoder",
        "description": "MorphMLP.build.lib.slowfast.datasets.decoder",
        "peekOfCode": "def pyav_decode(\n    container,\n    sampling_rate,\n    num_frames,\n    clip_idx,\n    num_clips=10,\n    target_fps=30,\n    use_offset=False,\n):\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.datasets.decoder",
        "documentation": {}
    },
    {
        "label": "decode",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.decoder",
        "description": "MorphMLP.build.lib.slowfast.datasets.decoder",
        "peekOfCode": "def decode(\n    container,\n    sampling_rate,\n    num_frames,\n    clip_idx=-1,\n    num_clips=10,\n    video_meta=None,\n    target_fps=30,\n    backend=\"pyav\",\n    max_spatial_scale=0,",
        "detail": "MorphMLP.build.lib.slowfast.datasets.decoder",
        "documentation": {}
    },
    {
        "label": "Imagenet",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.imagenet",
        "description": "MorphMLP.build.lib.slowfast.datasets.imagenet",
        "peekOfCode": "class Imagenet(torch.utils.data.Dataset):\n    \"\"\"ImageNet dataset.\"\"\"\n    def __init__(self, cfg, mode, num_retries=10):\n        self.num_retries = num_retries\n        self.cfg = cfg\n        self.mode = mode\n        self.data_path = cfg.DATA.PATH_TO_DATA_DIR\n        assert mode in [\n            \"train\",\n            \"val\",",
        "detail": "MorphMLP.build.lib.slowfast.datasets.imagenet",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.imagenet",
        "description": "MorphMLP.build.lib.slowfast.datasets.imagenet",
        "peekOfCode": "logger = logging.get_logger(__name__)\n@DATASET_REGISTRY.register()\nclass Imagenet(torch.utils.data.Dataset):\n    \"\"\"ImageNet dataset.\"\"\"\n    def __init__(self, cfg, mode, num_retries=10):\n        self.num_retries = num_retries\n        self.cfg = cfg\n        self.mode = mode\n        self.data_path = cfg.DATA.PATH_TO_DATA_DIR\n        assert mode in [",
        "detail": "MorphMLP.build.lib.slowfast.datasets.imagenet",
        "documentation": {}
    },
    {
        "label": "Kinetics",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.kinetics",
        "description": "MorphMLP.build.lib.slowfast.datasets.kinetics",
        "peekOfCode": "class Kinetics(torch.utils.data.Dataset):\n    \"\"\"\n    Kinetics video loader. Construct the Kinetics video loader, then sample\n    clips from the videos. For training and validation, a single clip is\n    randomly sampled from every video with random cropping, scaling, and\n    flipping. For testing, multiple clips are uniformaly sampled from every\n    video with uniform cropping. For uniform cropping, we take the left, center,\n    and right crop if the width is larger than height, or take top, center, and\n    bottom crop if the height is larger than the width.\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.datasets.kinetics",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.kinetics",
        "description": "MorphMLP.build.lib.slowfast.datasets.kinetics",
        "peekOfCode": "logger = logging.get_logger(__name__)\n@DATASET_REGISTRY.register()\nclass Kinetics(torch.utils.data.Dataset):\n    \"\"\"\n    Kinetics video loader. Construct the Kinetics video loader, then sample\n    clips from the videos. For training and validation, a single clip is\n    randomly sampled from every video with random cropping, scaling, and\n    flipping. For testing, multiple clips are uniformaly sampled from every\n    video with uniform cropping. For uniform cropping, we take the left, center,\n    and right crop if the width is larger than height, or take top, center, and",
        "detail": "MorphMLP.build.lib.slowfast.datasets.kinetics",
        "documentation": {}
    },
    {
        "label": "multiple_samples_collate",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.loader",
        "description": "MorphMLP.build.lib.slowfast.datasets.loader",
        "peekOfCode": "def multiple_samples_collate(batch, fold=False):\n    \"\"\"\n    Collate function for repeated augmentation. Each instance in the batch has\n    more than one sample.\n    Args:\n        batch (tuple or list): data batch to collate.\n    Returns:\n        (tuple): collated data batch.\n    \"\"\"\n    inputs, labels, video_idx, extra_data = zip(*batch)",
        "detail": "MorphMLP.build.lib.slowfast.datasets.loader",
        "documentation": {}
    },
    {
        "label": "detection_collate",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.loader",
        "description": "MorphMLP.build.lib.slowfast.datasets.loader",
        "peekOfCode": "def detection_collate(batch):\n    \"\"\"\n    Collate function for detection task. Concatanate bboxes, labels and\n    metadata from different samples in the first dimension instead of\n    stacking them to have a batch-size dimension.\n    Args:\n        batch (tuple or list): data batch to collate.\n    Returns:\n        (tuple): collated detection data batch.\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.datasets.loader",
        "documentation": {}
    },
    {
        "label": "construct_loader",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.loader",
        "description": "MorphMLP.build.lib.slowfast.datasets.loader",
        "peekOfCode": "def construct_loader(cfg, split, is_precise_bn=False):\n    \"\"\"\n    Constructs the data loader for the given dataset.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        split (str): the split of the data loader. Options include `train`,\n            `val`, and `test`.\n    \"\"\"\n    assert split in [\"train\", \"val\", \"test\"]",
        "detail": "MorphMLP.build.lib.slowfast.datasets.loader",
        "documentation": {}
    },
    {
        "label": "shuffle_dataset",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.loader",
        "description": "MorphMLP.build.lib.slowfast.datasets.loader",
        "peekOfCode": "def shuffle_dataset(loader, cur_epoch):\n    \"\"\" \"\n    Shuffles the data.\n    Args:\n        loader (loader): data loader to perform shuffle.\n        cur_epoch (int): number of the current epoch.\n    \"\"\"\n    if (\n        loader._dataset_kind\n        == torch.utils.data.dataloader._DatasetKind.Iterable",
        "detail": "MorphMLP.build.lib.slowfast.datasets.loader",
        "documentation": {}
    },
    {
        "label": "MixUp",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.mixup",
        "description": "MorphMLP.build.lib.slowfast.datasets.mixup",
        "peekOfCode": "class MixUp:\n    \"\"\"\n    Apply mixup and/or cutmix for videos at batch level.\n    mixup: Beyond Empirical Risk Minimization (https://arxiv.org/abs/1710.09412)\n    CutMix: Regularization Strategy to Train Strong Classifiers with Localizable\n        Features (https://arxiv.org/abs/1905.04899)\n    \"\"\"\n    def __init__(\n        self,\n        mixup_alpha=1.0,",
        "detail": "MorphMLP.build.lib.slowfast.datasets.mixup",
        "documentation": {}
    },
    {
        "label": "convert_to_one_hot",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.mixup",
        "description": "MorphMLP.build.lib.slowfast.datasets.mixup",
        "peekOfCode": "def convert_to_one_hot(targets, num_classes, on_value=1.0, off_value=0.0):\n    \"\"\"\n    This function converts target class indices to one-hot vectors, given the\n    number of classes.\n    Args:\n        targets (loader): Class labels.\n        num_classes (int): Total number of classes.\n        on_value (float): Target Value for ground truth class.\n        off_value (float): Target Value for other classes.This value is used for\n            label smoothing.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.mixup",
        "documentation": {}
    },
    {
        "label": "mixup_target",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.mixup",
        "description": "MorphMLP.build.lib.slowfast.datasets.mixup",
        "peekOfCode": "def mixup_target(target, num_classes, lam=1.0, smoothing=0.0):\n    \"\"\"\n    This function converts target class indices to one-hot vectors, given the\n    number of classes.\n    Args:\n        targets (loader): Class labels.\n        num_classes (int): Total number of classes.\n        lam (float): lamba value for mixup/cutmix.\n        smoothing (float): Label smoothing value.\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.datasets.mixup",
        "documentation": {}
    },
    {
        "label": "rand_bbox",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.mixup",
        "description": "MorphMLP.build.lib.slowfast.datasets.mixup",
        "peekOfCode": "def rand_bbox(img_shape, lam, margin=0.0, count=None):\n    \"\"\"\n    Generates a random square bbox based on lambda value.\n    Args:\n        img_shape (tuple): Image shape as tuple\n        lam (float): Cutmix lambda value\n        margin (float): Percentage of bbox dimension to enforce as margin (reduce amount of box outside image)\n        count (int): Number of bbox to generate\n    \"\"\"\n    ratio = np.sqrt(1 - lam)",
        "detail": "MorphMLP.build.lib.slowfast.datasets.mixup",
        "documentation": {}
    },
    {
        "label": "get_cutmix_bbox",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.mixup",
        "description": "MorphMLP.build.lib.slowfast.datasets.mixup",
        "peekOfCode": "def get_cutmix_bbox(img_shape, lam, correct_lam=True, count=None):\n    \"\"\"\n    Generates the box coordinates for cutmix.\n    Args:\n        img_shape (tuple): Image shape as tuple\n        lam (float): Cutmix lambda value\n        correct_lam (bool): Apply lambda correction when cutmix bbox clipped by\n            image borders.\n        count (int): Number of bbox to generate\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.datasets.mixup",
        "documentation": {}
    },
    {
        "label": "ShortCycleBatchSampler",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.multigrid_helper",
        "description": "MorphMLP.build.lib.slowfast.datasets.multigrid_helper",
        "peekOfCode": "class ShortCycleBatchSampler(Sampler):\n    \"\"\"\n    Extend Sampler to support \"short cycle\" sampling.\n    See paper \"A Multigrid Method for Efficiently Training Video Models\",\n    Wu et al., 2019 (https://arxiv.org/abs/1912.00998) for details.\n    \"\"\"\n    def __init__(self, sampler, batch_size, drop_last, cfg):\n        if not isinstance(sampler, Sampler):\n            raise ValueError(\n                \"sampler should be an instance of \"",
        "detail": "MorphMLP.build.lib.slowfast.datasets.multigrid_helper",
        "documentation": {}
    },
    {
        "label": "TORCH_MAJOR",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.multigrid_helper",
        "description": "MorphMLP.build.lib.slowfast.datasets.multigrid_helper",
        "peekOfCode": "TORCH_MAJOR = int(torch.__version__.split(\".\")[0])\nTORCH_MINOR = int(torch.__version__.split(\".\")[1])\nif TORCH_MAJOR >= 1 and TORCH_MINOR >= 8:\n    _int_classes = int\nelse:\n    from torch._six import int_classes as _int_classes\nclass ShortCycleBatchSampler(Sampler):\n    \"\"\"\n    Extend Sampler to support \"short cycle\" sampling.\n    See paper \"A Multigrid Method for Efficiently Training Video Models\",",
        "detail": "MorphMLP.build.lib.slowfast.datasets.multigrid_helper",
        "documentation": {}
    },
    {
        "label": "TORCH_MINOR",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.multigrid_helper",
        "description": "MorphMLP.build.lib.slowfast.datasets.multigrid_helper",
        "peekOfCode": "TORCH_MINOR = int(torch.__version__.split(\".\")[1])\nif TORCH_MAJOR >= 1 and TORCH_MINOR >= 8:\n    _int_classes = int\nelse:\n    from torch._six import int_classes as _int_classes\nclass ShortCycleBatchSampler(Sampler):\n    \"\"\"\n    Extend Sampler to support \"short cycle\" sampling.\n    See paper \"A Multigrid Method for Efficiently Training Video Models\",\n    Wu et al., 2019 (https://arxiv.org/abs/1912.00998) for details.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.multigrid_helper",
        "documentation": {}
    },
    {
        "label": "PTVDatasetWrapper",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "description": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "peekOfCode": "class PTVDatasetWrapper(torch.utils.data.IterableDataset):\n    \"\"\"\n    Wrapper for PyTorchVideo datasets.\n    \"\"\"\n    def __init__(self, num_videos, clips_per_video, crops_per_clip, dataset):\n        \"\"\"\n        Construct the dataset.\n        Args:\n            num_vidoes (int): number of videos in the dataset.\n            clips_per_video (int): number of clips per video in the dataset.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "documentation": {}
    },
    {
        "label": "PackPathway",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "description": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "peekOfCode": "class PackPathway(torch.nn.Module):\n    \"\"\"\n    Transform for converting video frames as a list of tensors. Each tensor\n    corresponding to a unique pathway.\n    \"\"\"\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n    def forward(self, x: torch.Tensor):\n        return utils.pack_pathway_output(self.cfg, x)",
        "detail": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "documentation": {}
    },
    {
        "label": "DictToTuple",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "description": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "peekOfCode": "class DictToTuple(torch.nn.Module):\n    \"\"\"\n    Transform for converting output from dict to a tuple following PySlowFast\n    dataset output format.\n    \"\"\"\n    def __init__(self, num_clips, num_crops):\n        super().__init__()\n        self._num_clips = num_clips\n        self._num_crops = num_crops\n    def forward(self, x: Dict[str, torch.Tensor]):",
        "detail": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "documentation": {}
    },
    {
        "label": "div255",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "description": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "peekOfCode": "def div255(x):\n    \"\"\"\n    Scale clip frames from [0, 255] to [0, 1].\n    Args:\n        x (Tensor): A tensor of the clip's RGB frames with shape:\n            (channel, time, height, width).\n    Returns:\n        x (Tensor): Scaled tensor by divide 255.\n    \"\"\"\n    return x / 255.0",
        "detail": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "documentation": {}
    },
    {
        "label": "Ptvkinetics",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "description": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "peekOfCode": "def Ptvkinetics(cfg, mode):\n    \"\"\"\n    Construct the Kinetics video loader with a given csv file. The format of\n    the csv file is:\n    ```\n    path_to_video_1 label_1\n    path_to_video_2 label_2\n    ...\n    path_to_video_N label_N\n    ```",
        "detail": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "documentation": {}
    },
    {
        "label": "process_charades_label",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "description": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "peekOfCode": "def process_charades_label(x, mode, num_classes):\n    \"\"\"\n    Process the video label for Charades dataset. Use video-level label for\n    training mode, otherwise use clip-level label. Then convert the label into\n    a binary vector.\n    Args:\n        x (dict): a video clip including label index.\n        mode (string): Options includes `train`, `val`, or `test` mode.\n        num_classes (int): Number of classes in the dataset.\n    Returns:",
        "detail": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "documentation": {}
    },
    {
        "label": "rgb2bgr",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "description": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "peekOfCode": "def rgb2bgr(x):\n    \"\"\"\n    Convert clip frames from RGB mode to BRG mode.\n    Args:\n        x (Tensor): A tensor of the clip's RGB frames with shape:\n            (channel, time, height, width).\n    Returns:\n        x (Tensor): Converted tensor\n    \"\"\"\n    return x[[2, 1, 0], ...]",
        "detail": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "documentation": {}
    },
    {
        "label": "Ptvcharades",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "description": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "peekOfCode": "def Ptvcharades(cfg, mode):\n    \"\"\"\n    Construct PyTorchVideo Charades video loader.\n    Load Charades data (frame paths, labels, etc. ) to Charades Dataset object.\n    The dataset could be downloaded from Chrades official website\n    (https://allenai.org/plato/charades/).\n    Please see datasets/DATASET.md for more information about the data format.\n    For `train` and `val` mode, a single clip is randomly sampled from every video\n    with random cropping, scaling, and flipping. For `test` mode, multiple clips are\n    uniformaly sampled from every video with center cropping.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "documentation": {}
    },
    {
        "label": "Ptvssv2",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "description": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "peekOfCode": "def Ptvssv2(cfg, mode):\n    \"\"\"\n    Construct PyTorchVideo Something-Something v2 SSv2 video loader.\n    Load SSv2 data (frame paths, labels, etc. ) to SSv2 Dataset object.\n    The dataset could be downloaded from Chrades official website\n    (https://20bn.com/datasets/something-something).\n    Please see datasets/DATASET.md for more information about the data format.\n    For training and validation, a single  clip is randomly sampled from every\n    video with random cropping and scaling. For testing, multiple clips are\n    uniformaly sampled from every video with uniform cropping. For uniform cropping,",
        "detail": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "description": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "peekOfCode": "logger = logging.get_logger(__name__)\nclass PTVDatasetWrapper(torch.utils.data.IterableDataset):\n    \"\"\"\n    Wrapper for PyTorchVideo datasets.\n    \"\"\"\n    def __init__(self, num_videos, clips_per_video, crops_per_clip, dataset):\n        \"\"\"\n        Construct the dataset.\n        Args:\n            num_vidoes (int): number of videos in the dataset.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.ptv_datasets",
        "documentation": {}
    },
    {
        "label": "RandomErasing",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.random_erasing",
        "description": "MorphMLP.build.lib.slowfast.datasets.random_erasing",
        "peekOfCode": "class RandomErasing:\n    \"\"\"Randomly selects a rectangle region in an image and erases its pixels.\n        'Random Erasing Data Augmentation' by Zhong et al.\n        See https://arxiv.org/pdf/1708.04896.pdf\n        This variant of RandomErasing is intended to be applied to either a batch\n        or single image tensor after it has been normalized by dataset mean and std.\n    Args:\n         probability: Probability that the Random Erasing operation will be performed.\n         min_area: Minimum percentage of erased area wrt input image area.\n         max_area: Maximum percentage of erased area wrt input image area.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.random_erasing",
        "documentation": {}
    },
    {
        "label": "AugmentOp",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "class AugmentOp:\n    \"\"\"\n    Apply for video.\n    \"\"\"\n    def __init__(self, name, prob=0.5, magnitude=10, hparams=None):\n        hparams = hparams or _HPARAMS_DEFAULT\n        self.aug_fn = NAME_TO_OP[name]\n        self.level_fn = LEVEL_TO_ARG[name]\n        self.prob = prob\n        self.magnitude = magnitude",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "RandAugment",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "class RandAugment:\n    def __init__(self, ops, num_layers=2, choice_weights=None):\n        self.ops = ops\n        self.num_layers = num_layers\n        self.choice_weights = choice_weights\n    def __call__(self, img):\n        # no replacement when using weighted choice\n        ops = np.random.choice(\n            self.ops,\n            self.num_layers,",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "shear_x",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "def shear_x(img, factor, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(\n        img.size, Image.AFFINE, (1, factor, 0, 0, 1, 0), **kwargs\n    )\ndef shear_y(img, factor, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(\n        img.size, Image.AFFINE, (1, 0, 0, factor, 1, 0), **kwargs\n    )",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "shear_y",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "def shear_y(img, factor, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(\n        img.size, Image.AFFINE, (1, 0, 0, factor, 1, 0), **kwargs\n    )\ndef translate_x_rel(img, pct, **kwargs):\n    pixels = pct * img.size[0]\n    _check_args_tf(kwargs)\n    return img.transform(\n        img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "translate_x_rel",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "def translate_x_rel(img, pct, **kwargs):\n    pixels = pct * img.size[0]\n    _check_args_tf(kwargs)\n    return img.transform(\n        img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs\n    )\ndef translate_y_rel(img, pct, **kwargs):\n    pixels = pct * img.size[1]\n    _check_args_tf(kwargs)\n    return img.transform(",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "translate_y_rel",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "def translate_y_rel(img, pct, **kwargs):\n    pixels = pct * img.size[1]\n    _check_args_tf(kwargs)\n    return img.transform(\n        img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels), **kwargs\n    )\ndef translate_x_abs(img, pixels, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(\n        img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "translate_x_abs",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "def translate_x_abs(img, pixels, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(\n        img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs\n    )\ndef translate_y_abs(img, pixels, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(\n        img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels), **kwargs\n    )",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "translate_y_abs",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "def translate_y_abs(img, pixels, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(\n        img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels), **kwargs\n    )\ndef rotate(img, degrees, **kwargs):\n    _check_args_tf(kwargs)\n    if _PIL_VER >= (5, 2):\n        return img.rotate(degrees, **kwargs)\n    elif _PIL_VER >= (5, 0):",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "rotate",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "def rotate(img, degrees, **kwargs):\n    _check_args_tf(kwargs)\n    if _PIL_VER >= (5, 2):\n        return img.rotate(degrees, **kwargs)\n    elif _PIL_VER >= (5, 0):\n        w, h = img.size\n        post_trans = (0, 0)\n        rotn_center = (w / 2.0, h / 2.0)\n        angle = -math.radians(degrees)\n        matrix = [",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "auto_contrast",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "def auto_contrast(img, **__):\n    return ImageOps.autocontrast(img)\ndef invert(img, **__):\n    return ImageOps.invert(img)\ndef equalize(img, **__):\n    return ImageOps.equalize(img)\ndef solarize(img, thresh, **__):\n    return ImageOps.solarize(img, thresh)\ndef solarize_add(img, add, thresh=128, **__):\n    lut = []",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "invert",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "def invert(img, **__):\n    return ImageOps.invert(img)\ndef equalize(img, **__):\n    return ImageOps.equalize(img)\ndef solarize(img, thresh, **__):\n    return ImageOps.solarize(img, thresh)\ndef solarize_add(img, add, thresh=128, **__):\n    lut = []\n    for i in range(256):\n        if i < thresh:",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "equalize",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "def equalize(img, **__):\n    return ImageOps.equalize(img)\ndef solarize(img, thresh, **__):\n    return ImageOps.solarize(img, thresh)\ndef solarize_add(img, add, thresh=128, **__):\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "solarize",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "def solarize(img, thresh, **__):\n    return ImageOps.solarize(img, thresh)\ndef solarize_add(img, add, thresh=128, **__):\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in (\"L\", \"RGB\"):",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "solarize_add",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "def solarize_add(img, add, thresh=128, **__):\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in (\"L\", \"RGB\"):\n        if img.mode == \"RGB\" and len(lut) == 256:\n            lut = lut + lut + lut",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "posterize",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "def posterize(img, bits_to_keep, **__):\n    if bits_to_keep >= 8:\n        return img\n    return ImageOps.posterize(img, bits_to_keep)\ndef contrast(img, factor, **__):\n    return ImageEnhance.Contrast(img).enhance(factor)\ndef color(img, factor, **__):\n    return ImageEnhance.Color(img).enhance(factor)\ndef brightness(img, factor, **__):\n    return ImageEnhance.Brightness(img).enhance(factor)",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "contrast",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "def contrast(img, factor, **__):\n    return ImageEnhance.Contrast(img).enhance(factor)\ndef color(img, factor, **__):\n    return ImageEnhance.Color(img).enhance(factor)\ndef brightness(img, factor, **__):\n    return ImageEnhance.Brightness(img).enhance(factor)\ndef sharpness(img, factor, **__):\n    return ImageEnhance.Sharpness(img).enhance(factor)\ndef _randomly_negate(v):\n    \"\"\"With 50% prob, negate the value\"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "color",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "def color(img, factor, **__):\n    return ImageEnhance.Color(img).enhance(factor)\ndef brightness(img, factor, **__):\n    return ImageEnhance.Brightness(img).enhance(factor)\ndef sharpness(img, factor, **__):\n    return ImageEnhance.Sharpness(img).enhance(factor)\ndef _randomly_negate(v):\n    \"\"\"With 50% prob, negate the value\"\"\"\n    return -v if random.random() > 0.5 else v\ndef _rotate_level_to_arg(level, _hparams):",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "brightness",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "def brightness(img, factor, **__):\n    return ImageEnhance.Brightness(img).enhance(factor)\ndef sharpness(img, factor, **__):\n    return ImageEnhance.Sharpness(img).enhance(factor)\ndef _randomly_negate(v):\n    \"\"\"With 50% prob, negate the value\"\"\"\n    return -v if random.random() > 0.5 else v\ndef _rotate_level_to_arg(level, _hparams):\n    # range [-30, 30]\n    level = (level / _MAX_LEVEL) * 30.0",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "sharpness",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "def sharpness(img, factor, **__):\n    return ImageEnhance.Sharpness(img).enhance(factor)\ndef _randomly_negate(v):\n    \"\"\"With 50% prob, negate the value\"\"\"\n    return -v if random.random() > 0.5 else v\ndef _rotate_level_to_arg(level, _hparams):\n    # range [-30, 30]\n    level = (level / _MAX_LEVEL) * 30.0\n    level = _randomly_negate(level)\n    return (level,)",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "rand_augment_ops",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "def rand_augment_ops(magnitude=10, hparams=None, transforms=None):\n    hparams = hparams or _HPARAMS_DEFAULT\n    transforms = transforms or _RAND_TRANSFORMS\n    return [\n        AugmentOp(name, prob=0.5, magnitude=magnitude, hparams=hparams)\n        for name in transforms\n    ]\nclass RandAugment:\n    def __init__(self, ops, num_layers=2, choice_weights=None):\n        self.ops = ops",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "rand_augment_transform",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "def rand_augment_transform(config_str, hparams):\n    \"\"\"\n    RandAugment: Practical automated data augmentation... - https://arxiv.org/abs/1909.13719\n    Create a RandAugment transform\n    :param config_str: String defining configuration of random augmentation. Consists of multiple sections separated by\n    dashes ('-'). The first section defines the specific variant of rand augment (currently only 'rand'). The remaining\n    sections, not order sepecific determine\n        'm' - integer magnitude of rand augment\n        'n' - integer num layers (number of transform ops selected per image)\n        'w' - integer probabiliy weight index (index of a set of weights to influence choice of op)",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "_PIL_VER",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "_PIL_VER = tuple([int(x) for x in PIL.__version__.split(\".\")[:2]])\n_FILL = (128, 128, 128)\n# This signifies the max integer that the controller RNN could predict for the\n# augmentation scheme.\n_MAX_LEVEL = 10.0\n_HPARAMS_DEFAULT = {\n    \"translate_const\": 250,\n    \"img_mean\": _FILL,\n}\n_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "_FILL",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "_FILL = (128, 128, 128)\n# This signifies the max integer that the controller RNN could predict for the\n# augmentation scheme.\n_MAX_LEVEL = 10.0\n_HPARAMS_DEFAULT = {\n    \"translate_const\": 250,\n    \"img_mean\": _FILL,\n}\n_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\ndef _interpolation(kwargs):",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "_MAX_LEVEL",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "_MAX_LEVEL = 10.0\n_HPARAMS_DEFAULT = {\n    \"translate_const\": 250,\n    \"img_mean\": _FILL,\n}\n_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\ndef _interpolation(kwargs):\n    interpolation = kwargs.pop(\"resample\", Image.BILINEAR)\n    if isinstance(interpolation, (list, tuple)):\n        return random.choice(interpolation)",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "_HPARAMS_DEFAULT",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "_HPARAMS_DEFAULT = {\n    \"translate_const\": 250,\n    \"img_mean\": _FILL,\n}\n_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\ndef _interpolation(kwargs):\n    interpolation = kwargs.pop(\"resample\", Image.BILINEAR)\n    if isinstance(interpolation, (list, tuple)):\n        return random.choice(interpolation)\n    else:",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "_RANDOM_INTERPOLATION",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\ndef _interpolation(kwargs):\n    interpolation = kwargs.pop(\"resample\", Image.BILINEAR)\n    if isinstance(interpolation, (list, tuple)):\n        return random.choice(interpolation)\n    else:\n        return interpolation\ndef _check_args_tf(kwargs):\n    if \"fillcolor\" in kwargs and _PIL_VER < (5, 0):\n        kwargs.pop(\"fillcolor\")",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "LEVEL_TO_ARG",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "LEVEL_TO_ARG = {\n    \"AutoContrast\": None,\n    \"Equalize\": None,\n    \"Invert\": None,\n    \"Rotate\": _rotate_level_to_arg,\n    # There are several variations of the posterize level scaling in various Tensorflow/Google repositories/papers\n    \"Posterize\": _posterize_level_to_arg,\n    \"PosterizeIncreasing\": _posterize_increasing_level_to_arg,\n    \"PosterizeOriginal\": _posterize_original_level_to_arg,\n    \"Solarize\": _solarize_level_to_arg,",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "NAME_TO_OP",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "NAME_TO_OP = {\n    \"AutoContrast\": auto_contrast,\n    \"Equalize\": equalize,\n    \"Invert\": invert,\n    \"Rotate\": rotate,\n    \"Posterize\": posterize,\n    \"PosterizeIncreasing\": posterize,\n    \"PosterizeOriginal\": posterize,\n    \"Solarize\": solarize,\n    \"SolarizeIncreasing\": solarize,",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "_RAND_TRANSFORMS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "_RAND_TRANSFORMS = [\n    \"AutoContrast\",\n    \"Equalize\",\n    \"Invert\",\n    \"Rotate\",\n    \"Posterize\",\n    \"Solarize\",\n    \"SolarizeAdd\",\n    \"Color\",\n    \"Contrast\",",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "_RAND_INCREASING_TRANSFORMS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "_RAND_INCREASING_TRANSFORMS = [\n    \"AutoContrast\",\n    \"Equalize\",\n    \"Invert\",\n    \"Rotate\",\n    \"PosterizeIncreasing\",\n    \"SolarizeIncreasing\",\n    \"SolarizeAdd\",\n    \"ColorIncreasing\",\n    \"ContrastIncreasing\",",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "_RAND_CHOICE_WEIGHTS_0",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "description": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "peekOfCode": "_RAND_CHOICE_WEIGHTS_0 = {\n    \"Rotate\": 0.3,\n    \"ShearX\": 0.2,\n    \"ShearY\": 0.2,\n    \"TranslateXRel\": 0.1,\n    \"TranslateYRel\": 0.1,\n    \"Color\": 0.025,\n    \"Sharpness\": 0.025,\n    \"AutoContrast\": 0.025,\n    \"Solarize\": 0.005,",
        "detail": "MorphMLP.build.lib.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "Ssv2",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.ssv2",
        "description": "MorphMLP.build.lib.slowfast.datasets.ssv2",
        "peekOfCode": "class Ssv2(torch.utils.data.Dataset):\n    \"\"\"\n    Something-Something v2 (SSV2) video loader. Construct the SSV2 video loader,\n    then sample clips from the videos. For training and validation, a single\n    clip is randomly sampled from every video with random cropping, scaling, and\n    flipping. For testing, multiple clips are uniformaly sampled from every\n    video with uniform cropping. For uniform cropping, we take the left, center,\n    and right crop if the width is larger than height, or take top, center, and\n    bottom crop if the height is larger than the width.\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.datasets.ssv2",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.ssv2",
        "description": "MorphMLP.build.lib.slowfast.datasets.ssv2",
        "peekOfCode": "logger = logging.get_logger(__name__)\n@DATASET_REGISTRY.register()\nclass Ssv2(torch.utils.data.Dataset):\n    \"\"\"\n    Something-Something v2 (SSV2) video loader. Construct the SSV2 video loader,\n    then sample clips from the videos. For training and validation, a single\n    clip is randomly sampled from every video with random cropping, scaling, and\n    flipping. For testing, multiple clips are uniformaly sampled from every\n    video with uniform cropping. For uniform cropping, we take the left, center,\n    and right crop if the width is larger than height, or take top, center, and",
        "detail": "MorphMLP.build.lib.slowfast.datasets.ssv2",
        "documentation": {}
    },
    {
        "label": "Sth",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.sth",
        "description": "MorphMLP.build.lib.slowfast.datasets.sth",
        "peekOfCode": "class Sth(torch.utils.data.Dataset):\n    \"\"\"\n    Something-Something video loader. Construct the Sth video loader,\n    then sample clips from the videos. For training and validation, a single\n    clip is randomly sampled from every video with random cropping, scaling, and\n    flipping. For testing, multiple clips are uniformaly sampled from every\n    video with uniform cropping. For uniform cropping, we take the left, center,\n    and right crop if the width is larger than height, or take top, center, and\n    bottom crop if the height is larger than the width.\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.datasets.sth",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.sth",
        "description": "MorphMLP.build.lib.slowfast.datasets.sth",
        "peekOfCode": "logger = logging.get_logger(__name__)\n@DATASET_REGISTRY.register()\nclass Sth(torch.utils.data.Dataset):\n    \"\"\"\n    Something-Something video loader. Construct the Sth video loader,\n    then sample clips from the videos. For training and validation, a single\n    clip is randomly sampled from every video with random cropping, scaling, and\n    flipping. For testing, multiple clips are uniformaly sampled from every\n    video with uniform cropping. For uniform cropping, we take the left, center,\n    and right crop if the width is larger than height, or take top, center, and",
        "detail": "MorphMLP.build.lib.slowfast.datasets.sth",
        "documentation": {}
    },
    {
        "label": "RandomResizedCropAndInterpolation",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "class RandomResizedCropAndInterpolation:\n    \"\"\"Crop the given PIL Image to random size and aspect ratio with random interpolation.\n    A crop of random size (default: of 0.08 to 1.0) of the original size and a random\n    aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop\n    is finally resized to given size.\n    This is popularly used to train the Inception networks.\n    Args:\n        size: expected output size of each edge\n        scale: range of size of the origin size cropped\n        ratio: range of aspect ratio of the origin aspect ratio cropped",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "GaussianBlur",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "class GaussianBlur(object):\n    \"\"\"Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709\"\"\"\n    def __init__(self, sigma=[0.1, 2.0]):\n        self.sigma = sigma\n    def __call__(self, x):\n        if len(self.sigma) == 2:\n            sigma = random.uniform(self.sigma[0], self.sigma[1])\n        elif len(self.sigma) == 1:\n            sigma = self.sigma[0]\n        x = x.filter(ImageFilter.GaussianBlur(radius=sigma))",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "GaussianBlurVideo",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "class GaussianBlurVideo(object):\n    def __init__(\n        self, sigma_min=[0.0, 0.1], sigma_max=[0.0, 2.0], use_PIL=False\n    ):\n        self.sigma_min = sigma_min\n        self.sigma_max = sigma_max\n    def __call__(self, frames):\n        sigma_y = sigma_x = random.uniform(self.sigma_min[1], self.sigma_max[1])\n        sigma_t = random.uniform(self.sigma_min[0], self.sigma_max[0])\n        frames = gaussian_filter(frames, sigma=(0.0, sigma_t, sigma_y, sigma_x))",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "random_short_side_scale_jitter",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "def random_short_side_scale_jitter(\n    images, min_size, max_size, boxes=None, inverse_uniform_sampling=False\n):\n    \"\"\"\n    Perform a spatial short scale jittering on the given images and\n    corresponding boxes.\n    Args:\n        images (tensor): images to perform scale jitter. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n        min_size (int): the minimal size to scale the frames.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "crop_boxes",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "def crop_boxes(boxes, x_offset, y_offset):\n    \"\"\"\n    Peform crop on the bounding boxes given the offsets.\n    Args:\n        boxes (ndarray or None): bounding boxes to peform crop. The dimension\n            is `num boxes` x 4.\n        x_offset (int): cropping offset in the x axis.\n        y_offset (int): cropping offset in the y axis.\n    Returns:\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "random_crop",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "def random_crop(images, size, boxes=None):\n    \"\"\"\n    Perform random spatial crop on the given images and corresponding boxes.\n    Args:\n        images (tensor): images to perform random crop. The dimension is\n            `num frames` x `channel` x `height` x `width`.\n        size (int): the size of height and width to crop on the image.\n        boxes (ndarray or None): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.\n    Returns:",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "horizontal_flip",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "def horizontal_flip(prob, images, boxes=None):\n    \"\"\"\n    Perform horizontal flip on the given images and corresponding boxes.\n    Args:\n        prob (float): probility to flip the images.\n        images (tensor): images to perform horizontal flip, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n        boxes (ndarray or None): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.\n    Returns:",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "uniform_crop",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "def uniform_crop(images, size, spatial_idx, boxes=None, scale_size=None):\n    \"\"\"\n    Perform uniform spatial sampling on the images and corresponding boxes.\n    Args:\n        images (tensor): images to perform uniform crop. The dimension is\n            `num frames` x `channel` x `height` x `width`.\n        size (int): size of height and weight to crop the images.\n        spatial_idx (int): 0, 1, or 2 for left, center, and right crop if width\n            is larger than height. Or 0, 1, or 2 for top, center, and bottom\n            crop if height is larger than width.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "clip_boxes_to_image",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "def clip_boxes_to_image(boxes, height, width):\n    \"\"\"\n    Clip an array of boxes to an image with the given height and width.\n    Args:\n        boxes (ndarray): bounding boxes to perform clipping.\n            Dimension is `num boxes` x 4.\n        height (int): given image height.\n        width (int): given image width.\n    Returns:\n        clipped_boxes (ndarray): the clipped boxes with dimension of",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "blend",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "def blend(images1, images2, alpha):\n    \"\"\"\n    Blend two images with a given weight alpha.\n    Args:\n        images1 (tensor): the first images to be blended, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n        images2 (tensor): the second images to be blended, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n        alpha (float): the blending weight.\n    Returns:",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "grayscale",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "def grayscale(images):\n    \"\"\"\n    Get the grayscale for the input images. The channels of images should be\n    in order BGR.\n    Args:\n        images (tensor): the input images for getting grayscale. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n    Returns:\n        img_gray (tensor): blended images, the dimension is\n            `num frames` x `channel` x `height` x `width`.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "color_jitter",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "def color_jitter(images, img_brightness=0, img_contrast=0, img_saturation=0):\n    \"\"\"\n    Perfrom a color jittering on the input images. The channels of images\n    should be in order BGR.\n    Args:\n        images (tensor): images to perform color jitter. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n        img_brightness (float): jitter ratio for brightness.\n        img_contrast (float): jitter ratio for contrast.\n        img_saturation (float): jitter ratio for saturation.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "brightness_jitter",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "def brightness_jitter(var, images):\n    \"\"\"\n    Perfrom brightness jittering on the input images. The channels of images\n    should be in order BGR.\n    Args:\n        var (float): jitter ratio for brightness.\n        images (tensor): images to perform color jitter. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n    Returns:\n        images (tensor): the jittered images, the dimension is",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "contrast_jitter",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "def contrast_jitter(var, images):\n    \"\"\"\n    Perfrom contrast jittering on the input images. The channels of images\n    should be in order BGR.\n    Args:\n        var (float): jitter ratio for contrast.\n        images (tensor): images to perform color jitter. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n    Returns:\n        images (tensor): the jittered images, the dimension is",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "saturation_jitter",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "def saturation_jitter(var, images):\n    \"\"\"\n    Perfrom saturation jittering on the input images. The channels of images\n    should be in order BGR.\n    Args:\n        var (float): jitter ratio for saturation.\n        images (tensor): images to perform color jitter. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n    Returns:\n        images (tensor): the jittered images, the dimension is",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "lighting_jitter",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "def lighting_jitter(images, alphastd, eigval, eigvec):\n    \"\"\"\n    Perform AlexNet-style PCA jitter on the given images.\n    Args:\n        images (tensor): images to perform lighting jitter. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n        alphastd (float): jitter ratio for PCA jitter.\n        eigval (list): eigenvalues for PCA jitter.\n        eigvec (list[list]): eigenvectors for PCA jitter.\n    Returns:",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "color_normalization",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "def color_normalization(images, mean, stddev):\n    \"\"\"\n    Perform color nomration on the given images.\n    Args:\n        images (tensor): images to perform color normalization. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n        mean (list): mean values for normalization.\n        stddev (list): standard deviations for normalization.\n    Returns:\n        out_images (tensor): the noramlized images, the dimension is",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "random_resized_crop",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "def random_resized_crop(\n    images,\n    target_height,\n    target_width,\n    scale=(0.8, 1.0),\n    ratio=(3.0 / 4.0, 4.0 / 3.0),\n):\n    \"\"\"\n    Crop the given images to random size and aspect ratio. A crop of random\n    size (default: of 0.08 to 1.0) of the original size and a random aspect",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "random_resized_crop_with_shift",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "def random_resized_crop_with_shift(\n    images,\n    target_height,\n    target_width,\n    scale=(0.8, 1.0),\n    ratio=(3.0 / 4.0, 4.0 / 3.0),\n):\n    \"\"\"\n    This is similar to random_resized_crop. However, it samples two different\n    boxes (for cropping) for the first and last frame. It then linearly",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "create_random_augment",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "def create_random_augment(\n    input_size,\n    auto_augment=None,\n    interpolation=\"bilinear\",\n):\n    \"\"\"\n    Get video randaug transform.\n    Args:\n        input_size: The size of the input video in tuple.\n        auto_augment: Parameters for randaug. An example:",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "random_sized_crop_img",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "def random_sized_crop_img(\n    im,\n    size,\n    jitter_scale=(0.08, 1.0),\n    jitter_aspect=(3.0 / 4.0, 4.0 / 3.0),\n    max_iter=10,\n):\n    \"\"\"\n    Performs Inception-style cropping (used for training).\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "transforms_imagenet_train",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "def transforms_imagenet_train(\n    img_size=224,\n    scale=None,\n    ratio=None,\n    hflip=0.5,\n    vflip=0.0,\n    color_jitter=0.4,\n    auto_augment=None,\n    interpolation=\"random\",\n    use_prefetcher=False,",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "temporal_difference",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "def temporal_difference(\n    frames,\n    use_grayscale=False,\n    absolute=False,\n):\n    if use_grayscale:\n        gray_channel = (\n            0.299 * frames[2, :] + 0.587 * frames[1, :] + 0.114 * frames[0, :]\n        )\n        frames[0, :] = gray_channel",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "color_jitter_video_ssl",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "def color_jitter_video_ssl(\n    frames,\n    bri_con_sat=[0.4] * 3,\n    hue=0.1,\n    p_convert_gray=0.0,\n    moco_v2_aug=False,\n    gaussan_sigma_min=[0.0, 0.1],\n    gaussan_sigma_max=[0.0, 2.0],\n):\n    # T H W C -> C T H W.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "augment_raw_frames",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "def augment_raw_frames(frames, time_diff_prob=0.0, gaussian_prob=0.0):\n    frames = frames.float()\n    if gaussian_prob > 0.0:\n        blur_trans = tv.transforms.RandomApply(\n            [GaussianBlurVideo()], p=gaussian_prob\n        )\n        frames = blur_trans(frames)\n    time_diff_out = False\n    if time_diff_prob > 0.0 and random.random() < time_diff_prob:\n        # T H W C -> C T H W.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "_pil_interpolation_to_str",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "_pil_interpolation_to_str = {\n    Image.NEAREST: \"PIL.Image.NEAREST\",\n    Image.BILINEAR: \"PIL.Image.BILINEAR\",\n    Image.BICUBIC: \"PIL.Image.BICUBIC\",\n    Image.LANCZOS: \"PIL.Image.LANCZOS\",\n    Image.HAMMING: \"PIL.Image.HAMMING\",\n    Image.BOX: \"PIL.Image.BOX\",\n}\n_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\ndef _pil_interp(method):",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "_RANDOM_INTERPOLATION",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\ndef _pil_interp(method):\n    if method == \"bicubic\":\n        return Image.BICUBIC\n    elif method == \"lanczos\":\n        return Image.LANCZOS\n    elif method == \"hamming\":\n        return Image.HAMMING\n    else:\n        return Image.BILINEAR",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.transform",
        "description": "MorphMLP.build.lib.slowfast.datasets.transform",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef random_short_side_scale_jitter(\n    images, min_size, max_size, boxes=None, inverse_uniform_sampling=False\n):\n    \"\"\"\n    Perform a spatial short scale jittering on the given images and\n    corresponding boxes.\n    Args:\n        images (tensor): images to perform scale jitter. Dimension is\n            `num frames` x `channel` x `height` x `width`.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "retry_load_images",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.utils",
        "description": "MorphMLP.build.lib.slowfast.datasets.utils",
        "peekOfCode": "def retry_load_images(image_paths, retry=10, backend=\"pytorch\"):\n    \"\"\"\n    This function is to load images with support of retrying for failed load.\n    Args:\n        image_paths (list): paths of images needed to be loaded.\n        retry (int, optional): maximum time of loading retrying. Defaults to 10.\n        backend (str): `pytorch` or `cv2`.\n    Returns:\n        imgs (list): list of loaded images.\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "get_sequence",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.utils",
        "description": "MorphMLP.build.lib.slowfast.datasets.utils",
        "peekOfCode": "def get_sequence(center_idx, half_len, sample_rate, num_frames):\n    \"\"\"\n    Sample frames among the corresponding clip.\n    Args:\n        center_idx (int): center frame idx for current clip\n        half_len (int): half of the clip length\n        sample_rate (int): sampling rate for sampling frames inside of the clip\n        num_frames (int): number of expected sampled frames\n    Returns:\n        seq (list): list of indexes of sampled frames in this clip.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "pack_pathway_output",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.utils",
        "description": "MorphMLP.build.lib.slowfast.datasets.utils",
        "peekOfCode": "def pack_pathway_output(cfg, frames):\n    \"\"\"\n    Prepare output as a list of tensors. Each tensor corresponding to a\n    unique pathway.\n    Args:\n        frames (tensor): frames of images sampled from the video. The\n            dimension is `channel` x `num frames` x `height` x `width`.\n    Returns:\n        frame_list (list): list of tensors with the dimension of\n            `channel` x `num frames` x `height` x `width`.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "spatial_sampling",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.utils",
        "description": "MorphMLP.build.lib.slowfast.datasets.utils",
        "peekOfCode": "def spatial_sampling(\n    frames,\n    spatial_idx=-1,\n    min_scale=256,\n    max_scale=320,\n    crop_size=224,\n    random_horizontal_flip=True,\n    inverse_uniform_sampling=False,\n    aspect_ratio=None,\n    scale=None,",
        "detail": "MorphMLP.build.lib.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "as_binary_vector",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.utils",
        "description": "MorphMLP.build.lib.slowfast.datasets.utils",
        "peekOfCode": "def as_binary_vector(labels, num_classes):\n    \"\"\"\n    Construct binary label vector given a list of label indices.\n    Args:\n        labels (list): The input label list.\n        num_classes (int): Number of classes of the label vector.\n    Returns:\n        labels (numpy array): the resulting binary vector.\n    \"\"\"\n    label_arr = np.zeros((num_classes,))",
        "detail": "MorphMLP.build.lib.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "aggregate_labels",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.utils",
        "description": "MorphMLP.build.lib.slowfast.datasets.utils",
        "peekOfCode": "def aggregate_labels(label_list):\n    \"\"\"\n    Join a list of label list.\n    Args:\n        labels (list): The input label list.\n    Returns:\n        labels (list): The joint list of all lists in input.\n    \"\"\"\n    all_labels = []\n    for labels in label_list:",
        "detail": "MorphMLP.build.lib.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "convert_to_video_level_labels",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.utils",
        "description": "MorphMLP.build.lib.slowfast.datasets.utils",
        "peekOfCode": "def convert_to_video_level_labels(labels):\n    \"\"\"\n    Aggregate annotations from all frames of a video to form video-level labels.\n    Args:\n        labels (list): The input label list.\n    Returns:\n        labels (list): Same as input, but with each label replaced by\n        a video-level one.\n    \"\"\"\n    for video_id in range(len(labels)):",
        "detail": "MorphMLP.build.lib.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "load_image_lists",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.utils",
        "description": "MorphMLP.build.lib.slowfast.datasets.utils",
        "peekOfCode": "def load_image_lists(frame_list_file, prefix=\"\", return_list=False):\n    \"\"\"\n    Load image paths and labels from a \"frame list\".\n    Each line of the frame list contains:\n    `original_vido_id video_id frame_id path labels`\n    Args:\n        frame_list_file (string): path to the frame list.\n        prefix (str): the prefix for the path.\n        return_list (bool): if True, return a list. If False, return a dict.\n    Returns:",
        "detail": "MorphMLP.build.lib.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "tensor_normalize",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.utils",
        "description": "MorphMLP.build.lib.slowfast.datasets.utils",
        "peekOfCode": "def tensor_normalize(tensor, mean, std, func=None):\n    \"\"\"\n    Normalize a given tensor by subtracting the mean and dividing the std.\n    Args:\n        tensor (tensor): tensor to normalize.\n        mean (tensor or list): mean value to subtract.\n        std (tensor or list): std to divide.\n    \"\"\"\n    if tensor.dtype == torch.uint8:\n        tensor = tensor.float()",
        "detail": "MorphMLP.build.lib.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "get_random_sampling_rate",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.utils",
        "description": "MorphMLP.build.lib.slowfast.datasets.utils",
        "peekOfCode": "def get_random_sampling_rate(long_cycle_sampling_rate, sampling_rate):\n    \"\"\"\n    When multigrid training uses a fewer number of frames, we randomly\n    increase the sampling rate so that some clips cover the original span.\n    \"\"\"\n    if long_cycle_sampling_rate > 0:\n        assert long_cycle_sampling_rate >= sampling_rate\n        return random.randint(sampling_rate, long_cycle_sampling_rate)\n    else:\n        return sampling_rate",
        "detail": "MorphMLP.build.lib.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "revert_tensor_normalize",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.utils",
        "description": "MorphMLP.build.lib.slowfast.datasets.utils",
        "peekOfCode": "def revert_tensor_normalize(tensor, mean, std):\n    \"\"\"\n    Revert normalization for a given tensor by multiplying by the std and adding the mean.\n    Args:\n        tensor (tensor): tensor to revert normalization.\n        mean (tensor or list): mean value to add.\n        std (tensor or list): std to multiply.\n    \"\"\"\n    if type(mean) == list:\n        mean = torch.tensor(mean)",
        "detail": "MorphMLP.build.lib.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "create_sampler",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.utils",
        "description": "MorphMLP.build.lib.slowfast.datasets.utils",
        "peekOfCode": "def create_sampler(dataset, shuffle, cfg):\n    \"\"\"\n    Create sampler for the given dataset.\n    Args:\n        dataset (torch.utils.data.Dataset): the given dataset.\n        shuffle (bool): set to ``True`` to have the data reshuffled\n            at every epoch.\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n    Returns:",
        "detail": "MorphMLP.build.lib.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "loader_worker_init_fn",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.utils",
        "description": "MorphMLP.build.lib.slowfast.datasets.utils",
        "peekOfCode": "def loader_worker_init_fn(dataset):\n    \"\"\"\n    Create init function passed to pytorch data loader.\n    Args:\n        dataset (torch.utils.data.Dataset): the given dataset.\n    \"\"\"\n    return None",
        "detail": "MorphMLP.build.lib.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.utils",
        "description": "MorphMLP.build.lib.slowfast.datasets.utils",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef retry_load_images(image_paths, retry=10, backend=\"pytorch\"):\n    \"\"\"\n    This function is to load images with support of retrying for failed load.\n    Args:\n        image_paths (list): paths of images needed to be loaded.\n        retry (int, optional): maximum time of loading retrying. Defaults to 10.\n        backend (str): `pytorch` or `cv2`.\n    Returns:\n        imgs (list): list of loaded images.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "get_video_container",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.datasets.video_container",
        "description": "MorphMLP.build.lib.slowfast.datasets.video_container",
        "peekOfCode": "def get_video_container(path_to_vid, multi_thread_decode=False, backend=\"torchvision\"):\n    \"\"\"\n    Given the path to the video, return the pyav video container.\n    Args:\n        path_to_vid (str): path to the video.\n        multi_thread_decode (bool): if True, perform multi-thread decoding.\n        backend (str): decoder backend, options include `pyav` and\n            `torchvision`, default is `pyav`.\n    Returns:\n        container (container): video container.",
        "detail": "MorphMLP.build.lib.slowfast.datasets.video_container",
        "documentation": {}
    },
    {
        "label": "MultiScaleAttention",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.attention",
        "description": "MorphMLP.build.lib.slowfast.models.attention",
        "peekOfCode": "class MultiScaleAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        input_size,\n        num_heads=8,\n        qkv_bias=False,\n        drop_rate=0.0,\n        kernel_q=(1, 1, 1),",
        "detail": "MorphMLP.build.lib.slowfast.models.attention",
        "documentation": {}
    },
    {
        "label": "MultiScaleBlock",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.attention",
        "description": "MorphMLP.build.lib.slowfast.models.attention",
        "peekOfCode": "class MultiScaleBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        num_heads,\n        input_size,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,",
        "detail": "MorphMLP.build.lib.slowfast.models.attention",
        "documentation": {}
    },
    {
        "label": "attention_pool",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.models.attention",
        "description": "MorphMLP.build.lib.slowfast.models.attention",
        "peekOfCode": "def attention_pool(tensor, pool, thw_shape, has_cls_embed=True, norm=None):\n    if pool is None:\n        return tensor, thw_shape\n    tensor_dim = tensor.ndim\n    if tensor_dim == 4:\n        pass\n    elif tensor_dim == 3:\n        tensor = tensor.unsqueeze(1)\n    else:\n        raise NotImplementedError(f\"Unsupported input dimension {tensor.shape}\")",
        "detail": "MorphMLP.build.lib.slowfast.models.attention",
        "documentation": {}
    },
    {
        "label": "get_rel_pos",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.models.attention",
        "description": "MorphMLP.build.lib.slowfast.models.attention",
        "peekOfCode": "def get_rel_pos(rel_pos, d):\n    if isinstance(d, int):\n        ori_d = rel_pos.shape[0]\n        if ori_d == d:\n            return rel_pos\n        else:\n            # Interpolate rel pos.\n            new_pos_embed = F.interpolate(\n                rel_pos.reshape(1, ori_d, -1).permute(0, 2, 1),\n                size=d,",
        "detail": "MorphMLP.build.lib.slowfast.models.attention",
        "documentation": {}
    },
    {
        "label": "cal_rel_pos_spatial",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.models.attention",
        "description": "MorphMLP.build.lib.slowfast.models.attention",
        "peekOfCode": "def cal_rel_pos_spatial(\n    attn, q, k, has_cls_embed, q_shape, k_shape, rel_pos_h, rel_pos_w\n):\n    \"\"\"\n    Decomposed Spatial Relative Positional Embeddings.\n    \"\"\"\n    sp_idx = 1 if has_cls_embed else 0\n    q_t, q_h, q_w = q_shape\n    k_t, k_h, k_w = k_shape\n    dh = int(2 * max(q_h, k_h) - 1)",
        "detail": "MorphMLP.build.lib.slowfast.models.attention",
        "documentation": {}
    },
    {
        "label": "cal_rel_pos_temporal",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.models.attention",
        "description": "MorphMLP.build.lib.slowfast.models.attention",
        "peekOfCode": "def cal_rel_pos_temporal(attn, q, has_cls_embed, q_shape, k_shape, rel_pos_t):\n    \"\"\"\n    Temporal Relative Positional Embeddings.\n    \"\"\"\n    sp_idx = 1 if has_cls_embed else 0\n    q_t, q_h, q_w = q_shape\n    k_t, k_h, k_w = k_shape\n    dt = int(2 * max(q_t, k_t) - 1)\n    # Intepolate rel pos if needed.\n    rel_pos_t = get_rel_pos(rel_pos_t, dt)",
        "detail": "MorphMLP.build.lib.slowfast.models.attention",
        "documentation": {}
    },
    {
        "label": "SubBatchNorm3d",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.batchnorm_helper",
        "description": "MorphMLP.build.lib.slowfast.models.batchnorm_helper",
        "peekOfCode": "class SubBatchNorm3d(nn.Module):\n    \"\"\"\n    The standard BN layer computes stats across all examples in a GPU. In some\n    cases it is desirable to compute stats across only a subset of examples\n    (e.g., in multigrid training https://arxiv.org/abs/1912.00998).\n    SubBatchNorm3d splits the batch dimension into N splits, and run BN on\n    each of them separately (so that the stats are computed on each subset of\n    examples (1/N of batch) independently. During evaluation, it aggregates\n    the stats from all splits into one BN.\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.models.batchnorm_helper",
        "documentation": {}
    },
    {
        "label": "get_norm",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.models.batchnorm_helper",
        "description": "MorphMLP.build.lib.slowfast.models.batchnorm_helper",
        "peekOfCode": "def get_norm(cfg):\n    \"\"\"\n    Args:\n        cfg (CfgNode): model building configs, details are in the comments of\n            the config file.\n    Returns:\n        nn.Module: the normalization layer.\n    \"\"\"\n    if cfg.BN.NORM_TYPE in {\"batchnorm\", \"sync_batchnorm_apex\"}:\n        return nn.BatchNorm3d",
        "detail": "MorphMLP.build.lib.slowfast.models.batchnorm_helper",
        "documentation": {}
    },
    {
        "label": "build_model",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.models.build",
        "description": "MorphMLP.build.lib.slowfast.models.build",
        "peekOfCode": "def build_model(cfg, gpu_id=None):\n    \"\"\"\n    Builds the video model.\n    Args:\n        cfg (configs): configs that contains the hyper-parameters to build the\n        backbone. Details can be seen in slowfast/config/defaults.py.\n        gpu_id (Optional[int]): specify the gpu index to build model.\n    \"\"\"\n    if torch.cuda.is_available():\n        assert (",
        "detail": "MorphMLP.build.lib.slowfast.models.build",
        "documentation": {}
    },
    {
        "label": "MODEL_REGISTRY",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.models.build",
        "description": "MorphMLP.build.lib.slowfast.models.build",
        "peekOfCode": "MODEL_REGISTRY = Registry(\"MODEL\")\nMODEL_REGISTRY.__doc__ = \"\"\"\nRegistry for video model.\nThe registered object will be called with `obj(cfg)`.\nThe call should return a `torch.nn.Module` object.\n\"\"\"\ndef build_model(cfg, gpu_id=None):\n    \"\"\"\n    Builds the video model.\n    Args:",
        "detail": "MorphMLP.build.lib.slowfast.models.build",
        "documentation": {}
    },
    {
        "label": "MODEL_REGISTRY.__doc__",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.models.build",
        "description": "MorphMLP.build.lib.slowfast.models.build",
        "peekOfCode": "MODEL_REGISTRY.__doc__ = \"\"\"\nRegistry for video model.\nThe registered object will be called with `obj(cfg)`.\nThe call should return a `torch.nn.Module` object.\n\"\"\"\ndef build_model(cfg, gpu_id=None):\n    \"\"\"\n    Builds the video model.\n    Args:\n        cfg (configs): configs that contains the hyper-parameters to build the",
        "detail": "MorphMLP.build.lib.slowfast.models.build",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.common",
        "description": "MorphMLP.build.lib.slowfast.models.common",
        "peekOfCode": "class Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop_rate=0.0,\n    ):\n        super().__init__()",
        "detail": "MorphMLP.build.lib.slowfast.models.common",
        "documentation": {}
    },
    {
        "label": "Permute",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.common",
        "description": "MorphMLP.build.lib.slowfast.models.common",
        "peekOfCode": "class Permute(nn.Module):\n    def __init__(self, dims):\n        super().__init__()\n        self.dims = dims\n    def forward(self, x):\n        return x.permute(*self.dims)\ndef drop_path(x, drop_prob: float = 0.0, training: bool = False):\n    \"\"\"\n    Stochastic Depth per sample.\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.models.common",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.common",
        "description": "MorphMLP.build.lib.slowfast.models.common",
        "peekOfCode": "class DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)",
        "detail": "MorphMLP.build.lib.slowfast.models.common",
        "documentation": {}
    },
    {
        "label": "drop_path",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.models.common",
        "description": "MorphMLP.build.lib.slowfast.models.common",
        "peekOfCode": "def drop_path(x, drop_prob: float = 0.0, training: bool = False):\n    \"\"\"\n    Stochastic Depth per sample.\n    \"\"\"\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (\n        x.ndim - 1\n    )  # work with diff dim tensors, not just 2D ConvNets",
        "detail": "MorphMLP.build.lib.slowfast.models.common",
        "documentation": {}
    },
    {
        "label": "ContrastiveModel",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.contrastive",
        "description": "MorphMLP.build.lib.slowfast.models.contrastive",
        "peekOfCode": "class ContrastiveModel(nn.Module):\n    \"\"\"\n    Contrastive Model, currently mainly focused on memory bank and CSC.\n    \"\"\"\n    def __init__(self, cfg):\n        super(ContrastiveModel, self).__init__()\n        # Construct the model.\n        self.backbone = _MODEL_TYPES[cfg.MODEL.ARCH](cfg)\n        self.type = cfg.CONTRASTIVE.TYPE\n        self.T = cfg.CONTRASTIVE.T",
        "detail": "MorphMLP.build.lib.slowfast.models.contrastive",
        "documentation": {}
    },
    {
        "label": "Normalize",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.contrastive",
        "description": "MorphMLP.build.lib.slowfast.models.contrastive",
        "peekOfCode": "class Normalize(nn.Module):\n    def __init__(self, power=2, dim=1):\n        super(Normalize, self).__init__()\n        self.dim = dim\n        self.power = power\n    def forward(self, x):\n        norm = (\n            x.pow(self.power).sum(self.dim, keepdim=True).pow(1.0 / self.power)\n        )\n        out = x.div(norm)",
        "detail": "MorphMLP.build.lib.slowfast.models.contrastive",
        "documentation": {}
    },
    {
        "label": "Memory",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.contrastive",
        "description": "MorphMLP.build.lib.slowfast.models.contrastive",
        "peekOfCode": "class Memory(nn.Module):\n    def __init__(self, length, duration, dim, cfg):\n        super(Memory, self).__init__()\n        self.length = length\n        self.duration = duration\n        self.dim = dim\n        stdv = 1.0 / math.sqrt(dim / 3)\n        self.register_buffer(\n            \"memory\",\n            torch.rand(length, duration, dim).mul_(2 * stdv).add_(-stdv),",
        "detail": "MorphMLP.build.lib.slowfast.models.contrastive",
        "documentation": {}
    },
    {
        "label": "Memory1D",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.contrastive",
        "description": "MorphMLP.build.lib.slowfast.models.contrastive",
        "peekOfCode": "class Memory1D(nn.Module):\n    def __init__(self, length, duration, dim, cfg):\n        super(Memory1D, self).__init__()\n        assert duration == 1\n        self.length = length\n        self.duration = duration\n        self.dim = dim\n        stdv = 1.0 / math.sqrt(dim / 3)\n        self.register_buffer(\n            \"memory\", torch.rand(length, dim).mul_(2 * stdv).add_(-stdv)",
        "detail": "MorphMLP.build.lib.slowfast.models.contrastive",
        "documentation": {}
    },
    {
        "label": "l2_loss",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.models.contrastive",
        "description": "MorphMLP.build.lib.slowfast.models.contrastive",
        "peekOfCode": "def l2_loss(x, y):\n    return 2 - 2 * (x * y).sum(dim=-1)\nclass Normalize(nn.Module):\n    def __init__(self, power=2, dim=1):\n        super(Normalize, self).__init__()\n        self.dim = dim\n        self.power = power\n    def forward(self, x):\n        norm = (\n            x.pow(self.power).sum(self.dim, keepdim=True).pow(1.0 / self.power)",
        "detail": "MorphMLP.build.lib.slowfast.models.contrastive",
        "documentation": {}
    },
    {
        "label": "cancel_swav_gradients",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.models.contrastive",
        "description": "MorphMLP.build.lib.slowfast.models.contrastive",
        "peekOfCode": "def cancel_swav_gradients(model, cfg, epoch_exact):\n    # cancel some gradients in first epoch of SwAV\n    if (\n        cfg.MODEL.MODEL_NAME == \"ContrastiveModel\"\n        and cfg.CONTRASTIVE.TYPE == \"swav\"\n        and epoch_exact <= 1.0\n    ):\n        for name, p in model.named_parameters():\n            if \"swav_prototypes\" in name:\n                p.grad = None",
        "detail": "MorphMLP.build.lib.slowfast.models.contrastive",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.models.contrastive",
        "description": "MorphMLP.build.lib.slowfast.models.contrastive",
        "peekOfCode": "logger = logging.get_logger(__name__)\n# Supported model types\n_MODEL_TYPES = {\n    \"slowfast\": SlowFast,\n    \"slow\": ResNet,\n    \"c2d\": ResNet,\n    \"i3d\": ResNet,\n    \"slow_c2d\": ResNet,\n    \"x3d\": X3D,\n    \"mvit\": MViT,",
        "detail": "MorphMLP.build.lib.slowfast.models.contrastive",
        "documentation": {}
    },
    {
        "label": "_MODEL_TYPES",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.models.contrastive",
        "description": "MorphMLP.build.lib.slowfast.models.contrastive",
        "peekOfCode": "_MODEL_TYPES = {\n    \"slowfast\": SlowFast,\n    \"slow\": ResNet,\n    \"c2d\": ResNet,\n    \"i3d\": ResNet,\n    \"slow_c2d\": ResNet,\n    \"x3d\": X3D,\n    \"mvit\": MViT,\n}\n@MODEL_REGISTRY.register()",
        "detail": "MorphMLP.build.lib.slowfast.models.contrastive",
        "documentation": {}
    },
    {
        "label": "ResNetRoIHead",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.head_helper",
        "description": "MorphMLP.build.lib.slowfast.models.head_helper",
        "peekOfCode": "class ResNetRoIHead(nn.Module):\n    \"\"\"\n    ResNe(X)t RoI head.\n    \"\"\"\n    def __init__(\n        self,\n        dim_in,\n        num_classes,\n        pool_size,\n        resolution,",
        "detail": "MorphMLP.build.lib.slowfast.models.head_helper",
        "documentation": {}
    },
    {
        "label": "MLPHead",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.head_helper",
        "description": "MorphMLP.build.lib.slowfast.models.head_helper",
        "peekOfCode": "class MLPHead(nn.Module):\n    def __init__(\n        self,\n        dim_in,\n        dim_out,\n        mlp_dim,\n        num_layers,\n        bn_on=False,\n        bias=True,\n        flatten=False,",
        "detail": "MorphMLP.build.lib.slowfast.models.head_helper",
        "documentation": {}
    },
    {
        "label": "ResNetBasicHead",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.head_helper",
        "description": "MorphMLP.build.lib.slowfast.models.head_helper",
        "peekOfCode": "class ResNetBasicHead(nn.Module):\n    \"\"\"\n    ResNe(X)t 3D head.\n    This layer performs a fully-connected projection during training, when the\n    input size is 1x1x1. It performs a convolutional projection during testing\n    when the input size is larger than 1x1x1. If the inputs are from multiple\n    different pathways, the inputs will be concatenated after pooling.\n    \"\"\"\n    def __init__(\n        self,",
        "detail": "MorphMLP.build.lib.slowfast.models.head_helper",
        "documentation": {}
    },
    {
        "label": "X3DHead",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.head_helper",
        "description": "MorphMLP.build.lib.slowfast.models.head_helper",
        "peekOfCode": "class X3DHead(nn.Module):\n    \"\"\"\n    X3D head.\n    This layer performs a fully-connected projection during training, when the\n    input size is 1x1x1. It performs a convolutional projection during testing\n    when the input size is larger than 1x1x1. If the inputs are from multiple\n    different pathways, the inputs will be concatenated after pooling.\n    \"\"\"\n    def __init__(\n        self,",
        "detail": "MorphMLP.build.lib.slowfast.models.head_helper",
        "documentation": {}
    },
    {
        "label": "TransformerBasicHead",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.head_helper",
        "description": "MorphMLP.build.lib.slowfast.models.head_helper",
        "peekOfCode": "class TransformerBasicHead(nn.Module):\n    \"\"\"\n    BasicHead. No pool.\n    \"\"\"\n    def __init__(\n        self,\n        dim_in,\n        num_classes,\n        dropout_rate=0.0,\n        act_func=\"softmax\",",
        "detail": "MorphMLP.build.lib.slowfast.models.head_helper",
        "documentation": {}
    },
    {
        "label": "ContrastiveLoss",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.losses",
        "description": "MorphMLP.build.lib.slowfast.models.losses",
        "peekOfCode": "class ContrastiveLoss(nn.Module):\n    def __init__(self, reduction=\"mean\"):\n        super(ContrastiveLoss, self).__init__()\n        self.reduction = reduction\n    def forward(self, inputs, dummy_labels=None):\n        targets = torch.zeros(inputs.shape[0], dtype=torch.long).cuda()\n        loss = nn.CrossEntropyLoss(reduction=self.reduction).cuda()(\n            inputs, targets\n        )\n        return loss",
        "detail": "MorphMLP.build.lib.slowfast.models.losses",
        "documentation": {}
    },
    {
        "label": "get_loss_func",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.models.losses",
        "description": "MorphMLP.build.lib.slowfast.models.losses",
        "peekOfCode": "def get_loss_func(loss_name):\n    \"\"\"\n    Retrieve the loss given the loss name.\n    Args (int):\n        loss_name: the name of the loss to use.\n    \"\"\"\n    if loss_name not in _LOSSES.keys():\n        raise NotImplementedError(\"Loss {} is not supported\".format(loss_name))\n    return _LOSSES[loss_name]",
        "detail": "MorphMLP.build.lib.slowfast.models.losses",
        "documentation": {}
    },
    {
        "label": "_LOSSES",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.models.losses",
        "description": "MorphMLP.build.lib.slowfast.models.losses",
        "peekOfCode": "_LOSSES = {\n    \"cross_entropy\": nn.CrossEntropyLoss,\n    \"bce\": nn.BCELoss,\n    \"bce_logit\": nn.BCEWithLogitsLoss,\n    \"soft_cross_entropy\": partial(SoftTargetCrossEntropyLoss, normalize_targets=False),\n    \"contrastive_loss\": ContrastiveLoss,\n}\ndef get_loss_func(loss_name):\n    \"\"\"\n    Retrieve the loss given the loss name.",
        "detail": "MorphMLP.build.lib.slowfast.models.losses",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "description": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "peekOfCode": "class Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n    def forward(self, x):",
        "detail": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "documentation": {}
    },
    {
        "label": "MorphFC_S2",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "description": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "peekOfCode": "class MorphFC_S2(nn.Module):\n    def __init__(self, dim, segment_dim=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.segment_dim = segment_dim\n        self.mlp_c = nn.Linear(dim, dim, bias=qkv_bias)\n        self.mlp_h = nn.Linear(dim, dim, bias=qkv_bias)\n        self.reweight = Mlp(dim, dim // 4, dim * 2)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n    def forward(self, x):",
        "detail": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "documentation": {}
    },
    {
        "label": "MorphFC_S",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "description": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "peekOfCode": "class MorphFC_S(nn.Module):\n    def __init__(self, dim, segment_dim=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.segment_dim = segment_dim\n        self.mlp_h = nn.Linear(dim, dim, bias=qkv_bias)\n        self.mlp_w = nn.Linear(dim, dim, bias=qkv_bias)\n        self.mlp_c = nn.Linear(dim, dim, bias=qkv_bias)\n        # init weight problem\n        self.reweight = Mlp(dim, dim // 4, dim * 3)\n        self.proj = nn.Linear(dim, dim)",
        "detail": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "documentation": {}
    },
    {
        "label": "MorphFC_T",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "description": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "peekOfCode": "class MorphFC_T(nn.Module):\n    def __init__(self, dim, segment_dim=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        global t_stride\n        self.segment_dim = 8\n        dim2 = dim\n        self.mlp_t = nn.Linear(dim2, dim2, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n    def forward(self, x):",
        "detail": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "documentation": {}
    },
    {
        "label": "PermutatorBlock",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "description": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "peekOfCode": "class PermutatorBlock(nn.Module):\n    def __init__(self, dim, segment_dim, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, skip_lam=1.0, mlp_fn=MorphFC_S):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.t_norm1 = norm_layer(dim)\n        self.t_fc = MorphFC_T(dim, segment_dim=segment_dim, qkv_bias=qkv_bias, qk_scale=None,\n                                        attn_drop=attn_drop)\n        self.fc = mlp_fn(dim, segment_dim=segment_dim, qkv_bias=qkv_bias, qk_scale=None, attn_drop=attn_drop)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here",
        "detail": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "description": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "peekOfCode": "class PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        self.proj1 = conv_3xnxn(in_chans, embed_dim//2, kernel_size=3, stride=2)\n        self.norm1= nn.BatchNorm3d(embed_dim//2)\n        self.act=nn.GELU()\n        self.proj2 = conv_1xnxn(embed_dim//2, embed_dim, kernel_size=3, stride=2)\n        self.norm2 = nn.BatchNorm3d(embed_dim)",
        "detail": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "documentation": {}
    },
    {
        "label": "Downsample",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "description": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "peekOfCode": "class Downsample(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n    def __init__(self, in_embed_dim, out_embed_dim, patch_size):\n        super().__init__()\n        self.proj = conv_1xnxn(in_embed_dim, out_embed_dim, kernel_size=3, stride=2)\n        self.norm=nn.LayerNorm(out_embed_dim)\n    def forward(self, x):\n        x = x.permute(0, 4, 1, 2, 3)\n        x = self.proj(x)  # B, C, T, H, W",
        "detail": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "documentation": {}
    },
    {
        "label": "MorphMLP",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "description": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "peekOfCode": "class MorphMLP(nn.Module):\n    \"\"\" MorphMLP\n    \"\"\"\n    def __init__(self, cfg):\n        super().__init__()\n        global t_stride\n        num_classes = cfg.MODEL.NUM_CLASSES\n        img_size = cfg.DATA.TRAIN_CROP_SIZE\n        in_chans = cfg.DATA.INPUT_CHANNEL_NUM[0]\n        layers = cfg.MORPH.LAYERS",
        "detail": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "documentation": {}
    },
    {
        "label": "conv_3xnxn",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "description": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "peekOfCode": "def conv_3xnxn(inp, oup, kernel_size=3, stride=3):\n    return nn.Conv3d(inp, oup, (3, kernel_size, kernel_size), (2, stride, stride), (1, 1, 1))\ndef conv_1xnxn(inp, oup, kernel_size=3, stride=3):\n    return nn.Conv3d(inp, oup, (1, kernel_size, kernel_size), (1, stride, stride), (0, 1, 1))\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)",
        "detail": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "documentation": {}
    },
    {
        "label": "conv_1xnxn",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "description": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "peekOfCode": "def conv_1xnxn(inp, oup, kernel_size=3, stride=3):\n    return nn.Conv3d(inp, oup, (1, kernel_size, kernel_size), (1, stride, stride), (0, 1, 1))\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)",
        "detail": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "documentation": {}
    },
    {
        "label": "t_stride",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "description": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "peekOfCode": "t_stride = 1\nmodel_path = {\n    's_sip_nods_s4': '/mnt/WXRC0020/users/junhao.zhang/tmp/slowfast/tools/s_sip_nods.pth',\n}\ndef conv_3xnxn(inp, oup, kernel_size=3, stride=3):\n    return nn.Conv3d(inp, oup, (3, kernel_size, kernel_size), (2, stride, stride), (1, 1, 1))\ndef conv_1xnxn(inp, oup, kernel_size=3, stride=3):\n    return nn.Conv3d(inp, oup, (1, kernel_size, kernel_size), (1, stride, stride), (0, 1, 1))\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):",
        "detail": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "documentation": {}
    },
    {
        "label": "model_path",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "description": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "peekOfCode": "model_path = {\n    's_sip_nods_s4': '/mnt/WXRC0020/users/junhao.zhang/tmp/slowfast/tools/s_sip_nods.pth',\n}\ndef conv_3xnxn(inp, oup, kernel_size=3, stride=3):\n    return nn.Conv3d(inp, oup, (3, kernel_size, kernel_size), (2, stride, stride), (1, 1, 1))\ndef conv_1xnxn(inp, oup, kernel_size=3, stride=3):\n    return nn.Conv3d(inp, oup, (1, kernel_size, kernel_size), (1, stride, stride), (0, 1, 1))\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()",
        "detail": "MorphMLP.build.lib.slowfast.models.morphmlp",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "description": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "peekOfCode": "class Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n    def forward(self, x):",
        "detail": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "documentation": {}
    },
    {
        "label": "MorphFC_S2",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "description": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "peekOfCode": "class MorphFC_S2(nn.Module):\n    def __init__(self, dim, segment_dim=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.segment_dim = segment_dim\n        self.mlp_c = nn.Linear(dim, dim, bias=qkv_bias)\n        self.mlp_h = nn.Linear(dim, dim, bias=qkv_bias)\n        self.reweight = Mlp(dim, dim // 4, dim * 2)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n    def forward(self, x):",
        "detail": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "documentation": {}
    },
    {
        "label": "MorphFC_S",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "description": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "peekOfCode": "class MorphFC_S(nn.Module):\n    def __init__(self, dim, segment_dim=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.segment_dim = segment_dim\n        self.mlp_h = nn.Linear(dim, dim, bias=qkv_bias)\n        self.mlp_w = nn.Linear(dim, dim, bias=qkv_bias)\n        self.mlp_c = nn.Linear(dim, dim, bias=qkv_bias)\n        # init weight problem\n        self.reweight = Mlp(dim, dim // 4, dim * 3)\n        self.proj = nn.Linear(dim, dim)",
        "detail": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "documentation": {}
    },
    {
        "label": "MorphFC_T",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "description": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "peekOfCode": "class MorphFC_T(nn.Module):\n    def __init__(self, dim, segment_dim=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        global t_stride\n        self.segment_dim = 16\n        dim2 = dim\n        if dim == 392:\n            dim2 = 392 // 14 * 16\n            self.segment_dim=14\n        self.mlp_t = nn.Linear(dim2, dim2, bias=qkv_bias)",
        "detail": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "documentation": {}
    },
    {
        "label": "PermutatorBlock",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "description": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "peekOfCode": "class PermutatorBlock(nn.Module):\n    def __init__(self, dim, segment_dim, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, skip_lam=1.0, mlp_fn=MorphFC_S):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.t_norm1 = norm_layer(dim)\n        self.t_fc = MorphFC_T(dim, segment_dim=segment_dim, qkv_bias=qkv_bias, qk_scale=None,\n                                        attn_drop=attn_drop)\n        self.fc = mlp_fn(dim, segment_dim=segment_dim, qkv_bias=qkv_bias, qk_scale=None, attn_drop=attn_drop)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here",
        "detail": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "description": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "peekOfCode": "class PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        self.proj1 = conv_3xnxn(in_chans, embed_dim//2, kernel_size=3, stride=2)\n        self.norm1= nn.BatchNorm3d(embed_dim//2)\n        self.act=nn.GELU()\n        self.proj2 = conv_1xnxn(embed_dim//2, embed_dim, kernel_size=3, stride=2)\n        self.norm2 = nn.BatchNorm3d(embed_dim)",
        "detail": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "documentation": {}
    },
    {
        "label": "Downsample",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "description": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "peekOfCode": "class Downsample(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n    def __init__(self, in_embed_dim, out_embed_dim, patch_size):\n        super().__init__()\n        self.proj = conv_1xnxn(in_embed_dim, out_embed_dim, kernel_size=3, stride=2)\n        self.norm=nn.LayerNorm(out_embed_dim)\n    def forward(self, x):\n        x = x.permute(0, 4, 1, 2, 3)\n        x = self.proj(x)  # B, C, T, H, W",
        "detail": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "documentation": {}
    },
    {
        "label": "MorphMLP_32",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "description": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "peekOfCode": "class MorphMLP_32(nn.Module):\n    \"\"\" MorphMLP\n    \"\"\"\n    def __init__(self, cfg):\n        super().__init__()\n        global t_stride\n        num_classes = cfg.MODEL.NUM_CLASSES\n        img_size = cfg.DATA.TRAIN_CROP_SIZE\n        in_chans = cfg.DATA.INPUT_CHANNEL_NUM[0]\n        layers = cfg.MORPH.LAYERS",
        "detail": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "documentation": {}
    },
    {
        "label": "conv_3xnxn",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "description": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "peekOfCode": "def conv_3xnxn(inp, oup, kernel_size=3, stride=3):\n    return nn.Conv3d(inp, oup, (3, kernel_size, kernel_size), (2, stride, stride), (1, 1, 1))\ndef conv_1xnxn(inp, oup, kernel_size=3, stride=3):\n    return nn.Conv3d(inp, oup, (1, kernel_size, kernel_size), (1, stride, stride), (0, 1, 1))\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)",
        "detail": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "documentation": {}
    },
    {
        "label": "conv_1xnxn",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "description": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "peekOfCode": "def conv_1xnxn(inp, oup, kernel_size=3, stride=3):\n    return nn.Conv3d(inp, oup, (1, kernel_size, kernel_size), (1, stride, stride), (0, 1, 1))\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)",
        "detail": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "documentation": {}
    },
    {
        "label": "t_stride",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "description": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "peekOfCode": "t_stride = 1\ndef conv_3xnxn(inp, oup, kernel_size=3, stride=3):\n    return nn.Conv3d(inp, oup, (3, kernel_size, kernel_size), (2, stride, stride), (1, 1, 1))\ndef conv_1xnxn(inp, oup, kernel_size=3, stride=3):\n    return nn.Conv3d(inp, oup, (1, kernel_size, kernel_size), (1, stride, stride), (0, 1, 1))\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features",
        "detail": "MorphMLP.build.lib.slowfast.models.morphmlp_32",
        "documentation": {}
    },
    {
        "label": "Nonlocal",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.nonlocal_helper",
        "description": "MorphMLP.build.lib.slowfast.models.nonlocal_helper",
        "peekOfCode": "class Nonlocal(nn.Module):\n    \"\"\"\n    Builds Non-local Neural Networks as a generic family of building\n    blocks for capturing long-range dependencies. Non-local Network\n    computes the response at a position as a weighted sum of the\n    features at all positions. This building block can be plugged into\n    many computer vision architectures.\n    More details in the paper: https://arxiv.org/pdf/1711.07971.pdf\n    \"\"\"\n    def __init__(",
        "detail": "MorphMLP.build.lib.slowfast.models.nonlocal_helper",
        "documentation": {}
    },
    {
        "label": "SE",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.operators",
        "description": "MorphMLP.build.lib.slowfast.models.operators",
        "peekOfCode": "class SE(nn.Module):\n    \"\"\"Squeeze-and-Excitation (SE) block w/ Swish: AvgPool, FC, Swish, FC, Sigmoid.\"\"\"\n    def _round_width(self, width, multiplier, min_width=8, divisor=8):\n        \"\"\"\n        Round width of filters based on width multiplier\n        Args:\n            width (int): the channel dimensions of the input.\n            multiplier (float): the multiplication factor.\n            min_width (int): the minimum width after multiplication.\n            divisor (int): the new width should be dividable by divisor.",
        "detail": "MorphMLP.build.lib.slowfast.models.operators",
        "documentation": {}
    },
    {
        "label": "construct_optimizer",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.models.optimizer",
        "description": "MorphMLP.build.lib.slowfast.models.optimizer",
        "peekOfCode": "def construct_optimizer(model, cfg):\n    \"\"\"\n    Construct a stochastic gradient descent or ADAM optimizer with momentum.\n    Details can be found in:\n    Herbert Robbins, and Sutton Monro. \"A stochastic approximation method.\"\n    and\n    Diederik P.Kingma, and Jimmy Ba.\n    \"Adam: A Method for Stochastic Optimization.\"\n    Args:\n        model (model): model to perform stochastic gradient descent",
        "detail": "MorphMLP.build.lib.slowfast.models.optimizer",
        "documentation": {}
    },
    {
        "label": "get_epoch_lr",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.models.optimizer",
        "description": "MorphMLP.build.lib.slowfast.models.optimizer",
        "peekOfCode": "def get_epoch_lr(cur_epoch, cfg):\n    \"\"\"\n    Retrieves the lr for the given epoch (as specified by the lr policy).\n    Args:\n        cfg (config): configs of hyper-parameters of ADAM, includes base\n        learning rate, betas, and weight decay.\n        cur_epoch (float): the number of epoch of the current training stage.\n    \"\"\"\n    return lr_policy.get_lr_at_epoch(cfg, cur_epoch)\ndef set_lr(optimizer, new_lr):",
        "detail": "MorphMLP.build.lib.slowfast.models.optimizer",
        "documentation": {}
    },
    {
        "label": "set_lr",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.models.optimizer",
        "description": "MorphMLP.build.lib.slowfast.models.optimizer",
        "peekOfCode": "def set_lr(optimizer, new_lr):\n    \"\"\"\n    Sets the optimizer lr to the specified value.\n    Args:\n        optimizer (optim): the optimizer using to optimize the current network.\n        new_lr (float): the new learning rate to set.\n    \"\"\"\n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = new_lr",
        "detail": "MorphMLP.build.lib.slowfast.models.optimizer",
        "documentation": {}
    },
    {
        "label": "PTVResNet",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.ptv_model_builder",
        "description": "MorphMLP.build.lib.slowfast.models.ptv_model_builder",
        "peekOfCode": "class PTVResNet(nn.Module):\n    \"\"\"\n    ResNet models using PyTorchVideo model builder.\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        The `__init__` method of any subclass should also contain these\n            arguments.\n        Args:\n            cfg (CfgNode): model building configs, details are in the",
        "detail": "MorphMLP.build.lib.slowfast.models.ptv_model_builder",
        "documentation": {}
    },
    {
        "label": "PTVSlowFast",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.ptv_model_builder",
        "description": "MorphMLP.build.lib.slowfast.models.ptv_model_builder",
        "peekOfCode": "class PTVSlowFast(nn.Module):\n    def __init__(self, cfg):\n        \"\"\"\n        The `__init__` method of any subclass should also contain these\n            arguments.\n        Args:\n            cfg (CfgNode): model building configs, details are in the\n                comments of the config file.\n        \"\"\"\n        super(PTVSlowFast, self).__init__()",
        "detail": "MorphMLP.build.lib.slowfast.models.ptv_model_builder",
        "documentation": {}
    },
    {
        "label": "PTVX3D",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.ptv_model_builder",
        "description": "MorphMLP.build.lib.slowfast.models.ptv_model_builder",
        "peekOfCode": "class PTVX3D(nn.Module):\n    def __init__(self, cfg):\n        \"\"\"\n        The `__init__` method of any subclass should also contain these\n            arguments.\n        Args:\n            cfg (CfgNode): model building configs, details are in the\n                comments of the config file.\n        \"\"\"\n        super(PTVX3D, self).__init__()",
        "detail": "MorphMLP.build.lib.slowfast.models.ptv_model_builder",
        "documentation": {}
    },
    {
        "label": "PTVCSN",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.ptv_model_builder",
        "description": "MorphMLP.build.lib.slowfast.models.ptv_model_builder",
        "peekOfCode": "class PTVCSN(nn.Module):\n    \"\"\"\n    CSN models using PyTorchVideo model builder.\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        The `__init__` method of any subclass should also contain these\n            arguments.\n        Args:\n            cfg (CfgNode): model building configs, details are in the",
        "detail": "MorphMLP.build.lib.slowfast.models.ptv_model_builder",
        "documentation": {}
    },
    {
        "label": "PTVR2plus1D",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.ptv_model_builder",
        "description": "MorphMLP.build.lib.slowfast.models.ptv_model_builder",
        "peekOfCode": "class PTVR2plus1D(nn.Module):\n    \"\"\"\n    R(2+1)D models using PyTorchVideo model builder.\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        The `__init__` method of any subclass should also contain these\n            arguments.\n        Args:\n            cfg (CfgNode): model building configs, details are in the",
        "detail": "MorphMLP.build.lib.slowfast.models.ptv_model_builder",
        "documentation": {}
    },
    {
        "label": "PTVMViT",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.ptv_model_builder",
        "description": "MorphMLP.build.lib.slowfast.models.ptv_model_builder",
        "peekOfCode": "class PTVMViT(nn.Module):\n    \"\"\"\n    MViT models using PyTorchVideo model builder.\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        The `__init__` method of any subclass should also contain these\n            arguments.\n        Args:\n            cfg (CfgNode): model building configs, details are in the",
        "detail": "MorphMLP.build.lib.slowfast.models.ptv_model_builder",
        "documentation": {}
    },
    {
        "label": "get_head_act",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.models.ptv_model_builder",
        "description": "MorphMLP.build.lib.slowfast.models.ptv_model_builder",
        "peekOfCode": "def get_head_act(act_func):\n    \"\"\"\n    Return the actual head activation function given the activation fucntion name.\n    Args:\n        act_func (string): activation function to use. 'softmax': applies\n        softmax on the output. 'sigmoid': applies sigmoid on the output.\n    Returns:\n        nn.Module: the activation layer.\n    \"\"\"\n    if act_func == \"softmax\":",
        "detail": "MorphMLP.build.lib.slowfast.models.ptv_model_builder",
        "documentation": {}
    },
    {
        "label": "BasicTransform",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.resnet_helper",
        "description": "MorphMLP.build.lib.slowfast.models.resnet_helper",
        "peekOfCode": "class BasicTransform(nn.Module):\n    \"\"\"\n    Basic transformation: Tx3x3, 1x3x3, where T is the size of temporal kernel.\n    \"\"\"\n    def __init__(\n        self,\n        dim_in,\n        dim_out,\n        temp_kernel_size,\n        stride,",
        "detail": "MorphMLP.build.lib.slowfast.models.resnet_helper",
        "documentation": {}
    },
    {
        "label": "X3DTransform",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.resnet_helper",
        "description": "MorphMLP.build.lib.slowfast.models.resnet_helper",
        "peekOfCode": "class X3DTransform(nn.Module):\n    \"\"\"\n    X3D transformation: 1x1x1, Tx3x3 (channelwise, num_groups=dim_in), 1x1x1,\n        augmented with (optional) SE (squeeze-excitation) on the 3x3x3 output.\n        T is the temporal kernel size (defaulting to 3)\n    \"\"\"\n    def __init__(\n        self,\n        dim_in,\n        dim_out,",
        "detail": "MorphMLP.build.lib.slowfast.models.resnet_helper",
        "documentation": {}
    },
    {
        "label": "BottleneckTransform",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.resnet_helper",
        "description": "MorphMLP.build.lib.slowfast.models.resnet_helper",
        "peekOfCode": "class BottleneckTransform(nn.Module):\n    \"\"\"\n    Bottleneck transformation: Tx1x1, 1x3x3, 1x1x1, where T is the size of\n        temporal kernel.\n    \"\"\"\n    def __init__(\n        self,\n        dim_in,\n        dim_out,\n        temp_kernel_size,",
        "detail": "MorphMLP.build.lib.slowfast.models.resnet_helper",
        "documentation": {}
    },
    {
        "label": "ResBlock",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.resnet_helper",
        "description": "MorphMLP.build.lib.slowfast.models.resnet_helper",
        "peekOfCode": "class ResBlock(nn.Module):\n    \"\"\"\n    Residual block.\n    \"\"\"\n    def __init__(\n        self,\n        dim_in,\n        dim_out,\n        temp_kernel_size,\n        stride,",
        "detail": "MorphMLP.build.lib.slowfast.models.resnet_helper",
        "documentation": {}
    },
    {
        "label": "ResStage",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.resnet_helper",
        "description": "MorphMLP.build.lib.slowfast.models.resnet_helper",
        "peekOfCode": "class ResStage(nn.Module):\n    \"\"\"\n    Stage of 3D ResNet. It expects to have one or more tensors as input for\n        single pathway (C2D, I3D, Slow), and multi-pathway (SlowFast) cases.\n        More details can be found here:\n        Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He.\n        \"SlowFast networks for video recognition.\"\n        https://arxiv.org/pdf/1812.03982.pdf\n    \"\"\"\n    def __init__(",
        "detail": "MorphMLP.build.lib.slowfast.models.resnet_helper",
        "documentation": {}
    },
    {
        "label": "get_trans_func",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.models.resnet_helper",
        "description": "MorphMLP.build.lib.slowfast.models.resnet_helper",
        "peekOfCode": "def get_trans_func(name):\n    \"\"\"\n    Retrieves the transformation module by name.\n    \"\"\"\n    trans_funcs = {\n        \"bottleneck_transform\": BottleneckTransform,\n        \"basic_transform\": BasicTransform,\n        \"x3d_transform\": X3DTransform,\n    }\n    assert (",
        "detail": "MorphMLP.build.lib.slowfast.models.resnet_helper",
        "documentation": {}
    },
    {
        "label": "VideoModelStem",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.stem_helper",
        "description": "MorphMLP.build.lib.slowfast.models.stem_helper",
        "peekOfCode": "class VideoModelStem(nn.Module):\n    \"\"\"\n    Video 3D stem module. Provides stem operations of Conv, BN, ReLU, MaxPool\n    on input data tensor for one or multiple pathways.\n    \"\"\"\n    def __init__(\n        self,\n        dim_in,\n        dim_out,\n        kernel,",
        "detail": "MorphMLP.build.lib.slowfast.models.stem_helper",
        "documentation": {}
    },
    {
        "label": "ResNetBasicStem",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.stem_helper",
        "description": "MorphMLP.build.lib.slowfast.models.stem_helper",
        "peekOfCode": "class ResNetBasicStem(nn.Module):\n    \"\"\"\n    ResNe(X)t 3D stem module.\n    Performs spatiotemporal Convolution, BN, and Relu following by a\n        spatiotemporal pooling.\n    \"\"\"\n    def __init__(\n        self,\n        dim_in,\n        dim_out,",
        "detail": "MorphMLP.build.lib.slowfast.models.stem_helper",
        "documentation": {}
    },
    {
        "label": "X3DStem",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.stem_helper",
        "description": "MorphMLP.build.lib.slowfast.models.stem_helper",
        "peekOfCode": "class X3DStem(nn.Module):\n    \"\"\"\n    X3D's 3D stem module.\n    Performs a spatial followed by a depthwise temporal Convolution, BN, and Relu following by a\n        spatiotemporal pooling.\n    \"\"\"\n    def __init__(\n        self,\n        dim_in,\n        dim_out,",
        "detail": "MorphMLP.build.lib.slowfast.models.stem_helper",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.stem_helper",
        "description": "MorphMLP.build.lib.slowfast.models.stem_helper",
        "peekOfCode": "class PatchEmbed(nn.Module):\n    \"\"\"\n    PatchEmbed.\n    \"\"\"\n    def __init__(\n        self,\n        dim_in=3,\n        dim_out=768,\n        kernel=(1, 16, 16),\n        stride=(1, 4, 4),",
        "detail": "MorphMLP.build.lib.slowfast.models.stem_helper",
        "documentation": {}
    },
    {
        "label": "get_stem_func",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.models.stem_helper",
        "description": "MorphMLP.build.lib.slowfast.models.stem_helper",
        "peekOfCode": "def get_stem_func(name):\n    \"\"\"\n    Retrieves the stem module by name.\n    \"\"\"\n    trans_funcs = {\"x3d_stem\": X3DStem, \"basic_stem\": ResNetBasicStem}\n    assert (\n        name in trans_funcs.keys()\n    ), \"Transformation function '{}' not supported\".format(name)\n    return trans_funcs[name]\nclass VideoModelStem(nn.Module):",
        "detail": "MorphMLP.build.lib.slowfast.models.stem_helper",
        "documentation": {}
    },
    {
        "label": "round_width",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.models.utils",
        "description": "MorphMLP.build.lib.slowfast.models.utils",
        "peekOfCode": "def round_width(width, multiplier, min_width=1, divisor=1, verbose=False):\n    if not multiplier:\n        return width\n    width *= multiplier\n    min_width = min_width or divisor\n    if verbose:\n        logger.info(f\"min width {min_width}\")\n        logger.info(f\"width {width} divisor {divisor}\")\n        logger.info(f\"other {int(width + divisor / 2) // divisor * divisor}\")\n    width_out = max(min_width, int(width + divisor / 2) // divisor * divisor)",
        "detail": "MorphMLP.build.lib.slowfast.models.utils",
        "documentation": {}
    },
    {
        "label": "validate_checkpoint_wrapper_import",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.models.utils",
        "description": "MorphMLP.build.lib.slowfast.models.utils",
        "peekOfCode": "def validate_checkpoint_wrapper_import(checkpoint_wrapper):\n    \"\"\"\n    Check if checkpoint_wrapper is imported.\n    \"\"\"\n    if checkpoint_wrapper is None:\n        raise ImportError(\"Please install fairscale.\")",
        "detail": "MorphMLP.build.lib.slowfast.models.utils",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.models.utils",
        "description": "MorphMLP.build.lib.slowfast.models.utils",
        "peekOfCode": "logger = logging.get_logger(__name__)\ndef round_width(width, multiplier, min_width=1, divisor=1, verbose=False):\n    if not multiplier:\n        return width\n    width *= multiplier\n    min_width = min_width or divisor\n    if verbose:\n        logger.info(f\"min width {min_width}\")\n        logger.info(f\"width {width} divisor {divisor}\")\n        logger.info(f\"other {int(width + divisor / 2) // divisor * divisor}\")",
        "detail": "MorphMLP.build.lib.slowfast.models.utils",
        "documentation": {}
    },
    {
        "label": "FuseFastToSlow",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.video_model_builder",
        "description": "MorphMLP.build.lib.slowfast.models.video_model_builder",
        "peekOfCode": "class FuseFastToSlow(nn.Module):\n    \"\"\"\n    Fuses the information from the Fast pathway to the Slow pathway. Given the\n    tensors from Slow pathway and Fast pathway, fuse information from Fast to\n    Slow, then return the fused tensors from Slow and Fast pathway in order.\n    \"\"\"\n    def __init__(\n        self,\n        dim_in,\n        fusion_conv_channel_ratio,",
        "detail": "MorphMLP.build.lib.slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "SlowFast",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.video_model_builder",
        "description": "MorphMLP.build.lib.slowfast.models.video_model_builder",
        "peekOfCode": "class SlowFast(nn.Module):\n    \"\"\"\n    SlowFast model builder for SlowFast network.\n    Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He.\n    \"SlowFast networks for video recognition.\"\n    https://arxiv.org/pdf/1812.03982.pdf\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        The `__init__` method of any subclass should also contain these",
        "detail": "MorphMLP.build.lib.slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "ResNet",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.video_model_builder",
        "description": "MorphMLP.build.lib.slowfast.models.video_model_builder",
        "peekOfCode": "class ResNet(nn.Module):\n    \"\"\"\n    ResNet model builder. It builds a ResNet like network backbone without\n    lateral connection (C2D, I3D, Slow).\n    Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He.\n    \"SlowFast networks for video recognition.\"\n    https://arxiv.org/pdf/1812.03982.pdf\n    Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He.\n    \"Non-local neural networks.\"\n    https://arxiv.org/pdf/1711.07971.pdf",
        "detail": "MorphMLP.build.lib.slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "X3D",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.video_model_builder",
        "description": "MorphMLP.build.lib.slowfast.models.video_model_builder",
        "peekOfCode": "class X3D(nn.Module):\n    \"\"\"\n    X3D model builder. It builds a X3D network backbone, which is a ResNet.\n    Christoph Feichtenhofer.\n    \"X3D: Expanding Architectures for Efficient Video Recognition.\"\n    https://arxiv.org/abs/2004.04730\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        The `__init__` method of any subclass should also contain these",
        "detail": "MorphMLP.build.lib.slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "MViT",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.models.video_model_builder",
        "description": "MorphMLP.build.lib.slowfast.models.video_model_builder",
        "peekOfCode": "class MViT(nn.Module):\n    \"\"\"\n    Model builder for MViTv1 and MViTv2.\n    \"MViTv2: Improved Multiscale Vision Transformers for Classification and Detection\"\n    Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, Christoph Feichtenhofer\n    https://arxiv.org/abs/2112.01526\n    \"Multiscale Vision Transformers\"\n    Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, Christoph Feichtenhofer\n    https://arxiv.org/abs/2104.11227\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "_MODEL_STAGE_DEPTH",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.models.video_model_builder",
        "description": "MorphMLP.build.lib.slowfast.models.video_model_builder",
        "peekOfCode": "_MODEL_STAGE_DEPTH = {18: (2, 2, 2, 2), 50: (3, 4, 6, 3), 101: (3, 4, 23, 3)}\n# Basis of temporal kernel sizes for each of the stage.\n_TEMPORAL_KERNEL_BASIS = {\n    \"2d\": [\n        [[1]],  # conv1 temporal kernel.\n        [[1]],  # res2 temporal kernel.\n        [[1]],  # res3 temporal kernel.\n        [[1]],  # res4 temporal kernel.\n        [[1]],  # res5 temporal kernel.\n    ],",
        "detail": "MorphMLP.build.lib.slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "_TEMPORAL_KERNEL_BASIS",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.models.video_model_builder",
        "description": "MorphMLP.build.lib.slowfast.models.video_model_builder",
        "peekOfCode": "_TEMPORAL_KERNEL_BASIS = {\n    \"2d\": [\n        [[1]],  # conv1 temporal kernel.\n        [[1]],  # res2 temporal kernel.\n        [[1]],  # res3 temporal kernel.\n        [[1]],  # res4 temporal kernel.\n        [[1]],  # res5 temporal kernel.\n    ],\n    \"c2d\": [\n        [[1]],  # conv1 temporal kernel.",
        "detail": "MorphMLP.build.lib.slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "_POOL1",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.models.video_model_builder",
        "description": "MorphMLP.build.lib.slowfast.models.video_model_builder",
        "peekOfCode": "_POOL1 = {\n    \"2d\": [[1, 1, 1]],\n    \"c2d\": [[2, 1, 1]],\n    \"slow_c2d\": [[1, 1, 1]],\n    \"i3d\": [[2, 1, 1]],\n    \"slow_i3d\": [[1, 1, 1]],\n    \"slow\": [[1, 1, 1]],\n    \"slowfast\": [[1, 1, 1], [1, 1, 1]],\n    \"x3d\": [[1, 1, 1]],\n}",
        "detail": "MorphMLP.build.lib.slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "create_category_index",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.label_map_util",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.label_map_util",
        "peekOfCode": "def create_category_index(categories):\n    \"\"\"Creates dictionary of COCO compatible categories keyed by category id.\n    Args:\n      categories: a list of dicts, each of which has the following keys:\n        'id': (required) an integer id uniquely identifying this category.\n        'name': (required) string representing category name\n          e.g., 'cat', 'dog', 'pizza'.\n    Returns:\n      category_index: a dict containing the same entries as categories, but keyed\n        by the 'id' field of each category.",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.label_map_util",
        "documentation": {}
    },
    {
        "label": "get_max_label_map_index",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.label_map_util",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.label_map_util",
        "peekOfCode": "def get_max_label_map_index(label_map):\n    \"\"\"Get maximum index in label map.\n    Args:\n      label_map: a StringIntLabelMapProto\n    Returns:\n      an integer\n    \"\"\"\n    return max([item.id for item in label_map.item])\ndef convert_label_map_to_categories(\n    label_map, max_num_classes, use_display_name=True",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.label_map_util",
        "documentation": {}
    },
    {
        "label": "convert_label_map_to_categories",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.label_map_util",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.label_map_util",
        "peekOfCode": "def convert_label_map_to_categories(\n    label_map, max_num_classes, use_display_name=True\n):\n    \"\"\"Loads label map proto and returns categories list compatible with eval.\n    This function loads a label map and returns a list of dicts, each of which\n    has the following keys:\n      'id': (required) an integer id uniquely identifying this category.\n      'name': (required) string representing category name\n        e.g., 'cat', 'dog', 'pizza'.\n    We only allow class into the list if its id-label_id_offset is",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.label_map_util",
        "documentation": {}
    },
    {
        "label": "load_labelmap",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.label_map_util",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.label_map_util",
        "peekOfCode": "def load_labelmap(path):\n    \"\"\"Loads label map proto.\n    Args:\n      path: path to StringIntLabelMap proto text file.\n    Returns:\n      a StringIntLabelMapProto\n    \"\"\"\n    with open(path, \"r\") as fid:\n        label_map_string = fid.read()\n        label_map = string_int_label_map_pb2.StringIntLabelMap()",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.label_map_util",
        "documentation": {}
    },
    {
        "label": "get_label_map_dict",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.label_map_util",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.label_map_util",
        "peekOfCode": "def get_label_map_dict(label_map_path, use_display_name=False):\n    \"\"\"Reads a label map and returns a dictionary of label names to id.\n    Args:\n      label_map_path: path to label_map.\n      use_display_name: whether to use the label map items' display names as keys.\n    Returns:\n      A dictionary mapping label names to id.\n    \"\"\"\n    label_map = load_labelmap(label_map_path)\n    label_map_dict = {}",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.label_map_util",
        "documentation": {}
    },
    {
        "label": "create_category_index_from_labelmap",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.label_map_util",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.label_map_util",
        "peekOfCode": "def create_category_index_from_labelmap(label_map_path):\n    \"\"\"Reads a label map and returns a category index.\n    Args:\n      label_map_path: Path to `StringIntLabelMap` proto text file.\n    Returns:\n      A category index, which is a dictionary that maps integer ids to dicts\n      containing categories, e.g.\n      {1: {'id': 1, 'name': 'dog'}, 2: {'id': 2, 'name': 'cat'}, ...}\n    \"\"\"\n    label_map = load_labelmap(label_map_path)",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.label_map_util",
        "documentation": {}
    },
    {
        "label": "create_class_agnostic_category_index",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.label_map_util",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.label_map_util",
        "peekOfCode": "def create_class_agnostic_category_index():\n    \"\"\"Creates a category index with a single `object` class.\"\"\"\n    return {1: {\"id\": 1, \"name\": \"object\"}}",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.label_map_util",
        "documentation": {}
    },
    {
        "label": "compute_precision_recall",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.metrics",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.metrics",
        "peekOfCode": "def compute_precision_recall(scores, labels, num_gt):\n    \"\"\"Compute precision and recall.\n    Args:\n      scores: A float numpy array representing detection score\n      labels: A boolean numpy array representing true/false positive labels\n      num_gt: Number of ground truth instances\n    Raises:\n      ValueError: if the input is not of the correct format\n    Returns:\n      precision: Fraction of positive instances over detected ones. This value is",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.metrics",
        "documentation": {}
    },
    {
        "label": "compute_average_precision",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.metrics",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.metrics",
        "peekOfCode": "def compute_average_precision(precision, recall):\n    \"\"\"Compute Average Precision according to the definition in VOCdevkit.\n    Precision is modified to ensure that it does not decrease as recall\n    decrease.\n    Args:\n      precision: A float [N, 1] numpy array of precisions\n      recall: A float [N, 1] numpy array of recalls\n    Raises:\n      ValueError: if the input is not of the correct format\n    Returns:",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.metrics",
        "documentation": {}
    },
    {
        "label": "compute_cor_loc",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.metrics",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.metrics",
        "peekOfCode": "def compute_cor_loc(\n    num_gt_imgs_per_class, num_images_correctly_detected_per_class\n):\n    \"\"\"Compute CorLoc according to the definition in the following paper.\n    https://www.robots.ox.ac.uk/~vgg/rg/papers/deselaers-eccv10.pdf\n    Returns nans if there are no ground truth images for a class.\n    Args:\n      num_gt_imgs_per_class: 1D array, representing number of images containing\n          at least one object instance of a particular class\n      num_images_correctly_detected_per_class: 1D array, representing number of",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.metrics",
        "documentation": {}
    },
    {
        "label": "BoxList",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list",
        "peekOfCode": "class BoxList(object):\n    \"\"\"Box collection.\n    BoxList represents a list of bounding boxes as numpy array, where each\n    bounding box is represented as a row of 4 numbers,\n    [y_min, x_min, y_max, x_max].  It is assumed that all bounding boxes within a\n    given list correspond to a single image.\n    Optionally, users can add additional related fields (such as\n    objectness/classification scores).\n    \"\"\"\n    def __init__(self, data):",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list",
        "documentation": {}
    },
    {
        "label": "SortOrder",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "class SortOrder(object):\n    \"\"\"Enum class for sort order.\n    Attributes:\n      ascend: ascend order.\n      descend: descend order.\n    \"\"\"\n    ASCEND = 1\n    DESCEND = 2\ndef area(boxlist):\n    \"\"\"Computes area of boxes.",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "area",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def area(boxlist):\n    \"\"\"Computes area of boxes.\n    Args:\n      boxlist: BoxList holding N boxes\n    Returns:\n      a numpy array with shape [N*1] representing box areas\n    \"\"\"\n    y_min, x_min, y_max, x_max = boxlist.get_coordinates()\n    return (y_max - y_min) * (x_max - x_min)\ndef intersection(boxlist1, boxlist2):",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "intersection",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def intersection(boxlist1, boxlist2):\n    \"\"\"Compute pairwise intersection areas between boxes.\n    Args:\n      boxlist1: BoxList holding N boxes\n      boxlist2: BoxList holding M boxes\n    Returns:\n      a numpy array with shape [N*M] representing pairwise intersection area\n    \"\"\"\n    return np_box_ops.intersection(boxlist1.get(), boxlist2.get())\ndef iou(boxlist1, boxlist2):",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "iou",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def iou(boxlist1, boxlist2):\n    \"\"\"Computes pairwise intersection-over-union between box collections.\n    Args:\n      boxlist1: BoxList holding N boxes\n      boxlist2: BoxList holding M boxes\n    Returns:\n      a numpy array with shape [N, M] representing pairwise iou scores.\n    \"\"\"\n    return np_box_ops.iou(boxlist1.get(), boxlist2.get())\ndef ioa(boxlist1, boxlist2):",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "ioa",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def ioa(boxlist1, boxlist2):\n    \"\"\"Computes pairwise intersection-over-area between box collections.\n    Intersection-over-area (ioa) between two boxes box1 and box2 is defined as\n    their intersection area over box2's area. Note that ioa is not symmetric,\n    that is, IOA(box1, box2) != IOA(box2, box1).\n    Args:\n      boxlist1: BoxList holding N boxes\n      boxlist2: BoxList holding M boxes\n    Returns:\n      a numpy array with shape [N, M] representing pairwise ioa scores.",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "gather",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def gather(boxlist, indices, fields=None):\n    \"\"\"Gather boxes from BoxList according to indices and return new BoxList.\n    By default, gather returns boxes corresponding to the input index list, as\n    well as all additional fields stored in the boxlist (indexing into the\n    first dimension).  However one can optionally only gather from a\n    subset of fields.\n    Args:\n      boxlist: BoxList holding N boxes\n      indices: a 1-d numpy array of type int_\n      fields: (optional) list of fields to also gather from.  If None (default),",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "sort_by_field",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def sort_by_field(boxlist, field, order=SortOrder.DESCEND):\n    \"\"\"Sort boxes and associated fields according to a scalar field.\n    A common use case is reordering the boxes according to descending scores.\n    Args:\n      boxlist: BoxList holding N boxes.\n      field: A BoxList field for sorting and reordering the BoxList.\n      order: (Optional) 'descend' or 'ascend'. Default is descend.\n    Returns:\n      sorted_boxlist: A sorted BoxList with the field in the specified order.\n    Raises:",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "non_max_suppression",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def non_max_suppression(\n    boxlist, max_output_size=10000, iou_threshold=1.0, score_threshold=-10.0\n):\n    \"\"\"Non maximum suppression.\n    This op greedily selects a subset of detection bounding boxes, pruning\n    away boxes that have high IOU (intersection over union) overlap (> thresh)\n    with already selected boxes. In each iteration, the detected bounding box with\n    highest score in the available pool is selected.\n    Args:\n      boxlist: BoxList holding N boxes.  Must contain a 'scores' field",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "multi_class_non_max_suppression",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def multi_class_non_max_suppression(\n    boxlist, score_thresh, iou_thresh, max_output_size\n):\n    \"\"\"Multi-class version of non maximum suppression.\n    This op greedily selects a subset of detection bounding boxes, pruning\n    away boxes that have high IOU (intersection over union) overlap (> thresh)\n    with already selected boxes.  It operates independently for each class for\n    which scores are provided (via the scores field of the input box_list),\n    pruning boxes with score less than a provided threshold prior to\n    applying NMS.",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "scale",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def scale(boxlist, y_scale, x_scale):\n    \"\"\"Scale box coordinates in x and y dimensions.\n    Args:\n      boxlist: BoxList holding N boxes\n      y_scale: float\n      x_scale: float\n    Returns:\n      boxlist: BoxList holding N boxes\n    \"\"\"\n    y_min, x_min, y_max, x_max = np.array_split(boxlist.get(), 4, axis=1)",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "clip_to_window",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def clip_to_window(boxlist, window):\n    \"\"\"Clip bounding boxes to a window.\n    This op clips input bounding boxes (represented by bounding box\n    corners) to a window, optionally filtering out boxes that do not\n    overlap at all with the window.\n    Args:\n      boxlist: BoxList holding M_in boxes\n      window: a numpy array of shape [4] representing the\n              [y_min, x_min, y_max, x_max] window to which the op\n              should clip boxes.",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "prune_non_overlapping_boxes",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def prune_non_overlapping_boxes(boxlist1, boxlist2, minoverlap=0.0):\n    \"\"\"Prunes the boxes in boxlist1 that overlap less than thresh with boxlist2.\n    For each box in boxlist1, we want its IOA to be more than minoverlap with\n    at least one of the boxes in boxlist2. If it does not, we remove it.\n    Args:\n      boxlist1: BoxList holding N boxes.\n      boxlist2: BoxList holding M boxes.\n      minoverlap: Minimum required overlap between boxes, to count them as\n                  overlapping.\n    Returns:",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "prune_outside_window",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def prune_outside_window(boxlist, window):\n    \"\"\"Prunes bounding boxes that fall outside a given window.\n    This function prunes bounding boxes that even partially fall outside the given\n    window. See also ClipToWindow which only prunes bounding boxes that fall\n    completely outside the window, and clips any bounding boxes that partially\n    overflow.\n    Args:\n      boxlist: a BoxList holding M_in boxes.\n      window: a numpy array of size 4, representing [ymin, xmin, ymax, xmax]\n              of the window.",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "concatenate",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def concatenate(boxlists, fields=None):\n    \"\"\"Concatenate list of BoxLists.\n    This op concatenates a list of input BoxLists into a larger BoxList.  It also\n    handles concatenation of BoxList fields as long as the field tensor shapes\n    are equal except for the first dimension.\n    Args:\n      boxlists: list of BoxList objects\n      fields: optional list of fields to also concatenate.  By default, all\n        fields from the first BoxList in the list are included in the\n        concatenation.",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "filter_scores_greater_than",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def filter_scores_greater_than(boxlist, thresh):\n    \"\"\"Filter to keep only boxes with score exceeding a given threshold.\n    This op keeps the collection of boxes whose corresponding scores are\n    greater than the input threshold.\n    Args:\n      boxlist: BoxList holding N boxes.  Must contain a 'scores' field\n        representing detection scores.\n      thresh: scalar threshold\n    Returns:\n      a BoxList holding M boxes where M <= N",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "change_coordinate_frame",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def change_coordinate_frame(boxlist, window):\n    \"\"\"Change coordinate frame of the boxlist to be relative to window's frame.\n    Given a window of the form [ymin, xmin, ymax, xmax],\n    changes bounding box coordinates from boxlist to be relative to this window\n    (e.g., the min corner maps to (0,0) and the max corner maps to (1,1)).\n    An example use case is data augmentation: where we are given groundtruth\n    boxes (boxlist) and would like to randomly crop the image to some\n    window (window). In this case we need to change the coordinate frame of\n    each groundtruth box to be relative to this new window.\n    Args:",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "BoxMaskList",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list",
        "peekOfCode": "class BoxMaskList(np_box_list.BoxList):\n    \"\"\"Convenience wrapper for BoxList with masks.\n    BoxMaskList extends the np_box_list.BoxList to contain masks as well.\n    In particular, its constructor receives both boxes and masks. Note that the\n    masks correspond to the full image.\n    \"\"\"\n    def __init__(self, box_data, mask_data):\n        \"\"\"Constructs box collection.\n        Args:\n          box_data: a numpy array of shape [N, 4] representing box coordinates",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list",
        "documentation": {}
    },
    {
        "label": "box_list_to_box_mask_list",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "peekOfCode": "def box_list_to_box_mask_list(boxlist):\n    \"\"\"Converts a BoxList containing 'masks' into a BoxMaskList.\n    Args:\n      boxlist: An np_box_list.BoxList object.\n    Returns:\n      An np_box_mask_list.BoxMaskList object.\n    Raises:\n      ValueError: If boxlist does not contain `masks` as a field.\n    \"\"\"\n    if not boxlist.has_field(\"masks\"):",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "documentation": {}
    },
    {
        "label": "area",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "peekOfCode": "def area(box_mask_list):\n    \"\"\"Computes area of masks.\n    Args:\n      box_mask_list: np_box_mask_list.BoxMaskList holding N boxes and masks\n    Returns:\n      a numpy array with shape [N*1] representing mask areas\n    \"\"\"\n    return np_mask_ops.area(box_mask_list.get_masks())\ndef intersection(box_mask_list1, box_mask_list2):\n    \"\"\"Compute pairwise intersection areas between masks.",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "documentation": {}
    },
    {
        "label": "intersection",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "peekOfCode": "def intersection(box_mask_list1, box_mask_list2):\n    \"\"\"Compute pairwise intersection areas between masks.\n    Args:\n      box_mask_list1: BoxMaskList holding N boxes and masks\n      box_mask_list2: BoxMaskList holding M boxes and masks\n    Returns:\n      a numpy array with shape [N*M] representing pairwise intersection area\n    \"\"\"\n    return np_mask_ops.intersection(\n        box_mask_list1.get_masks(), box_mask_list2.get_masks()",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "documentation": {}
    },
    {
        "label": "iou",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "peekOfCode": "def iou(box_mask_list1, box_mask_list2):\n    \"\"\"Computes pairwise intersection-over-union between box and mask collections.\n    Args:\n      box_mask_list1: BoxMaskList holding N boxes and masks\n      box_mask_list2: BoxMaskList holding M boxes and masks\n    Returns:\n      a numpy array with shape [N, M] representing pairwise iou scores.\n    \"\"\"\n    return np_mask_ops.iou(\n        box_mask_list1.get_masks(), box_mask_list2.get_masks()",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "documentation": {}
    },
    {
        "label": "ioa",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "peekOfCode": "def ioa(box_mask_list1, box_mask_list2):\n    \"\"\"Computes pairwise intersection-over-area between box and mask collections.\n    Intersection-over-area (ioa) between two masks mask1 and mask2 is defined as\n    their intersection area over mask2's area. Note that ioa is not symmetric,\n    that is, IOA(mask1, mask2) != IOA(mask2, mask1).\n    Args:\n      box_mask_list1: np_box_mask_list.BoxMaskList holding N boxes and masks\n      box_mask_list2: np_box_mask_list.BoxMaskList holding M boxes and masks\n    Returns:\n      a numpy array with shape [N, M] representing pairwise ioa scores.",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "documentation": {}
    },
    {
        "label": "gather",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "peekOfCode": "def gather(box_mask_list, indices, fields=None):\n    \"\"\"Gather boxes from np_box_mask_list.BoxMaskList according to indices.\n    By default, gather returns boxes corresponding to the input index list, as\n    well as all additional fields stored in the box_mask_list (indexing into the\n    first dimension).  However one can optionally only gather from a\n    subset of fields.\n    Args:\n      box_mask_list: np_box_mask_list.BoxMaskList holding N boxes\n      indices: a 1-d numpy array of type int_\n      fields: (optional) list of fields to also gather from.  If None (default),",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "documentation": {}
    },
    {
        "label": "sort_by_field",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "peekOfCode": "def sort_by_field(\n    box_mask_list, field, order=np_box_list_ops.SortOrder.DESCEND\n):\n    \"\"\"Sort boxes and associated fields according to a scalar field.\n    A common use case is reordering the boxes according to descending scores.\n    Args:\n      box_mask_list: BoxMaskList holding N boxes.\n      field: A BoxMaskList field for sorting and reordering the BoxMaskList.\n      order: (Optional) 'descend' or 'ascend'. Default is descend.\n    Returns:",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "documentation": {}
    },
    {
        "label": "non_max_suppression",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "peekOfCode": "def non_max_suppression(\n    box_mask_list,\n    max_output_size=10000,\n    iou_threshold=1.0,\n    score_threshold=-10.0,\n):\n    \"\"\"Non maximum suppression.\n    This op greedily selects a subset of detection bounding boxes, pruning\n    away boxes that have high IOU (intersection over union) overlap (> thresh)\n    with already selected boxes. In each iteration, the detected bounding box with",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "documentation": {}
    },
    {
        "label": "multi_class_non_max_suppression",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "peekOfCode": "def multi_class_non_max_suppression(\n    box_mask_list, score_thresh, iou_thresh, max_output_size\n):\n    \"\"\"Multi-class version of non maximum suppression.\n    This op greedily selects a subset of detection bounding boxes, pruning\n    away boxes that have high IOU (intersection over union) overlap (> thresh)\n    with already selected boxes.  It operates independently for each class for\n    which scores are provided (via the scores field of the input box_list),\n    pruning boxes with score less than a provided threshold prior to\n    applying NMS.",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "documentation": {}
    },
    {
        "label": "prune_non_overlapping_masks",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "peekOfCode": "def prune_non_overlapping_masks(box_mask_list1, box_mask_list2, minoverlap=0.0):\n    \"\"\"Prunes the boxes in list1 that overlap less than thresh with list2.\n    For each mask in box_mask_list1, we want its IOA to be more than minoverlap\n    with at least one of the masks in box_mask_list2. If it does not, we remove\n    it. If the masks are not full size image, we do the pruning based on boxes.\n    Args:\n      box_mask_list1: np_box_mask_list.BoxMaskList holding N boxes and masks.\n      box_mask_list2: np_box_mask_list.BoxMaskList holding M boxes and masks.\n      minoverlap: Minimum required overlap between boxes, to count them as\n                  overlapping.",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "documentation": {}
    },
    {
        "label": "concatenate",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "peekOfCode": "def concatenate(box_mask_lists, fields=None):\n    \"\"\"Concatenate list of box_mask_lists.\n    This op concatenates a list of input box_mask_lists into a larger\n    box_mask_list.  It also\n    handles concatenation of box_mask_list fields as long as the field tensor\n    shapes are equal except for the first dimension.\n    Args:\n      box_mask_lists: list of np_box_mask_list.BoxMaskList objects\n      fields: optional list of fields to also concatenate.  By default, all\n        fields from the first BoxMaskList in the list are included in the",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "documentation": {}
    },
    {
        "label": "filter_scores_greater_than",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "peekOfCode": "def filter_scores_greater_than(box_mask_list, thresh):\n    \"\"\"Filter to keep only boxes and masks with score exceeding a given threshold.\n    This op keeps the collection of boxes and masks whose corresponding scores are\n    greater than the input threshold.\n    Args:\n      box_mask_list: BoxMaskList holding N boxes and masks.  Must contain a\n        'scores' field representing detection scores.\n      thresh: scalar threshold\n    Returns:\n      a BoxMaskList holding M boxes and masks where M <= N",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "documentation": {}
    },
    {
        "label": "area",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_ops",
        "peekOfCode": "def area(boxes):\n    \"\"\"Computes area of boxes.\n    Args:\n      boxes: Numpy array with shape [N, 4] holding N boxes\n    Returns:\n      a numpy array with shape [N*1] representing box areas\n    \"\"\"\n    return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\ndef intersection(boxes1, boxes2):\n    \"\"\"Compute pairwise intersection areas between boxes.",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_ops",
        "documentation": {}
    },
    {
        "label": "intersection",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_ops",
        "peekOfCode": "def intersection(boxes1, boxes2):\n    \"\"\"Compute pairwise intersection areas between boxes.\n    Args:\n      boxes1: a numpy array with shape [N, 4] holding N boxes\n      boxes2: a numpy array with shape [M, 4] holding M boxes\n    Returns:\n      a numpy array with shape [N*M] representing pairwise intersection area\n    \"\"\"\n    [y_min1, x_min1, y_max1, x_max1] = np.split(boxes1, 4, axis=1)\n    [y_min2, x_min2, y_max2, x_max2] = np.split(boxes2, 4, axis=1)",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_ops",
        "documentation": {}
    },
    {
        "label": "iou",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_ops",
        "peekOfCode": "def iou(boxes1, boxes2):\n    \"\"\"Computes pairwise intersection-over-union between box collections.\n    Args:\n      boxes1: a numpy array with shape [N, 4] holding N boxes.\n      boxes2: a numpy array with shape [M, 4] holding N boxes.\n    Returns:\n      a numpy array with shape [N, M] representing pairwise iou scores.\n    \"\"\"\n    intersect = intersection(boxes1, boxes2)\n    area1 = area(boxes1)",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_ops",
        "documentation": {}
    },
    {
        "label": "ioa",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_ops",
        "peekOfCode": "def ioa(boxes1, boxes2):\n    \"\"\"Computes pairwise intersection-over-area between box collections.\n    Intersection-over-area (ioa) between two boxes box1 and box2 is defined as\n    their intersection area over box2's area. Note that ioa is not symmetric,\n    that is, IOA(box1, box2) != IOA(box2, box1).\n    Args:\n      boxes1: a numpy array with shape [N, 4] holding N boxes.\n      boxes2: a numpy array with shape [M, 4] holding N boxes.\n    Returns:\n      a numpy array with shape [N, M] representing pairwise ioa scores.",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_box_ops",
        "documentation": {}
    },
    {
        "label": "area",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_mask_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_mask_ops",
        "peekOfCode": "def area(masks):\n    \"\"\"Computes area of masks.\n    Args:\n      masks: Numpy array with shape [N, height, width] holding N masks. Masks\n        values are of type np.uint8 and values are in {0,1}.\n    Returns:\n      a numpy array with shape [N*1] representing mask areas.\n    Raises:\n      ValueError: If masks.dtype is not np.uint8\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_mask_ops",
        "documentation": {}
    },
    {
        "label": "intersection",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_mask_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_mask_ops",
        "peekOfCode": "def intersection(masks1, masks2):\n    \"\"\"Compute pairwise intersection areas between masks.\n    Args:\n      masks1: a numpy array with shape [N, height, width] holding N masks. Masks\n        values are of type np.uint8 and values are in {0,1}.\n      masks2: a numpy array with shape [M, height, width] holding M masks. Masks\n        values are of type np.uint8 and values are in {0,1}.\n    Returns:\n      a numpy array with shape [N*M] representing pairwise intersection area.\n    Raises:",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_mask_ops",
        "documentation": {}
    },
    {
        "label": "iou",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_mask_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_mask_ops",
        "peekOfCode": "def iou(masks1, masks2):\n    \"\"\"Computes pairwise intersection-over-union between mask collections.\n    Args:\n      masks1: a numpy array with shape [N, height, width] holding N masks. Masks\n        values are of type np.uint8 and values are in {0,1}.\n      masks2: a numpy array with shape [M, height, width] holding N masks. Masks\n        values are of type np.uint8 and values are in {0,1}.\n    Returns:\n      a numpy array with shape [N, M] representing pairwise iou scores.\n    Raises:",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_mask_ops",
        "documentation": {}
    },
    {
        "label": "ioa",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_mask_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_mask_ops",
        "peekOfCode": "def ioa(masks1, masks2):\n    \"\"\"Computes pairwise intersection-over-area between box collections.\n    Intersection-over-area (ioa) between two masks, mask1 and mask2 is defined as\n    their intersection area over mask2's area. Note that ioa is not symmetric,\n    that is, IOA(mask1, mask2) != IOA(mask2, mask1).\n    Args:\n      masks1: a numpy array with shape [N, height, width] holding N masks. Masks\n        values are of type np.uint8 and values are in {0,1}.\n      masks2: a numpy array with shape [M, height, width] holding N masks. Masks\n        values are of type np.uint8 and values are in {0,1}.",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_mask_ops",
        "documentation": {}
    },
    {
        "label": "EPSILON",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_mask_ops",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_mask_ops",
        "peekOfCode": "EPSILON = 1e-7\ndef area(masks):\n    \"\"\"Computes area of masks.\n    Args:\n      masks: Numpy array with shape [N, height, width] holding N masks. Masks\n        values are of type np.uint8 and values are in {0,1}.\n    Returns:\n      a numpy array with shape [N*1] representing mask areas.\n    Raises:\n      ValueError: If masks.dtype is not np.uint8",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.np_mask_ops",
        "documentation": {}
    },
    {
        "label": "DetectionEvaluator",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "peekOfCode": "class DetectionEvaluator(object):\n    \"\"\"Interface for object detection evalution classes.\n    Example usage of the Evaluator:\n    ------------------------------\n    evaluator = DetectionEvaluator(categories)\n    # Detections and groundtruth for image 1.\n    evaluator.add_single_groundtruth_image_info(...)\n    evaluator.add_single_detected_image_info(...)\n    # Detections and groundtruth for image 2.\n    evaluator.add_single_groundtruth_image_info(...)",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "documentation": {}
    },
    {
        "label": "ObjectDetectionEvaluator",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "peekOfCode": "class ObjectDetectionEvaluator(DetectionEvaluator):\n    \"\"\"A class to evaluate detections.\"\"\"\n    def __init__(\n        self,\n        categories,\n        matching_iou_threshold=0.5,\n        evaluate_corlocs=False,\n        metric_prefix=None,\n        use_weighted_mean_ap=False,\n        evaluate_masks=False,",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "documentation": {}
    },
    {
        "label": "PascalDetectionEvaluator",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "peekOfCode": "class PascalDetectionEvaluator(ObjectDetectionEvaluator):\n    \"\"\"A class to evaluate detections using PASCAL metrics.\"\"\"\n    def __init__(self, categories, matching_iou_threshold=0.5):\n        super(PascalDetectionEvaluator, self).__init__(\n            categories,\n            matching_iou_threshold=matching_iou_threshold,\n            evaluate_corlocs=False,\n            metric_prefix=\"PascalBoxes\",\n            use_weighted_mean_ap=False,\n        )",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "documentation": {}
    },
    {
        "label": "WeightedPascalDetectionEvaluator",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "peekOfCode": "class WeightedPascalDetectionEvaluator(ObjectDetectionEvaluator):\n    \"\"\"A class to evaluate detections using weighted PASCAL metrics.\n    Weighted PASCAL metrics computes the mean average precision as the average\n    precision given the scores and tp_fp_labels of all classes. In comparison,\n    PASCAL metrics computes the mean average precision as the mean of the\n    per-class average precisions.\n    This definition is very similar to the mean of the per-class average\n    precisions weighted by class frequency. However, they are typically not the\n    same as the average precision is not a linear function of the scores and\n    tp_fp_labels.",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "documentation": {}
    },
    {
        "label": "PascalInstanceSegmentationEvaluator",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "peekOfCode": "class PascalInstanceSegmentationEvaluator(ObjectDetectionEvaluator):\n    \"\"\"A class to evaluate instance masks using PASCAL metrics.\"\"\"\n    def __init__(self, categories, matching_iou_threshold=0.5):\n        super(PascalInstanceSegmentationEvaluator, self).__init__(\n            categories,\n            matching_iou_threshold=matching_iou_threshold,\n            evaluate_corlocs=False,\n            metric_prefix=\"PascalMasks\",\n            use_weighted_mean_ap=False,\n            evaluate_masks=True,",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "documentation": {}
    },
    {
        "label": "WeightedPascalInstanceSegmentationEvaluator",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "peekOfCode": "class WeightedPascalInstanceSegmentationEvaluator(ObjectDetectionEvaluator):\n    \"\"\"A class to evaluate instance masks using weighted PASCAL metrics.\n    Weighted PASCAL metrics computes the mean average precision as the average\n    precision given the scores and tp_fp_labels of all classes. In comparison,\n    PASCAL metrics computes the mean average precision as the mean of the\n    per-class average precisions.\n    This definition is very similar to the mean of the per-class average\n    precisions weighted by class frequency. However, they are typically not the\n    same as the average precision is not a linear function of the scores and\n    tp_fp_labels.",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "documentation": {}
    },
    {
        "label": "OpenImagesDetectionEvaluator",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "peekOfCode": "class OpenImagesDetectionEvaluator(ObjectDetectionEvaluator):\n    \"\"\"A class to evaluate detections using Open Images V2 metrics.\n    Open Images V2 introduce group_of type of bounding boxes and this metric\n    handles those boxes appropriately.\n    \"\"\"\n    def __init__(\n        self, categories, matching_iou_threshold=0.5, evaluate_corlocs=False\n    ):\n        \"\"\"Constructor.\n        Args:",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "documentation": {}
    },
    {
        "label": "ObjectDetectionEvaluation",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "peekOfCode": "class ObjectDetectionEvaluation(object):\n    \"\"\"Internal implementation of Pascal object detection metrics.\"\"\"\n    def __init__(\n        self,\n        num_groundtruth_classes,\n        matching_iou_threshold=0.5,\n        nms_iou_threshold=1.0,\n        nms_max_output_boxes=10000,\n        use_weighted_mean_ap=False,\n        label_id_offset=0,",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "documentation": {}
    },
    {
        "label": "ObjectDetectionEvalMetrics",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "peekOfCode": "ObjectDetectionEvalMetrics = collections.namedtuple(\n    \"ObjectDetectionEvalMetrics\",\n    [\n        \"average_precisions\",\n        \"mean_ap\",\n        \"precisions\",\n        \"recalls\",\n        \"corlocs\",\n        \"mean_corloc\",\n    ],",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "documentation": {}
    },
    {
        "label": "PerImageEvaluation",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.per_image_evaluation",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.per_image_evaluation",
        "peekOfCode": "class PerImageEvaluation(object):\n    \"\"\"Evaluate detection result of a single image.\"\"\"\n    def __init__(self, num_groundtruth_classes, matching_iou_threshold=0.5):\n        \"\"\"Initialized PerImageEvaluation by evaluation parameters.\n        Args:\n          num_groundtruth_classes: Number of ground truth object classes\n          matching_iou_threshold: A ratio of area intersection to union, which is\n              the threshold to consider whether a detection is true positive or not\n        \"\"\"\n        self.matching_iou_threshold = matching_iou_threshold",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.per_image_evaluation",
        "documentation": {}
    },
    {
        "label": "InputDataFields",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.standard_fields",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.standard_fields",
        "peekOfCode": "class InputDataFields(object):\n    \"\"\"Names for the input tensors.\n    Holds the standard data field names to use for identifying input tensors. This\n    should be used by the decoder to identify keys for the returned tensor_dict\n    containing input tensors. And it should be used by the model to identify the\n    tensors it needs.\n    Attributes:\n      image: image.\n      original_image: image in the original input size.\n      key: unique key corresponding to image.",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.standard_fields",
        "documentation": {}
    },
    {
        "label": "DetectionResultFields",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.standard_fields",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.standard_fields",
        "peekOfCode": "class DetectionResultFields(object):\n    \"\"\"Naming conventions for storing the output of the detector.\n    Attributes:\n      source_id: source of the original image.\n      key: unique key corresponding to image.\n      detection_boxes: coordinates of the detection boxes in the image.\n      detection_scores: detection scores for the detection boxes in the image.\n      detection_classes: detection-level class labels.\n      detection_masks: contains a segmentation mask for each detection box.\n      detection_boundaries: contains an object boundary for each detection box.",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.standard_fields",
        "documentation": {}
    },
    {
        "label": "BoxListFields",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.standard_fields",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.standard_fields",
        "peekOfCode": "class BoxListFields(object):\n    \"\"\"Naming conventions for BoxLists.\n    Attributes:\n      boxes: bounding box coordinates.\n      classes: classes per bounding box.\n      scores: scores per bounding box.\n      weights: sample weights per bounding box.\n      objectness: objectness score per bounding box.\n      masks: masks per bounding box.\n      boundaries: boundaries per bounding box.",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.standard_fields",
        "documentation": {}
    },
    {
        "label": "TfExampleFields",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.standard_fields",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.standard_fields",
        "peekOfCode": "class TfExampleFields(object):\n    \"\"\"TF-example proto feature names for object detection.\n    Holds the standard feature names to load from an Example proto for object\n    detection.\n    Attributes:\n      image_encoded: JPEG encoded string\n      image_format: image format, e.g. \"JPEG\"\n      filename: filename\n      channels: number of channels of image\n      colorspace: colorspace, e.g. \"RGB\"",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_evaluation.standard_fields",
        "documentation": {}
    },
    {
        "label": "make_image_key",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "peekOfCode": "def make_image_key(video_id, timestamp):\n    \"\"\"Returns a unique identifier for a video id & timestamp.\"\"\"\n    return \"%s,%04d\" % (video_id, int(timestamp))\ndef read_csv(csv_file, class_whitelist=None, load_score=False):\n    \"\"\"Loads boxes and class labels from a CSV file in the AVA format.\n    CSV file format described at https://research.google.com/ava/download.html.\n    Args:\n      csv_file: A file object.\n      class_whitelist: If provided, boxes corresponding to (integer) class labels\n        not in this set are skipped.",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "read_csv",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "peekOfCode": "def read_csv(csv_file, class_whitelist=None, load_score=False):\n    \"\"\"Loads boxes and class labels from a CSV file in the AVA format.\n    CSV file format described at https://research.google.com/ava/download.html.\n    Args:\n      csv_file: A file object.\n      class_whitelist: If provided, boxes corresponding to (integer) class labels\n        not in this set are skipped.\n    Returns:\n      boxes: A dictionary mapping each unique image key (string) to a list of\n        boxes, given as coordinates [y1, x1, y2, x2].",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "read_exclusions",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "peekOfCode": "def read_exclusions(exclusions_file):\n    \"\"\"Reads a CSV file of excluded timestamps.\n    Args:\n      exclusions_file: A file object containing a csv of video-id,timestamp.\n    Returns:\n      A set of strings containing excluded image keys, e.g. \"aaaaaaaaaaa,0904\",\n      or an empty set if exclusions file is None.\n    \"\"\"\n    excluded = set()\n    if exclusions_file:",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "read_labelmap",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "peekOfCode": "def read_labelmap(labelmap_file):\n    \"\"\"Read label map and class ids.\"\"\"\n    labelmap = []\n    class_ids = set()\n    name = \"\"\n    class_id = \"\"\n    with pathmgr.open(labelmap_file, \"r\") as f:\n        for line in f:\n            if line.startswith(\"  name:\"):\n                name = line.split('\"')[1]",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "evaluate_ava_from_files",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "peekOfCode": "def evaluate_ava_from_files(labelmap, groundtruth, detections, exclusions):\n    \"\"\"Run AVA evaluation given annotation/prediction files.\"\"\"\n    categories, class_whitelist = read_labelmap(labelmap)\n    excluded_keys = read_exclusions(exclusions)\n    groundtruth = read_csv(groundtruth, class_whitelist, load_score=False)\n    detections = read_csv(detections, class_whitelist, load_score=True)\n    run_evaluation(categories, groundtruth, detections, excluded_keys)\ndef evaluate_ava(\n    preds,\n    original_boxes,",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "evaluate_ava",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "peekOfCode": "def evaluate_ava(\n    preds,\n    original_boxes,\n    metadata,\n    excluded_keys,\n    class_whitelist,\n    categories,\n    groundtruth=None,\n    video_idx_to_name=None,\n    name=\"latest\",",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "run_evaluation",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "peekOfCode": "def run_evaluation(\n    categories, groundtruth, detections, excluded_keys, verbose=True\n):\n    \"\"\"AVA evaluation main logic.\"\"\"\n    pascal_evaluator = object_detection_evaluation.PascalDetectionEvaluator(\n        categories\n    )\n    boxes, labels, _ = groundtruth\n    gt_keys = []\n    pred_keys = []",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "get_ava_eval_data",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "peekOfCode": "def get_ava_eval_data(\n    scores,\n    boxes,\n    metadata,\n    class_whitelist,\n    verbose=False,\n    video_idx_to_name=None,\n):\n    \"\"\"\n    Convert our data format into the data format used in official AVA",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "write_results",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "peekOfCode": "def write_results(detections, filename):\n    \"\"\"Write prediction results into official formats.\"\"\"\n    start = time.time()\n    boxes, labels, scores = detections\n    with pathmgr.open(filename, \"w\") as f:\n        for key in boxes.keys():\n            for box, label, score in zip(boxes[key], labels[key], scores[key]):\n                f.write(\n                    \"%s,%.03f,%.03f,%.03f,%.03f,%d,%.04f\\n\"\n                    % (key, box[1], box[0], box[3], box[2], label, score)",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "description": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef make_image_key(video_id, timestamp):\n    \"\"\"Returns a unique identifier for a video id & timestamp.\"\"\"\n    return \"%s,%04d\" % (video_id, int(timestamp))\ndef read_csv(csv_file, class_whitelist=None, load_score=False):\n    \"\"\"Loads boxes and class labels from a CSV file in the AVA format.\n    CSV file format described at https://research.google.com/ava/download.html.\n    Args:\n      csv_file: A file object.\n      class_whitelist: If provided, boxes corresponding to (integer) class labels",
        "detail": "MorphMLP.build.lib.slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "benchmark_data_loading",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.benchmark",
        "description": "MorphMLP.build.lib.slowfast.utils.benchmark",
        "peekOfCode": "def benchmark_data_loading(cfg):\n    \"\"\"\n    Benchmark the speed of data loading in PySlowFast.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n    \"\"\"\n    # Set up environment.\n    setup_environment()\n    # Set random seed from configs.",
        "detail": "MorphMLP.build.lib.slowfast.utils.benchmark",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.utils.benchmark",
        "description": "MorphMLP.build.lib.slowfast.utils.benchmark",
        "peekOfCode": "logger = logging.get_logger(__name__)\ndef benchmark_data_loading(cfg):\n    \"\"\"\n    Benchmark the speed of data loading in PySlowFast.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n    \"\"\"\n    # Set up environment.\n    setup_environment()",
        "detail": "MorphMLP.build.lib.slowfast.utils.benchmark",
        "documentation": {}
    },
    {
        "label": "compute_and_update_bn_stats",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.bn_helper",
        "description": "MorphMLP.build.lib.slowfast.utils.bn_helper",
        "peekOfCode": "def compute_and_update_bn_stats(model, data_loader, num_batches=200):\n    \"\"\"\n    Compute and update the batch norm stats to make it more precise. During\n    training both bn stats and the weight are changing after every iteration,\n    so the bn can not precisely reflect the latest stats of the current model.\n    Here the bn stats is recomputed without change of weights, to make the\n    running mean and running var more precise.\n    Args:\n        model (model): the model using to compute and update the bn stats.\n        data_loader (dataloader): dataloader using to provide inputs.",
        "detail": "MorphMLP.build.lib.slowfast.utils.bn_helper",
        "documentation": {}
    },
    {
        "label": "get_name_convert_func",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.c2_model_loading",
        "description": "MorphMLP.build.lib.slowfast.utils.c2_model_loading",
        "peekOfCode": "def get_name_convert_func():\n    \"\"\"\n    Get the function to convert Caffe2 layer names to PyTorch layer names.\n    Returns:\n        (func): function to convert parameter name from Caffe2 format to PyTorch\n        format.\n    \"\"\"\n    pairs = [\n        # ------------------------------------------------------------\n        # 'nonlocal_conv3_1_theta_w' -> 's3.pathway0_nonlocal3.conv_g.weight'",
        "detail": "MorphMLP.build.lib.slowfast.utils.c2_model_loading",
        "documentation": {}
    },
    {
        "label": "make_checkpoint_dir",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "peekOfCode": "def make_checkpoint_dir(path_to_job):\n    \"\"\"\n    Creates the checkpoint directory (if not present already).\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n    \"\"\"\n    checkpoint_dir = os.path.join(path_to_job, \"checkpoints\")\n    # Create the checkpoint dir from the master process\n    if du.is_master_proc() and not g_pathmgr.exists(checkpoint_dir):\n        try:",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "get_checkpoint_dir",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "peekOfCode": "def get_checkpoint_dir(path_to_job):\n    \"\"\"\n    Get path for storing checkpoints.\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n    \"\"\"\n    return os.path.join(path_to_job, \"checkpoints\")\ndef get_path_to_checkpoint(path_to_job, epoch):\n    \"\"\"\n    Get the full path to a checkpoint file.",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "get_path_to_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "peekOfCode": "def get_path_to_checkpoint(path_to_job, epoch):\n    \"\"\"\n    Get the full path to a checkpoint file.\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n        epoch (int): the number of epoch for the checkpoint.\n    \"\"\"\n    name = \"checkpoint_epoch_{:05d}.pyth\".format(epoch)\n    return os.path.join(get_checkpoint_dir(path_to_job), name)\ndef get_last_checkpoint(path_to_job):",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "peekOfCode": "def get_last_checkpoint(path_to_job):\n    \"\"\"\n    Get the last checkpoint from the checkpointing folder.\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n    \"\"\"\n    d = get_checkpoint_dir(path_to_job)\n    names = g_pathmgr.ls(d) if g_pathmgr.exists(d) else []\n    names = [f for f in names if \"checkpoint\" in f]\n    assert len(names), \"No checkpoints found in '{}'.\".format(d)",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "has_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "peekOfCode": "def has_checkpoint(path_to_job):\n    \"\"\"\n    Determines if the given directory contains a checkpoint.\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n    \"\"\"\n    d = get_checkpoint_dir(path_to_job)\n    files = g_pathmgr.ls(d) if g_pathmgr.exists(d) else []\n    return any(\"checkpoint\" in f for f in files)\ndef is_checkpoint_epoch(cfg, cur_epoch, multigrid_schedule=None):",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "is_checkpoint_epoch",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "peekOfCode": "def is_checkpoint_epoch(cfg, cur_epoch, multigrid_schedule=None):\n    \"\"\"\n    Determine if a checkpoint should be saved on current epoch.\n    Args:\n        cfg (CfgNode): configs to save.\n        cur_epoch (int): current number of epoch of the model.\n        multigrid_schedule (List): schedule for multigrid training.\n    \"\"\"\n    if cur_epoch + 1 == cfg.SOLVER.MAX_EPOCH:\n        return True",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "save_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "peekOfCode": "def save_checkpoint(path_to_job, model, optimizer, loss_scaler, epoch, cfg):\n    \"\"\"\n    Save a checkpoint.\n    Args:\n        model (model): model to save the weight to the checkpoint.\n        optimizer (optim): optimizer to save the historical state.\n        loss_scaler (scaler): scaler for loss.\n        epoch (int): current number of epoch of the model.\n        cfg (CfgNode): configs to save.\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "inflate_weight",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "peekOfCode": "def inflate_weight(state_dict_2d, state_dict_3d):\n    \"\"\"\n    Inflate 2D model weights in state_dict_2d to the 3D model weights in\n    state_dict_3d. The details can be found in:\n    Joao Carreira, and Andrew Zisserman.\n    \"Quo vadis, action recognition? a new model and the kinetics dataset.\"\n    Args:\n        state_dict_2d (OrderedDict): a dict of parameters from a 2D model.\n        state_dict_3d (OrderedDict): a dict of parameters from a 3D model.\n    Returns:",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "peekOfCode": "def load_checkpoint(\n    path_to_checkpoint,\n    model,\n    loss_scaler=None,\n    data_parallel=True,\n    optimizer=None,\n    inflation=False,\n    convert_from_caffe2=False,\n    epoch_reset=False,\n    clear_name_pattern=(),",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "sub_to_normal_bn",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "peekOfCode": "def sub_to_normal_bn(sd):\n    \"\"\"\n    Convert the Sub-BN paprameters to normal BN parameters in a state dict.\n    There are two copies of BN layers in a Sub-BN implementation: `bn.bn` and\n    `bn.split_bn`. `bn.split_bn` is used during training and\n    \"compute_precise_bn\". Before saving or evaluation, its stats are copied to\n    `bn.bn`. We rename `bn.bn` to `bn` and store it to be consistent with normal\n    BN layers.\n    Args:\n        sd (OrderedDict): a dict of parameters whitch might contain Sub-BN",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "c2_normal_to_sub_bn",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "peekOfCode": "def c2_normal_to_sub_bn(key, model_keys):\n    \"\"\"\n    Convert BN parameters to Sub-BN parameters if model contains Sub-BNs.\n    Args:\n        key (OrderedDict): source dict of parameters.\n        mdoel_key (OrderedDict): target dict of parameters.\n    Returns:\n        new_sd (OrderedDict): converted dict of parameters.\n    \"\"\"\n    if \"bn.running_\" in key:",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "normal_to_sub_bn",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "peekOfCode": "def normal_to_sub_bn(checkpoint_sd, model_sd):\n    \"\"\"\n    Convert BN parameters to Sub-BN parameters if model contains Sub-BNs.\n    Args:\n        checkpoint_sd (OrderedDict): source dict of parameters.\n        model_sd (OrderedDict): target dict of parameters.\n    Returns:\n        new_sd (OrderedDict): converted dict of parameters.\n    \"\"\"\n    for key in model_sd:",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "load_test_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "peekOfCode": "def load_test_checkpoint(cfg, model):\n    \"\"\"\n    Loading checkpoint logic for testing.\n    \"\"\"\n    # Load a checkpoint to test if applicable.\n    if cfg.TEST.CHECKPOINT_FILE_PATH != \"\":\n        # If no checkpoint found in MODEL_VIS.CHECKPOINT_FILE_PATH or in the current\n        # checkpoint folder, try to load checkpoint from\n        # TEST.CHECKPOINT_FILE_PATH and test it.\n        load_checkpoint(",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "load_train_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "peekOfCode": "def load_train_checkpoint(cfg, model, optimizer, loss_scaler):\n    \"\"\"\n    Loading checkpoint logic for training.\n    \"\"\"\n    if cfg.TRAIN.AUTO_RESUME and has_checkpoint(cfg.OUTPUT_DIR):\n        last_checkpoint = get_last_checkpoint(cfg.OUTPUT_DIR)\n        logger.info(\"Load from last checkpoint, {}.\".format(last_checkpoint))\n        checkpoint_epoch = load_checkpoint(\n            last_checkpoint, model, loss_scaler, cfg.NUM_GPUS > 1, optimizer\n        )",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "peekOfCode": "logger = logging.get_logger(__name__)\ndef make_checkpoint_dir(path_to_job):\n    \"\"\"\n    Creates the checkpoint directory (if not present already).\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n    \"\"\"\n    checkpoint_dir = os.path.join(path_to_job, \"checkpoints\")\n    # Create the checkpoint dir from the master process\n    if du.is_master_proc() and not g_pathmgr.exists(checkpoint_dir):",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "make_checkpoint_dir",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def make_checkpoint_dir(path_to_job):\n    \"\"\"\n    Creates the checkpoint directory (if not present already).\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n    \"\"\"\n    checkpoint_dir = os.path.join(path_to_job, \"checkpoints\")\n    # Create the checkpoint dir from the master process\n    if du.is_master_proc() and not g_pathmgr.exists(checkpoint_dir):\n        try:",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "get_checkpoint_dir",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def get_checkpoint_dir(path_to_job):\n    \"\"\"\n    Get path for storing checkpoints.\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n    \"\"\"\n    return os.path.join(path_to_job, \"checkpoints\")\ndef get_path_to_checkpoint(path_to_job, epoch):\n    \"\"\"\n    Get the full path to a checkpoint file.",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "get_path_to_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def get_path_to_checkpoint(path_to_job, epoch):\n    \"\"\"\n    Get the full path to a checkpoint file.\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n        epoch (int): the number of epoch for the checkpoint.\n    \"\"\"\n    name = \"checkpoint_epoch_{:05d}.pyth\".format(epoch)\n    return os.path.join(get_checkpoint_dir(path_to_job), name)\ndef get_last_checkpoint(path_to_job):",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def get_last_checkpoint(path_to_job):\n    \"\"\"\n    Get the last checkpoint from the checkpointing folder.\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n    \"\"\"\n    d = get_checkpoint_dir(path_to_job)\n    names = g_pathmgr.ls(d) if g_pathmgr.exists(d) else []\n    names = [f for f in names if \"checkpoint\" in f]\n    assert len(names), \"No checkpoints found in '{}'.\".format(d)",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "has_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def has_checkpoint(path_to_job):\n    \"\"\"\n    Determines if the given directory contains a checkpoint.\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n    \"\"\"\n    d = get_checkpoint_dir(path_to_job)\n    files = g_pathmgr.ls(d) if g_pathmgr.exists(d) else []\n    return any(\"checkpoint\" in f for f in files)\ndef is_checkpoint_epoch(cfg, cur_epoch, multigrid_schedule=None):",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "is_checkpoint_epoch",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def is_checkpoint_epoch(cfg, cur_epoch, multigrid_schedule=None):\n    \"\"\"\n    Determine if a checkpoint should be saved on current epoch.\n    Args:\n        cfg (CfgNode): configs to save.\n        cur_epoch (int): current number of epoch of the model.\n        multigrid_schedule (List): schedule for multigrid training.\n    \"\"\"\n    if cur_epoch + 1 == cfg.SOLVER.MAX_EPOCH:\n        return True",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "save_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def save_checkpoint(path_to_job, model, optimizer, loss_scaler, epoch, cfg):\n    \"\"\"\n    Save a checkpoint.\n    Args:\n        model (model): model to save the weight to the checkpoint.\n        optimizer (optim): optimizer to save the historical state.\n        loss_scaler (scaler): scaler for loss.\n        epoch (int): current number of epoch of the model.\n        cfg (CfgNode): configs to save.\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "inflate_weight",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def inflate_weight(state_dict_2d, state_dict_3d):\n    \"\"\"\n    Inflate 2D model weights in state_dict_2d to the 3D model weights in\n    state_dict_3d. The details can be found in:\n    Joao Carreira, and Andrew Zisserman.\n    \"Quo vadis, action recognition? a new model and the kinetics dataset.\"\n    Args:\n        state_dict_2d (OrderedDict): a dict of parameters from a 2D model.\n        state_dict_3d (OrderedDict): a dict of parameters from a 3D model.\n    Returns:",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def load_checkpoint(\n    path_to_checkpoint,\n    model,\n    loss_scaler=None,\n    data_parallel=True,\n    optimizer=None,\n    inflation=False,\n    convert_from_caffe2=False,\n    epoch_reset=False,\n    clear_name_pattern=(),",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "sub_to_normal_bn",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def sub_to_normal_bn(sd):\n    \"\"\"\n    Convert the Sub-BN paprameters to normal BN parameters in a state dict.\n    There are two copies of BN layers in a Sub-BN implementation: `bn.bn` and\n    `bn.split_bn`. `bn.split_bn` is used during training and\n    \"compute_precise_bn\". Before saving or evaluation, its stats are copied to\n    `bn.bn`. We rename `bn.bn` to `bn` and store it to be consistent with normal\n    BN layers.\n    Args:\n        sd (OrderedDict): a dict of parameters whitch might contain Sub-BN",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "c2_normal_to_sub_bn",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def c2_normal_to_sub_bn(key, model_keys):\n    \"\"\"\n    Convert BN parameters to Sub-BN parameters if model contains Sub-BNs.\n    Args:\n        key (OrderedDict): source dict of parameters.\n        mdoel_key (OrderedDict): target dict of parameters.\n    Returns:\n        new_sd (OrderedDict): converted dict of parameters.\n    \"\"\"\n    if \"bn.running_\" in key:",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "normal_to_sub_bn",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def normal_to_sub_bn(checkpoint_sd, model_sd):\n    \"\"\"\n    Convert BN parameters to Sub-BN parameters if model contains Sub-BNs.\n    Args:\n        checkpoint_sd (OrderedDict): source dict of parameters.\n        model_sd (OrderedDict): target dict of parameters.\n    Returns:\n        new_sd (OrderedDict): converted dict of parameters.\n    \"\"\"\n    for key in model_sd:",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "load_test_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def load_test_checkpoint(cfg, model):\n    \"\"\"\n    Loading checkpoint logic for testing.\n    \"\"\"\n    # Load a checkpoint to test if applicable.\n    if cfg.TEST.CHECKPOINT_FILE_PATH != \"\":\n        # If no checkpoint found in MODEL_VIS.CHECKPOINT_FILE_PATH or in the current\n        # checkpoint folder, try to load checkpoint from\n        # TEST.CHECKPOINT_FILE_PATH and test it.\n        load_checkpoint(",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "load_train_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def load_train_checkpoint(cfg, model, optimizer, loss_scaler):\n    \"\"\"\n    Loading checkpoint logic for training.\n    \"\"\"\n    if cfg.TRAIN.AUTO_RESUME and has_checkpoint(cfg.OUTPUT_DIR):\n        last_checkpoint = get_last_checkpoint(cfg.OUTPUT_DIR)\n        logger.info(\"Load from last checkpoint, {}.\".format(last_checkpoint))\n        checkpoint_epoch = load_checkpoint(\n            last_checkpoint, model, loss_scaler, cfg.NUM_GPUS > 1, optimizer\n        )",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "peekOfCode": "logger = logging.get_logger(__name__)\ndef make_checkpoint_dir(path_to_job):\n    \"\"\"\n    Creates the checkpoint directory (if not present already).\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n    \"\"\"\n    checkpoint_dir = os.path.join(path_to_job, \"checkpoints\")\n    # Create the checkpoint dir from the master process\n    if du.is_master_proc() and not g_pathmgr.exists(checkpoint_dir):",
        "detail": "MorphMLP.build.lib.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "GatherLayer",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.utils.distributed",
        "description": "MorphMLP.build.lib.slowfast.utils.distributed",
        "peekOfCode": "class GatherLayer(torch.autograd.Function):\n    \"\"\"Gather tensors from all process, supporting backward propagation.\"\"\"\n    @staticmethod\n    def forward(ctx, input):\n        ctx.save_for_backward(input)\n        output = [torch.zeros_like(input) for _ in range(dist.get_world_size())]\n        dist.all_gather(output, input)\n        return tuple(output)\n    @staticmethod\n    def backward(ctx, *grads):",
        "detail": "MorphMLP.build.lib.slowfast.utils.distributed",
        "documentation": {}
    },
    {
        "label": "AllGatherWithGradient",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.utils.distributed",
        "description": "MorphMLP.build.lib.slowfast.utils.distributed",
        "peekOfCode": "class AllGatherWithGradient(torch.autograd.Function):\n    \"\"\"AllGatherWithGradient\"\"\"\n    @staticmethod\n    def forward(ctx, input):\n        world_size = dist.get_world_size()\n        x_gather = [torch.ones_like(input) for _ in range(world_size)]\n        torch.distributed.all_gather(x_gather, input, async_op=False)\n        x_gather = torch.cat(x_gather, dim=0)\n        return x_gather\n    @staticmethod",
        "detail": "MorphMLP.build.lib.slowfast.utils.distributed",
        "documentation": {}
    },
    {
        "label": "all_gather",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.distributed",
        "description": "MorphMLP.build.lib.slowfast.utils.distributed",
        "peekOfCode": "def all_gather(tensors):\n    \"\"\"\n    All gathers the provided tensors from all processes across machines.\n    Args:\n        tensors (list): tensors to perform all gather across all processes in\n        all machines.\n    \"\"\"\n    gather_list = []\n    output_tensor = []\n    world_size = dist.get_world_size()",
        "detail": "MorphMLP.build.lib.slowfast.utils.distributed",
        "documentation": {}
    },
    {
        "label": "all_reduce",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.distributed",
        "description": "MorphMLP.build.lib.slowfast.utils.distributed",
        "peekOfCode": "def all_reduce(tensors, average=True):\n    \"\"\"\n    All reduce the provided tensors from all processes across machines.\n    Args:\n        tensors (list): tensors to perform all reduce across all processes in\n        all machines.\n        average (bool): scales the reduced tensor by the number of overall\n        processes across all machines.\n    \"\"\"\n    for tensor in tensors:",
        "detail": "MorphMLP.build.lib.slowfast.utils.distributed",
        "documentation": {}
    },
    {
        "label": "init_process_group",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.distributed",
        "description": "MorphMLP.build.lib.slowfast.utils.distributed",
        "peekOfCode": "def init_process_group(\n    local_rank,\n    local_world_size,\n    shard_id,\n    num_shards,\n    init_method,\n    dist_backend=\"nccl\",\n):\n    \"\"\"\n    Initializes the default process group.",
        "detail": "MorphMLP.build.lib.slowfast.utils.distributed",
        "documentation": {}
    },
    {
        "label": "is_master_proc",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.distributed",
        "description": "MorphMLP.build.lib.slowfast.utils.distributed",
        "peekOfCode": "def is_master_proc(num_gpus=8):\n    \"\"\"\n    Determines if the current process is the master process.\n    \"\"\"\n    if torch.distributed.is_initialized():\n        return dist.get_rank() % num_gpus == 0\n    else:\n        return True\ndef is_root_proc():\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.utils.distributed",
        "documentation": {}
    },
    {
        "label": "is_root_proc",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.distributed",
        "description": "MorphMLP.build.lib.slowfast.utils.distributed",
        "peekOfCode": "def is_root_proc():\n    \"\"\"\n    Determines if the current process is the root process.\n    \"\"\"\n    if torch.distributed.is_initialized():\n        return dist.get_rank() == 0\n    else:\n        return True\ndef get_rank():\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.utils.distributed",
        "documentation": {}
    },
    {
        "label": "get_rank",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.distributed",
        "description": "MorphMLP.build.lib.slowfast.utils.distributed",
        "peekOfCode": "def get_rank():\n    \"\"\"\n    Get the rank of the current process.\n    \"\"\"\n    if not dist.is_available():\n        return 0\n    if not dist.is_initialized():\n        return 0\n    return dist.get_rank()\ndef synchronize():",
        "detail": "MorphMLP.build.lib.slowfast.utils.distributed",
        "documentation": {}
    },
    {
        "label": "synchronize",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.distributed",
        "description": "MorphMLP.build.lib.slowfast.utils.distributed",
        "peekOfCode": "def synchronize():\n    \"\"\"\n    Helper function to synchronize (barrier) among all processes when\n    using distributed training\n    \"\"\"\n    if not dist.is_available():\n        return\n    if not dist.is_initialized():\n        return\n    world_size = dist.get_world_size()",
        "detail": "MorphMLP.build.lib.slowfast.utils.distributed",
        "documentation": {}
    },
    {
        "label": "all_gather_unaligned",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.distributed",
        "description": "MorphMLP.build.lib.slowfast.utils.distributed",
        "peekOfCode": "def all_gather_unaligned(data, group=None):\n    \"\"\"\n    Run all_gather on arbitrary picklable data (not necessarily tensors).\n    Args:\n        data: any picklable object\n        group: a torch process group. By default, will use a group which\n            contains all ranks on gloo backend.\n    Returns:\n        list[data]: list of data gathered from each rank\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.utils.distributed",
        "documentation": {}
    },
    {
        "label": "setup_environment",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.env",
        "description": "MorphMLP.build.lib.slowfast.utils.env",
        "peekOfCode": "def setup_environment():\n    global _ENV_SETUP_DONE\n    if _ENV_SETUP_DONE:\n        return\n    _ENV_SETUP_DONE = True",
        "detail": "MorphMLP.build.lib.slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "_ENV_SETUP_DONE",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.utils.env",
        "description": "MorphMLP.build.lib.slowfast.utils.env",
        "peekOfCode": "_ENV_SETUP_DONE = False\npathmgr = PathManagerFactory.get(key=\"pyslowfast\")\ncheckpoint_pathmgr = PathManagerFactory.get(key=\"pyslowfast_checkpoint\")\ndef setup_environment():\n    global _ENV_SETUP_DONE\n    if _ENV_SETUP_DONE:\n        return\n    _ENV_SETUP_DONE = True",
        "detail": "MorphMLP.build.lib.slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "pathmgr",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.utils.env",
        "description": "MorphMLP.build.lib.slowfast.utils.env",
        "peekOfCode": "pathmgr = PathManagerFactory.get(key=\"pyslowfast\")\ncheckpoint_pathmgr = PathManagerFactory.get(key=\"pyslowfast_checkpoint\")\ndef setup_environment():\n    global _ENV_SETUP_DONE\n    if _ENV_SETUP_DONE:\n        return\n    _ENV_SETUP_DONE = True",
        "detail": "MorphMLP.build.lib.slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "checkpoint_pathmgr",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.utils.env",
        "description": "MorphMLP.build.lib.slowfast.utils.env",
        "peekOfCode": "checkpoint_pathmgr = PathManagerFactory.get(key=\"pyslowfast_checkpoint\")\ndef setup_environment():\n    global _ENV_SETUP_DONE\n    if _ENV_SETUP_DONE:\n        return\n    _ENV_SETUP_DONE = True",
        "detail": "MorphMLP.build.lib.slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "setup_logging",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.logging",
        "description": "MorphMLP.build.lib.slowfast.utils.logging",
        "peekOfCode": "def setup_logging(output_dir=None):\n    \"\"\"\n    Sets up the logging for multiple processes. Only enable the logging for the\n    master process, and suppress logging for the non-master processes.\n    \"\"\"\n    # Set up logging format.\n    _FORMAT = \"[%(levelname)s: %(filename)s: %(lineno)4d]: %(message)s\"\n    if du.is_master_proc():\n        # Enable logging for the master process.\n        logging.root.handlers = []",
        "detail": "MorphMLP.build.lib.slowfast.utils.logging",
        "documentation": {}
    },
    {
        "label": "get_logger",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.logging",
        "description": "MorphMLP.build.lib.slowfast.utils.logging",
        "peekOfCode": "def get_logger(name):\n    \"\"\"\n    Retrieve the logger with the specified name or, if name is None, return a\n    logger which is the root logger of the hierarchy.\n    Args:\n        name (string): name of the logger.\n    \"\"\"\n    return logging.getLogger(name)\ndef log_json_stats(stats, output_dir=None):\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.utils.logging",
        "documentation": {}
    },
    {
        "label": "log_json_stats",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.logging",
        "description": "MorphMLP.build.lib.slowfast.utils.logging",
        "peekOfCode": "def log_json_stats(stats, output_dir=None):\n    \"\"\"\n    Logs json stats.\n    Args:\n        stats (dict): a dictionary of statistical information to log.\n    \"\"\"\n    stats = {\n        k: decimal.Decimal(\"{:.3f}\".format(v)) if isinstance(v, float) else v\n        for k, v in stats.items()\n    }",
        "detail": "MorphMLP.build.lib.slowfast.utils.logging",
        "documentation": {}
    },
    {
        "label": "get_lr_at_epoch",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.lr_policy",
        "description": "MorphMLP.build.lib.slowfast.utils.lr_policy",
        "peekOfCode": "def get_lr_at_epoch(cfg, cur_epoch):\n    \"\"\"\n    Retrieve the learning rate of the current epoch with the option to perform\n    warm up in the beginning of the training stage.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        cur_epoch (float): the number of epoch of the current training stage.\n    \"\"\"\n    lr = get_lr_func(cfg.SOLVER.LR_POLICY)(cfg, cur_epoch)",
        "detail": "MorphMLP.build.lib.slowfast.utils.lr_policy",
        "documentation": {}
    },
    {
        "label": "lr_func_cosine",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.lr_policy",
        "description": "MorphMLP.build.lib.slowfast.utils.lr_policy",
        "peekOfCode": "def lr_func_cosine(cfg, cur_epoch):\n    \"\"\"\n    Retrieve the learning rate to specified values at specified epoch with the\n    cosine learning rate schedule. Details can be found in:\n    Ilya Loshchilov, and  Frank Hutter\n    SGDR: Stochastic Gradient Descent With Warm Restarts.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        cur_epoch (float): the number of epoch of the current training stage.",
        "detail": "MorphMLP.build.lib.slowfast.utils.lr_policy",
        "documentation": {}
    },
    {
        "label": "lr_func_steps_with_relative_lrs",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.lr_policy",
        "description": "MorphMLP.build.lib.slowfast.utils.lr_policy",
        "peekOfCode": "def lr_func_steps_with_relative_lrs(cfg, cur_epoch):\n    \"\"\"\n    Retrieve the learning rate to specified values at specified epoch with the\n    steps with relative learning rate schedule.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        cur_epoch (float): the number of epoch of the current training stage.\n    \"\"\"\n    ind = get_step_index(cfg, cur_epoch)",
        "detail": "MorphMLP.build.lib.slowfast.utils.lr_policy",
        "documentation": {}
    },
    {
        "label": "get_step_index",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.lr_policy",
        "description": "MorphMLP.build.lib.slowfast.utils.lr_policy",
        "peekOfCode": "def get_step_index(cfg, cur_epoch):\n    \"\"\"\n    Retrieves the lr step index for the given epoch.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        cur_epoch (float): the number of epoch of the current training stage.\n    \"\"\"\n    steps = cfg.SOLVER.STEPS + [cfg.SOLVER.MAX_EPOCH]\n    for ind, step in enumerate(steps):  # NoQA",
        "detail": "MorphMLP.build.lib.slowfast.utils.lr_policy",
        "documentation": {}
    },
    {
        "label": "get_lr_func",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.lr_policy",
        "description": "MorphMLP.build.lib.slowfast.utils.lr_policy",
        "peekOfCode": "def get_lr_func(lr_policy):\n    \"\"\"\n    Given the configs, retrieve the specified lr policy function.\n    Args:\n        lr_policy (string): the learning rate policy to use for the job.\n    \"\"\"\n    policy = \"lr_func_\" + lr_policy\n    if policy not in globals():\n        raise NotImplementedError(\"Unknown LR policy: {}\".format(lr_policy))\n    else:",
        "detail": "MorphMLP.build.lib.slowfast.utils.lr_policy",
        "documentation": {}
    },
    {
        "label": "AVAMeter",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.utils.meters",
        "description": "MorphMLP.build.lib.slowfast.utils.meters",
        "peekOfCode": "class AVAMeter(object):\n    \"\"\"\n    Measure the AVA train, val, and test stats.\n    \"\"\"\n    def __init__(self, overall_iters, cfg, mode):\n        \"\"\"\n        overall_iters (int): the overall number of iterations of one epoch.\n        cfg (CfgNode): configs.\n        mode (str): `train`, `val`, or `test` mode.\n        \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.utils.meters",
        "documentation": {}
    },
    {
        "label": "TestMeter",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.utils.meters",
        "description": "MorphMLP.build.lib.slowfast.utils.meters",
        "peekOfCode": "class TestMeter(object):\n    \"\"\"\n    Perform the multi-view ensemble for testing: each video with an unique index\n    will be sampled with multiple clips, and the predictions of the clips will\n    be aggregated to produce the final prediction for the video.\n    The accuracy is calculated with the given ground truth labels.\n    \"\"\"\n    def __init__(\n        self,\n        num_videos,",
        "detail": "MorphMLP.build.lib.slowfast.utils.meters",
        "documentation": {}
    },
    {
        "label": "ScalarMeter",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.utils.meters",
        "description": "MorphMLP.build.lib.slowfast.utils.meters",
        "peekOfCode": "class ScalarMeter(object):\n    \"\"\"\n    A scalar meter uses a deque to track a series of scaler values with a given\n    window size. It supports calculating the median and average values of the\n    window, and also supports calculating the global average.\n    \"\"\"\n    def __init__(self, window_size):\n        \"\"\"\n        Args:\n            window_size (int): size of the max length of the deque.",
        "detail": "MorphMLP.build.lib.slowfast.utils.meters",
        "documentation": {}
    },
    {
        "label": "TrainMeter",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.utils.meters",
        "description": "MorphMLP.build.lib.slowfast.utils.meters",
        "peekOfCode": "class TrainMeter(object):\n    \"\"\"\n    Measure training stats.\n    \"\"\"\n    def __init__(self, epoch_iters, cfg):\n        \"\"\"\n        Args:\n            epoch_iters (int): the overall number of iterations of one epoch.\n            cfg (CfgNode): configs.\n        \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.utils.meters",
        "documentation": {}
    },
    {
        "label": "ValMeter",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.utils.meters",
        "description": "MorphMLP.build.lib.slowfast.utils.meters",
        "peekOfCode": "class ValMeter(object):\n    \"\"\"\n    Measures validation stats.\n    \"\"\"\n    def __init__(self, max_iter, cfg):\n        \"\"\"\n        Args:\n            max_iter (int): the max number of iteration of the current epoch.\n            cfg (CfgNode): configs.\n        \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.utils.meters",
        "documentation": {}
    },
    {
        "label": "EpochTimer",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.utils.meters",
        "description": "MorphMLP.build.lib.slowfast.utils.meters",
        "peekOfCode": "class EpochTimer:\n    \"\"\"\n    A timer which computes the epoch time.\n    \"\"\"\n    def __init__(self) -> None:\n        self.timer = Timer()\n        self.timer.reset()\n        self.epoch_times = []\n    def reset(self) -> None:\n        \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.utils.meters",
        "documentation": {}
    },
    {
        "label": "get_ava_mini_groundtruth",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.meters",
        "description": "MorphMLP.build.lib.slowfast.utils.meters",
        "peekOfCode": "def get_ava_mini_groundtruth(full_groundtruth):\n    \"\"\"\n    Get the groundtruth annotations corresponding the \"subset\" of AVA val set.\n    We define the subset to be the frames such that (second % 4 == 0).\n    We optionally use subset for faster evaluation during training\n    (in order to track training progress).\n    Args:\n        full_groundtruth(dict): list of groundtruth.\n    \"\"\"\n    ret = [defaultdict(list), defaultdict(list), defaultdict(list)]",
        "detail": "MorphMLP.build.lib.slowfast.utils.meters",
        "documentation": {}
    },
    {
        "label": "get_map",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.meters",
        "description": "MorphMLP.build.lib.slowfast.utils.meters",
        "peekOfCode": "def get_map(preds, labels):\n    \"\"\"\n    Compute mAP for multi-label case.\n    Args:\n        preds (numpy tensor): num_examples x num_classes.\n        labels (numpy tensor): num_examples x num_classes.\n    Returns:\n        mean_ap (int): final mAP score.\n    \"\"\"\n    logger.info(\"Getting mAP for {} examples\".format(preds.shape[0]))",
        "detail": "MorphMLP.build.lib.slowfast.utils.meters",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.utils.meters",
        "description": "MorphMLP.build.lib.slowfast.utils.meters",
        "peekOfCode": "logger = logging.get_logger(__name__)\ndef get_ava_mini_groundtruth(full_groundtruth):\n    \"\"\"\n    Get the groundtruth annotations corresponding the \"subset\" of AVA val set.\n    We define the subset to be the frames such that (second % 4 == 0).\n    We optionally use subset for faster evaluation during training\n    (in order to track training progress).\n    Args:\n        full_groundtruth(dict): list of groundtruth.\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.utils.meters",
        "documentation": {}
    },
    {
        "label": "topks_correct",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.metrics",
        "description": "MorphMLP.build.lib.slowfast.utils.metrics",
        "peekOfCode": "def topks_correct(preds, labels, ks):\n    \"\"\"\n    Given the predictions, labels, and a list of top-k values, compute the\n    number of correct predictions for each top-k value.\n    Args:\n        preds (array): array of predictions. Dimension is batchsize\n            N x ClassNum.\n        labels (array): array of labels. Dimension is batchsize N.\n        ks (list): list of top-k values. For example, ks = [1, 5] correspods\n            to top-1 and top-5.",
        "detail": "MorphMLP.build.lib.slowfast.utils.metrics",
        "documentation": {}
    },
    {
        "label": "topk_errors",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.metrics",
        "description": "MorphMLP.build.lib.slowfast.utils.metrics",
        "peekOfCode": "def topk_errors(preds, labels, ks):\n    \"\"\"\n    Computes the top-k error for each k.\n    Args:\n        preds (array): array of predictions. Dimension is N.\n        labels (array): array of labels. Dimension is N.\n        ks (list): list of ks to calculate the top accuracies.\n    \"\"\"\n    num_topks_correct = topks_correct(preds, labels, ks)\n    return [(1.0 - x / preds.size(0)) * 100.0 for x in num_topks_correct]",
        "detail": "MorphMLP.build.lib.slowfast.utils.metrics",
        "documentation": {}
    },
    {
        "label": "topk_accuracies",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.metrics",
        "description": "MorphMLP.build.lib.slowfast.utils.metrics",
        "peekOfCode": "def topk_accuracies(preds, labels, ks):\n    \"\"\"\n    Computes the top-k accuracy for each k.\n    Args:\n        preds (array): array of predictions. Dimension is N.\n        labels (array): array of labels. Dimension is N.\n        ks (list): list of ks to calculate the top accuracies.\n    \"\"\"\n    num_topks_correct = topks_correct(preds, labels, ks)\n    return [(x / preds.size(0)) * 100.0 for x in num_topks_correct]",
        "detail": "MorphMLP.build.lib.slowfast.utils.metrics",
        "documentation": {}
    },
    {
        "label": "check_nan_losses",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.misc",
        "description": "MorphMLP.build.lib.slowfast.utils.misc",
        "peekOfCode": "def check_nan_losses(loss):\n    \"\"\"\n    Determine whether the loss is NaN (not a number).\n    Args:\n        loss (loss): loss to check whether is NaN.\n    \"\"\"\n    if math.isnan(loss):\n        raise RuntimeError(\"ERROR: Got NaN losses {}\".format(datetime.now()))\ndef params_count(model, ignore_bn=False):\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "params_count",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.misc",
        "description": "MorphMLP.build.lib.slowfast.utils.misc",
        "peekOfCode": "def params_count(model, ignore_bn=False):\n    \"\"\"\n    Compute the number of parameters.\n    Args:\n        model (model): model to count the number of parameters.\n    \"\"\"\n    if not ignore_bn:\n        return np.sum([p.numel() for p in model.parameters()]).item()\n    else:\n        count = 0",
        "detail": "MorphMLP.build.lib.slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "gpu_mem_usage",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.misc",
        "description": "MorphMLP.build.lib.slowfast.utils.misc",
        "peekOfCode": "def gpu_mem_usage():\n    \"\"\"\n    Compute the GPU memory usage for the current device (GB).\n    \"\"\"\n    if torch.cuda.is_available():\n        mem_usage_bytes = torch.cuda.max_memory_allocated()\n    else:\n        mem_usage_bytes = 0\n    return mem_usage_bytes / 1024**3\ndef cpu_mem_usage():",
        "detail": "MorphMLP.build.lib.slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "cpu_mem_usage",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.misc",
        "description": "MorphMLP.build.lib.slowfast.utils.misc",
        "peekOfCode": "def cpu_mem_usage():\n    \"\"\"\n    Compute the system memory (RAM) usage for the current device (GB).\n    Returns:\n        usage (float): used memory (GB).\n        total (float): total memory (GB).\n    \"\"\"\n    vram = psutil.virtual_memory()\n    usage = (vram.total - vram.available) / 1024**3\n    total = vram.total / 1024**3",
        "detail": "MorphMLP.build.lib.slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "get_model_stats",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.misc",
        "description": "MorphMLP.build.lib.slowfast.utils.misc",
        "peekOfCode": "def get_model_stats(model, cfg, mode, use_train_input):\n    \"\"\"\n    Compute statistics for the current model given the config.\n    Args:\n        model (model): model to perform analysis.\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        mode (str): Options include `flop` or `activation`. Compute either flop\n            (gflops) or activation count (mega).\n        use_train_input (bool): if True, compute statistics for training. Otherwise,",
        "detail": "MorphMLP.build.lib.slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "log_model_info",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.misc",
        "description": "MorphMLP.build.lib.slowfast.utils.misc",
        "peekOfCode": "def log_model_info(model, cfg, use_train_input=True):\n    \"\"\"\n    Log info, includes number of parameters, gpu usage, gflops and activation count.\n        The model info is computed when the model is in validation mode.\n    Args:\n        model (model): model to log the info.\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        use_train_input (bool): if True, log info for training. Otherwise,\n            log info for testing.",
        "detail": "MorphMLP.build.lib.slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "is_eval_epoch",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.misc",
        "description": "MorphMLP.build.lib.slowfast.utils.misc",
        "peekOfCode": "def is_eval_epoch(cfg, cur_epoch, multigrid_schedule):\n    \"\"\"\n    Determine if the model should be evaluated at the current epoch.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        cur_epoch (int): current epoch.\n        multigrid_schedule (List): schedule for multigrid training.\n    \"\"\"\n    if cur_epoch + 1 == cfg.SOLVER.MAX_EPOCH:",
        "detail": "MorphMLP.build.lib.slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "plot_input",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.misc",
        "description": "MorphMLP.build.lib.slowfast.utils.misc",
        "peekOfCode": "def plot_input(tensor, bboxes=(), texts=(), path=\"./tmp_vis.png\"):\n    \"\"\"\n    Plot the input tensor with the optional bounding box and save it to disk.\n    Args:\n        tensor (tensor): a tensor with shape of `NxCxHxW`.\n        bboxes (tuple): bounding boxes with format of [[x, y, h, w]].\n        texts (tuple): a tuple of string to plot.\n        path (str): path to the image to save to.\n    \"\"\"\n    tensor = tensor.float()",
        "detail": "MorphMLP.build.lib.slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "frozen_bn_stats",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.misc",
        "description": "MorphMLP.build.lib.slowfast.utils.misc",
        "peekOfCode": "def frozen_bn_stats(model):\n    \"\"\"\n    Set all the bn layers to eval mode.\n    Args:\n        model (model): model to set bn layers to eval mode.\n    \"\"\"\n    for m in model.modules():\n        if isinstance(m, nn.BatchNorm3d):\n            m.eval()\ndef aggregate_sub_bn_stats(module):",
        "detail": "MorphMLP.build.lib.slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "aggregate_sub_bn_stats",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.misc",
        "description": "MorphMLP.build.lib.slowfast.utils.misc",
        "peekOfCode": "def aggregate_sub_bn_stats(module):\n    \"\"\"\n    Recursively find all SubBN modules and aggregate sub-BN stats.\n    Args:\n        module (nn.Module)\n    Returns:\n        count (int): number of SubBN module found.\n    \"\"\"\n    count = 0\n    for child in module.children():",
        "detail": "MorphMLP.build.lib.slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "launch_job",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.misc",
        "description": "MorphMLP.build.lib.slowfast.utils.misc",
        "peekOfCode": "def launch_job(cfg, init_method, func, daemon=False):\n    \"\"\"\n    Run 'func' on one or more GPUs, specified in cfg\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        init_method (str): initialization method to launch the job with multiple\n            devices.\n        func (function): job to run on GPU(s)\n        daemon (bool): The spawned processes’ daemon flag. If set to True,",
        "detail": "MorphMLP.build.lib.slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "get_class_names",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.misc",
        "description": "MorphMLP.build.lib.slowfast.utils.misc",
        "peekOfCode": "def get_class_names(path, parent_path=None, subset_path=None):\n    \"\"\"\n    Read json file with entries {classname: index} and return\n    an array of class names in order.\n    If parent_path is provided, load and map all children to their ids.\n    Args:\n        path (str): path to class ids json file.\n            File must be in the format {\"class1\": id1, \"class2\": id2, ...}\n        parent_path (Optional[str]): path to parent-child json file.\n            File must be in the format {\"parent1\": [\"child1\", \"child2\", ...], ...}",
        "detail": "MorphMLP.build.lib.slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.utils.misc",
        "description": "MorphMLP.build.lib.slowfast.utils.misc",
        "peekOfCode": "logger = logging.get_logger(__name__)\ndef check_nan_losses(loss):\n    \"\"\"\n    Determine whether the loss is NaN (not a number).\n    Args:\n        loss (loss): loss to check whether is NaN.\n    \"\"\"\n    if math.isnan(loss):\n        raise RuntimeError(\"ERROR: Got NaN losses {}\".format(datetime.now()))\ndef params_count(model, ignore_bn=False):",
        "detail": "MorphMLP.build.lib.slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "MultigridSchedule",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.utils.multigrid",
        "description": "MorphMLP.build.lib.slowfast.utils.multigrid",
        "peekOfCode": "class MultigridSchedule(object):\n    \"\"\"\n    This class defines multigrid training schedule and update cfg accordingly.\n    \"\"\"\n    def init_multigrid(self, cfg):\n        \"\"\"\n        Update cfg based on multigrid settings.\n        Args:\n            cfg (configs): configs that contains training and multigrid specific\n                hyperparameters. Details can be seen in",
        "detail": "MorphMLP.build.lib.slowfast.utils.multigrid",
        "documentation": {}
    },
    {
        "label": "print_schedule",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.multigrid",
        "description": "MorphMLP.build.lib.slowfast.utils.multigrid",
        "peekOfCode": "def print_schedule(schedule):\n    \"\"\"\n    Log schedule.\n    \"\"\"\n    logger.info(\"Long cycle index\\tBase shape\\tEpochs\")\n    for s in schedule:\n        logger.info(\"{}\\t{}\\t{}\".format(s[0], s[1], s[2]))\ndef get_current_long_cycle_shape(schedule, epoch):\n    \"\"\"\n    Given a schedule and epoch index, return the long cycle base shape.",
        "detail": "MorphMLP.build.lib.slowfast.utils.multigrid",
        "documentation": {}
    },
    {
        "label": "get_current_long_cycle_shape",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.multigrid",
        "description": "MorphMLP.build.lib.slowfast.utils.multigrid",
        "peekOfCode": "def get_current_long_cycle_shape(schedule, epoch):\n    \"\"\"\n    Given a schedule and epoch index, return the long cycle base shape.\n    Args:\n        schedule (configs): configs that contains training and multigrid specific\n            hyperparameters. Details can be seen in\n            slowfast/config/defaults.py.\n        cur_epoch (int): current epoch index.\n    Returns:\n        shapes (list): A list describing the base shape in a long cycle:",
        "detail": "MorphMLP.build.lib.slowfast.utils.multigrid",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.utils.multigrid",
        "description": "MorphMLP.build.lib.slowfast.utils.multigrid",
        "peekOfCode": "logger = logging.get_logger(__name__)\nclass MultigridSchedule(object):\n    \"\"\"\n    This class defines multigrid training schedule and update cfg accordingly.\n    \"\"\"\n    def init_multigrid(self, cfg):\n        \"\"\"\n        Update cfg based on multigrid settings.\n        Args:\n            cfg (configs): configs that contains training and multigrid specific",
        "detail": "MorphMLP.build.lib.slowfast.utils.multigrid",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.multiprocessing",
        "description": "MorphMLP.build.lib.slowfast.utils.multiprocessing",
        "peekOfCode": "def run(\n    local_rank,\n    num_proc,\n    func,\n    init_method,\n    shard_id,\n    num_shards,\n    backend,\n    cfg,\n    output_queue=None,",
        "detail": "MorphMLP.build.lib.slowfast.utils.multiprocessing",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.parser",
        "description": "MorphMLP.build.lib.slowfast.utils.parser",
        "peekOfCode": "def parse_args():\n    \"\"\"\n    Parse the following arguments for a default parser for PySlowFast users.\n    Args:\n        shard_id (int): shard id for the current machine. Starts from 0 to\n            num_shards - 1. If single machine is used, then set shard id to 0.\n        num_shards (int): number of shards using by the job.\n        init_method (str): initialization method to launch the job with multiple\n            devices. Options includes TCP or shared file-system for\n            initialization. details can be find in",
        "detail": "MorphMLP.build.lib.slowfast.utils.parser",
        "documentation": {}
    },
    {
        "label": "load_config",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.parser",
        "description": "MorphMLP.build.lib.slowfast.utils.parser",
        "peekOfCode": "def load_config(args):\n    \"\"\"\n    Given the arguemnts, load and initialize the configs.\n    Args:\n        args (argument): arguments includes `shard_id`, `num_shards`,\n            `init_method`, `cfg_file`, and `opts`.\n    \"\"\"\n    # Setup cfg.\n    cfg = get_cfg()\n    # Load config from cfg.",
        "detail": "MorphMLP.build.lib.slowfast.utils.parser",
        "documentation": {}
    },
    {
        "label": "init_weights",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.utils.weight_init_helper",
        "description": "MorphMLP.build.lib.slowfast.utils.weight_init_helper",
        "peekOfCode": "def init_weights(\n    model, fc_init_std=0.01, zero_init_final_bn=True, zero_init_final_conv=False\n):\n    \"\"\"\n    Performs ResNet style weight initialization.\n    Args:\n        fc_init_std (float): the expected standard deviation for fc layer.\n        zero_init_final_bn (bool): if True, zero initialize the final bn for\n            every bottleneck.\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.utils.weight_init_helper",
        "documentation": {}
    },
    {
        "label": "AsycnActionPredictor",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.async_predictor",
        "description": "MorphMLP.build.lib.slowfast.visualization.async_predictor",
        "peekOfCode": "class AsycnActionPredictor:\n    class _Predictor(mp.Process):\n        def __init__(self, cfg, task_queue, result_queue, gpu_id=None):\n            \"\"\"\n            Predict Worker for Detectron2.\n            Args:\n                cfg (CfgNode): configs. Details can be found in\n                    slowfast/config/defaults.py\n                task_queue (mp.Queue): a shared queue for incoming task.\n                result_queue (mp.Queue): a shared queue for predicted results.",
        "detail": "MorphMLP.build.lib.slowfast.visualization.async_predictor",
        "documentation": {}
    },
    {
        "label": "AsyncVis",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.async_predictor",
        "description": "MorphMLP.build.lib.slowfast.visualization.async_predictor",
        "peekOfCode": "class AsyncVis:\n    class _VisWorker(mp.Process):\n        def __init__(self, video_vis, task_queue, result_queue):\n            \"\"\"\n            Visualization Worker for AsyncVis.\n            Args:\n                video_vis (VideoVisualizer object): object with tools for visualization.\n                task_queue (mp.Queue): a shared queue for incoming task for visualization.\n                result_queue (mp.Queue): a shared queue for visualized results.\n            \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.visualization.async_predictor",
        "documentation": {}
    },
    {
        "label": "_StopToken",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.async_predictor",
        "description": "MorphMLP.build.lib.slowfast.visualization.async_predictor",
        "peekOfCode": "class _StopToken:\n    pass\nclass AsyncDemo:\n    \"\"\"\n    Asynchronous Action Prediction and Visualization pipeline with AsyncVis.\n    \"\"\"\n    def __init__(self, cfg, async_vis):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in",
        "detail": "MorphMLP.build.lib.slowfast.visualization.async_predictor",
        "documentation": {}
    },
    {
        "label": "AsyncDemo",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.async_predictor",
        "description": "MorphMLP.build.lib.slowfast.visualization.async_predictor",
        "peekOfCode": "class AsyncDemo:\n    \"\"\"\n    Asynchronous Action Prediction and Visualization pipeline with AsyncVis.\n    \"\"\"\n    def __init__(self, cfg, async_vis):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in\n                slowfast/config/defaults.py\n            async_vis (AsyncVis object): asynchronous visualizer.",
        "detail": "MorphMLP.build.lib.slowfast.visualization.async_predictor",
        "documentation": {}
    },
    {
        "label": "draw_predictions",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.async_predictor",
        "description": "MorphMLP.build.lib.slowfast.visualization.async_predictor",
        "peekOfCode": "def draw_predictions(task, video_vis):\n    \"\"\"\n    Draw prediction for the given task.\n    Args:\n        task (TaskInfo object): task object that contain\n            the necessary information for visualization. (e.g. frames, preds)\n            All attributes must lie on CPU devices.\n        video_vis (VideoVisualizer object): the video visualizer object.\n    \"\"\"\n    boxes = task.bboxes",
        "detail": "MorphMLP.build.lib.slowfast.visualization.async_predictor",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.async_predictor",
        "description": "MorphMLP.build.lib.slowfast.visualization.async_predictor",
        "peekOfCode": "logger = logging.get_logger(__name__)\nclass AsycnActionPredictor:\n    class _Predictor(mp.Process):\n        def __init__(self, cfg, task_queue, result_queue, gpu_id=None):\n            \"\"\"\n            Predict Worker for Detectron2.\n            Args:\n                cfg (CfgNode): configs. Details can be found in\n                    slowfast/config/defaults.py\n                task_queue (mp.Queue): a shared queue for incoming task.",
        "detail": "MorphMLP.build.lib.slowfast.visualization.async_predictor",
        "documentation": {}
    },
    {
        "label": "AVAVisualizerWithPrecomputedBox",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.ava_demo_precomputed_boxes",
        "description": "MorphMLP.build.lib.slowfast.visualization.ava_demo_precomputed_boxes",
        "peekOfCode": "class AVAVisualizerWithPrecomputedBox:\n    \"\"\"\n    Visualize action predictions for videos or folder of images with precomputed\n    and ground-truth boxes in AVA format.\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in\n                slowfast/config/defaults.py",
        "detail": "MorphMLP.build.lib.slowfast.visualization.ava_demo_precomputed_boxes",
        "documentation": {}
    },
    {
        "label": "merge_pred_gt_boxes",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.ava_demo_precomputed_boxes",
        "description": "MorphMLP.build.lib.slowfast.visualization.ava_demo_precomputed_boxes",
        "peekOfCode": "def merge_pred_gt_boxes(pred_dict, gt_dict=None):\n    \"\"\"\n    Merge data from precomputed and ground-truth boxes dictionaries.\n    Args:\n        pred_dict (dict): a dict which maps from `frame_idx` to a list of `boxes`\n            and `labels`. Each `box` is a list of 4 box coordinates. `labels[i]` is\n            a list of labels for `boxes[i]`.\n        gt_dict (Optional[dict]): a dict which maps from `frame_idx` to a list of `boxes`\n            and `labels`. Each `box` is a list of 4 box coordinates. `labels[i]` is\n            a list of labels for `boxes[i]`. Note that label is -1 for predicted boxes.",
        "detail": "MorphMLP.build.lib.slowfast.visualization.ava_demo_precomputed_boxes",
        "documentation": {}
    },
    {
        "label": "load_boxes_labels",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.ava_demo_precomputed_boxes",
        "description": "MorphMLP.build.lib.slowfast.visualization.ava_demo_precomputed_boxes",
        "peekOfCode": "def load_boxes_labels(cfg, video_name, fps, img_width, img_height):\n    \"\"\"\n    Loading boxes and labels from AVA bounding boxes csv files.\n    Args:\n        cfg (CfgNode): config.\n        video_name (str): name of the given video.\n        fps (int or float): frames per second of the input video/images folder.\n        img_width (int): width of images in input video/images folder.\n        img_height (int): height of images in input video/images folder.\n    Returns:",
        "detail": "MorphMLP.build.lib.slowfast.visualization.ava_demo_precomputed_boxes",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.ava_demo_precomputed_boxes",
        "description": "MorphMLP.build.lib.slowfast.visualization.ava_demo_precomputed_boxes",
        "peekOfCode": "logger = logging.get_logger(__name__)\nclass AVAVisualizerWithPrecomputedBox:\n    \"\"\"\n    Visualize action predictions for videos or folder of images with precomputed\n    and ground-truth boxes in AVA format.\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in",
        "detail": "MorphMLP.build.lib.slowfast.visualization.ava_demo_precomputed_boxes",
        "documentation": {}
    },
    {
        "label": "VideoManager",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.demo_loader",
        "description": "MorphMLP.build.lib.slowfast.visualization.demo_loader",
        "peekOfCode": "class VideoManager:\n    \"\"\"\n    VideoManager object for getting frames from video source for inference.\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.visualization.demo_loader",
        "documentation": {}
    },
    {
        "label": "ThreadVideoManager",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.demo_loader",
        "description": "MorphMLP.build.lib.slowfast.visualization.demo_loader",
        "peekOfCode": "class ThreadVideoManager:\n    \"\"\"\n    VideoManager object for getting frames from video source for inference\n    using multithreading for read and write frames.\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py",
        "detail": "MorphMLP.build.lib.slowfast.visualization.demo_loader",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.demo_loader",
        "description": "MorphMLP.build.lib.slowfast.visualization.demo_loader",
        "peekOfCode": "logger = logging.get_logger(__name__)\nclass VideoManager:\n    \"\"\"\n    VideoManager object for getting frames from video source for inference.\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py",
        "detail": "MorphMLP.build.lib.slowfast.visualization.demo_loader",
        "documentation": {}
    },
    {
        "label": "GradCAM",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.gradcam_utils",
        "description": "MorphMLP.build.lib.slowfast.visualization.gradcam_utils",
        "peekOfCode": "class GradCAM:\n    \"\"\"\n    GradCAM class helps create localization maps using the Grad-CAM method for input videos\n    and overlap the maps over the input videos as heatmaps.\n    https://arxiv.org/pdf/1610.02391.pdf\n    \"\"\"\n    def __init__(\n        self, model, target_layers, data_mean, data_std, colormap=\"viridis\"\n    ):\n        \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.visualization.gradcam_utils",
        "documentation": {}
    },
    {
        "label": "WrongPredictionVis",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.prediction_vis",
        "description": "MorphMLP.build.lib.slowfast.visualization.prediction_vis",
        "peekOfCode": "class WrongPredictionVis:\n    \"\"\"\n    WrongPredictionVis class for visualizing video inputs to Tensorboard\n    for instances that the model makes wrong predictions.\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in\n                slowfast/config/defaults.py",
        "detail": "MorphMLP.build.lib.slowfast.visualization.prediction_vis",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.prediction_vis",
        "description": "MorphMLP.build.lib.slowfast.visualization.prediction_vis",
        "peekOfCode": "logger = logging.get_logger(__name__)\nclass WrongPredictionVis:\n    \"\"\"\n    WrongPredictionVis class for visualizing video inputs to Tensorboard\n    for instances that the model makes wrong predictions.\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in",
        "detail": "MorphMLP.build.lib.slowfast.visualization.prediction_vis",
        "documentation": {}
    },
    {
        "label": "Predictor",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.predictor",
        "description": "MorphMLP.build.lib.slowfast.visualization.predictor",
        "peekOfCode": "class Predictor:\n    \"\"\"\n    Action Predictor for action recognition.\n    \"\"\"\n    def __init__(self, cfg, gpu_id=None):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in\n                slowfast/config/defaults.py\n            gpu_id (Optional[int]): GPU id.",
        "detail": "MorphMLP.build.lib.slowfast.visualization.predictor",
        "documentation": {}
    },
    {
        "label": "ActionPredictor",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.predictor",
        "description": "MorphMLP.build.lib.slowfast.visualization.predictor",
        "peekOfCode": "class ActionPredictor:\n    \"\"\"\n    Synchronous Action Prediction and Visualization pipeline with AsyncVis.\n    \"\"\"\n    def __init__(self, cfg, async_vis=None, gpu_id=None):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in\n                slowfast/config/defaults.py\n            async_vis (AsyncVis object): asynchronous visualizer.",
        "detail": "MorphMLP.build.lib.slowfast.visualization.predictor",
        "documentation": {}
    },
    {
        "label": "Detectron2Predictor",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.predictor",
        "description": "MorphMLP.build.lib.slowfast.visualization.predictor",
        "peekOfCode": "class Detectron2Predictor:\n    \"\"\"\n    Wrapper around Detectron2 to return the required predicted bounding boxes\n    as a ndarray.\n    \"\"\"\n    def __init__(self, cfg, gpu_id=None):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in\n                slowfast/config/defaults.py",
        "detail": "MorphMLP.build.lib.slowfast.visualization.predictor",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.predictor",
        "description": "MorphMLP.build.lib.slowfast.visualization.predictor",
        "peekOfCode": "logger = logging.get_logger(__name__)\nclass Predictor:\n    \"\"\"\n    Action Predictor for action recognition.\n    \"\"\"\n    def __init__(self, cfg, gpu_id=None):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in\n                slowfast/config/defaults.py",
        "detail": "MorphMLP.build.lib.slowfast.visualization.predictor",
        "documentation": {}
    },
    {
        "label": "TensorboardWriter",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.tensorboard_vis",
        "description": "MorphMLP.build.lib.slowfast.visualization.tensorboard_vis",
        "peekOfCode": "class TensorboardWriter(object):\n    \"\"\"\n    Helper class to log information to Tensorboard.\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in\n                slowfast/config/defaults.py\n        \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.visualization.tensorboard_vis",
        "documentation": {}
    },
    {
        "label": "add_confusion_matrix",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.tensorboard_vis",
        "description": "MorphMLP.build.lib.slowfast.visualization.tensorboard_vis",
        "peekOfCode": "def add_confusion_matrix(\n    writer,\n    cmtx,\n    num_classes,\n    global_step=None,\n    subset_ids=None,\n    class_names=None,\n    tag=\"Confusion Matrix\",\n    figsize=None,\n):",
        "detail": "MorphMLP.build.lib.slowfast.visualization.tensorboard_vis",
        "documentation": {}
    },
    {
        "label": "plot_hist",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.tensorboard_vis",
        "description": "MorphMLP.build.lib.slowfast.visualization.tensorboard_vis",
        "peekOfCode": "def plot_hist(\n    writer,\n    cmtx,\n    num_classes,\n    k=10,\n    global_step=None,\n    subset_ids=None,\n    class_names=None,\n    figsize=None,\n):",
        "detail": "MorphMLP.build.lib.slowfast.visualization.tensorboard_vis",
        "documentation": {}
    },
    {
        "label": "add_ndim_array",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.tensorboard_vis",
        "description": "MorphMLP.build.lib.slowfast.visualization.tensorboard_vis",
        "peekOfCode": "def add_ndim_array(\n    writer,\n    array,\n    name,\n    nrow=None,\n    normalize=False,\n    global_step=None,\n    heat_map=True,\n):\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.visualization.tensorboard_vis",
        "documentation": {}
    },
    {
        "label": "add_heatmap",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.tensorboard_vis",
        "description": "MorphMLP.build.lib.slowfast.visualization.tensorboard_vis",
        "peekOfCode": "def add_heatmap(tensor):\n    \"\"\"\n    Add heatmap to 2D tensor.\n    Args:\n        tensor (tensor): a 2D tensor. Tensor value must be in [0..1] range.\n    Returns:\n        heatmap (tensor): a 3D tensor. Result of applying heatmap to the 2D tensor.\n    \"\"\"\n    assert tensor.ndim == 2, \"Only support 2D tensors.\"\n    # Move tensor to cpu if necessary.",
        "detail": "MorphMLP.build.lib.slowfast.visualization.tensorboard_vis",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.tensorboard_vis",
        "description": "MorphMLP.build.lib.slowfast.visualization.tensorboard_vis",
        "peekOfCode": "logger = logging.get_logger(__name__)\nlog.getLogger(\"matplotlib\").setLevel(log.ERROR)\nclass TensorboardWriter(object):\n    \"\"\"\n    Helper class to log information to Tensorboard.\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in",
        "detail": "MorphMLP.build.lib.slowfast.visualization.tensorboard_vis",
        "documentation": {}
    },
    {
        "label": "GetWeightAndActivation",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.utils",
        "description": "MorphMLP.build.lib.slowfast.visualization.utils",
        "peekOfCode": "class GetWeightAndActivation:\n    \"\"\"\n    A class used to get weights and activations from specified layers from a Pytorch model.\n    \"\"\"\n    def __init__(self, model, layers):\n        \"\"\"\n        Args:\n            model (nn.Module): the model containing layers to obtain weights and activations from.\n            layers (list of strings): a list of layer names to obtain weights and activations from.\n                Names are hierarchical, separated by /. For example, If a layer follow a path",
        "detail": "MorphMLP.build.lib.slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "TaskInfo",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.utils",
        "description": "MorphMLP.build.lib.slowfast.visualization.utils",
        "peekOfCode": "class TaskInfo:\n    def __init__(self):\n        self.frames = None\n        self.id = -1\n        self.bboxes = None\n        self.action_preds = None\n        self.num_buffer_frames = 0\n        self.img_height = -1\n        self.img_width = -1\n        self.crop_size = -1",
        "detail": "MorphMLP.build.lib.slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "get_confusion_matrix",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.utils",
        "description": "MorphMLP.build.lib.slowfast.visualization.utils",
        "peekOfCode": "def get_confusion_matrix(preds, labels, num_classes, normalize=\"true\"):\n    \"\"\"\n    Calculate confusion matrix on the provided preds and labels.\n    Args:\n        preds (tensor or lists of tensors): predictions. Each tensor is in\n            in the shape of (n_batch, num_classes). Tensor(s) must be on CPU.\n        labels (tensor or lists of tensors): corresponding labels. Each tensor is\n            in the shape of either (n_batch,) or (n_batch, num_classes).\n        num_classes (int): number of classes. Tensor(s) must be on CPU.\n        normalize (Optional[str]) : {‘true’, ‘pred’, ‘all’}, default=\"true\"",
        "detail": "MorphMLP.build.lib.slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "plot_confusion_matrix",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.utils",
        "description": "MorphMLP.build.lib.slowfast.visualization.utils",
        "peekOfCode": "def plot_confusion_matrix(cmtx, num_classes, class_names=None, figsize=None):\n    \"\"\"\n    A function to create a colored and labeled confusion matrix matplotlib figure\n    given true labels and preds.\n    Args:\n        cmtx (ndarray): confusion matrix.\n        num_classes (int): total number of classes.\n        class_names (Optional[list of strs]): a list of class names.\n        figsize (Optional[float, float]): the figure size of the confusion matrix.\n            If None, default to [6.4, 4.8].",
        "detail": "MorphMLP.build.lib.slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "plot_topk_histogram",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.utils",
        "description": "MorphMLP.build.lib.slowfast.visualization.utils",
        "peekOfCode": "def plot_topk_histogram(tag, array, k=10, class_names=None, figsize=None):\n    \"\"\"\n    Plot histogram of top-k value from the given array.\n    Args:\n        tag (str): histogram title.\n        array (tensor): a tensor to draw top k value from.\n        k (int): number of top values to draw from array.\n            Defaut to 10.\n        class_names (list of strings, optional):\n            a list of names for values in array.",
        "detail": "MorphMLP.build.lib.slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "get_indexing",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.utils",
        "description": "MorphMLP.build.lib.slowfast.visualization.utils",
        "peekOfCode": "def get_indexing(string):\n    \"\"\"\n    Parse numpy-like fancy indexing from a string.\n    Args:\n        string (str): string represent the indices to take\n            a subset of from array. Indices for each dimension\n            are separated by `,`; indices for different dimensions\n            are separated by `;`.\n            e.g.: For a numpy array `arr` of shape (3,3,3), the string \"1,2;1,2\"\n            means taking the sub-array `arr[[1,2], [1,2]]",
        "detail": "MorphMLP.build.lib.slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "process_layer_index_data",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.utils",
        "description": "MorphMLP.build.lib.slowfast.visualization.utils",
        "peekOfCode": "def process_layer_index_data(layer_ls, layer_name_prefix=\"\"):\n    \"\"\"\n    Extract layer names and numpy-like fancy indexing from a string.\n    Args:\n        layer_ls (list of strs): list of strings containing data about layer names\n            and their indexing. For each string, layer name and indexing is separated by whitespaces.\n            e.g.: [layer1 1,2;2, layer2, layer3 150;3,4]\n        layer_name_prefix (Optional[str]): prefix to be added to each layer name.\n    Returns:\n        layer_name (list of strings): a list of layer names.",
        "detail": "MorphMLP.build.lib.slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "process_cv2_inputs",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.utils",
        "description": "MorphMLP.build.lib.slowfast.visualization.utils",
        "peekOfCode": "def process_cv2_inputs(frames, cfg):\n    \"\"\"\n    Normalize and prepare inputs as a list of tensors. Each tensor\n    correspond to a unique pathway.\n    Args:\n        frames (list of array): list of input images (correspond to one clip) in range [0, 255].\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n    \"\"\"\n    inputs = torch.from_numpy(np.array(frames)).float() / 255",
        "detail": "MorphMLP.build.lib.slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "get_layer",
        "kind": 2,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.utils",
        "description": "MorphMLP.build.lib.slowfast.visualization.utils",
        "peekOfCode": "def get_layer(model, layer_name):\n    \"\"\"\n    Return the targeted layer (nn.Module Object) given a hierarchical layer name,\n    separated by /.\n    Args:\n        model (model): model to get layers from.\n        layer_name (str): name of the layer.\n    Returns:\n        prev_module (nn.Module): the layer from the model with `layer_name` name.\n    \"\"\"",
        "detail": "MorphMLP.build.lib.slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.utils",
        "description": "MorphMLP.build.lib.slowfast.visualization.utils",
        "peekOfCode": "logger = logging.get_logger(__name__)\ndef get_confusion_matrix(preds, labels, num_classes, normalize=\"true\"):\n    \"\"\"\n    Calculate confusion matrix on the provided preds and labels.\n    Args:\n        preds (tensor or lists of tensors): predictions. Each tensor is in\n            in the shape of (n_batch, num_classes). Tensor(s) must be on CPU.\n        labels (tensor or lists of tensors): corresponding labels. Each tensor is\n            in the shape of either (n_batch,) or (n_batch, num_classes).\n        num_classes (int): number of classes. Tensor(s) must be on CPU.",
        "detail": "MorphMLP.build.lib.slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "ImgVisualizer",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.video_visualizer",
        "description": "MorphMLP.build.lib.slowfast.visualization.video_visualizer",
        "peekOfCode": "class ImgVisualizer(Visualizer):\n    def __init__(self, img_rgb, meta, **kwargs):\n        \"\"\"\n        See https://github.com/facebookresearch/detectron2/blob/master/detectron2/utils/visualizer.py\n        for more details.\n        Args:\n            img_rgb: a tensor or numpy array of shape (H, W, C), where H and W correspond to\n                the height and width of the image respectively. C is the number of\n                color channels. The image is required to be in RGB format since that\n                is a requirement of the Matplotlib library. The image is also expected",
        "detail": "MorphMLP.build.lib.slowfast.visualization.video_visualizer",
        "documentation": {}
    },
    {
        "label": "VideoVisualizer",
        "kind": 6,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.video_visualizer",
        "description": "MorphMLP.build.lib.slowfast.visualization.video_visualizer",
        "peekOfCode": "class VideoVisualizer:\n    def __init__(\n        self,\n        num_classes,\n        class_names_path,\n        top_k=1,\n        colormap=\"rainbow\",\n        thres=0.7,\n        lower_thres=0.3,\n        common_class_names=None,",
        "detail": "MorphMLP.build.lib.slowfast.visualization.video_visualizer",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.build.lib.slowfast.visualization.video_visualizer",
        "description": "MorphMLP.build.lib.slowfast.visualization.video_visualizer",
        "peekOfCode": "logger = logging.get_logger(__name__)\nlog.getLogger(\"matplotlib\").setLevel(log.ERROR)\ndef _create_text_labels(classes, scores, class_names, ground_truth=False):\n    \"\"\"\n    Create text labels.\n    Args:\n        classes (list[int]): a list of class ids for each example.\n        scores (list[float] or None): list of scores for each example.\n        class_names (list[str]): a list of class names, ordered by their ids.\n        ground_truth (bool): whether the labels are ground truth.",
        "detail": "MorphMLP.build.lib.slowfast.visualization.video_visualizer",
        "documentation": {}
    },
    {
        "label": "add_custom_config",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.config.custom_config",
        "description": "MorphMLP.slowfast.config.custom_config",
        "peekOfCode": "def add_custom_config(_C):\n    # Add your own customized configs.\n    pass",
        "detail": "MorphMLP.slowfast.config.custom_config",
        "documentation": {}
    },
    {
        "label": "assert_and_infer_cfg",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "def assert_and_infer_cfg(cfg):\n    # BN assertions.\n    if cfg.BN.USE_PRECISE_STATS:\n        assert cfg.BN.NUM_BATCHES_PRECISE >= 0\n    # TRAIN assertions.\n    assert cfg.TRAIN.CHECKPOINT_TYPE in [\"pytorch\", \"caffe2\"]\n    assert cfg.NUM_GPUS == 0 or cfg.TRAIN.BATCH_SIZE % cfg.NUM_GPUS == 0\n    # TEST assertions.\n    assert cfg.TEST.CHECKPOINT_TYPE in [\"pytorch\", \"caffe2\"]\n    assert cfg.NUM_GPUS == 0 or cfg.TEST.BATCH_SIZE % cfg.NUM_GPUS == 0",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "get_cfg",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "def get_cfg():\n    \"\"\"\n    Get a copy of the default config.\n    \"\"\"\n    return _C.clone()",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C = CfgNode()\n# ---------------------------------------------------------------------------- #\n# Batch norm options\n# ---------------------------------------------------------------------------- #\n_C.BN = CfgNode()\n# Precise BN stats.\n_C.BN.USE_PRECISE_STATS = False\n# Number of samples use to compute precise bn.\n_C.BN.NUM_BATCHES_PRECISE = 200\n# Weight decay value that applies on BN.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.BN",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.BN = CfgNode()\n# Precise BN stats.\n_C.BN.USE_PRECISE_STATS = False\n# Number of samples use to compute precise bn.\n_C.BN.NUM_BATCHES_PRECISE = 200\n# Weight decay value that applies on BN.\n_C.BN.WEIGHT_DECAY = 0.0\n# Norm type, options include `batchnorm`, `sub_batchnorm`, `sync_batchnorm`\n_C.BN.NORM_TYPE = \"batchnorm\"\n# Parameter for SubBatchNorm, where it splits the batch dimension into",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.BN.USE_PRECISE_STATS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.BN.USE_PRECISE_STATS = False\n# Number of samples use to compute precise bn.\n_C.BN.NUM_BATCHES_PRECISE = 200\n# Weight decay value that applies on BN.\n_C.BN.WEIGHT_DECAY = 0.0\n# Norm type, options include `batchnorm`, `sub_batchnorm`, `sync_batchnorm`\n_C.BN.NORM_TYPE = \"batchnorm\"\n# Parameter for SubBatchNorm, where it splits the batch dimension into\n# NUM_SPLITS splits, and run BN on each of them separately independently.\n_C.BN.NUM_SPLITS = 1",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.BN.NUM_BATCHES_PRECISE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.BN.NUM_BATCHES_PRECISE = 200\n# Weight decay value that applies on BN.\n_C.BN.WEIGHT_DECAY = 0.0\n# Norm type, options include `batchnorm`, `sub_batchnorm`, `sync_batchnorm`\n_C.BN.NORM_TYPE = \"batchnorm\"\n# Parameter for SubBatchNorm, where it splits the batch dimension into\n# NUM_SPLITS splits, and run BN on each of them separately independently.\n_C.BN.NUM_SPLITS = 1\n# Parameter for NaiveSyncBatchNorm3d, where the stats across `NUM_SYNC_DEVICES`\n# devices will be synchronized.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.BN.WEIGHT_DECAY",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.BN.WEIGHT_DECAY = 0.0\n# Norm type, options include `batchnorm`, `sub_batchnorm`, `sync_batchnorm`\n_C.BN.NORM_TYPE = \"batchnorm\"\n# Parameter for SubBatchNorm, where it splits the batch dimension into\n# NUM_SPLITS splits, and run BN on each of them separately independently.\n_C.BN.NUM_SPLITS = 1\n# Parameter for NaiveSyncBatchNorm3d, where the stats across `NUM_SYNC_DEVICES`\n# devices will be synchronized.\n_C.BN.NUM_SYNC_DEVICES = 1\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.BN.NORM_TYPE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.BN.NORM_TYPE = \"batchnorm\"\n# Parameter for SubBatchNorm, where it splits the batch dimension into\n# NUM_SPLITS splits, and run BN on each of them separately independently.\n_C.BN.NUM_SPLITS = 1\n# Parameter for NaiveSyncBatchNorm3d, where the stats across `NUM_SYNC_DEVICES`\n# devices will be synchronized.\n_C.BN.NUM_SYNC_DEVICES = 1\n# ---------------------------------------------------------------------------- #\n# Training options.\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.BN.NUM_SPLITS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.BN.NUM_SPLITS = 1\n# Parameter for NaiveSyncBatchNorm3d, where the stats across `NUM_SYNC_DEVICES`\n# devices will be synchronized.\n_C.BN.NUM_SYNC_DEVICES = 1\n# ---------------------------------------------------------------------------- #\n# Training options.\n# ---------------------------------------------------------------------------- #\n_C.TRAIN = CfgNode()\n# If True Train the model, else skip training.\n_C.TRAIN.ENABLE = True",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.BN.NUM_SYNC_DEVICES",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.BN.NUM_SYNC_DEVICES = 1\n# ---------------------------------------------------------------------------- #\n# Training options.\n# ---------------------------------------------------------------------------- #\n_C.TRAIN = CfgNode()\n# If True Train the model, else skip training.\n_C.TRAIN.ENABLE = True\n# Dataset.\n_C.TRAIN.DATASET = \"kinetics\"\n# Total mini-batch size.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TRAIN",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TRAIN = CfgNode()\n# If True Train the model, else skip training.\n_C.TRAIN.ENABLE = True\n# Dataset.\n_C.TRAIN.DATASET = \"kinetics\"\n# Total mini-batch size.\n_C.TRAIN.BATCH_SIZE = 64\n# Evaluate model on test data every eval period epochs.\n_C.TRAIN.EVAL_PERIOD = 10\n# Save model checkpoint every checkpoint period epochs.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TRAIN.ENABLE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TRAIN.ENABLE = True\n# Dataset.\n_C.TRAIN.DATASET = \"kinetics\"\n# Total mini-batch size.\n_C.TRAIN.BATCH_SIZE = 64\n# Evaluate model on test data every eval period epochs.\n_C.TRAIN.EVAL_PERIOD = 10\n# Save model checkpoint every checkpoint period epochs.\n_C.TRAIN.CHECKPOINT_PERIOD = 10\n# Resume training from the latest checkpoint in the output directory.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TRAIN.DATASET",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TRAIN.DATASET = \"kinetics\"\n# Total mini-batch size.\n_C.TRAIN.BATCH_SIZE = 64\n# Evaluate model on test data every eval period epochs.\n_C.TRAIN.EVAL_PERIOD = 10\n# Save model checkpoint every checkpoint period epochs.\n_C.TRAIN.CHECKPOINT_PERIOD = 10\n# Resume training from the latest checkpoint in the output directory.\n_C.TRAIN.AUTO_RESUME = True\n# Path to the checkpoint to load the initial weight.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TRAIN.BATCH_SIZE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TRAIN.BATCH_SIZE = 64\n# Evaluate model on test data every eval period epochs.\n_C.TRAIN.EVAL_PERIOD = 10\n# Save model checkpoint every checkpoint period epochs.\n_C.TRAIN.CHECKPOINT_PERIOD = 10\n# Resume training from the latest checkpoint in the output directory.\n_C.TRAIN.AUTO_RESUME = True\n# Path to the checkpoint to load the initial weight.\n_C.TRAIN.CHECKPOINT_FILE_PATH = \"\"\n# Checkpoint types include `caffe2` or `pytorch`.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TRAIN.EVAL_PERIOD",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TRAIN.EVAL_PERIOD = 10\n# Save model checkpoint every checkpoint period epochs.\n_C.TRAIN.CHECKPOINT_PERIOD = 10\n# Resume training from the latest checkpoint in the output directory.\n_C.TRAIN.AUTO_RESUME = True\n# Path to the checkpoint to load the initial weight.\n_C.TRAIN.CHECKPOINT_FILE_PATH = \"\"\n# Checkpoint types include `caffe2` or `pytorch`.\n_C.TRAIN.CHECKPOINT_TYPE = \"pytorch\"\n# If True, perform inflation when loading checkpoint.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TRAIN.CHECKPOINT_PERIOD",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TRAIN.CHECKPOINT_PERIOD = 10\n# Resume training from the latest checkpoint in the output directory.\n_C.TRAIN.AUTO_RESUME = True\n# Path to the checkpoint to load the initial weight.\n_C.TRAIN.CHECKPOINT_FILE_PATH = \"\"\n# Checkpoint types include `caffe2` or `pytorch`.\n_C.TRAIN.CHECKPOINT_TYPE = \"pytorch\"\n# If True, perform inflation when loading checkpoint.\n_C.TRAIN.CHECKPOINT_INFLATE = False\n# If True, reset epochs when loading checkpoint.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TRAIN.AUTO_RESUME",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TRAIN.AUTO_RESUME = True\n# Path to the checkpoint to load the initial weight.\n_C.TRAIN.CHECKPOINT_FILE_PATH = \"\"\n# Checkpoint types include `caffe2` or `pytorch`.\n_C.TRAIN.CHECKPOINT_TYPE = \"pytorch\"\n# If True, perform inflation when loading checkpoint.\n_C.TRAIN.CHECKPOINT_INFLATE = False\n# If True, reset epochs when loading checkpoint.\n_C.TRAIN.CHECKPOINT_EPOCH_RESET = False\n# If set, clear all layer names according to the pattern provided.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TRAIN.CHECKPOINT_FILE_PATH",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TRAIN.CHECKPOINT_FILE_PATH = \"\"\n# Checkpoint types include `caffe2` or `pytorch`.\n_C.TRAIN.CHECKPOINT_TYPE = \"pytorch\"\n# If True, perform inflation when loading checkpoint.\n_C.TRAIN.CHECKPOINT_INFLATE = False\n# If True, reset epochs when loading checkpoint.\n_C.TRAIN.CHECKPOINT_EPOCH_RESET = False\n# If set, clear all layer names according to the pattern provided.\n_C.TRAIN.CHECKPOINT_CLEAR_NAME_PATTERN = ()  # (\"backbone.\",)\n# If True, use FP16 for activations",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TRAIN.CHECKPOINT_TYPE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TRAIN.CHECKPOINT_TYPE = \"pytorch\"\n# If True, perform inflation when loading checkpoint.\n_C.TRAIN.CHECKPOINT_INFLATE = False\n# If True, reset epochs when loading checkpoint.\n_C.TRAIN.CHECKPOINT_EPOCH_RESET = False\n# If set, clear all layer names according to the pattern provided.\n_C.TRAIN.CHECKPOINT_CLEAR_NAME_PATTERN = ()  # (\"backbone.\",)\n# If True, use FP16 for activations\n_C.TRAIN.MIXED_PRECISION = False\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TRAIN.CHECKPOINT_INFLATE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TRAIN.CHECKPOINT_INFLATE = False\n# If True, reset epochs when loading checkpoint.\n_C.TRAIN.CHECKPOINT_EPOCH_RESET = False\n# If set, clear all layer names according to the pattern provided.\n_C.TRAIN.CHECKPOINT_CLEAR_NAME_PATTERN = ()  # (\"backbone.\",)\n# If True, use FP16 for activations\n_C.TRAIN.MIXED_PRECISION = False\n# ---------------------------------------------------------------------------- #\n# Augmentation options.\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TRAIN.CHECKPOINT_EPOCH_RESET",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TRAIN.CHECKPOINT_EPOCH_RESET = False\n# If set, clear all layer names according to the pattern provided.\n_C.TRAIN.CHECKPOINT_CLEAR_NAME_PATTERN = ()  # (\"backbone.\",)\n# If True, use FP16 for activations\n_C.TRAIN.MIXED_PRECISION = False\n# ---------------------------------------------------------------------------- #\n# Augmentation options.\n# ---------------------------------------------------------------------------- #\n_C.AUG = CfgNode()\n# Whether to enable randaug.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TRAIN.CHECKPOINT_CLEAR_NAME_PATTERN",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TRAIN.CHECKPOINT_CLEAR_NAME_PATTERN = ()  # (\"backbone.\",)\n# If True, use FP16 for activations\n_C.TRAIN.MIXED_PRECISION = False\n# ---------------------------------------------------------------------------- #\n# Augmentation options.\n# ---------------------------------------------------------------------------- #\n_C.AUG = CfgNode()\n# Whether to enable randaug.\n_C.AUG.ENABLE = False\n# Number of repeated augmentations to used during training.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TRAIN.MIXED_PRECISION",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TRAIN.MIXED_PRECISION = False\n# ---------------------------------------------------------------------------- #\n# Augmentation options.\n# ---------------------------------------------------------------------------- #\n_C.AUG = CfgNode()\n# Whether to enable randaug.\n_C.AUG.ENABLE = False\n# Number of repeated augmentations to used during training.\n# If this is greater than 1, then the actual batch size is\n# TRAIN.BATCH_SIZE * AUG.NUM_SAMPLE.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AUG",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AUG = CfgNode()\n# Whether to enable randaug.\n_C.AUG.ENABLE = False\n# Number of repeated augmentations to used during training.\n# If this is greater than 1, then the actual batch size is\n# TRAIN.BATCH_SIZE * AUG.NUM_SAMPLE.\n_C.AUG.NUM_SAMPLE = 1\n# Not used if using randaug.\n_C.AUG.COLOR_JITTER = 0.4\n# RandAug parameters.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AUG.ENABLE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AUG.ENABLE = False\n# Number of repeated augmentations to used during training.\n# If this is greater than 1, then the actual batch size is\n# TRAIN.BATCH_SIZE * AUG.NUM_SAMPLE.\n_C.AUG.NUM_SAMPLE = 1\n# Not used if using randaug.\n_C.AUG.COLOR_JITTER = 0.4\n# RandAug parameters.\n_C.AUG.AA_TYPE = \"rand-m9-mstd0.5-inc1\"\n# Interpolation method.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AUG.NUM_SAMPLE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AUG.NUM_SAMPLE = 1\n# Not used if using randaug.\n_C.AUG.COLOR_JITTER = 0.4\n# RandAug parameters.\n_C.AUG.AA_TYPE = \"rand-m9-mstd0.5-inc1\"\n# Interpolation method.\n_C.AUG.INTERPOLATION = \"bicubic\"\n# Probability of random erasing.\n_C.AUG.RE_PROB = 0.25\n# Random erasing mode.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AUG.COLOR_JITTER",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AUG.COLOR_JITTER = 0.4\n# RandAug parameters.\n_C.AUG.AA_TYPE = \"rand-m9-mstd0.5-inc1\"\n# Interpolation method.\n_C.AUG.INTERPOLATION = \"bicubic\"\n# Probability of random erasing.\n_C.AUG.RE_PROB = 0.25\n# Random erasing mode.\n_C.AUG.RE_MODE = \"pixel\"\n# Random erase count.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AUG.AA_TYPE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AUG.AA_TYPE = \"rand-m9-mstd0.5-inc1\"\n# Interpolation method.\n_C.AUG.INTERPOLATION = \"bicubic\"\n# Probability of random erasing.\n_C.AUG.RE_PROB = 0.25\n# Random erasing mode.\n_C.AUG.RE_MODE = \"pixel\"\n# Random erase count.\n_C.AUG.RE_COUNT = 1\n# Do not random erase first (clean) augmentation split.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AUG.INTERPOLATION",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AUG.INTERPOLATION = \"bicubic\"\n# Probability of random erasing.\n_C.AUG.RE_PROB = 0.25\n# Random erasing mode.\n_C.AUG.RE_MODE = \"pixel\"\n# Random erase count.\n_C.AUG.RE_COUNT = 1\n# Do not random erase first (clean) augmentation split.\n_C.AUG.RE_SPLIT = False\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AUG.RE_PROB",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AUG.RE_PROB = 0.25\n# Random erasing mode.\n_C.AUG.RE_MODE = \"pixel\"\n# Random erase count.\n_C.AUG.RE_COUNT = 1\n# Do not random erase first (clean) augmentation split.\n_C.AUG.RE_SPLIT = False\n# ---------------------------------------------------------------------------- #\n# MipUp options.\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AUG.RE_MODE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AUG.RE_MODE = \"pixel\"\n# Random erase count.\n_C.AUG.RE_COUNT = 1\n# Do not random erase first (clean) augmentation split.\n_C.AUG.RE_SPLIT = False\n# ---------------------------------------------------------------------------- #\n# MipUp options.\n# ---------------------------------------------------------------------------- #\n_C.MIXUP = CfgNode()\n# Whether to use mixup.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AUG.RE_COUNT",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AUG.RE_COUNT = 1\n# Do not random erase first (clean) augmentation split.\n_C.AUG.RE_SPLIT = False\n# ---------------------------------------------------------------------------- #\n# MipUp options.\n# ---------------------------------------------------------------------------- #\n_C.MIXUP = CfgNode()\n# Whether to use mixup.\n_C.MIXUP.ENABLE = False\n# Mixup alpha.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AUG.RE_SPLIT",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AUG.RE_SPLIT = False\n# ---------------------------------------------------------------------------- #\n# MipUp options.\n# ---------------------------------------------------------------------------- #\n_C.MIXUP = CfgNode()\n# Whether to use mixup.\n_C.MIXUP.ENABLE = False\n# Mixup alpha.\n_C.MIXUP.ALPHA = 0.8\n# Cutmix alpha.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MIXUP",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MIXUP = CfgNode()\n# Whether to use mixup.\n_C.MIXUP.ENABLE = False\n# Mixup alpha.\n_C.MIXUP.ALPHA = 0.8\n# Cutmix alpha.\n_C.MIXUP.CUTMIX_ALPHA = 1.0\n# Probability of performing mixup or cutmix when either/both is enabled.\n_C.MIXUP.PROB = 1.0\n# Probability of switching to cutmix when both mixup and cutmix enabled.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MIXUP.ENABLE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MIXUP.ENABLE = False\n# Mixup alpha.\n_C.MIXUP.ALPHA = 0.8\n# Cutmix alpha.\n_C.MIXUP.CUTMIX_ALPHA = 1.0\n# Probability of performing mixup or cutmix when either/both is enabled.\n_C.MIXUP.PROB = 1.0\n# Probability of switching to cutmix when both mixup and cutmix enabled.\n_C.MIXUP.SWITCH_PROB = 0.5\n# Label smoothing.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MIXUP.ALPHA",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MIXUP.ALPHA = 0.8\n# Cutmix alpha.\n_C.MIXUP.CUTMIX_ALPHA = 1.0\n# Probability of performing mixup or cutmix when either/both is enabled.\n_C.MIXUP.PROB = 1.0\n# Probability of switching to cutmix when both mixup and cutmix enabled.\n_C.MIXUP.SWITCH_PROB = 0.5\n# Label smoothing.\n_C.MIXUP.LABEL_SMOOTH_VALUE = 0.1\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MIXUP.CUTMIX_ALPHA",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MIXUP.CUTMIX_ALPHA = 1.0\n# Probability of performing mixup or cutmix when either/both is enabled.\n_C.MIXUP.PROB = 1.0\n# Probability of switching to cutmix when both mixup and cutmix enabled.\n_C.MIXUP.SWITCH_PROB = 0.5\n# Label smoothing.\n_C.MIXUP.LABEL_SMOOTH_VALUE = 0.1\n# ---------------------------------------------------------------------------- #\n# Testing options\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MIXUP.PROB",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MIXUP.PROB = 1.0\n# Probability of switching to cutmix when both mixup and cutmix enabled.\n_C.MIXUP.SWITCH_PROB = 0.5\n# Label smoothing.\n_C.MIXUP.LABEL_SMOOTH_VALUE = 0.1\n# ---------------------------------------------------------------------------- #\n# Testing options\n# ---------------------------------------------------------------------------- #\n_C.TEST = CfgNode()\n# If True test the model, else skip the testing.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MIXUP.SWITCH_PROB",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MIXUP.SWITCH_PROB = 0.5\n# Label smoothing.\n_C.MIXUP.LABEL_SMOOTH_VALUE = 0.1\n# ---------------------------------------------------------------------------- #\n# Testing options\n# ---------------------------------------------------------------------------- #\n_C.TEST = CfgNode()\n# If True test the model, else skip the testing.\n_C.TEST.ENABLE = True\n# Dataset for testing.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MIXUP.LABEL_SMOOTH_VALUE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MIXUP.LABEL_SMOOTH_VALUE = 0.1\n# ---------------------------------------------------------------------------- #\n# Testing options\n# ---------------------------------------------------------------------------- #\n_C.TEST = CfgNode()\n# If True test the model, else skip the testing.\n_C.TEST.ENABLE = True\n# Dataset for testing.\n_C.TEST.DATASET = \"kinetics\"\n# Total mini-batch size",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TEST",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TEST = CfgNode()\n# If True test the model, else skip the testing.\n_C.TEST.ENABLE = True\n# Dataset for testing.\n_C.TEST.DATASET = \"kinetics\"\n# Total mini-batch size\n_C.TEST.BATCH_SIZE = 8\n# Path to the checkpoint to load the initial weight.\n_C.TEST.CHECKPOINT_FILE_PATH = \"\"\n# Number of clips to sample from a video uniformly for aggregating the",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TEST.ENABLE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TEST.ENABLE = True\n# Dataset for testing.\n_C.TEST.DATASET = \"kinetics\"\n# Total mini-batch size\n_C.TEST.BATCH_SIZE = 8\n# Path to the checkpoint to load the initial weight.\n_C.TEST.CHECKPOINT_FILE_PATH = \"\"\n# Number of clips to sample from a video uniformly for aggregating the\n# prediction results.\n_C.TEST.NUM_ENSEMBLE_VIEWS = 10",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TEST.DATASET",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TEST.DATASET = \"kinetics\"\n# Total mini-batch size\n_C.TEST.BATCH_SIZE = 8\n# Path to the checkpoint to load the initial weight.\n_C.TEST.CHECKPOINT_FILE_PATH = \"\"\n# Number of clips to sample from a video uniformly for aggregating the\n# prediction results.\n_C.TEST.NUM_ENSEMBLE_VIEWS = 10\n# Number of crops to sample from a frame spatially for aggregating the\n# prediction results.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TEST.BATCH_SIZE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TEST.BATCH_SIZE = 8\n# Path to the checkpoint to load the initial weight.\n_C.TEST.CHECKPOINT_FILE_PATH = \"\"\n# Number of clips to sample from a video uniformly for aggregating the\n# prediction results.\n_C.TEST.NUM_ENSEMBLE_VIEWS = 10\n# Number of crops to sample from a frame spatially for aggregating the\n# prediction results.\n_C.TEST.NUM_SPATIAL_CROPS = 3\n# Checkpoint types include `caffe2` or `pytorch`.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TEST.CHECKPOINT_FILE_PATH",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TEST.CHECKPOINT_FILE_PATH = \"\"\n# Number of clips to sample from a video uniformly for aggregating the\n# prediction results.\n_C.TEST.NUM_ENSEMBLE_VIEWS = 10\n# Number of crops to sample from a frame spatially for aggregating the\n# prediction results.\n_C.TEST.NUM_SPATIAL_CROPS = 3\n# Checkpoint types include `caffe2` or `pytorch`.\n_C.TEST.CHECKPOINT_TYPE = \"pytorch\"\n# Path to saving prediction results file.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TEST.NUM_ENSEMBLE_VIEWS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TEST.NUM_ENSEMBLE_VIEWS = 10\n# Number of crops to sample from a frame spatially for aggregating the\n# prediction results.\n_C.TEST.NUM_SPATIAL_CROPS = 3\n# Checkpoint types include `caffe2` or `pytorch`.\n_C.TEST.CHECKPOINT_TYPE = \"pytorch\"\n# Path to saving prediction results file.\n_C.TEST.SAVE_RESULTS_PATH = \"\"\n# -----------------------------------------------------------------------------\n# ResNet options",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TEST.NUM_SPATIAL_CROPS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TEST.NUM_SPATIAL_CROPS = 3\n# Checkpoint types include `caffe2` or `pytorch`.\n_C.TEST.CHECKPOINT_TYPE = \"pytorch\"\n# Path to saving prediction results file.\n_C.TEST.SAVE_RESULTS_PATH = \"\"\n# -----------------------------------------------------------------------------\n# ResNet options\n# -----------------------------------------------------------------------------\n_C.RESNET = CfgNode()\n# Transformation function.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TEST.CHECKPOINT_TYPE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TEST.CHECKPOINT_TYPE = \"pytorch\"\n# Path to saving prediction results file.\n_C.TEST.SAVE_RESULTS_PATH = \"\"\n# -----------------------------------------------------------------------------\n# ResNet options\n# -----------------------------------------------------------------------------\n_C.RESNET = CfgNode()\n# Transformation function.\n_C.RESNET.TRANS_FUNC = \"bottleneck_transform\"\n# Number of groups. 1 for ResNet, and larger than 1 for ResNeXt).",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TEST.SAVE_RESULTS_PATH",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TEST.SAVE_RESULTS_PATH = \"\"\n# -----------------------------------------------------------------------------\n# ResNet options\n# -----------------------------------------------------------------------------\n_C.RESNET = CfgNode()\n# Transformation function.\n_C.RESNET.TRANS_FUNC = \"bottleneck_transform\"\n# Number of groups. 1 for ResNet, and larger than 1 for ResNeXt).\n_C.RESNET.NUM_GROUPS = 1\n# Width of each group (64 -> ResNet; 4 -> ResNeXt).",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.RESNET",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.RESNET = CfgNode()\n# Transformation function.\n_C.RESNET.TRANS_FUNC = \"bottleneck_transform\"\n# Number of groups. 1 for ResNet, and larger than 1 for ResNeXt).\n_C.RESNET.NUM_GROUPS = 1\n# Width of each group (64 -> ResNet; 4 -> ResNeXt).\n_C.RESNET.WIDTH_PER_GROUP = 64\n# Apply relu in a inplace manner.\n_C.RESNET.INPLACE_RELU = True\n# Apply stride to 1x1 conv.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.RESNET.TRANS_FUNC",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.RESNET.TRANS_FUNC = \"bottleneck_transform\"\n# Number of groups. 1 for ResNet, and larger than 1 for ResNeXt).\n_C.RESNET.NUM_GROUPS = 1\n# Width of each group (64 -> ResNet; 4 -> ResNeXt).\n_C.RESNET.WIDTH_PER_GROUP = 64\n# Apply relu in a inplace manner.\n_C.RESNET.INPLACE_RELU = True\n# Apply stride to 1x1 conv.\n_C.RESNET.STRIDE_1X1 = False\n#  If true, initialize the gamma of the final BN of each block to zero.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.RESNET.NUM_GROUPS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.RESNET.NUM_GROUPS = 1\n# Width of each group (64 -> ResNet; 4 -> ResNeXt).\n_C.RESNET.WIDTH_PER_GROUP = 64\n# Apply relu in a inplace manner.\n_C.RESNET.INPLACE_RELU = True\n# Apply stride to 1x1 conv.\n_C.RESNET.STRIDE_1X1 = False\n#  If true, initialize the gamma of the final BN of each block to zero.\n_C.RESNET.ZERO_INIT_FINAL_BN = False\n# Number of weight layers.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.RESNET.WIDTH_PER_GROUP",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.RESNET.WIDTH_PER_GROUP = 64\n# Apply relu in a inplace manner.\n_C.RESNET.INPLACE_RELU = True\n# Apply stride to 1x1 conv.\n_C.RESNET.STRIDE_1X1 = False\n#  If true, initialize the gamma of the final BN of each block to zero.\n_C.RESNET.ZERO_INIT_FINAL_BN = False\n# Number of weight layers.\n_C.RESNET.DEPTH = 50\n# If the current block has more than NUM_BLOCK_TEMP_KERNEL blocks, use temporal",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.RESNET.INPLACE_RELU",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.RESNET.INPLACE_RELU = True\n# Apply stride to 1x1 conv.\n_C.RESNET.STRIDE_1X1 = False\n#  If true, initialize the gamma of the final BN of each block to zero.\n_C.RESNET.ZERO_INIT_FINAL_BN = False\n# Number of weight layers.\n_C.RESNET.DEPTH = 50\n# If the current block has more than NUM_BLOCK_TEMP_KERNEL blocks, use temporal\n# kernel of 1 for the rest of the blocks.\n_C.RESNET.NUM_BLOCK_TEMP_KERNEL = [[3], [4], [6], [3]]",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.RESNET.STRIDE_1X1",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.RESNET.STRIDE_1X1 = False\n#  If true, initialize the gamma of the final BN of each block to zero.\n_C.RESNET.ZERO_INIT_FINAL_BN = False\n# Number of weight layers.\n_C.RESNET.DEPTH = 50\n# If the current block has more than NUM_BLOCK_TEMP_KERNEL blocks, use temporal\n# kernel of 1 for the rest of the blocks.\n_C.RESNET.NUM_BLOCK_TEMP_KERNEL = [[3], [4], [6], [3]]\n# Size of stride on different res stages.\n_C.RESNET.SPATIAL_STRIDES = [[1], [2], [2], [2]]",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.RESNET.ZERO_INIT_FINAL_BN",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.RESNET.ZERO_INIT_FINAL_BN = False\n# Number of weight layers.\n_C.RESNET.DEPTH = 50\n# If the current block has more than NUM_BLOCK_TEMP_KERNEL blocks, use temporal\n# kernel of 1 for the rest of the blocks.\n_C.RESNET.NUM_BLOCK_TEMP_KERNEL = [[3], [4], [6], [3]]\n# Size of stride on different res stages.\n_C.RESNET.SPATIAL_STRIDES = [[1], [2], [2], [2]]\n# Size of dilation on different res stages.\n_C.RESNET.SPATIAL_DILATIONS = [[1], [1], [1], [1]]",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.RESNET.DEPTH",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.RESNET.DEPTH = 50\n# If the current block has more than NUM_BLOCK_TEMP_KERNEL blocks, use temporal\n# kernel of 1 for the rest of the blocks.\n_C.RESNET.NUM_BLOCK_TEMP_KERNEL = [[3], [4], [6], [3]]\n# Size of stride on different res stages.\n_C.RESNET.SPATIAL_STRIDES = [[1], [2], [2], [2]]\n# Size of dilation on different res stages.\n_C.RESNET.SPATIAL_DILATIONS = [[1], [1], [1], [1]]\n# -----------------------------------------------------------------------------\n# MORPH options",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.RESNET.NUM_BLOCK_TEMP_KERNEL",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.RESNET.NUM_BLOCK_TEMP_KERNEL = [[3], [4], [6], [3]]\n# Size of stride on different res stages.\n_C.RESNET.SPATIAL_STRIDES = [[1], [2], [2], [2]]\n# Size of dilation on different res stages.\n_C.RESNET.SPATIAL_DILATIONS = [[1], [1], [1], [1]]\n# -----------------------------------------------------------------------------\n# MORPH options\n# -----------------------------------------------------------------------------\n_C.MORPH = CfgNode()\n# layers.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.RESNET.SPATIAL_STRIDES",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.RESNET.SPATIAL_STRIDES = [[1], [2], [2], [2]]\n# Size of dilation on different res stages.\n_C.RESNET.SPATIAL_DILATIONS = [[1], [1], [1], [1]]\n# -----------------------------------------------------------------------------\n# MORPH options\n# -----------------------------------------------------------------------------\n_C.MORPH = CfgNode()\n# layers.\n_C.MORPH.LAYERS = [4, 3, 8, 3]\n# whether downsample in the end.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.RESNET.SPATIAL_DILATIONS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.RESNET.SPATIAL_DILATIONS = [[1], [1], [1], [1]]\n# -----------------------------------------------------------------------------\n# MORPH options\n# -----------------------------------------------------------------------------\n_C.MORPH = CfgNode()\n# layers.\n_C.MORPH.LAYERS = [4, 3, 8, 3]\n# whether downsample in the end.\n_C.MORPH.TRANSITIONS = [True, False, False, False]\n# dimesion of segment.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MORPH",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MORPH = CfgNode()\n# layers.\n_C.MORPH.LAYERS = [4, 3, 8, 3]\n# whether downsample in the end.\n_C.MORPH.TRANSITIONS = [True, False, False, False]\n# dimesion of segment.\n_C.MORPH.SEGMENT_DIM = [32, 16, 16, 16]\n# temporal stride.\n_C.MORPH.T_STRIDE = 1\n# mlp ratio of different layer.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MORPH.LAYERS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MORPH.LAYERS = [4, 3, 8, 3]\n# whether downsample in the end.\n_C.MORPH.TRANSITIONS = [True, False, False, False]\n# dimesion of segment.\n_C.MORPH.SEGMENT_DIM = [32, 16, 16, 16]\n# temporal stride.\n_C.MORPH.T_STRIDE = 1\n# mlp ratio of different layer.\n_C.MORPH.MLP_RATIOS = [3, 3, 3, 3]\n# embedding dimensions of different layer.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MORPH.TRANSITIONS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MORPH.TRANSITIONS = [True, False, False, False]\n# dimesion of segment.\n_C.MORPH.SEGMENT_DIM = [32, 16, 16, 16]\n# temporal stride.\n_C.MORPH.T_STRIDE = 1\n# mlp ratio of different layer.\n_C.MORPH.MLP_RATIOS = [3, 3, 3, 3]\n# embedding dimensions of different layer.\n_C.MORPH.EMBED_DIMS = [192, 384, 384, 384]\n# patch size.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MORPH.SEGMENT_DIM",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MORPH.SEGMENT_DIM = [32, 16, 16, 16]\n# temporal stride.\n_C.MORPH.T_STRIDE = 1\n# mlp ratio of different layer.\n_C.MORPH.MLP_RATIOS = [3, 3, 3, 3]\n# embedding dimensions of different layer.\n_C.MORPH.EMBED_DIMS = [192, 384, 384, 384]\n# patch size.\n_C.MORPH.PATCH_SIZE = 7\n# override default qk scale of head_dim ** -0.5 if set.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MORPH.T_STRIDE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MORPH.T_STRIDE = 1\n# mlp ratio of different layer.\n_C.MORPH.MLP_RATIOS = [3, 3, 3, 3]\n# embedding dimensions of different layer.\n_C.MORPH.EMBED_DIMS = [192, 384, 384, 384]\n# patch size.\n_C.MORPH.PATCH_SIZE = 7\n# override default qk scale of head_dim ** -0.5 if set.\n_C.MORPH.QKV_SCALE = None\n# enable bias for qkv if True.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MORPH.MLP_RATIOS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MORPH.MLP_RATIOS = [3, 3, 3, 3]\n# embedding dimensions of different layer.\n_C.MORPH.EMBED_DIMS = [192, 384, 384, 384]\n# patch size.\n_C.MORPH.PATCH_SIZE = 7\n# override default qk scale of head_dim ** -0.5 if set.\n_C.MORPH.QKV_SCALE = None\n# enable bias for qkv if True.\n_C.MORPH.QKV_BIAS = True\n# attention dropout rate.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MORPH.EMBED_DIMS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MORPH.EMBED_DIMS = [192, 384, 384, 384]\n# patch size.\n_C.MORPH.PATCH_SIZE = 7\n# override default qk scale of head_dim ** -0.5 if set.\n_C.MORPH.QKV_SCALE = None\n# enable bias for qkv if True.\n_C.MORPH.QKV_BIAS = True\n# attention dropout rate.\n_C.MORPH.ATTENTION_DROPOUT_RATE = 0\n# stochastic depth rate.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MORPH.PATCH_SIZE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MORPH.PATCH_SIZE = 7\n# override default qk scale of head_dim ** -0.5 if set.\n_C.MORPH.QKV_SCALE = None\n# enable bias for qkv if True.\n_C.MORPH.QKV_BIAS = True\n# attention dropout rate.\n_C.MORPH.ATTENTION_DROPOUT_RATE = 0\n# stochastic depth rate.\n_C.MORPH.DROP_DEPTH_RATE = 0.1\n# pretrained path",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MORPH.QKV_SCALE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MORPH.QKV_SCALE = None\n# enable bias for qkv if True.\n_C.MORPH.QKV_BIAS = True\n# attention dropout rate.\n_C.MORPH.ATTENTION_DROPOUT_RATE = 0\n# stochastic depth rate.\n_C.MORPH.DROP_DEPTH_RATE = 0.1\n# pretrained path\n_C.MORPH.PRETRAIN_PATH = None\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MORPH.QKV_BIAS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MORPH.QKV_BIAS = True\n# attention dropout rate.\n_C.MORPH.ATTENTION_DROPOUT_RATE = 0\n# stochastic depth rate.\n_C.MORPH.DROP_DEPTH_RATE = 0.1\n# pretrained path\n_C.MORPH.PRETRAIN_PATH = None\n# ---------------------------------------------------------------------------- #\n# X3D  options\n# See https://arxiv.org/abs/2004.04730 for details about X3D Networks.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MORPH.ATTENTION_DROPOUT_RATE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MORPH.ATTENTION_DROPOUT_RATE = 0\n# stochastic depth rate.\n_C.MORPH.DROP_DEPTH_RATE = 0.1\n# pretrained path\n_C.MORPH.PRETRAIN_PATH = None\n# ---------------------------------------------------------------------------- #\n# X3D  options\n# See https://arxiv.org/abs/2004.04730 for details about X3D Networks.\n# ---------------------------------------------------------------------------- #\n_C.X3D = CfgNode()",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MORPH.DROP_DEPTH_RATE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MORPH.DROP_DEPTH_RATE = 0.1\n# pretrained path\n_C.MORPH.PRETRAIN_PATH = None\n# ---------------------------------------------------------------------------- #\n# X3D  options\n# See https://arxiv.org/abs/2004.04730 for details about X3D Networks.\n# ---------------------------------------------------------------------------- #\n_C.X3D = CfgNode()\n# Width expansion factor.\n_C.X3D.WIDTH_FACTOR = 1.0",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MORPH.PRETRAIN_PATH",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MORPH.PRETRAIN_PATH = None\n# ---------------------------------------------------------------------------- #\n# X3D  options\n# See https://arxiv.org/abs/2004.04730 for details about X3D Networks.\n# ---------------------------------------------------------------------------- #\n_C.X3D = CfgNode()\n# Width expansion factor.\n_C.X3D.WIDTH_FACTOR = 1.0\n# Depth expansion factor.\n_C.X3D.DEPTH_FACTOR = 1.0",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.X3D",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.X3D = CfgNode()\n# Width expansion factor.\n_C.X3D.WIDTH_FACTOR = 1.0\n# Depth expansion factor.\n_C.X3D.DEPTH_FACTOR = 1.0\n# Bottleneck expansion factor for the 3x3x3 conv.\n_C.X3D.BOTTLENECK_FACTOR = 1.0  #\n# Dimensions of the last linear layer before classificaiton.\n_C.X3D.DIM_C5 = 2048\n# Dimensions of the first 3x3 conv layer.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.X3D.WIDTH_FACTOR",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.X3D.WIDTH_FACTOR = 1.0\n# Depth expansion factor.\n_C.X3D.DEPTH_FACTOR = 1.0\n# Bottleneck expansion factor for the 3x3x3 conv.\n_C.X3D.BOTTLENECK_FACTOR = 1.0  #\n# Dimensions of the last linear layer before classificaiton.\n_C.X3D.DIM_C5 = 2048\n# Dimensions of the first 3x3 conv layer.\n_C.X3D.DIM_C1 = 12\n# Whether to scale the width of Res2, default is false.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.X3D.DEPTH_FACTOR",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.X3D.DEPTH_FACTOR = 1.0\n# Bottleneck expansion factor for the 3x3x3 conv.\n_C.X3D.BOTTLENECK_FACTOR = 1.0  #\n# Dimensions of the last linear layer before classificaiton.\n_C.X3D.DIM_C5 = 2048\n# Dimensions of the first 3x3 conv layer.\n_C.X3D.DIM_C1 = 12\n# Whether to scale the width of Res2, default is false.\n_C.X3D.SCALE_RES2 = False\n# Whether to use a BatchNorm (BN) layer before the classifier, default is false.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.X3D.BOTTLENECK_FACTOR",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.X3D.BOTTLENECK_FACTOR = 1.0  #\n# Dimensions of the last linear layer before classificaiton.\n_C.X3D.DIM_C5 = 2048\n# Dimensions of the first 3x3 conv layer.\n_C.X3D.DIM_C1 = 12\n# Whether to scale the width of Res2, default is false.\n_C.X3D.SCALE_RES2 = False\n# Whether to use a BatchNorm (BN) layer before the classifier, default is false.\n_C.X3D.BN_LIN5 = False\n# Whether to use channelwise (=depthwise) convolution in the center (3x3x3)",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.X3D.DIM_C5",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.X3D.DIM_C5 = 2048\n# Dimensions of the first 3x3 conv layer.\n_C.X3D.DIM_C1 = 12\n# Whether to scale the width of Res2, default is false.\n_C.X3D.SCALE_RES2 = False\n# Whether to use a BatchNorm (BN) layer before the classifier, default is false.\n_C.X3D.BN_LIN5 = False\n# Whether to use channelwise (=depthwise) convolution in the center (3x3x3)\n# convolution operation of the residual blocks.\n_C.X3D.CHANNELWISE_3x3x3 = True",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.X3D.DIM_C1",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.X3D.DIM_C1 = 12\n# Whether to scale the width of Res2, default is false.\n_C.X3D.SCALE_RES2 = False\n# Whether to use a BatchNorm (BN) layer before the classifier, default is false.\n_C.X3D.BN_LIN5 = False\n# Whether to use channelwise (=depthwise) convolution in the center (3x3x3)\n# convolution operation of the residual blocks.\n_C.X3D.CHANNELWISE_3x3x3 = True\n# -----------------------------------------------------------------------------\n# Nonlocal options",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.X3D.SCALE_RES2",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.X3D.SCALE_RES2 = False\n# Whether to use a BatchNorm (BN) layer before the classifier, default is false.\n_C.X3D.BN_LIN5 = False\n# Whether to use channelwise (=depthwise) convolution in the center (3x3x3)\n# convolution operation of the residual blocks.\n_C.X3D.CHANNELWISE_3x3x3 = True\n# -----------------------------------------------------------------------------\n# Nonlocal options\n# -----------------------------------------------------------------------------\n_C.NONLOCAL = CfgNode()",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.X3D.BN_LIN5",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.X3D.BN_LIN5 = False\n# Whether to use channelwise (=depthwise) convolution in the center (3x3x3)\n# convolution operation of the residual blocks.\n_C.X3D.CHANNELWISE_3x3x3 = True\n# -----------------------------------------------------------------------------\n# Nonlocal options\n# -----------------------------------------------------------------------------\n_C.NONLOCAL = CfgNode()\n# Index of each stage and block to add nonlocal layers.\n_C.NONLOCAL.LOCATION = [[[]], [[]], [[]], [[]]]",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.X3D.CHANNELWISE_3x3x3",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.X3D.CHANNELWISE_3x3x3 = True\n# -----------------------------------------------------------------------------\n# Nonlocal options\n# -----------------------------------------------------------------------------\n_C.NONLOCAL = CfgNode()\n# Index of each stage and block to add nonlocal layers.\n_C.NONLOCAL.LOCATION = [[[]], [[]], [[]], [[]]]\n# Number of group for nonlocal for each stage.\n_C.NONLOCAL.GROUP = [[1], [1], [1], [1]]\n# Instatiation to use for non-local layer.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.NONLOCAL",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.NONLOCAL = CfgNode()\n# Index of each stage and block to add nonlocal layers.\n_C.NONLOCAL.LOCATION = [[[]], [[]], [[]], [[]]]\n# Number of group for nonlocal for each stage.\n_C.NONLOCAL.GROUP = [[1], [1], [1], [1]]\n# Instatiation to use for non-local layer.\n_C.NONLOCAL.INSTANTIATION = \"dot_product\"\n# Size of pooling layers used in Non-Local.\n_C.NONLOCAL.POOL = [\n    # Res2",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.NONLOCAL.LOCATION",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.NONLOCAL.LOCATION = [[[]], [[]], [[]], [[]]]\n# Number of group for nonlocal for each stage.\n_C.NONLOCAL.GROUP = [[1], [1], [1], [1]]\n# Instatiation to use for non-local layer.\n_C.NONLOCAL.INSTANTIATION = \"dot_product\"\n# Size of pooling layers used in Non-Local.\n_C.NONLOCAL.POOL = [\n    # Res2\n    [[1, 2, 2], [1, 2, 2]],\n    # Res3",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.NONLOCAL.GROUP",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.NONLOCAL.GROUP = [[1], [1], [1], [1]]\n# Instatiation to use for non-local layer.\n_C.NONLOCAL.INSTANTIATION = \"dot_product\"\n# Size of pooling layers used in Non-Local.\n_C.NONLOCAL.POOL = [\n    # Res2\n    [[1, 2, 2], [1, 2, 2]],\n    # Res3\n    [[1, 2, 2], [1, 2, 2]],\n    # Res4",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.NONLOCAL.INSTANTIATION",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.NONLOCAL.INSTANTIATION = \"dot_product\"\n# Size of pooling layers used in Non-Local.\n_C.NONLOCAL.POOL = [\n    # Res2\n    [[1, 2, 2], [1, 2, 2]],\n    # Res3\n    [[1, 2, 2], [1, 2, 2]],\n    # Res4\n    [[1, 2, 2], [1, 2, 2]],\n    # Res5",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.NONLOCAL.POOL",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.NONLOCAL.POOL = [\n    # Res2\n    [[1, 2, 2], [1, 2, 2]],\n    # Res3\n    [[1, 2, 2], [1, 2, 2]],\n    # Res4\n    [[1, 2, 2], [1, 2, 2]],\n    # Res5\n    [[1, 2, 2], [1, 2, 2]],\n]",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MODEL",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MODEL = CfgNode()\n# Model architecture.\n_C.MODEL.ARCH = \"slowfast\"\n_C.MODEL.USE_CHECKPOINT=True\n_C.MODEL.CHECKPOINT_NUM=2\n# Model name\n_C.MODEL.MODEL_NAME = \"SlowFast\"\n# The number of classes to predict for the model.\n_C.MODEL.NUM_CLASSES = 400\n# Loss function.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MODEL.ARCH",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MODEL.ARCH = \"slowfast\"\n_C.MODEL.USE_CHECKPOINT=True\n_C.MODEL.CHECKPOINT_NUM=2\n# Model name\n_C.MODEL.MODEL_NAME = \"SlowFast\"\n# The number of classes to predict for the model.\n_C.MODEL.NUM_CLASSES = 400\n# Loss function.\n_C.MODEL.LOSS_FUNC = \"cross_entropy\"\n# Model architectures that has one single pathway.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MODEL.MODEL_NAME",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MODEL.MODEL_NAME = \"SlowFast\"\n# The number of classes to predict for the model.\n_C.MODEL.NUM_CLASSES = 400\n# Loss function.\n_C.MODEL.LOSS_FUNC = \"cross_entropy\"\n# Model architectures that has one single pathway.\n_C.MODEL.SINGLE_PATHWAY_ARCH = [\"2d\", \"c2d\", \"i3d\", \"slow\", \"x3d\", \"mvit\", \"morph\"]\n# Model architectures that has multiple pathways.\n_C.MODEL.MULTI_PATHWAY_ARCH = [\"slowfast\"]\n# Dropout rate before final projection in the backbone.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MODEL.NUM_CLASSES",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MODEL.NUM_CLASSES = 400\n# Loss function.\n_C.MODEL.LOSS_FUNC = \"cross_entropy\"\n# Model architectures that has one single pathway.\n_C.MODEL.SINGLE_PATHWAY_ARCH = [\"2d\", \"c2d\", \"i3d\", \"slow\", \"x3d\", \"mvit\", \"morph\"]\n# Model architectures that has multiple pathways.\n_C.MODEL.MULTI_PATHWAY_ARCH = [\"slowfast\"]\n# Dropout rate before final projection in the backbone.\n_C.MODEL.DROPOUT_RATE = 0.5\n# Randomly drop rate for Res-blocks, linearly increase from res2 to res5",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MODEL.LOSS_FUNC",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MODEL.LOSS_FUNC = \"cross_entropy\"\n# Model architectures that has one single pathway.\n_C.MODEL.SINGLE_PATHWAY_ARCH = [\"2d\", \"c2d\", \"i3d\", \"slow\", \"x3d\", \"mvit\", \"morph\"]\n# Model architectures that has multiple pathways.\n_C.MODEL.MULTI_PATHWAY_ARCH = [\"slowfast\"]\n# Dropout rate before final projection in the backbone.\n_C.MODEL.DROPOUT_RATE = 0.5\n# Randomly drop rate for Res-blocks, linearly increase from res2 to res5\n_C.MODEL.DROPCONNECT_RATE = 0.0\n# The std to initialize the fc layer(s).",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MODEL.SINGLE_PATHWAY_ARCH",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MODEL.SINGLE_PATHWAY_ARCH = [\"2d\", \"c2d\", \"i3d\", \"slow\", \"x3d\", \"mvit\", \"morph\"]\n# Model architectures that has multiple pathways.\n_C.MODEL.MULTI_PATHWAY_ARCH = [\"slowfast\"]\n# Dropout rate before final projection in the backbone.\n_C.MODEL.DROPOUT_RATE = 0.5\n# Randomly drop rate for Res-blocks, linearly increase from res2 to res5\n_C.MODEL.DROPCONNECT_RATE = 0.0\n# The std to initialize the fc layer(s).\n_C.MODEL.FC_INIT_STD = 0.01\n# Activation layer for the output head.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MODEL.MULTI_PATHWAY_ARCH",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MODEL.MULTI_PATHWAY_ARCH = [\"slowfast\"]\n# Dropout rate before final projection in the backbone.\n_C.MODEL.DROPOUT_RATE = 0.5\n# Randomly drop rate for Res-blocks, linearly increase from res2 to res5\n_C.MODEL.DROPCONNECT_RATE = 0.0\n# The std to initialize the fc layer(s).\n_C.MODEL.FC_INIT_STD = 0.01\n# Activation layer for the output head.\n_C.MODEL.HEAD_ACT = \"softmax\"\n# -----------------------------------------------------------------------------",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MODEL.DROPOUT_RATE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MODEL.DROPOUT_RATE = 0.5\n# Randomly drop rate for Res-blocks, linearly increase from res2 to res5\n_C.MODEL.DROPCONNECT_RATE = 0.0\n# The std to initialize the fc layer(s).\n_C.MODEL.FC_INIT_STD = 0.01\n# Activation layer for the output head.\n_C.MODEL.HEAD_ACT = \"softmax\"\n# -----------------------------------------------------------------------------\n# MViT options\n# -----------------------------------------------------------------------------",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MODEL.DROPCONNECT_RATE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MODEL.DROPCONNECT_RATE = 0.0\n# The std to initialize the fc layer(s).\n_C.MODEL.FC_INIT_STD = 0.01\n# Activation layer for the output head.\n_C.MODEL.HEAD_ACT = \"softmax\"\n# -----------------------------------------------------------------------------\n# MViT options\n# -----------------------------------------------------------------------------\n_C.MVIT = CfgNode()\n# Options include `conv`, `max`.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MODEL.FC_INIT_STD",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MODEL.FC_INIT_STD = 0.01\n# Activation layer for the output head.\n_C.MODEL.HEAD_ACT = \"softmax\"\n# -----------------------------------------------------------------------------\n# MViT options\n# -----------------------------------------------------------------------------\n_C.MVIT = CfgNode()\n# Options include `conv`, `max`.\n_C.MVIT.MODE = \"conv\"\n# If True, perform pool before projection in attention.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MODEL.HEAD_ACT",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MODEL.HEAD_ACT = \"softmax\"\n# -----------------------------------------------------------------------------\n# MViT options\n# -----------------------------------------------------------------------------\n_C.MVIT = CfgNode()\n# Options include `conv`, `max`.\n_C.MVIT.MODE = \"conv\"\n# If True, perform pool before projection in attention.\n_C.MVIT.POOL_FIRST = False\n# If True, use cls embed in the network, otherwise don't use cls_embed in transformer.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT = CfgNode()\n# Options include `conv`, `max`.\n_C.MVIT.MODE = \"conv\"\n# If True, perform pool before projection in attention.\n_C.MVIT.POOL_FIRST = False\n# If True, use cls embed in the network, otherwise don't use cls_embed in transformer.\n_C.MVIT.CLS_EMBED_ON = True\n# Kernel size for patchtification.\n_C.MVIT.PATCH_KERNEL = [3, 7, 7]\n# Stride size for patchtification.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.MODE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.MODE = \"conv\"\n# If True, perform pool before projection in attention.\n_C.MVIT.POOL_FIRST = False\n# If True, use cls embed in the network, otherwise don't use cls_embed in transformer.\n_C.MVIT.CLS_EMBED_ON = True\n# Kernel size for patchtification.\n_C.MVIT.PATCH_KERNEL = [3, 7, 7]\n# Stride size for patchtification.\n_C.MVIT.PATCH_STRIDE = [2, 4, 4]\n# Padding size for patchtification.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.POOL_FIRST",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.POOL_FIRST = False\n# If True, use cls embed in the network, otherwise don't use cls_embed in transformer.\n_C.MVIT.CLS_EMBED_ON = True\n# Kernel size for patchtification.\n_C.MVIT.PATCH_KERNEL = [3, 7, 7]\n# Stride size for patchtification.\n_C.MVIT.PATCH_STRIDE = [2, 4, 4]\n# Padding size for patchtification.\n_C.MVIT.PATCH_PADDING = [2, 4, 4]\n# If True, use 2d patch, otherwise use 3d patch.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.CLS_EMBED_ON",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.CLS_EMBED_ON = True\n# Kernel size for patchtification.\n_C.MVIT.PATCH_KERNEL = [3, 7, 7]\n# Stride size for patchtification.\n_C.MVIT.PATCH_STRIDE = [2, 4, 4]\n# Padding size for patchtification.\n_C.MVIT.PATCH_PADDING = [2, 4, 4]\n# If True, use 2d patch, otherwise use 3d patch.\n_C.MVIT.PATCH_2D = False\n# Base embedding dimension for the transformer.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.PATCH_KERNEL",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.PATCH_KERNEL = [3, 7, 7]\n# Stride size for patchtification.\n_C.MVIT.PATCH_STRIDE = [2, 4, 4]\n# Padding size for patchtification.\n_C.MVIT.PATCH_PADDING = [2, 4, 4]\n# If True, use 2d patch, otherwise use 3d patch.\n_C.MVIT.PATCH_2D = False\n# Base embedding dimension for the transformer.\n_C.MVIT.EMBED_DIM = 96\n# Base num of heads for the transformer.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.PATCH_STRIDE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.PATCH_STRIDE = [2, 4, 4]\n# Padding size for patchtification.\n_C.MVIT.PATCH_PADDING = [2, 4, 4]\n# If True, use 2d patch, otherwise use 3d patch.\n_C.MVIT.PATCH_2D = False\n# Base embedding dimension for the transformer.\n_C.MVIT.EMBED_DIM = 96\n# Base num of heads for the transformer.\n_C.MVIT.NUM_HEADS = 1\n# Dimension reduction ratio for the MLP layers.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.PATCH_PADDING",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.PATCH_PADDING = [2, 4, 4]\n# If True, use 2d patch, otherwise use 3d patch.\n_C.MVIT.PATCH_2D = False\n# Base embedding dimension for the transformer.\n_C.MVIT.EMBED_DIM = 96\n# Base num of heads for the transformer.\n_C.MVIT.NUM_HEADS = 1\n# Dimension reduction ratio for the MLP layers.\n_C.MVIT.MLP_RATIO = 4.0\n# If use, use bias term in attention fc layers.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.PATCH_2D",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.PATCH_2D = False\n# Base embedding dimension for the transformer.\n_C.MVIT.EMBED_DIM = 96\n# Base num of heads for the transformer.\n_C.MVIT.NUM_HEADS = 1\n# Dimension reduction ratio for the MLP layers.\n_C.MVIT.MLP_RATIO = 4.0\n# If use, use bias term in attention fc layers.\n_C.MVIT.QKV_BIAS = True\n# Drop path rate for the tranfomer.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.EMBED_DIM",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.EMBED_DIM = 96\n# Base num of heads for the transformer.\n_C.MVIT.NUM_HEADS = 1\n# Dimension reduction ratio for the MLP layers.\n_C.MVIT.MLP_RATIO = 4.0\n# If use, use bias term in attention fc layers.\n_C.MVIT.QKV_BIAS = True\n# Drop path rate for the tranfomer.\n_C.MVIT.DROPPATH_RATE = 0.1\n# Depth of the transformer.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.NUM_HEADS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.NUM_HEADS = 1\n# Dimension reduction ratio for the MLP layers.\n_C.MVIT.MLP_RATIO = 4.0\n# If use, use bias term in attention fc layers.\n_C.MVIT.QKV_BIAS = True\n# Drop path rate for the tranfomer.\n_C.MVIT.DROPPATH_RATE = 0.1\n# Depth of the transformer.\n_C.MVIT.DEPTH = 16\n# Normalization layer for the transformer. Only layernorm is supported now.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.MLP_RATIO",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.MLP_RATIO = 4.0\n# If use, use bias term in attention fc layers.\n_C.MVIT.QKV_BIAS = True\n# Drop path rate for the tranfomer.\n_C.MVIT.DROPPATH_RATE = 0.1\n# Depth of the transformer.\n_C.MVIT.DEPTH = 16\n# Normalization layer for the transformer. Only layernorm is supported now.\n_C.MVIT.NORM = \"layernorm\"\n# Dimension multiplication at layer i. If 2.0 is used, then the next block will increase",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.QKV_BIAS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.QKV_BIAS = True\n# Drop path rate for the tranfomer.\n_C.MVIT.DROPPATH_RATE = 0.1\n# Depth of the transformer.\n_C.MVIT.DEPTH = 16\n# Normalization layer for the transformer. Only layernorm is supported now.\n_C.MVIT.NORM = \"layernorm\"\n# Dimension multiplication at layer i. If 2.0 is used, then the next block will increase\n# the dimension by 2 times. Format: [depth_i: mul_dim_ratio]\n_C.MVIT.DIM_MUL = []",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.DROPPATH_RATE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.DROPPATH_RATE = 0.1\n# Depth of the transformer.\n_C.MVIT.DEPTH = 16\n# Normalization layer for the transformer. Only layernorm is supported now.\n_C.MVIT.NORM = \"layernorm\"\n# Dimension multiplication at layer i. If 2.0 is used, then the next block will increase\n# the dimension by 2 times. Format: [depth_i: mul_dim_ratio]\n_C.MVIT.DIM_MUL = []\n# Head number multiplication at layer i. If 2.0 is used, then the next block will\n# increase the number of heads by 2 times. Format: [depth_i: head_mul_ratio]",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.DEPTH",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.DEPTH = 16\n# Normalization layer for the transformer. Only layernorm is supported now.\n_C.MVIT.NORM = \"layernorm\"\n# Dimension multiplication at layer i. If 2.0 is used, then the next block will increase\n# the dimension by 2 times. Format: [depth_i: mul_dim_ratio]\n_C.MVIT.DIM_MUL = []\n# Head number multiplication at layer i. If 2.0 is used, then the next block will\n# increase the number of heads by 2 times. Format: [depth_i: head_mul_ratio]\n_C.MVIT.HEAD_MUL = []\n# Stride size for the Pool KV at layer i.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.NORM",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.NORM = \"layernorm\"\n# Dimension multiplication at layer i. If 2.0 is used, then the next block will increase\n# the dimension by 2 times. Format: [depth_i: mul_dim_ratio]\n_C.MVIT.DIM_MUL = []\n# Head number multiplication at layer i. If 2.0 is used, then the next block will\n# increase the number of heads by 2 times. Format: [depth_i: head_mul_ratio]\n_C.MVIT.HEAD_MUL = []\n# Stride size for the Pool KV at layer i.\n# Format: [[i, stride_t_i, stride_h_i, stride_w_i], ...,]\n_C.MVIT.POOL_KV_STRIDE = None",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.DIM_MUL",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.DIM_MUL = []\n# Head number multiplication at layer i. If 2.0 is used, then the next block will\n# increase the number of heads by 2 times. Format: [depth_i: head_mul_ratio]\n_C.MVIT.HEAD_MUL = []\n# Stride size for the Pool KV at layer i.\n# Format: [[i, stride_t_i, stride_h_i, stride_w_i], ...,]\n_C.MVIT.POOL_KV_STRIDE = None\n# Initial stride size for KV at layer 1. The stride size will be further reduced with\n# the raio of MVIT.DIM_MUL. If will overwrite MVIT.POOL_KV_STRIDE if not None.\n_C.MVIT.POOL_KV_STRIDE_ADAPTIVE = None",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.HEAD_MUL",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.HEAD_MUL = []\n# Stride size for the Pool KV at layer i.\n# Format: [[i, stride_t_i, stride_h_i, stride_w_i], ...,]\n_C.MVIT.POOL_KV_STRIDE = None\n# Initial stride size for KV at layer 1. The stride size will be further reduced with\n# the raio of MVIT.DIM_MUL. If will overwrite MVIT.POOL_KV_STRIDE if not None.\n_C.MVIT.POOL_KV_STRIDE_ADAPTIVE = None\n# Stride size for the Pool Q at layer i.\n# Format: [[i, stride_t_i, stride_h_i, stride_w_i], ...,]\n_C.MVIT.POOL_Q_STRIDE = []",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.POOL_KV_STRIDE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.POOL_KV_STRIDE = None\n# Initial stride size for KV at layer 1. The stride size will be further reduced with\n# the raio of MVIT.DIM_MUL. If will overwrite MVIT.POOL_KV_STRIDE if not None.\n_C.MVIT.POOL_KV_STRIDE_ADAPTIVE = None\n# Stride size for the Pool Q at layer i.\n# Format: [[i, stride_t_i, stride_h_i, stride_w_i], ...,]\n_C.MVIT.POOL_Q_STRIDE = []\n# If not None, overwrite the KV_KERNEL and Q_KERNEL size with POOL_KVQ_CONV_SIZ.\n# Otherwise the kernel_size is [s + 1 if s > 1 else s for s in stride_size].\n_C.MVIT.POOL_KVQ_KERNEL = None",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.POOL_KV_STRIDE_ADAPTIVE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.POOL_KV_STRIDE_ADAPTIVE = None\n# Stride size for the Pool Q at layer i.\n# Format: [[i, stride_t_i, stride_h_i, stride_w_i], ...,]\n_C.MVIT.POOL_Q_STRIDE = []\n# If not None, overwrite the KV_KERNEL and Q_KERNEL size with POOL_KVQ_CONV_SIZ.\n# Otherwise the kernel_size is [s + 1 if s > 1 else s for s in stride_size].\n_C.MVIT.POOL_KVQ_KERNEL = None\n# If True, perform no decay on positional embedding and cls embedding.\n_C.MVIT.ZERO_DECAY_POS_CLS = True\n# If True, use norm after stem.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.POOL_Q_STRIDE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.POOL_Q_STRIDE = []\n# If not None, overwrite the KV_KERNEL and Q_KERNEL size with POOL_KVQ_CONV_SIZ.\n# Otherwise the kernel_size is [s + 1 if s > 1 else s for s in stride_size].\n_C.MVIT.POOL_KVQ_KERNEL = None\n# If True, perform no decay on positional embedding and cls embedding.\n_C.MVIT.ZERO_DECAY_POS_CLS = True\n# If True, use norm after stem.\n_C.MVIT.NORM_STEM = False\n# If True, perform separate positional embedding.\n_C.MVIT.SEP_POS_EMBED = False",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.POOL_KVQ_KERNEL",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.POOL_KVQ_KERNEL = None\n# If True, perform no decay on positional embedding and cls embedding.\n_C.MVIT.ZERO_DECAY_POS_CLS = True\n# If True, use norm after stem.\n_C.MVIT.NORM_STEM = False\n# If True, perform separate positional embedding.\n_C.MVIT.SEP_POS_EMBED = False\n# Dropout rate for the MViT backbone.\n_C.MVIT.DROPOUT_RATE = 0.0\n# -----------------------------------------------------------------------------",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.ZERO_DECAY_POS_CLS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.ZERO_DECAY_POS_CLS = True\n# If True, use norm after stem.\n_C.MVIT.NORM_STEM = False\n# If True, perform separate positional embedding.\n_C.MVIT.SEP_POS_EMBED = False\n# Dropout rate for the MViT backbone.\n_C.MVIT.DROPOUT_RATE = 0.0\n# -----------------------------------------------------------------------------\n# SlowFast options\n# -----------------------------------------------------------------------------",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.NORM_STEM",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.NORM_STEM = False\n# If True, perform separate positional embedding.\n_C.MVIT.SEP_POS_EMBED = False\n# Dropout rate for the MViT backbone.\n_C.MVIT.DROPOUT_RATE = 0.0\n# -----------------------------------------------------------------------------\n# SlowFast options\n# -----------------------------------------------------------------------------\n_C.SLOWFAST = CfgNode()\n# Corresponds to the inverse of the channel reduction ratio, $\\beta$ between",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.SEP_POS_EMBED",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.SEP_POS_EMBED = False\n# Dropout rate for the MViT backbone.\n_C.MVIT.DROPOUT_RATE = 0.0\n# -----------------------------------------------------------------------------\n# SlowFast options\n# -----------------------------------------------------------------------------\n_C.SLOWFAST = CfgNode()\n# Corresponds to the inverse of the channel reduction ratio, $\\beta$ between\n# the Slow and Fast pathways.\n_C.SLOWFAST.BETA_INV = 8",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MVIT.DROPOUT_RATE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MVIT.DROPOUT_RATE = 0.0\n# -----------------------------------------------------------------------------\n# SlowFast options\n# -----------------------------------------------------------------------------\n_C.SLOWFAST = CfgNode()\n# Corresponds to the inverse of the channel reduction ratio, $\\beta$ between\n# the Slow and Fast pathways.\n_C.SLOWFAST.BETA_INV = 8\n# Corresponds to the frame rate reduction ratio, $\\alpha$ between the Slow and\n# Fast pathways.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SLOWFAST",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SLOWFAST = CfgNode()\n# Corresponds to the inverse of the channel reduction ratio, $\\beta$ between\n# the Slow and Fast pathways.\n_C.SLOWFAST.BETA_INV = 8\n# Corresponds to the frame rate reduction ratio, $\\alpha$ between the Slow and\n# Fast pathways.\n_C.SLOWFAST.ALPHA = 8\n# Ratio of channel dimensions between the Slow and Fast pathways.\n_C.SLOWFAST.FUSION_CONV_CHANNEL_RATIO = 2\n# Kernel dimension used for fusing information from Fast pathway to Slow",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SLOWFAST.BETA_INV",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SLOWFAST.BETA_INV = 8\n# Corresponds to the frame rate reduction ratio, $\\alpha$ between the Slow and\n# Fast pathways.\n_C.SLOWFAST.ALPHA = 8\n# Ratio of channel dimensions between the Slow and Fast pathways.\n_C.SLOWFAST.FUSION_CONV_CHANNEL_RATIO = 2\n# Kernel dimension used for fusing information from Fast pathway to Slow\n# pathway.\n_C.SLOWFAST.FUSION_KERNEL_SZ = 5\n# -----------------------------------------------------------------------------",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SLOWFAST.ALPHA",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SLOWFAST.ALPHA = 8\n# Ratio of channel dimensions between the Slow and Fast pathways.\n_C.SLOWFAST.FUSION_CONV_CHANNEL_RATIO = 2\n# Kernel dimension used for fusing information from Fast pathway to Slow\n# pathway.\n_C.SLOWFAST.FUSION_KERNEL_SZ = 5\n# -----------------------------------------------------------------------------\n# Data options\n# -----------------------------------------------------------------------------\n_C.DATA = CfgNode()",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SLOWFAST.FUSION_CONV_CHANNEL_RATIO",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SLOWFAST.FUSION_CONV_CHANNEL_RATIO = 2\n# Kernel dimension used for fusing information from Fast pathway to Slow\n# pathway.\n_C.SLOWFAST.FUSION_KERNEL_SZ = 5\n# -----------------------------------------------------------------------------\n# Data options\n# -----------------------------------------------------------------------------\n_C.DATA = CfgNode()\n_C.DATA.LABEL_PATH_TEMPLATE=\"somesomev1_rgb_{}_split.txt\"\n_C.DATA.IMAGE_TEMPLATE=\"{:05d}.jpg\"",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SLOWFAST.FUSION_KERNEL_SZ",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SLOWFAST.FUSION_KERNEL_SZ = 5\n# -----------------------------------------------------------------------------\n# Data options\n# -----------------------------------------------------------------------------\n_C.DATA = CfgNode()\n_C.DATA.LABEL_PATH_TEMPLATE=\"somesomev1_rgb_{}_split.txt\"\n_C.DATA.IMAGE_TEMPLATE=\"{:05d}.jpg\"\n# The path to the data directory.\n_C.DATA.PATH_TO_DATA_DIR = \"/mnt/bd/jh-backbone/UniFormer/video_classification/data_list/sthv1\"\n# The separator used between path and label.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA = CfgNode()\n_C.DATA.LABEL_PATH_TEMPLATE=\"somesomev1_rgb_{}_split.txt\"\n_C.DATA.IMAGE_TEMPLATE=\"{:05d}.jpg\"\n# The path to the data directory.\n_C.DATA.PATH_TO_DATA_DIR = \"/mnt/bd/jh-backbone/UniFormer/video_classification/data_list/sthv1\"\n# The separator used between path and label.\n_C.DATA.PATH_LABEL_SEPARATOR = \" \"\n# Video path prefix if any.\n_C.DATA.PATH_PREFIX = \"\"\n# The number of frames of the input clip.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.PATH_TO_DATA_DIR",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.PATH_TO_DATA_DIR = \"/mnt/bd/jh-backbone/UniFormer/video_classification/data_list/sthv1\"\n# The separator used between path and label.\n_C.DATA.PATH_LABEL_SEPARATOR = \" \"\n# Video path prefix if any.\n_C.DATA.PATH_PREFIX = \"\"\n# The number of frames of the input clip.\n_C.DATA.NUM_FRAMES = 8\n# The video sampling rate of the input clip.\n_C.DATA.SAMPLING_RATE = 8\n# Eigenvalues for PCA jittering. Note PCA is RGB based.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.PATH_LABEL_SEPARATOR",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.PATH_LABEL_SEPARATOR = \" \"\n# Video path prefix if any.\n_C.DATA.PATH_PREFIX = \"\"\n# The number of frames of the input clip.\n_C.DATA.NUM_FRAMES = 8\n# The video sampling rate of the input clip.\n_C.DATA.SAMPLING_RATE = 8\n# Eigenvalues for PCA jittering. Note PCA is RGB based.\n_C.DATA.TRAIN_PCA_EIGVAL = [0.225, 0.224, 0.229]\n# Eigenvectors for PCA jittering.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.PATH_PREFIX",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.PATH_PREFIX = \"\"\n# The number of frames of the input clip.\n_C.DATA.NUM_FRAMES = 8\n# The video sampling rate of the input clip.\n_C.DATA.SAMPLING_RATE = 8\n# Eigenvalues for PCA jittering. Note PCA is RGB based.\n_C.DATA.TRAIN_PCA_EIGVAL = [0.225, 0.224, 0.229]\n# Eigenvectors for PCA jittering.\n_C.DATA.TRAIN_PCA_EIGVEC = [\n    [-0.5675, 0.7192, 0.4009],",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.NUM_FRAMES",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.NUM_FRAMES = 8\n# The video sampling rate of the input clip.\n_C.DATA.SAMPLING_RATE = 8\n# Eigenvalues for PCA jittering. Note PCA is RGB based.\n_C.DATA.TRAIN_PCA_EIGVAL = [0.225, 0.224, 0.229]\n# Eigenvectors for PCA jittering.\n_C.DATA.TRAIN_PCA_EIGVEC = [\n    [-0.5675, 0.7192, 0.4009],\n    [-0.5808, -0.0045, -0.8140],\n    [-0.5836, -0.6948, 0.4203],",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.SAMPLING_RATE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.SAMPLING_RATE = 8\n# Eigenvalues for PCA jittering. Note PCA is RGB based.\n_C.DATA.TRAIN_PCA_EIGVAL = [0.225, 0.224, 0.229]\n# Eigenvectors for PCA jittering.\n_C.DATA.TRAIN_PCA_EIGVEC = [\n    [-0.5675, 0.7192, 0.4009],\n    [-0.5808, -0.0045, -0.8140],\n    [-0.5836, -0.6948, 0.4203],\n]\n# If a imdb have been dumpped to a local file with the following format:",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.TRAIN_PCA_EIGVAL",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.TRAIN_PCA_EIGVAL = [0.225, 0.224, 0.229]\n# Eigenvectors for PCA jittering.\n_C.DATA.TRAIN_PCA_EIGVEC = [\n    [-0.5675, 0.7192, 0.4009],\n    [-0.5808, -0.0045, -0.8140],\n    [-0.5836, -0.6948, 0.4203],\n]\n# If a imdb have been dumpped to a local file with the following format:\n# `{\"im_path\": im_path, \"class\": cont_id}`\n# then we can skip the construction of imdb and load it from the local file.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.TRAIN_PCA_EIGVEC",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.TRAIN_PCA_EIGVEC = [\n    [-0.5675, 0.7192, 0.4009],\n    [-0.5808, -0.0045, -0.8140],\n    [-0.5836, -0.6948, 0.4203],\n]\n# If a imdb have been dumpped to a local file with the following format:\n# `{\"im_path\": im_path, \"class\": cont_id}`\n# then we can skip the construction of imdb and load it from the local file.\n_C.DATA.PATH_TO_PRELOAD_IMDB = \"\"\n# The mean value of the video raw pixels across the R G B channels.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.PATH_TO_PRELOAD_IMDB",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.PATH_TO_PRELOAD_IMDB = \"\"\n# The mean value of the video raw pixels across the R G B channels.\n_C.DATA.MEAN = [0.45, 0.45, 0.45]\n# List of input frame channel dimensions.\n_C.DATA.INPUT_CHANNEL_NUM = [3, 3]\n# The std value of the video raw pixels across the R G B channels.\n_C.DATA.STD = [0.225, 0.225, 0.225]\n# The spatial augmentation jitter scales for training.\n_C.DATA.TRAIN_JITTER_SCALES = [256, 320]\n# The relative scale range of Inception-style area based random resizing augmentation.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.MEAN",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.MEAN = [0.45, 0.45, 0.45]\n# List of input frame channel dimensions.\n_C.DATA.INPUT_CHANNEL_NUM = [3, 3]\n# The std value of the video raw pixels across the R G B channels.\n_C.DATA.STD = [0.225, 0.225, 0.225]\n# The spatial augmentation jitter scales for training.\n_C.DATA.TRAIN_JITTER_SCALES = [256, 320]\n# The relative scale range of Inception-style area based random resizing augmentation.\n# If this is provided, DATA.TRAIN_JITTER_SCALES above is ignored.\n_C.DATA.TRAIN_JITTER_SCALES_RELATIVE = []",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.INPUT_CHANNEL_NUM",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.INPUT_CHANNEL_NUM = [3, 3]\n# The std value of the video raw pixels across the R G B channels.\n_C.DATA.STD = [0.225, 0.225, 0.225]\n# The spatial augmentation jitter scales for training.\n_C.DATA.TRAIN_JITTER_SCALES = [256, 320]\n# The relative scale range of Inception-style area based random resizing augmentation.\n# If this is provided, DATA.TRAIN_JITTER_SCALES above is ignored.\n_C.DATA.TRAIN_JITTER_SCALES_RELATIVE = []\n# The relative aspect ratio range of Inception-style area based random resizing\n# augmentation.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.STD",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.STD = [0.225, 0.225, 0.225]\n# The spatial augmentation jitter scales for training.\n_C.DATA.TRAIN_JITTER_SCALES = [256, 320]\n# The relative scale range of Inception-style area based random resizing augmentation.\n# If this is provided, DATA.TRAIN_JITTER_SCALES above is ignored.\n_C.DATA.TRAIN_JITTER_SCALES_RELATIVE = []\n# The relative aspect ratio range of Inception-style area based random resizing\n# augmentation.\n_C.DATA.TRAIN_JITTER_ASPECT_RELATIVE = []\n# If True, perform stride length uniform temporal sampling.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.TRAIN_JITTER_SCALES",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.TRAIN_JITTER_SCALES = [256, 320]\n# The relative scale range of Inception-style area based random resizing augmentation.\n# If this is provided, DATA.TRAIN_JITTER_SCALES above is ignored.\n_C.DATA.TRAIN_JITTER_SCALES_RELATIVE = []\n# The relative aspect ratio range of Inception-style area based random resizing\n# augmentation.\n_C.DATA.TRAIN_JITTER_ASPECT_RELATIVE = []\n# If True, perform stride length uniform temporal sampling.\n_C.DATA.USE_OFFSET_SAMPLING = False\n# Whether to apply motion shift for augmentation.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.TRAIN_JITTER_SCALES_RELATIVE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.TRAIN_JITTER_SCALES_RELATIVE = []\n# The relative aspect ratio range of Inception-style area based random resizing\n# augmentation.\n_C.DATA.TRAIN_JITTER_ASPECT_RELATIVE = []\n# If True, perform stride length uniform temporal sampling.\n_C.DATA.USE_OFFSET_SAMPLING = False\n# Whether to apply motion shift for augmentation.\n_C.DATA.TRAIN_JITTER_MOTION_SHIFT = False\n# The spatial crop size for training.\n_C.DATA.TRAIN_CROP_SIZE = 224",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.TRAIN_JITTER_ASPECT_RELATIVE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.TRAIN_JITTER_ASPECT_RELATIVE = []\n# If True, perform stride length uniform temporal sampling.\n_C.DATA.USE_OFFSET_SAMPLING = False\n# Whether to apply motion shift for augmentation.\n_C.DATA.TRAIN_JITTER_MOTION_SHIFT = False\n# The spatial crop size for training.\n_C.DATA.TRAIN_CROP_SIZE = 224\n# The spatial crop size for testing.\n_C.DATA.TEST_CROP_SIZE = 256\n# Input videos may has different fps, convert it to the target video fps before",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.USE_OFFSET_SAMPLING",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.USE_OFFSET_SAMPLING = False\n# Whether to apply motion shift for augmentation.\n_C.DATA.TRAIN_JITTER_MOTION_SHIFT = False\n# The spatial crop size for training.\n_C.DATA.TRAIN_CROP_SIZE = 224\n# The spatial crop size for testing.\n_C.DATA.TEST_CROP_SIZE = 256\n# Input videos may has different fps, convert it to the target video fps before\n# frame sampling.\n_C.DATA.TARGET_FPS = 30",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.TRAIN_JITTER_MOTION_SHIFT",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.TRAIN_JITTER_MOTION_SHIFT = False\n# The spatial crop size for training.\n_C.DATA.TRAIN_CROP_SIZE = 224\n# The spatial crop size for testing.\n_C.DATA.TEST_CROP_SIZE = 256\n# Input videos may has different fps, convert it to the target video fps before\n# frame sampling.\n_C.DATA.TARGET_FPS = 30\n# Decoding backend, options include `pyav` or `torchvision`\n_C.DATA.DECODING_BACKEND = \"pyav\"",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.TRAIN_CROP_SIZE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.TRAIN_CROP_SIZE = 224\n# The spatial crop size for testing.\n_C.DATA.TEST_CROP_SIZE = 256\n# Input videos may has different fps, convert it to the target video fps before\n# frame sampling.\n_C.DATA.TARGET_FPS = 30\n# Decoding backend, options include `pyav` or `torchvision`\n_C.DATA.DECODING_BACKEND = \"pyav\"\n# if True, sample uniformly in [1 / max_scale, 1 / min_scale] and take a\n# reciprocal to get the scale. If False, take a uniform sample from",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.TEST_CROP_SIZE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.TEST_CROP_SIZE = 256\n# Input videos may has different fps, convert it to the target video fps before\n# frame sampling.\n_C.DATA.TARGET_FPS = 30\n# Decoding backend, options include `pyav` or `torchvision`\n_C.DATA.DECODING_BACKEND = \"pyav\"\n# if True, sample uniformly in [1 / max_scale, 1 / min_scale] and take a\n# reciprocal to get the scale. If False, take a uniform sample from\n# [min_scale, max_scale].\n_C.DATA.INV_UNIFORM_SAMPLE = False",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.TARGET_FPS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.TARGET_FPS = 30\n# Decoding backend, options include `pyav` or `torchvision`\n_C.DATA.DECODING_BACKEND = \"pyav\"\n# if True, sample uniformly in [1 / max_scale, 1 / min_scale] and take a\n# reciprocal to get the scale. If False, take a uniform sample from\n# [min_scale, max_scale].\n_C.DATA.INV_UNIFORM_SAMPLE = False\n# If True, perform random horizontal flip on the video frames during training.\n_C.DATA.RANDOM_FLIP = True\n# If True, calculdate the map as metric.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.DECODING_BACKEND",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.DECODING_BACKEND = \"pyav\"\n# if True, sample uniformly in [1 / max_scale, 1 / min_scale] and take a\n# reciprocal to get the scale. If False, take a uniform sample from\n# [min_scale, max_scale].\n_C.DATA.INV_UNIFORM_SAMPLE = False\n# If True, perform random horizontal flip on the video frames during training.\n_C.DATA.RANDOM_FLIP = True\n# If True, calculdate the map as metric.\n_C.DATA.MULTI_LABEL = False\n# Method to perform the ensemble, options include \"sum\" and \"max\".",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.INV_UNIFORM_SAMPLE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.INV_UNIFORM_SAMPLE = False\n# If True, perform random horizontal flip on the video frames during training.\n_C.DATA.RANDOM_FLIP = True\n# If True, calculdate the map as metric.\n_C.DATA.MULTI_LABEL = False\n# Method to perform the ensemble, options include \"sum\" and \"max\".\n_C.DATA.ENSEMBLE_METHOD = \"sum\"\n# If True, revert the default input channel (RBG <-> BGR).\n_C.DATA.REVERSE_INPUT_CHANNEL = False\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.RANDOM_FLIP",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.RANDOM_FLIP = True\n# If True, calculdate the map as metric.\n_C.DATA.MULTI_LABEL = False\n# Method to perform the ensemble, options include \"sum\" and \"max\".\n_C.DATA.ENSEMBLE_METHOD = \"sum\"\n# If True, revert the default input channel (RBG <-> BGR).\n_C.DATA.REVERSE_INPUT_CHANNEL = False\n# ---------------------------------------------------------------------------- #\n# Optimizer options\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.MULTI_LABEL",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.MULTI_LABEL = False\n# Method to perform the ensemble, options include \"sum\" and \"max\".\n_C.DATA.ENSEMBLE_METHOD = \"sum\"\n# If True, revert the default input channel (RBG <-> BGR).\n_C.DATA.REVERSE_INPUT_CHANNEL = False\n# ---------------------------------------------------------------------------- #\n# Optimizer options\n# ---------------------------------------------------------------------------- #\n_C.SOLVER = CfgNode()\n# Base learning rate.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.ENSEMBLE_METHOD",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.ENSEMBLE_METHOD = \"sum\"\n# If True, revert the default input channel (RBG <-> BGR).\n_C.DATA.REVERSE_INPUT_CHANNEL = False\n# ---------------------------------------------------------------------------- #\n# Optimizer options\n# ---------------------------------------------------------------------------- #\n_C.SOLVER = CfgNode()\n# Base learning rate.\n_C.SOLVER.BASE_LR = 0.1\n# Learning rate policy (see utils/lr_policy.py for options and examples).",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA.REVERSE_INPUT_CHANNEL",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA.REVERSE_INPUT_CHANNEL = False\n# ---------------------------------------------------------------------------- #\n# Optimizer options\n# ---------------------------------------------------------------------------- #\n_C.SOLVER = CfgNode()\n# Base learning rate.\n_C.SOLVER.BASE_LR = 0.1\n# Learning rate policy (see utils/lr_policy.py for options and examples).\n_C.SOLVER.LR_POLICY = \"cosine\"\n# Final learning rates for 'cosine' policy.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER = CfgNode()\n# Base learning rate.\n_C.SOLVER.BASE_LR = 0.1\n# Learning rate policy (see utils/lr_policy.py for options and examples).\n_C.SOLVER.LR_POLICY = \"cosine\"\n# Final learning rates for 'cosine' policy.\n_C.SOLVER.COSINE_END_LR = 0.0\n# Exponential decay factor.\n_C.SOLVER.GAMMA = 0.1\n# Step size for 'exp' and 'cos' policies (in epochs).",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.BASE_LR",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.BASE_LR = 0.1\n# Learning rate policy (see utils/lr_policy.py for options and examples).\n_C.SOLVER.LR_POLICY = \"cosine\"\n# Final learning rates for 'cosine' policy.\n_C.SOLVER.COSINE_END_LR = 0.0\n# Exponential decay factor.\n_C.SOLVER.GAMMA = 0.1\n# Step size for 'exp' and 'cos' policies (in epochs).\n_C.SOLVER.STEP_SIZE = 1\n# Steps for 'steps_' policies (in epochs).",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.LR_POLICY",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.LR_POLICY = \"cosine\"\n# Final learning rates for 'cosine' policy.\n_C.SOLVER.COSINE_END_LR = 0.0\n# Exponential decay factor.\n_C.SOLVER.GAMMA = 0.1\n# Step size for 'exp' and 'cos' policies (in epochs).\n_C.SOLVER.STEP_SIZE = 1\n# Steps for 'steps_' policies (in epochs).\n_C.SOLVER.STEPS = []\n# Learning rates for 'steps_' policies.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.COSINE_END_LR",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.COSINE_END_LR = 0.0\n# Exponential decay factor.\n_C.SOLVER.GAMMA = 0.1\n# Step size for 'exp' and 'cos' policies (in epochs).\n_C.SOLVER.STEP_SIZE = 1\n# Steps for 'steps_' policies (in epochs).\n_C.SOLVER.STEPS = []\n# Learning rates for 'steps_' policies.\n_C.SOLVER.LRS = []\n# Maximal number of epochs.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.GAMMA",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.GAMMA = 0.1\n# Step size for 'exp' and 'cos' policies (in epochs).\n_C.SOLVER.STEP_SIZE = 1\n# Steps for 'steps_' policies (in epochs).\n_C.SOLVER.STEPS = []\n# Learning rates for 'steps_' policies.\n_C.SOLVER.LRS = []\n# Maximal number of epochs.\n_C.SOLVER.MAX_EPOCH = 300\n# Momentum.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.STEP_SIZE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.STEP_SIZE = 1\n# Steps for 'steps_' policies (in epochs).\n_C.SOLVER.STEPS = []\n# Learning rates for 'steps_' policies.\n_C.SOLVER.LRS = []\n# Maximal number of epochs.\n_C.SOLVER.MAX_EPOCH = 300\n# Momentum.\n_C.SOLVER.MOMENTUM = 0.9\n# Momentum dampening.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.STEPS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.STEPS = []\n# Learning rates for 'steps_' policies.\n_C.SOLVER.LRS = []\n# Maximal number of epochs.\n_C.SOLVER.MAX_EPOCH = 300\n# Momentum.\n_C.SOLVER.MOMENTUM = 0.9\n# Momentum dampening.\n_C.SOLVER.DAMPENING = 0.0\n# Nesterov momentum.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.LRS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.LRS = []\n# Maximal number of epochs.\n_C.SOLVER.MAX_EPOCH = 300\n# Momentum.\n_C.SOLVER.MOMENTUM = 0.9\n# Momentum dampening.\n_C.SOLVER.DAMPENING = 0.0\n# Nesterov momentum.\n_C.SOLVER.NESTEROV = True\n# L2 regularization.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.MAX_EPOCH",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.MAX_EPOCH = 300\n# Momentum.\n_C.SOLVER.MOMENTUM = 0.9\n# Momentum dampening.\n_C.SOLVER.DAMPENING = 0.0\n# Nesterov momentum.\n_C.SOLVER.NESTEROV = True\n# L2 regularization.\n_C.SOLVER.WEIGHT_DECAY = 1e-4\n# Start the warm up from SOLVER.BASE_LR * SOLVER.WARMUP_FACTOR.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.MOMENTUM",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.MOMENTUM = 0.9\n# Momentum dampening.\n_C.SOLVER.DAMPENING = 0.0\n# Nesterov momentum.\n_C.SOLVER.NESTEROV = True\n# L2 regularization.\n_C.SOLVER.WEIGHT_DECAY = 1e-4\n# Start the warm up from SOLVER.BASE_LR * SOLVER.WARMUP_FACTOR.\n_C.SOLVER.WARMUP_FACTOR = 0.1\n# Gradually warm up the SOLVER.BASE_LR over this number of epochs.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.DAMPENING",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.DAMPENING = 0.0\n# Nesterov momentum.\n_C.SOLVER.NESTEROV = True\n# L2 regularization.\n_C.SOLVER.WEIGHT_DECAY = 1e-4\n# Start the warm up from SOLVER.BASE_LR * SOLVER.WARMUP_FACTOR.\n_C.SOLVER.WARMUP_FACTOR = 0.1\n# Gradually warm up the SOLVER.BASE_LR over this number of epochs.\n_C.SOLVER.WARMUP_EPOCHS = 0.0\n# The start learning rate of the warm up.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.NESTEROV",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.NESTEROV = True\n# L2 regularization.\n_C.SOLVER.WEIGHT_DECAY = 1e-4\n# Start the warm up from SOLVER.BASE_LR * SOLVER.WARMUP_FACTOR.\n_C.SOLVER.WARMUP_FACTOR = 0.1\n# Gradually warm up the SOLVER.BASE_LR over this number of epochs.\n_C.SOLVER.WARMUP_EPOCHS = 0.0\n# The start learning rate of the warm up.\n_C.SOLVER.WARMUP_START_LR = 0.01\n# Optimization method.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.WEIGHT_DECAY",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.WEIGHT_DECAY = 1e-4\n# Start the warm up from SOLVER.BASE_LR * SOLVER.WARMUP_FACTOR.\n_C.SOLVER.WARMUP_FACTOR = 0.1\n# Gradually warm up the SOLVER.BASE_LR over this number of epochs.\n_C.SOLVER.WARMUP_EPOCHS = 0.0\n# The start learning rate of the warm up.\n_C.SOLVER.WARMUP_START_LR = 0.01\n# Optimization method.\n_C.SOLVER.OPTIMIZING_METHOD = \"sgd\"\n# Base learning rate is linearly scaled with NUM_SHARDS.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.WARMUP_FACTOR",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.WARMUP_FACTOR = 0.1\n# Gradually warm up the SOLVER.BASE_LR over this number of epochs.\n_C.SOLVER.WARMUP_EPOCHS = 0.0\n# The start learning rate of the warm up.\n_C.SOLVER.WARMUP_START_LR = 0.01\n# Optimization method.\n_C.SOLVER.OPTIMIZING_METHOD = \"sgd\"\n# Base learning rate is linearly scaled with NUM_SHARDS.\n_C.SOLVER.BASE_LR_SCALE_NUM_SHARDS = False\n# If True, start from the peak cosine learning rate after warm up.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.WARMUP_EPOCHS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.WARMUP_EPOCHS = 0.0\n# The start learning rate of the warm up.\n_C.SOLVER.WARMUP_START_LR = 0.01\n# Optimization method.\n_C.SOLVER.OPTIMIZING_METHOD = \"sgd\"\n# Base learning rate is linearly scaled with NUM_SHARDS.\n_C.SOLVER.BASE_LR_SCALE_NUM_SHARDS = False\n# If True, start from the peak cosine learning rate after warm up.\n_C.SOLVER.COSINE_AFTER_WARMUP = False\n# If True, perform no weight decay on parameter with one dimension (bias term, etc).",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.WARMUP_START_LR",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.WARMUP_START_LR = 0.01\n# Optimization method.\n_C.SOLVER.OPTIMIZING_METHOD = \"sgd\"\n# Base learning rate is linearly scaled with NUM_SHARDS.\n_C.SOLVER.BASE_LR_SCALE_NUM_SHARDS = False\n# If True, start from the peak cosine learning rate after warm up.\n_C.SOLVER.COSINE_AFTER_WARMUP = False\n# If True, perform no weight decay on parameter with one dimension (bias term, etc).\n_C.SOLVER.ZERO_WD_1D_PARAM = False\n# Clip gradient at this value before optimizer update",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.OPTIMIZING_METHOD",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.OPTIMIZING_METHOD = \"sgd\"\n# Base learning rate is linearly scaled with NUM_SHARDS.\n_C.SOLVER.BASE_LR_SCALE_NUM_SHARDS = False\n# If True, start from the peak cosine learning rate after warm up.\n_C.SOLVER.COSINE_AFTER_WARMUP = False\n# If True, perform no weight decay on parameter with one dimension (bias term, etc).\n_C.SOLVER.ZERO_WD_1D_PARAM = False\n# Clip gradient at this value before optimizer update\n_C.SOLVER.CLIP_GRAD_VAL = None\n# Clip gradient at this norm before optimizer update",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.BASE_LR_SCALE_NUM_SHARDS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.BASE_LR_SCALE_NUM_SHARDS = False\n# If True, start from the peak cosine learning rate after warm up.\n_C.SOLVER.COSINE_AFTER_WARMUP = False\n# If True, perform no weight decay on parameter with one dimension (bias term, etc).\n_C.SOLVER.ZERO_WD_1D_PARAM = False\n# Clip gradient at this value before optimizer update\n_C.SOLVER.CLIP_GRAD_VAL = None\n# Clip gradient at this norm before optimizer update\n_C.SOLVER.CLIP_GRAD_L2NORM = None\n_C.SOLVER.CLIP_GRADIENT = 20",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.COSINE_AFTER_WARMUP",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.COSINE_AFTER_WARMUP = False\n# If True, perform no weight decay on parameter with one dimension (bias term, etc).\n_C.SOLVER.ZERO_WD_1D_PARAM = False\n# Clip gradient at this value before optimizer update\n_C.SOLVER.CLIP_GRAD_VAL = None\n# Clip gradient at this norm before optimizer update\n_C.SOLVER.CLIP_GRAD_L2NORM = None\n_C.SOLVER.CLIP_GRADIENT = 20\n# ---------------------------------------------------------------------------- #\n# Misc options",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.ZERO_WD_1D_PARAM",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.ZERO_WD_1D_PARAM = False\n# Clip gradient at this value before optimizer update\n_C.SOLVER.CLIP_GRAD_VAL = None\n# Clip gradient at this norm before optimizer update\n_C.SOLVER.CLIP_GRAD_L2NORM = None\n_C.SOLVER.CLIP_GRADIENT = 20\n# ---------------------------------------------------------------------------- #\n# Misc options\n# ---------------------------------------------------------------------------- #\n# Number of GPUs to use (applies to both training and testing).",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.CLIP_GRAD_VAL",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.CLIP_GRAD_VAL = None\n# Clip gradient at this norm before optimizer update\n_C.SOLVER.CLIP_GRAD_L2NORM = None\n_C.SOLVER.CLIP_GRADIENT = 20\n# ---------------------------------------------------------------------------- #\n# Misc options\n# ---------------------------------------------------------------------------- #\n# Number of GPUs to use (applies to both training and testing).\n_C.NUM_GPUS = 1\n# Number of machine to use for the job.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.CLIP_GRAD_L2NORM",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.CLIP_GRAD_L2NORM = None\n_C.SOLVER.CLIP_GRADIENT = 20\n# ---------------------------------------------------------------------------- #\n# Misc options\n# ---------------------------------------------------------------------------- #\n# Number of GPUs to use (applies to both training and testing).\n_C.NUM_GPUS = 1\n# Number of machine to use for the job.\n_C.NUM_SHARDS = 1\n# The index of the current machine.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SOLVER.CLIP_GRADIENT",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SOLVER.CLIP_GRADIENT = 20\n# ---------------------------------------------------------------------------- #\n# Misc options\n# ---------------------------------------------------------------------------- #\n# Number of GPUs to use (applies to both training and testing).\n_C.NUM_GPUS = 1\n# Number of machine to use for the job.\n_C.NUM_SHARDS = 1\n# The index of the current machine.\n_C.SHARD_ID = 0",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.NUM_GPUS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.NUM_GPUS = 1\n# Number of machine to use for the job.\n_C.NUM_SHARDS = 1\n# The index of the current machine.\n_C.SHARD_ID = 0\n# Output basedir.\n_C.OUTPUT_DIR = \"./tmp\"\n# Note that non-determinism may still be present due to non-deterministic\n# operator implementations in GPU operator libraries.\n_C.RNG_SEED = 1",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.NUM_SHARDS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.NUM_SHARDS = 1\n# The index of the current machine.\n_C.SHARD_ID = 0\n# Output basedir.\n_C.OUTPUT_DIR = \"./tmp\"\n# Note that non-determinism may still be present due to non-deterministic\n# operator implementations in GPU operator libraries.\n_C.RNG_SEED = 1\n# Log period in iters.\n_C.LOG_PERIOD = 10",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.SHARD_ID",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.SHARD_ID = 0\n# Output basedir.\n_C.OUTPUT_DIR = \"./tmp\"\n# Note that non-determinism may still be present due to non-deterministic\n# operator implementations in GPU operator libraries.\n_C.RNG_SEED = 1\n# Log period in iters.\n_C.LOG_PERIOD = 10\n# If True, log the model info.\n_C.LOG_MODEL_INFO = True",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.OUTPUT_DIR",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.OUTPUT_DIR = \"./tmp\"\n# Note that non-determinism may still be present due to non-deterministic\n# operator implementations in GPU operator libraries.\n_C.RNG_SEED = 1\n# Log period in iters.\n_C.LOG_PERIOD = 10\n# If True, log the model info.\n_C.LOG_MODEL_INFO = True\n# Distributed backend.\n_C.DIST_BACKEND = \"nccl\"",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.RNG_SEED",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.RNG_SEED = 1\n# Log period in iters.\n_C.LOG_PERIOD = 10\n# If True, log the model info.\n_C.LOG_MODEL_INFO = True\n# Distributed backend.\n_C.DIST_BACKEND = \"nccl\"\n# ---------------------------------------------------------------------------- #\n# Benchmark options\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.LOG_PERIOD",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.LOG_PERIOD = 10\n# If True, log the model info.\n_C.LOG_MODEL_INFO = True\n# Distributed backend.\n_C.DIST_BACKEND = \"nccl\"\n# ---------------------------------------------------------------------------- #\n# Benchmark options\n# ---------------------------------------------------------------------------- #\n_C.BENCHMARK = CfgNode()\n# Number of epochs for data loading benchmark.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.LOG_MODEL_INFO",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.LOG_MODEL_INFO = True\n# Distributed backend.\n_C.DIST_BACKEND = \"nccl\"\n# ---------------------------------------------------------------------------- #\n# Benchmark options\n# ---------------------------------------------------------------------------- #\n_C.BENCHMARK = CfgNode()\n# Number of epochs for data loading benchmark.\n_C.BENCHMARK.NUM_EPOCHS = 5\n# Log period in iters for data loading benchmark.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DIST_BACKEND",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DIST_BACKEND = \"nccl\"\n# ---------------------------------------------------------------------------- #\n# Benchmark options\n# ---------------------------------------------------------------------------- #\n_C.BENCHMARK = CfgNode()\n# Number of epochs for data loading benchmark.\n_C.BENCHMARK.NUM_EPOCHS = 5\n# Log period in iters for data loading benchmark.\n_C.BENCHMARK.LOG_PERIOD = 100\n# If True, shuffle dataloader for epoch during benchmark.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.BENCHMARK",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.BENCHMARK = CfgNode()\n# Number of epochs for data loading benchmark.\n_C.BENCHMARK.NUM_EPOCHS = 5\n# Log period in iters for data loading benchmark.\n_C.BENCHMARK.LOG_PERIOD = 100\n# If True, shuffle dataloader for epoch during benchmark.\n_C.BENCHMARK.SHUFFLE = False\n# ---------------------------------------------------------------------------- #\n# Common train/test data loader options\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.BENCHMARK.NUM_EPOCHS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.BENCHMARK.NUM_EPOCHS = 5\n# Log period in iters for data loading benchmark.\n_C.BENCHMARK.LOG_PERIOD = 100\n# If True, shuffle dataloader for epoch during benchmark.\n_C.BENCHMARK.SHUFFLE = False\n# ---------------------------------------------------------------------------- #\n# Common train/test data loader options\n# ---------------------------------------------------------------------------- #\n_C.DATA_LOADER = CfgNode()\n# Number of data loader workers per training process.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.BENCHMARK.LOG_PERIOD",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.BENCHMARK.LOG_PERIOD = 100\n# If True, shuffle dataloader for epoch during benchmark.\n_C.BENCHMARK.SHUFFLE = False\n# ---------------------------------------------------------------------------- #\n# Common train/test data loader options\n# ---------------------------------------------------------------------------- #\n_C.DATA_LOADER = CfgNode()\n# Number of data loader workers per training process.\n_C.DATA_LOADER.NUM_WORKERS = 8\n# Load data to pinned host memory.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.BENCHMARK.SHUFFLE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.BENCHMARK.SHUFFLE = False\n# ---------------------------------------------------------------------------- #\n# Common train/test data loader options\n# ---------------------------------------------------------------------------- #\n_C.DATA_LOADER = CfgNode()\n# Number of data loader workers per training process.\n_C.DATA_LOADER.NUM_WORKERS = 8\n# Load data to pinned host memory.\n_C.DATA_LOADER.PIN_MEMORY = True\n# Enable multi thread decoding.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA_LOADER",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA_LOADER = CfgNode()\n# Number of data loader workers per training process.\n_C.DATA_LOADER.NUM_WORKERS = 8\n# Load data to pinned host memory.\n_C.DATA_LOADER.PIN_MEMORY = True\n# Enable multi thread decoding.\n_C.DATA_LOADER.ENABLE_MULTI_THREAD_DECODE = False\n# ---------------------------------------------------------------------------- #\n# Detection options.\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA_LOADER.NUM_WORKERS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA_LOADER.NUM_WORKERS = 8\n# Load data to pinned host memory.\n_C.DATA_LOADER.PIN_MEMORY = True\n# Enable multi thread decoding.\n_C.DATA_LOADER.ENABLE_MULTI_THREAD_DECODE = False\n# ---------------------------------------------------------------------------- #\n# Detection options.\n# ---------------------------------------------------------------------------- #\n_C.DETECTION = CfgNode()\n# Whether enable video detection.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA_LOADER.PIN_MEMORY",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA_LOADER.PIN_MEMORY = True\n# Enable multi thread decoding.\n_C.DATA_LOADER.ENABLE_MULTI_THREAD_DECODE = False\n# ---------------------------------------------------------------------------- #\n# Detection options.\n# ---------------------------------------------------------------------------- #\n_C.DETECTION = CfgNode()\n# Whether enable video detection.\n_C.DETECTION.ENABLE = False\n# Aligned version of RoI. More details can be found at slowfast/models/head_helper.py",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DATA_LOADER.ENABLE_MULTI_THREAD_DECODE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DATA_LOADER.ENABLE_MULTI_THREAD_DECODE = False\n# ---------------------------------------------------------------------------- #\n# Detection options.\n# ---------------------------------------------------------------------------- #\n_C.DETECTION = CfgNode()\n# Whether enable video detection.\n_C.DETECTION.ENABLE = False\n# Aligned version of RoI. More details can be found at slowfast/models/head_helper.py\n_C.DETECTION.ALIGNED = True\n# Spatial scale factor.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DETECTION",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DETECTION = CfgNode()\n# Whether enable video detection.\n_C.DETECTION.ENABLE = False\n# Aligned version of RoI. More details can be found at slowfast/models/head_helper.py\n_C.DETECTION.ALIGNED = True\n# Spatial scale factor.\n_C.DETECTION.SPATIAL_SCALE_FACTOR = 16\n# RoI tranformation resolution.\n_C.DETECTION.ROI_XFORM_RESOLUTION = 7\n# -----------------------------------------------------------------------------",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DETECTION.ENABLE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DETECTION.ENABLE = False\n# Aligned version of RoI. More details can be found at slowfast/models/head_helper.py\n_C.DETECTION.ALIGNED = True\n# Spatial scale factor.\n_C.DETECTION.SPATIAL_SCALE_FACTOR = 16\n# RoI tranformation resolution.\n_C.DETECTION.ROI_XFORM_RESOLUTION = 7\n# -----------------------------------------------------------------------------\n# AVA Dataset options\n# -----------------------------------------------------------------------------",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DETECTION.ALIGNED",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DETECTION.ALIGNED = True\n# Spatial scale factor.\n_C.DETECTION.SPATIAL_SCALE_FACTOR = 16\n# RoI tranformation resolution.\n_C.DETECTION.ROI_XFORM_RESOLUTION = 7\n# -----------------------------------------------------------------------------\n# AVA Dataset options\n# -----------------------------------------------------------------------------\n_C.AVA = CfgNode()\n# Directory path of frames.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DETECTION.SPATIAL_SCALE_FACTOR",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DETECTION.SPATIAL_SCALE_FACTOR = 16\n# RoI tranformation resolution.\n_C.DETECTION.ROI_XFORM_RESOLUTION = 7\n# -----------------------------------------------------------------------------\n# AVA Dataset options\n# -----------------------------------------------------------------------------\n_C.AVA = CfgNode()\n# Directory path of frames.\n_C.AVA.FRAME_DIR = \"/mnt/fair-flash3-east/ava_trainval_frames.img/\"\n# Directory path for files of frame lists.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DETECTION.ROI_XFORM_RESOLUTION",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DETECTION.ROI_XFORM_RESOLUTION = 7\n# -----------------------------------------------------------------------------\n# AVA Dataset options\n# -----------------------------------------------------------------------------\n_C.AVA = CfgNode()\n# Directory path of frames.\n_C.AVA.FRAME_DIR = \"/mnt/fair-flash3-east/ava_trainval_frames.img/\"\n# Directory path for files of frame lists.\n_C.AVA.FRAME_LIST_DIR = (\n    \"/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/\"",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AVA = CfgNode()\n# Directory path of frames.\n_C.AVA.FRAME_DIR = \"/mnt/fair-flash3-east/ava_trainval_frames.img/\"\n# Directory path for files of frame lists.\n_C.AVA.FRAME_LIST_DIR = (\n    \"/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/\"\n)\n# Directory path for annotation files.\n_C.AVA.ANNOTATION_DIR = (\n    \"/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/\"",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.FRAME_DIR",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.FRAME_DIR = \"/mnt/fair-flash3-east/ava_trainval_frames.img/\"\n# Directory path for files of frame lists.\n_C.AVA.FRAME_LIST_DIR = (\n    \"/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/\"\n)\n# Directory path for annotation files.\n_C.AVA.ANNOTATION_DIR = (\n    \"/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/\"\n)\n# Filenames of training samples list files.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.FRAME_LIST_DIR",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.FRAME_LIST_DIR = (\n    \"/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/\"\n)\n# Directory path for annotation files.\n_C.AVA.ANNOTATION_DIR = (\n    \"/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/\"\n)\n# Filenames of training samples list files.\n_C.AVA.TRAIN_LISTS = [\"train.csv\"]\n# Filenames of test samples list files.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.ANNOTATION_DIR",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.ANNOTATION_DIR = (\n    \"/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/\"\n)\n# Filenames of training samples list files.\n_C.AVA.TRAIN_LISTS = [\"train.csv\"]\n# Filenames of test samples list files.\n_C.AVA.TEST_LISTS = [\"val.csv\"]\n# Filenames of box list files for training. Note that we assume files which\n# contains predicted boxes will have a suffix \"predicted_boxes\" in the\n# filename.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.TRAIN_LISTS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.TRAIN_LISTS = [\"train.csv\"]\n# Filenames of test samples list files.\n_C.AVA.TEST_LISTS = [\"val.csv\"]\n# Filenames of box list files for training. Note that we assume files which\n# contains predicted boxes will have a suffix \"predicted_boxes\" in the\n# filename.\n_C.AVA.TRAIN_GT_BOX_LISTS = [\"ava_train_v2.2.csv\"]\n_C.AVA.TRAIN_PREDICT_BOX_LISTS = []\n# Filenames of box list files for test.\n_C.AVA.TEST_PREDICT_BOX_LISTS = [\"ava_val_predicted_boxes.csv\"]",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.TEST_LISTS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.TEST_LISTS = [\"val.csv\"]\n# Filenames of box list files for training. Note that we assume files which\n# contains predicted boxes will have a suffix \"predicted_boxes\" in the\n# filename.\n_C.AVA.TRAIN_GT_BOX_LISTS = [\"ava_train_v2.2.csv\"]\n_C.AVA.TRAIN_PREDICT_BOX_LISTS = []\n# Filenames of box list files for test.\n_C.AVA.TEST_PREDICT_BOX_LISTS = [\"ava_val_predicted_boxes.csv\"]\n# This option controls the score threshold for the predicted boxes to use.\n_C.AVA.DETECTION_SCORE_THRESH = 0.9",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.TRAIN_GT_BOX_LISTS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.TRAIN_GT_BOX_LISTS = [\"ava_train_v2.2.csv\"]\n_C.AVA.TRAIN_PREDICT_BOX_LISTS = []\n# Filenames of box list files for test.\n_C.AVA.TEST_PREDICT_BOX_LISTS = [\"ava_val_predicted_boxes.csv\"]\n# This option controls the score threshold for the predicted boxes to use.\n_C.AVA.DETECTION_SCORE_THRESH = 0.9\n# If use BGR as the format of input frames.\n_C.AVA.BGR = False\n# Training augmentation parameters\n# Whether to use color augmentation method.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.TRAIN_PREDICT_BOX_LISTS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.TRAIN_PREDICT_BOX_LISTS = []\n# Filenames of box list files for test.\n_C.AVA.TEST_PREDICT_BOX_LISTS = [\"ava_val_predicted_boxes.csv\"]\n# This option controls the score threshold for the predicted boxes to use.\n_C.AVA.DETECTION_SCORE_THRESH = 0.9\n# If use BGR as the format of input frames.\n_C.AVA.BGR = False\n# Training augmentation parameters\n# Whether to use color augmentation method.\n_C.AVA.TRAIN_USE_COLOR_AUGMENTATION = False",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.TEST_PREDICT_BOX_LISTS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.TEST_PREDICT_BOX_LISTS = [\"ava_val_predicted_boxes.csv\"]\n# This option controls the score threshold for the predicted boxes to use.\n_C.AVA.DETECTION_SCORE_THRESH = 0.9\n# If use BGR as the format of input frames.\n_C.AVA.BGR = False\n# Training augmentation parameters\n# Whether to use color augmentation method.\n_C.AVA.TRAIN_USE_COLOR_AUGMENTATION = False\n# Whether to only use PCA jitter augmentation when using color augmentation\n# method (otherwise combine with color jitter method).",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.DETECTION_SCORE_THRESH",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.DETECTION_SCORE_THRESH = 0.9\n# If use BGR as the format of input frames.\n_C.AVA.BGR = False\n# Training augmentation parameters\n# Whether to use color augmentation method.\n_C.AVA.TRAIN_USE_COLOR_AUGMENTATION = False\n# Whether to only use PCA jitter augmentation when using color augmentation\n# method (otherwise combine with color jitter method).\n_C.AVA.TRAIN_PCA_JITTER_ONLY = True\n# Whether to do horizontal flipping during test.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.BGR",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.BGR = False\n# Training augmentation parameters\n# Whether to use color augmentation method.\n_C.AVA.TRAIN_USE_COLOR_AUGMENTATION = False\n# Whether to only use PCA jitter augmentation when using color augmentation\n# method (otherwise combine with color jitter method).\n_C.AVA.TRAIN_PCA_JITTER_ONLY = True\n# Whether to do horizontal flipping during test.\n_C.AVA.TEST_FORCE_FLIP = False\n# Whether to use full test set for validation split.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.TRAIN_USE_COLOR_AUGMENTATION",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.TRAIN_USE_COLOR_AUGMENTATION = False\n# Whether to only use PCA jitter augmentation when using color augmentation\n# method (otherwise combine with color jitter method).\n_C.AVA.TRAIN_PCA_JITTER_ONLY = True\n# Whether to do horizontal flipping during test.\n_C.AVA.TEST_FORCE_FLIP = False\n# Whether to use full test set for validation split.\n_C.AVA.FULL_TEST_ON_VAL = False\n# The name of the file to the ava label map.\n_C.AVA.LABEL_MAP_FILE = \"ava_action_list_v2.2_for_activitynet_2019.pbtxt\"",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.TRAIN_PCA_JITTER_ONLY",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.TRAIN_PCA_JITTER_ONLY = True\n# Whether to do horizontal flipping during test.\n_C.AVA.TEST_FORCE_FLIP = False\n# Whether to use full test set for validation split.\n_C.AVA.FULL_TEST_ON_VAL = False\n# The name of the file to the ava label map.\n_C.AVA.LABEL_MAP_FILE = \"ava_action_list_v2.2_for_activitynet_2019.pbtxt\"\n# The name of the file to the ava exclusion.\n_C.AVA.EXCLUSION_FILE = \"ava_val_excluded_timestamps_v2.2.csv\"\n# The name of the file to the ava groundtruth.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.TEST_FORCE_FLIP",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.TEST_FORCE_FLIP = False\n# Whether to use full test set for validation split.\n_C.AVA.FULL_TEST_ON_VAL = False\n# The name of the file to the ava label map.\n_C.AVA.LABEL_MAP_FILE = \"ava_action_list_v2.2_for_activitynet_2019.pbtxt\"\n# The name of the file to the ava exclusion.\n_C.AVA.EXCLUSION_FILE = \"ava_val_excluded_timestamps_v2.2.csv\"\n# The name of the file to the ava groundtruth.\n_C.AVA.GROUNDTRUTH_FILE = \"ava_val_v2.2.csv\"\n# Backend to process image, includes `pytorch` and `cv2`.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.FULL_TEST_ON_VAL",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.FULL_TEST_ON_VAL = False\n# The name of the file to the ava label map.\n_C.AVA.LABEL_MAP_FILE = \"ava_action_list_v2.2_for_activitynet_2019.pbtxt\"\n# The name of the file to the ava exclusion.\n_C.AVA.EXCLUSION_FILE = \"ava_val_excluded_timestamps_v2.2.csv\"\n# The name of the file to the ava groundtruth.\n_C.AVA.GROUNDTRUTH_FILE = \"ava_val_v2.2.csv\"\n# Backend to process image, includes `pytorch` and `cv2`.\n_C.AVA.IMG_PROC_BACKEND = \"cv2\"\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.LABEL_MAP_FILE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.LABEL_MAP_FILE = \"ava_action_list_v2.2_for_activitynet_2019.pbtxt\"\n# The name of the file to the ava exclusion.\n_C.AVA.EXCLUSION_FILE = \"ava_val_excluded_timestamps_v2.2.csv\"\n# The name of the file to the ava groundtruth.\n_C.AVA.GROUNDTRUTH_FILE = \"ava_val_v2.2.csv\"\n# Backend to process image, includes `pytorch` and `cv2`.\n_C.AVA.IMG_PROC_BACKEND = \"cv2\"\n# ---------------------------------------------------------------------------- #\n# Multigrid training options\n# See https://arxiv.org/abs/1912.00998 for details about multigrid training.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.EXCLUSION_FILE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.EXCLUSION_FILE = \"ava_val_excluded_timestamps_v2.2.csv\"\n# The name of the file to the ava groundtruth.\n_C.AVA.GROUNDTRUTH_FILE = \"ava_val_v2.2.csv\"\n# Backend to process image, includes `pytorch` and `cv2`.\n_C.AVA.IMG_PROC_BACKEND = \"cv2\"\n# ---------------------------------------------------------------------------- #\n# Multigrid training options\n# See https://arxiv.org/abs/1912.00998 for details about multigrid training.\n# ---------------------------------------------------------------------------- #\n_C.MULTIGRID = CfgNode()",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.GROUNDTRUTH_FILE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.GROUNDTRUTH_FILE = \"ava_val_v2.2.csv\"\n# Backend to process image, includes `pytorch` and `cv2`.\n_C.AVA.IMG_PROC_BACKEND = \"cv2\"\n# ---------------------------------------------------------------------------- #\n# Multigrid training options\n# See https://arxiv.org/abs/1912.00998 for details about multigrid training.\n# ---------------------------------------------------------------------------- #\n_C.MULTIGRID = CfgNode()\n# Multigrid training allows us to train for more epochs with fewer iterations.\n# This hyperparameter specifies how many times more epochs to train.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.AVA.IMG_PROC_BACKEND",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.AVA.IMG_PROC_BACKEND = \"cv2\"\n# ---------------------------------------------------------------------------- #\n# Multigrid training options\n# See https://arxiv.org/abs/1912.00998 for details about multigrid training.\n# ---------------------------------------------------------------------------- #\n_C.MULTIGRID = CfgNode()\n# Multigrid training allows us to train for more epochs with fewer iterations.\n# This hyperparameter specifies how many times more epochs to train.\n# The default setting in paper trains for 1.5x more epochs than baseline.\n_C.MULTIGRID.EPOCH_FACTOR = 1.5",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MULTIGRID",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MULTIGRID = CfgNode()\n# Multigrid training allows us to train for more epochs with fewer iterations.\n# This hyperparameter specifies how many times more epochs to train.\n# The default setting in paper trains for 1.5x more epochs than baseline.\n_C.MULTIGRID.EPOCH_FACTOR = 1.5\n# Enable short cycles.\n_C.MULTIGRID.SHORT_CYCLE = False\n# Short cycle additional spatial dimensions relative to the default crop size.\n_C.MULTIGRID.SHORT_CYCLE_FACTORS = [0.5, 0.5 ** 0.5]\n_C.MULTIGRID.LONG_CYCLE = False",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MULTIGRID.EPOCH_FACTOR",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MULTIGRID.EPOCH_FACTOR = 1.5\n# Enable short cycles.\n_C.MULTIGRID.SHORT_CYCLE = False\n# Short cycle additional spatial dimensions relative to the default crop size.\n_C.MULTIGRID.SHORT_CYCLE_FACTORS = [0.5, 0.5 ** 0.5]\n_C.MULTIGRID.LONG_CYCLE = False\n# (Temporal, Spatial) dimensions relative to the default shape.\n_C.MULTIGRID.LONG_CYCLE_FACTORS = [\n    (0.25, 0.5 ** 0.5),\n    (0.5, 0.5 ** 0.5),",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MULTIGRID.SHORT_CYCLE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MULTIGRID.SHORT_CYCLE = False\n# Short cycle additional spatial dimensions relative to the default crop size.\n_C.MULTIGRID.SHORT_CYCLE_FACTORS = [0.5, 0.5 ** 0.5]\n_C.MULTIGRID.LONG_CYCLE = False\n# (Temporal, Spatial) dimensions relative to the default shape.\n_C.MULTIGRID.LONG_CYCLE_FACTORS = [\n    (0.25, 0.5 ** 0.5),\n    (0.5, 0.5 ** 0.5),\n    (0.5, 1),\n    (1, 1),",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MULTIGRID.SHORT_CYCLE_FACTORS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MULTIGRID.SHORT_CYCLE_FACTORS = [0.5, 0.5 ** 0.5]\n_C.MULTIGRID.LONG_CYCLE = False\n# (Temporal, Spatial) dimensions relative to the default shape.\n_C.MULTIGRID.LONG_CYCLE_FACTORS = [\n    (0.25, 0.5 ** 0.5),\n    (0.5, 0.5 ** 0.5),\n    (0.5, 1),\n    (1, 1),\n]\n# While a standard BN computes stats across all examples in a GPU,",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MULTIGRID.LONG_CYCLE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MULTIGRID.LONG_CYCLE = False\n# (Temporal, Spatial) dimensions relative to the default shape.\n_C.MULTIGRID.LONG_CYCLE_FACTORS = [\n    (0.25, 0.5 ** 0.5),\n    (0.5, 0.5 ** 0.5),\n    (0.5, 1),\n    (1, 1),\n]\n# While a standard BN computes stats across all examples in a GPU,\n# for multigrid training we fix the number of clips to compute BN stats on.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MULTIGRID.LONG_CYCLE_FACTORS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MULTIGRID.LONG_CYCLE_FACTORS = [\n    (0.25, 0.5 ** 0.5),\n    (0.5, 0.5 ** 0.5),\n    (0.5, 1),\n    (1, 1),\n]\n# While a standard BN computes stats across all examples in a GPU,\n# for multigrid training we fix the number of clips to compute BN stats on.\n# See https://arxiv.org/abs/1912.00998 for details.\n_C.MULTIGRID.BN_BASE_SIZE = 8",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MULTIGRID.BN_BASE_SIZE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MULTIGRID.BN_BASE_SIZE = 8\n# Multigrid training epochs are not proportional to actual training time or\n# computations, so _C.TRAIN.EVAL_PERIOD leads to too frequent or rare\n# evaluation. We use a multigrid-specific rule to determine when to evaluate:\n# This hyperparameter defines how many times to evaluate a model per long\n# cycle shape.\n_C.MULTIGRID.EVAL_FREQ = 3\n# No need to specify; Set automatically and used as global variables.\n_C.MULTIGRID.LONG_CYCLE_SAMPLING_RATE = 0\n_C.MULTIGRID.DEFAULT_B = 0",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MULTIGRID.EVAL_FREQ",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MULTIGRID.EVAL_FREQ = 3\n# No need to specify; Set automatically and used as global variables.\n_C.MULTIGRID.LONG_CYCLE_SAMPLING_RATE = 0\n_C.MULTIGRID.DEFAULT_B = 0\n_C.MULTIGRID.DEFAULT_T = 0\n_C.MULTIGRID.DEFAULT_S = 0\n# -----------------------------------------------------------------------------\n# Tensorboard Visualization Options\n# -----------------------------------------------------------------------------\n_C.TENSORBOARD = CfgNode()",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MULTIGRID.LONG_CYCLE_SAMPLING_RATE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MULTIGRID.LONG_CYCLE_SAMPLING_RATE = 0\n_C.MULTIGRID.DEFAULT_B = 0\n_C.MULTIGRID.DEFAULT_T = 0\n_C.MULTIGRID.DEFAULT_S = 0\n# -----------------------------------------------------------------------------\n# Tensorboard Visualization Options\n# -----------------------------------------------------------------------------\n_C.TENSORBOARD = CfgNode()\n# Log to summary writer, this will automatically.\n# log loss, lr and metrics during train/eval.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MULTIGRID.DEFAULT_B",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MULTIGRID.DEFAULT_B = 0\n_C.MULTIGRID.DEFAULT_T = 0\n_C.MULTIGRID.DEFAULT_S = 0\n# -----------------------------------------------------------------------------\n# Tensorboard Visualization Options\n# -----------------------------------------------------------------------------\n_C.TENSORBOARD = CfgNode()\n# Log to summary writer, this will automatically.\n# log loss, lr and metrics during train/eval.\n_C.TENSORBOARD.ENABLE = False",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MULTIGRID.DEFAULT_T",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MULTIGRID.DEFAULT_T = 0\n_C.MULTIGRID.DEFAULT_S = 0\n# -----------------------------------------------------------------------------\n# Tensorboard Visualization Options\n# -----------------------------------------------------------------------------\n_C.TENSORBOARD = CfgNode()\n# Log to summary writer, this will automatically.\n# log loss, lr and metrics during train/eval.\n_C.TENSORBOARD.ENABLE = False\n# Provide path to prediction results for visualization.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.MULTIGRID.DEFAULT_S",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.MULTIGRID.DEFAULT_S = 0\n# -----------------------------------------------------------------------------\n# Tensorboard Visualization Options\n# -----------------------------------------------------------------------------\n_C.TENSORBOARD = CfgNode()\n# Log to summary writer, this will automatically.\n# log loss, lr and metrics during train/eval.\n_C.TENSORBOARD.ENABLE = False\n# Provide path to prediction results for visualization.\n# This is a pickle file of [prediction_tensor, label_tensor]",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD = CfgNode()\n# Log to summary writer, this will automatically.\n# log loss, lr and metrics during train/eval.\n_C.TENSORBOARD.ENABLE = False\n# Provide path to prediction results for visualization.\n# This is a pickle file of [prediction_tensor, label_tensor]\n_C.TENSORBOARD.PREDICTIONS_PATH = \"\"\n# Path to directory for tensorboard logs.\n# Default to to cfg.OUTPUT_DIR/runs-{cfg.TRAIN.DATASET}.\n_C.TENSORBOARD.LOG_DIR = \"\"",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.ENABLE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.ENABLE = False\n# Provide path to prediction results for visualization.\n# This is a pickle file of [prediction_tensor, label_tensor]\n_C.TENSORBOARD.PREDICTIONS_PATH = \"\"\n# Path to directory for tensorboard logs.\n# Default to to cfg.OUTPUT_DIR/runs-{cfg.TRAIN.DATASET}.\n_C.TENSORBOARD.LOG_DIR = \"\"\n# Path to a json file providing class_name - id mapping\n# in the format {\"class_name1\": id1, \"class_name2\": id2, ...}.\n# This file must be provided to enable plotting confusion matrix",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.PREDICTIONS_PATH",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.PREDICTIONS_PATH = \"\"\n# Path to directory for tensorboard logs.\n# Default to to cfg.OUTPUT_DIR/runs-{cfg.TRAIN.DATASET}.\n_C.TENSORBOARD.LOG_DIR = \"\"\n# Path to a json file providing class_name - id mapping\n# in the format {\"class_name1\": id1, \"class_name2\": id2, ...}.\n# This file must be provided to enable plotting confusion matrix\n# by a subset or parent categories.\n_C.TENSORBOARD.CLASS_NAMES_PATH = \"\"\n# Path to a json file for categories -> classes mapping",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.LOG_DIR",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.LOG_DIR = \"\"\n# Path to a json file providing class_name - id mapping\n# in the format {\"class_name1\": id1, \"class_name2\": id2, ...}.\n# This file must be provided to enable plotting confusion matrix\n# by a subset or parent categories.\n_C.TENSORBOARD.CLASS_NAMES_PATH = \"\"\n# Path to a json file for categories -> classes mapping\n# in the format {\"parent_class\": [\"child_class1\", \"child_class2\",...], ...}.\n_C.TENSORBOARD.CATEGORIES_PATH = \"\"\n# Config for confusion matrices visualization.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.CLASS_NAMES_PATH",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.CLASS_NAMES_PATH = \"\"\n# Path to a json file for categories -> classes mapping\n# in the format {\"parent_class\": [\"child_class1\", \"child_class2\",...], ...}.\n_C.TENSORBOARD.CATEGORIES_PATH = \"\"\n# Config for confusion matrices visualization.\n_C.TENSORBOARD.CONFUSION_MATRIX = CfgNode()\n# Visualize confusion matrix.\n_C.TENSORBOARD.CONFUSION_MATRIX.ENABLE = False\n# Figure size of the confusion matrices plotted.\n_C.TENSORBOARD.CONFUSION_MATRIX.FIGSIZE = [8, 8]",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.CATEGORIES_PATH",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.CATEGORIES_PATH = \"\"\n# Config for confusion matrices visualization.\n_C.TENSORBOARD.CONFUSION_MATRIX = CfgNode()\n# Visualize confusion matrix.\n_C.TENSORBOARD.CONFUSION_MATRIX.ENABLE = False\n# Figure size of the confusion matrices plotted.\n_C.TENSORBOARD.CONFUSION_MATRIX.FIGSIZE = [8, 8]\n# Path to a subset of categories to visualize.\n# File contains class names separated by newline characters.\n_C.TENSORBOARD.CONFUSION_MATRIX.SUBSET_PATH = \"\"",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.CONFUSION_MATRIX",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.CONFUSION_MATRIX = CfgNode()\n# Visualize confusion matrix.\n_C.TENSORBOARD.CONFUSION_MATRIX.ENABLE = False\n# Figure size of the confusion matrices plotted.\n_C.TENSORBOARD.CONFUSION_MATRIX.FIGSIZE = [8, 8]\n# Path to a subset of categories to visualize.\n# File contains class names separated by newline characters.\n_C.TENSORBOARD.CONFUSION_MATRIX.SUBSET_PATH = \"\"\n# Config for histogram visualization.\n_C.TENSORBOARD.HISTOGRAM = CfgNode()",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.CONFUSION_MATRIX.ENABLE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.CONFUSION_MATRIX.ENABLE = False\n# Figure size of the confusion matrices plotted.\n_C.TENSORBOARD.CONFUSION_MATRIX.FIGSIZE = [8, 8]\n# Path to a subset of categories to visualize.\n# File contains class names separated by newline characters.\n_C.TENSORBOARD.CONFUSION_MATRIX.SUBSET_PATH = \"\"\n# Config for histogram visualization.\n_C.TENSORBOARD.HISTOGRAM = CfgNode()\n# Visualize histograms.\n_C.TENSORBOARD.HISTOGRAM.ENABLE = False",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.CONFUSION_MATRIX.FIGSIZE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.CONFUSION_MATRIX.FIGSIZE = [8, 8]\n# Path to a subset of categories to visualize.\n# File contains class names separated by newline characters.\n_C.TENSORBOARD.CONFUSION_MATRIX.SUBSET_PATH = \"\"\n# Config for histogram visualization.\n_C.TENSORBOARD.HISTOGRAM = CfgNode()\n# Visualize histograms.\n_C.TENSORBOARD.HISTOGRAM.ENABLE = False\n# Path to a subset of classes to plot histograms.\n# Class names must be separated by newline characters.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.CONFUSION_MATRIX.SUBSET_PATH",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.CONFUSION_MATRIX.SUBSET_PATH = \"\"\n# Config for histogram visualization.\n_C.TENSORBOARD.HISTOGRAM = CfgNode()\n# Visualize histograms.\n_C.TENSORBOARD.HISTOGRAM.ENABLE = False\n# Path to a subset of classes to plot histograms.\n# Class names must be separated by newline characters.\n_C.TENSORBOARD.HISTOGRAM.SUBSET_PATH = \"\"\n# Visualize top-k most predicted classes on histograms for each\n# chosen true label.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.HISTOGRAM",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.HISTOGRAM = CfgNode()\n# Visualize histograms.\n_C.TENSORBOARD.HISTOGRAM.ENABLE = False\n# Path to a subset of classes to plot histograms.\n# Class names must be separated by newline characters.\n_C.TENSORBOARD.HISTOGRAM.SUBSET_PATH = \"\"\n# Visualize top-k most predicted classes on histograms for each\n# chosen true label.\n_C.TENSORBOARD.HISTOGRAM.TOPK = 10\n# Figure size of the histograms plotted.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.HISTOGRAM.ENABLE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.HISTOGRAM.ENABLE = False\n# Path to a subset of classes to plot histograms.\n# Class names must be separated by newline characters.\n_C.TENSORBOARD.HISTOGRAM.SUBSET_PATH = \"\"\n# Visualize top-k most predicted classes on histograms for each\n# chosen true label.\n_C.TENSORBOARD.HISTOGRAM.TOPK = 10\n# Figure size of the histograms plotted.\n_C.TENSORBOARD.HISTOGRAM.FIGSIZE = [8, 8]\n# Config for layers' weights and activations visualization.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.HISTOGRAM.SUBSET_PATH",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.HISTOGRAM.SUBSET_PATH = \"\"\n# Visualize top-k most predicted classes on histograms for each\n# chosen true label.\n_C.TENSORBOARD.HISTOGRAM.TOPK = 10\n# Figure size of the histograms plotted.\n_C.TENSORBOARD.HISTOGRAM.FIGSIZE = [8, 8]\n# Config for layers' weights and activations visualization.\n# _C.TENSORBOARD.ENABLE must be True.\n_C.TENSORBOARD.MODEL_VIS = CfgNode()\n# If False, skip model visualization.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.HISTOGRAM.TOPK",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.HISTOGRAM.TOPK = 10\n# Figure size of the histograms plotted.\n_C.TENSORBOARD.HISTOGRAM.FIGSIZE = [8, 8]\n# Config for layers' weights and activations visualization.\n# _C.TENSORBOARD.ENABLE must be True.\n_C.TENSORBOARD.MODEL_VIS = CfgNode()\n# If False, skip model visualization.\n_C.TENSORBOARD.MODEL_VIS.ENABLE = False\n# If False, skip visualizing model weights.\n_C.TENSORBOARD.MODEL_VIS.MODEL_WEIGHTS = False",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.HISTOGRAM.FIGSIZE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.HISTOGRAM.FIGSIZE = [8, 8]\n# Config for layers' weights and activations visualization.\n# _C.TENSORBOARD.ENABLE must be True.\n_C.TENSORBOARD.MODEL_VIS = CfgNode()\n# If False, skip model visualization.\n_C.TENSORBOARD.MODEL_VIS.ENABLE = False\n# If False, skip visualizing model weights.\n_C.TENSORBOARD.MODEL_VIS.MODEL_WEIGHTS = False\n# If False, skip visualizing model activations.\n_C.TENSORBOARD.MODEL_VIS.ACTIVATIONS = False",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.MODEL_VIS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.MODEL_VIS = CfgNode()\n# If False, skip model visualization.\n_C.TENSORBOARD.MODEL_VIS.ENABLE = False\n# If False, skip visualizing model weights.\n_C.TENSORBOARD.MODEL_VIS.MODEL_WEIGHTS = False\n# If False, skip visualizing model activations.\n_C.TENSORBOARD.MODEL_VIS.ACTIVATIONS = False\n# If False, skip visualizing input videos.\n_C.TENSORBOARD.MODEL_VIS.INPUT_VIDEO = False\n# List of strings containing data about layer names and their indexing to",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.MODEL_VIS.ENABLE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.MODEL_VIS.ENABLE = False\n# If False, skip visualizing model weights.\n_C.TENSORBOARD.MODEL_VIS.MODEL_WEIGHTS = False\n# If False, skip visualizing model activations.\n_C.TENSORBOARD.MODEL_VIS.ACTIVATIONS = False\n# If False, skip visualizing input videos.\n_C.TENSORBOARD.MODEL_VIS.INPUT_VIDEO = False\n# List of strings containing data about layer names and their indexing to\n# visualize weights and activations for. The indexing is meant for\n# choosing a subset of activations outputed by a layer for visualization.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.MODEL_VIS.MODEL_WEIGHTS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.MODEL_VIS.MODEL_WEIGHTS = False\n# If False, skip visualizing model activations.\n_C.TENSORBOARD.MODEL_VIS.ACTIVATIONS = False\n# If False, skip visualizing input videos.\n_C.TENSORBOARD.MODEL_VIS.INPUT_VIDEO = False\n# List of strings containing data about layer names and their indexing to\n# visualize weights and activations for. The indexing is meant for\n# choosing a subset of activations outputed by a layer for visualization.\n# If indexing is not specified, visualize all activations outputed by the layer.\n# For each string, layer name and indexing is separated by whitespaces.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.MODEL_VIS.ACTIVATIONS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.MODEL_VIS.ACTIVATIONS = False\n# If False, skip visualizing input videos.\n_C.TENSORBOARD.MODEL_VIS.INPUT_VIDEO = False\n# List of strings containing data about layer names and their indexing to\n# visualize weights and activations for. The indexing is meant for\n# choosing a subset of activations outputed by a layer for visualization.\n# If indexing is not specified, visualize all activations outputed by the layer.\n# For each string, layer name and indexing is separated by whitespaces.\n# e.g.: [layer1 1,2;1,2, layer2, layer3 150,151;3,4]; this means for each array `arr`\n# along the batch dimension in `layer1`, we take arr[[1, 2], [1, 2]]",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.MODEL_VIS.INPUT_VIDEO",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.MODEL_VIS.INPUT_VIDEO = False\n# List of strings containing data about layer names and their indexing to\n# visualize weights and activations for. The indexing is meant for\n# choosing a subset of activations outputed by a layer for visualization.\n# If indexing is not specified, visualize all activations outputed by the layer.\n# For each string, layer name and indexing is separated by whitespaces.\n# e.g.: [layer1 1,2;1,2, layer2, layer3 150,151;3,4]; this means for each array `arr`\n# along the batch dimension in `layer1`, we take arr[[1, 2], [1, 2]]\n_C.TENSORBOARD.MODEL_VIS.LAYER_LIST = []\n# Top-k predictions to plot on videos",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.MODEL_VIS.LAYER_LIST",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.MODEL_VIS.LAYER_LIST = []\n# Top-k predictions to plot on videos\n_C.TENSORBOARD.MODEL_VIS.TOPK_PREDS = 1\n# Colormap to for text boxes and bounding boxes colors\n_C.TENSORBOARD.MODEL_VIS.COLORMAP = \"Pastel2\"\n# Config for visualization video inputs with Grad-CAM.\n# _C.TENSORBOARD.ENABLE must be True.\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM = CfgNode()\n# Whether to run visualization using Grad-CAM technique.\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.ENABLE = True",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.MODEL_VIS.TOPK_PREDS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.MODEL_VIS.TOPK_PREDS = 1\n# Colormap to for text boxes and bounding boxes colors\n_C.TENSORBOARD.MODEL_VIS.COLORMAP = \"Pastel2\"\n# Config for visualization video inputs with Grad-CAM.\n# _C.TENSORBOARD.ENABLE must be True.\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM = CfgNode()\n# Whether to run visualization using Grad-CAM technique.\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.ENABLE = True\n# CNN layers to use for Grad-CAM. The number of layers must be equal to\n# number of pathway(s).",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.MODEL_VIS.COLORMAP",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.MODEL_VIS.COLORMAP = \"Pastel2\"\n# Config for visualization video inputs with Grad-CAM.\n# _C.TENSORBOARD.ENABLE must be True.\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM = CfgNode()\n# Whether to run visualization using Grad-CAM technique.\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.ENABLE = True\n# CNN layers to use for Grad-CAM. The number of layers must be equal to\n# number of pathway(s).\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.LAYER_LIST = []\n# If True, visualize Grad-CAM using true labels for each instances.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.MODEL_VIS.GRAD_CAM",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.MODEL_VIS.GRAD_CAM = CfgNode()\n# Whether to run visualization using Grad-CAM technique.\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.ENABLE = True\n# CNN layers to use for Grad-CAM. The number of layers must be equal to\n# number of pathway(s).\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.LAYER_LIST = []\n# If True, visualize Grad-CAM using true labels for each instances.\n# If False, use the highest predicted class.\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.USE_TRUE_LABEL = False\n# Colormap to for text boxes and bounding boxes colors",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.ENABLE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.ENABLE = True\n# CNN layers to use for Grad-CAM. The number of layers must be equal to\n# number of pathway(s).\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.LAYER_LIST = []\n# If True, visualize Grad-CAM using true labels for each instances.\n# If False, use the highest predicted class.\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.USE_TRUE_LABEL = False\n# Colormap to for text boxes and bounding boxes colors\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.COLORMAP = \"viridis\"\n# Config for visualization for wrong prediction visualization.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.LAYER_LIST",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.LAYER_LIST = []\n# If True, visualize Grad-CAM using true labels for each instances.\n# If False, use the highest predicted class.\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.USE_TRUE_LABEL = False\n# Colormap to for text boxes and bounding boxes colors\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.COLORMAP = \"viridis\"\n# Config for visualization for wrong prediction visualization.\n# _C.TENSORBOARD.ENABLE must be True.\n_C.TENSORBOARD.WRONG_PRED_VIS = CfgNode()\n_C.TENSORBOARD.WRONG_PRED_VIS.ENABLE = False",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.USE_TRUE_LABEL",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.USE_TRUE_LABEL = False\n# Colormap to for text boxes and bounding boxes colors\n_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.COLORMAP = \"viridis\"\n# Config for visualization for wrong prediction visualization.\n# _C.TENSORBOARD.ENABLE must be True.\n_C.TENSORBOARD.WRONG_PRED_VIS = CfgNode()\n_C.TENSORBOARD.WRONG_PRED_VIS.ENABLE = False\n# Folder tag to origanize model eval videos under.\n_C.TENSORBOARD.WRONG_PRED_VIS.TAG = \"Incorrectly classified videos.\"\n# Subset of labels to visualize. Only wrong predictions with true labels",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.COLORMAP",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.MODEL_VIS.GRAD_CAM.COLORMAP = \"viridis\"\n# Config for visualization for wrong prediction visualization.\n# _C.TENSORBOARD.ENABLE must be True.\n_C.TENSORBOARD.WRONG_PRED_VIS = CfgNode()\n_C.TENSORBOARD.WRONG_PRED_VIS.ENABLE = False\n# Folder tag to origanize model eval videos under.\n_C.TENSORBOARD.WRONG_PRED_VIS.TAG = \"Incorrectly classified videos.\"\n# Subset of labels to visualize. Only wrong predictions with true labels\n# within this subset is visualized.\n_C.TENSORBOARD.WRONG_PRED_VIS.SUBSET_PATH = \"\"",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.WRONG_PRED_VIS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.WRONG_PRED_VIS = CfgNode()\n_C.TENSORBOARD.WRONG_PRED_VIS.ENABLE = False\n# Folder tag to origanize model eval videos under.\n_C.TENSORBOARD.WRONG_PRED_VIS.TAG = \"Incorrectly classified videos.\"\n# Subset of labels to visualize. Only wrong predictions with true labels\n# within this subset is visualized.\n_C.TENSORBOARD.WRONG_PRED_VIS.SUBSET_PATH = \"\"\n# ---------------------------------------------------------------------------- #\n# Demo options\n# ---------------------------------------------------------------------------- #",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.WRONG_PRED_VIS.ENABLE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.WRONG_PRED_VIS.ENABLE = False\n# Folder tag to origanize model eval videos under.\n_C.TENSORBOARD.WRONG_PRED_VIS.TAG = \"Incorrectly classified videos.\"\n# Subset of labels to visualize. Only wrong predictions with true labels\n# within this subset is visualized.\n_C.TENSORBOARD.WRONG_PRED_VIS.SUBSET_PATH = \"\"\n# ---------------------------------------------------------------------------- #\n# Demo options\n# ---------------------------------------------------------------------------- #\n_C.DEMO = CfgNode()",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.WRONG_PRED_VIS.TAG",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.WRONG_PRED_VIS.TAG = \"Incorrectly classified videos.\"\n# Subset of labels to visualize. Only wrong predictions with true labels\n# within this subset is visualized.\n_C.TENSORBOARD.WRONG_PRED_VIS.SUBSET_PATH = \"\"\n# ---------------------------------------------------------------------------- #\n# Demo options\n# ---------------------------------------------------------------------------- #\n_C.DEMO = CfgNode()\n# Run model in DEMO mode.\n_C.DEMO.ENABLE = False",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.TENSORBOARD.WRONG_PRED_VIS.SUBSET_PATH",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.TENSORBOARD.WRONG_PRED_VIS.SUBSET_PATH = \"\"\n# ---------------------------------------------------------------------------- #\n# Demo options\n# ---------------------------------------------------------------------------- #\n_C.DEMO = CfgNode()\n# Run model in DEMO mode.\n_C.DEMO.ENABLE = False\n# Path to a json file providing class_name - id mapping\n# in the format {\"class_name1\": id1, \"class_name2\": id2, ...}.\n_C.DEMO.LABEL_FILE_PATH = \"\"",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO = CfgNode()\n# Run model in DEMO mode.\n_C.DEMO.ENABLE = False\n# Path to a json file providing class_name - id mapping\n# in the format {\"class_name1\": id1, \"class_name2\": id2, ...}.\n_C.DEMO.LABEL_FILE_PATH = \"\"\n# Specify a camera device as input. This will be prioritized\n# over input video if set.\n# If -1, use input video instead.\n_C.DEMO.WEBCAM = -1",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.ENABLE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.ENABLE = False\n# Path to a json file providing class_name - id mapping\n# in the format {\"class_name1\": id1, \"class_name2\": id2, ...}.\n_C.DEMO.LABEL_FILE_PATH = \"\"\n# Specify a camera device as input. This will be prioritized\n# over input video if set.\n# If -1, use input video instead.\n_C.DEMO.WEBCAM = -1\n# Path to input video for demo.\n_C.DEMO.INPUT_VIDEO = \"\"",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.LABEL_FILE_PATH",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.LABEL_FILE_PATH = \"\"\n# Specify a camera device as input. This will be prioritized\n# over input video if set.\n# If -1, use input video instead.\n_C.DEMO.WEBCAM = -1\n# Path to input video for demo.\n_C.DEMO.INPUT_VIDEO = \"\"\n# Custom width for reading input video data.\n_C.DEMO.DISPLAY_WIDTH = 0\n# Custom height for reading input video data.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.WEBCAM",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.WEBCAM = -1\n# Path to input video for demo.\n_C.DEMO.INPUT_VIDEO = \"\"\n# Custom width for reading input video data.\n_C.DEMO.DISPLAY_WIDTH = 0\n# Custom height for reading input video data.\n_C.DEMO.DISPLAY_HEIGHT = 0\n# Path to Detectron2 object detection model configuration,\n# only used for detection tasks.\n_C.DEMO.DETECTRON2_CFG = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.INPUT_VIDEO",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.INPUT_VIDEO = \"\"\n# Custom width for reading input video data.\n_C.DEMO.DISPLAY_WIDTH = 0\n# Custom height for reading input video data.\n_C.DEMO.DISPLAY_HEIGHT = 0\n# Path to Detectron2 object detection model configuration,\n# only used for detection tasks.\n_C.DEMO.DETECTRON2_CFG = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n# Path to Detectron2 object detection model pre-trained weights.\n_C.DEMO.DETECTRON2_WEIGHTS = \"detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\"",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.DISPLAY_WIDTH",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.DISPLAY_WIDTH = 0\n# Custom height for reading input video data.\n_C.DEMO.DISPLAY_HEIGHT = 0\n# Path to Detectron2 object detection model configuration,\n# only used for detection tasks.\n_C.DEMO.DETECTRON2_CFG = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n# Path to Detectron2 object detection model pre-trained weights.\n_C.DEMO.DETECTRON2_WEIGHTS = \"detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\"\n# Threshold for choosing predicted bounding boxes by Detectron2.\n_C.DEMO.DETECTRON2_THRESH = 0.9",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.DISPLAY_HEIGHT",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.DISPLAY_HEIGHT = 0\n# Path to Detectron2 object detection model configuration,\n# only used for detection tasks.\n_C.DEMO.DETECTRON2_CFG = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n# Path to Detectron2 object detection model pre-trained weights.\n_C.DEMO.DETECTRON2_WEIGHTS = \"detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\"\n# Threshold for choosing predicted bounding boxes by Detectron2.\n_C.DEMO.DETECTRON2_THRESH = 0.9\n# Number of overlapping frames between 2 consecutive clips.\n# Increase this number for more frequent action predictions.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.DETECTRON2_CFG",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.DETECTRON2_CFG = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n# Path to Detectron2 object detection model pre-trained weights.\n_C.DEMO.DETECTRON2_WEIGHTS = \"detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\"\n# Threshold for choosing predicted bounding boxes by Detectron2.\n_C.DEMO.DETECTRON2_THRESH = 0.9\n# Number of overlapping frames between 2 consecutive clips.\n# Increase this number for more frequent action predictions.\n# The number of overlapping frames cannot be larger than\n# half of the sequence length `cfg.DATA.NUM_FRAMES * cfg.DATA.SAMPLING_RATE`\n_C.DEMO.BUFFER_SIZE = 0",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.DETECTRON2_WEIGHTS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.DETECTRON2_WEIGHTS = \"detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\"\n# Threshold for choosing predicted bounding boxes by Detectron2.\n_C.DEMO.DETECTRON2_THRESH = 0.9\n# Number of overlapping frames between 2 consecutive clips.\n# Increase this number for more frequent action predictions.\n# The number of overlapping frames cannot be larger than\n# half of the sequence length `cfg.DATA.NUM_FRAMES * cfg.DATA.SAMPLING_RATE`\n_C.DEMO.BUFFER_SIZE = 0\n# If specified, the visualized outputs will be written this a video file of\n# this path. Otherwise, the visualized outputs will be displayed in a window.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.DETECTRON2_THRESH",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.DETECTRON2_THRESH = 0.9\n# Number of overlapping frames between 2 consecutive clips.\n# Increase this number for more frequent action predictions.\n# The number of overlapping frames cannot be larger than\n# half of the sequence length `cfg.DATA.NUM_FRAMES * cfg.DATA.SAMPLING_RATE`\n_C.DEMO.BUFFER_SIZE = 0\n# If specified, the visualized outputs will be written this a video file of\n# this path. Otherwise, the visualized outputs will be displayed in a window.\n_C.DEMO.OUTPUT_FILE = \"\"\n# Frames per second rate for writing to output video file.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.BUFFER_SIZE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.BUFFER_SIZE = 0\n# If specified, the visualized outputs will be written this a video file of\n# this path. Otherwise, the visualized outputs will be displayed in a window.\n_C.DEMO.OUTPUT_FILE = \"\"\n# Frames per second rate for writing to output video file.\n# If not set (-1), use fps rate from input file.\n_C.DEMO.OUTPUT_FPS = -1\n# Input format from demo video reader (\"RGB\" or \"BGR\").\n_C.DEMO.INPUT_FORMAT = \"BGR\"\n# Draw visualization frames in [keyframe_idx - CLIP_VIS_SIZE, keyframe_idx + CLIP_VIS_SIZE] inclusively.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.OUTPUT_FILE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.OUTPUT_FILE = \"\"\n# Frames per second rate for writing to output video file.\n# If not set (-1), use fps rate from input file.\n_C.DEMO.OUTPUT_FPS = -1\n# Input format from demo video reader (\"RGB\" or \"BGR\").\n_C.DEMO.INPUT_FORMAT = \"BGR\"\n# Draw visualization frames in [keyframe_idx - CLIP_VIS_SIZE, keyframe_idx + CLIP_VIS_SIZE] inclusively.\n_C.DEMO.CLIP_VIS_SIZE = 10\n# Number of processes to run video visualizer.\n_C.DEMO.NUM_VIS_INSTANCES = 2",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.OUTPUT_FPS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.OUTPUT_FPS = -1\n# Input format from demo video reader (\"RGB\" or \"BGR\").\n_C.DEMO.INPUT_FORMAT = \"BGR\"\n# Draw visualization frames in [keyframe_idx - CLIP_VIS_SIZE, keyframe_idx + CLIP_VIS_SIZE] inclusively.\n_C.DEMO.CLIP_VIS_SIZE = 10\n# Number of processes to run video visualizer.\n_C.DEMO.NUM_VIS_INSTANCES = 2\n# Path to pre-computed predicted boxes\n_C.DEMO.PREDS_BOXES = \"\"\n# Whether to run in with multi-threaded video reader.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.INPUT_FORMAT",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.INPUT_FORMAT = \"BGR\"\n# Draw visualization frames in [keyframe_idx - CLIP_VIS_SIZE, keyframe_idx + CLIP_VIS_SIZE] inclusively.\n_C.DEMO.CLIP_VIS_SIZE = 10\n# Number of processes to run video visualizer.\n_C.DEMO.NUM_VIS_INSTANCES = 2\n# Path to pre-computed predicted boxes\n_C.DEMO.PREDS_BOXES = \"\"\n# Whether to run in with multi-threaded video reader.\n_C.DEMO.THREAD_ENABLE = False\n# Take one clip for every `DEMO.NUM_CLIPS_SKIP` + 1 for prediction and visualization.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.CLIP_VIS_SIZE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.CLIP_VIS_SIZE = 10\n# Number of processes to run video visualizer.\n_C.DEMO.NUM_VIS_INSTANCES = 2\n# Path to pre-computed predicted boxes\n_C.DEMO.PREDS_BOXES = \"\"\n# Whether to run in with multi-threaded video reader.\n_C.DEMO.THREAD_ENABLE = False\n# Take one clip for every `DEMO.NUM_CLIPS_SKIP` + 1 for prediction and visualization.\n# This is used for fast demo speed by reducing the prediction/visualiztion frequency.\n# If -1, take the most recent read clip for visualization. This mode is only supported",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.NUM_VIS_INSTANCES",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.NUM_VIS_INSTANCES = 2\n# Path to pre-computed predicted boxes\n_C.DEMO.PREDS_BOXES = \"\"\n# Whether to run in with multi-threaded video reader.\n_C.DEMO.THREAD_ENABLE = False\n# Take one clip for every `DEMO.NUM_CLIPS_SKIP` + 1 for prediction and visualization.\n# This is used for fast demo speed by reducing the prediction/visualiztion frequency.\n# If -1, take the most recent read clip for visualization. This mode is only supported\n# if `DEMO.THREAD_ENABLE` is set to True.\n_C.DEMO.NUM_CLIPS_SKIP = 0",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.PREDS_BOXES",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.PREDS_BOXES = \"\"\n# Whether to run in with multi-threaded video reader.\n_C.DEMO.THREAD_ENABLE = False\n# Take one clip for every `DEMO.NUM_CLIPS_SKIP` + 1 for prediction and visualization.\n# This is used for fast demo speed by reducing the prediction/visualiztion frequency.\n# If -1, take the most recent read clip for visualization. This mode is only supported\n# if `DEMO.THREAD_ENABLE` is set to True.\n_C.DEMO.NUM_CLIPS_SKIP = 0\n# Path to ground-truth boxes and labels (optional)\n_C.DEMO.GT_BOXES = \"\"",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.THREAD_ENABLE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.THREAD_ENABLE = False\n# Take one clip for every `DEMO.NUM_CLIPS_SKIP` + 1 for prediction and visualization.\n# This is used for fast demo speed by reducing the prediction/visualiztion frequency.\n# If -1, take the most recent read clip for visualization. This mode is only supported\n# if `DEMO.THREAD_ENABLE` is set to True.\n_C.DEMO.NUM_CLIPS_SKIP = 0\n# Path to ground-truth boxes and labels (optional)\n_C.DEMO.GT_BOXES = \"\"\n# The starting second of the video w.r.t bounding boxes file.\n_C.DEMO.STARTING_SECOND = 900",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.NUM_CLIPS_SKIP",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.NUM_CLIPS_SKIP = 0\n# Path to ground-truth boxes and labels (optional)\n_C.DEMO.GT_BOXES = \"\"\n# The starting second of the video w.r.t bounding boxes file.\n_C.DEMO.STARTING_SECOND = 900\n# Frames per second of the input video/folder of images.\n_C.DEMO.FPS = 30\n# Visualize with top-k predictions or predictions above certain threshold(s).\n# Option: {\"thres\", \"top-k\"}\n_C.DEMO.VIS_MODE = \"thres\"",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.GT_BOXES",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.GT_BOXES = \"\"\n# The starting second of the video w.r.t bounding boxes file.\n_C.DEMO.STARTING_SECOND = 900\n# Frames per second of the input video/folder of images.\n_C.DEMO.FPS = 30\n# Visualize with top-k predictions or predictions above certain threshold(s).\n# Option: {\"thres\", \"top-k\"}\n_C.DEMO.VIS_MODE = \"thres\"\n# Threshold for common class names.\n_C.DEMO.COMMON_CLASS_THRES = 0.7",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.STARTING_SECOND",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.STARTING_SECOND = 900\n# Frames per second of the input video/folder of images.\n_C.DEMO.FPS = 30\n# Visualize with top-k predictions or predictions above certain threshold(s).\n# Option: {\"thres\", \"top-k\"}\n_C.DEMO.VIS_MODE = \"thres\"\n# Threshold for common class names.\n_C.DEMO.COMMON_CLASS_THRES = 0.7\n# Theshold for uncommon class names. This will not be\n# used if `_C.DEMO.COMMON_CLASS_NAMES` is empty.",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.FPS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.FPS = 30\n# Visualize with top-k predictions or predictions above certain threshold(s).\n# Option: {\"thres\", \"top-k\"}\n_C.DEMO.VIS_MODE = \"thres\"\n# Threshold for common class names.\n_C.DEMO.COMMON_CLASS_THRES = 0.7\n# Theshold for uncommon class names. This will not be\n# used if `_C.DEMO.COMMON_CLASS_NAMES` is empty.\n_C.DEMO.UNCOMMON_CLASS_THRES = 0.3\n# This is chosen based on distribution of examples in",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.VIS_MODE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.VIS_MODE = \"thres\"\n# Threshold for common class names.\n_C.DEMO.COMMON_CLASS_THRES = 0.7\n# Theshold for uncommon class names. This will not be\n# used if `_C.DEMO.COMMON_CLASS_NAMES` is empty.\n_C.DEMO.UNCOMMON_CLASS_THRES = 0.3\n# This is chosen based on distribution of examples in\n# each classes in AVA dataset.\n_C.DEMO.COMMON_CLASS_NAMES = [\n    \"watch (a person)\",",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.COMMON_CLASS_THRES",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.COMMON_CLASS_THRES = 0.7\n# Theshold for uncommon class names. This will not be\n# used if `_C.DEMO.COMMON_CLASS_NAMES` is empty.\n_C.DEMO.UNCOMMON_CLASS_THRES = 0.3\n# This is chosen based on distribution of examples in\n# each classes in AVA dataset.\n_C.DEMO.COMMON_CLASS_NAMES = [\n    \"watch (a person)\",\n    \"talk to (e.g., self, a person, a group)\",\n    \"listen to (a person)\",",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.UNCOMMON_CLASS_THRES",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.UNCOMMON_CLASS_THRES = 0.3\n# This is chosen based on distribution of examples in\n# each classes in AVA dataset.\n_C.DEMO.COMMON_CLASS_NAMES = [\n    \"watch (a person)\",\n    \"talk to (e.g., self, a person, a group)\",\n    \"listen to (a person)\",\n    \"touch (an object)\",\n    \"carry/hold (an object)\",\n    \"walk\",",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.COMMON_CLASS_NAMES",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.COMMON_CLASS_NAMES = [\n    \"watch (a person)\",\n    \"talk to (e.g., self, a person, a group)\",\n    \"listen to (a person)\",\n    \"touch (an object)\",\n    \"carry/hold (an object)\",\n    \"walk\",\n    \"sit\",\n    \"lie/sleep\",\n    \"bend/bow (at the waist)\",",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "_C.DEMO.SLOWMO",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.config.defaults",
        "description": "MorphMLP.slowfast.config.defaults",
        "peekOfCode": "_C.DEMO.SLOWMO = 1\n# Add custom config with default values.\ncustom_config.add_custom_config(_C)\ndef assert_and_infer_cfg(cfg):\n    # BN assertions.\n    if cfg.BN.USE_PRECISE_STATS:\n        assert cfg.BN.NUM_BATCHES_PRECISE >= 0\n    # TRAIN assertions.\n    assert cfg.TRAIN.CHECKPOINT_TYPE in [\"pytorch\", \"caffe2\"]\n    assert cfg.NUM_GPUS == 0 or cfg.TRAIN.BATCH_SIZE % cfg.NUM_GPUS == 0",
        "detail": "MorphMLP.slowfast.config.defaults",
        "documentation": {}
    },
    {
        "label": "Ava",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.datasets.ava_dataset",
        "description": "MorphMLP.slowfast.datasets.ava_dataset",
        "peekOfCode": "class Ava(torch.utils.data.Dataset):\n    \"\"\"\n    AVA Dataset\n    \"\"\"\n    def __init__(self, cfg, split):\n        self.cfg = cfg\n        self._split = split\n        self._sample_rate = cfg.DATA.SAMPLING_RATE\n        self._video_length = cfg.DATA.NUM_FRAMES\n        self._seq_len = self._video_length * self._sample_rate",
        "detail": "MorphMLP.slowfast.datasets.ava_dataset",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.ava_dataset",
        "description": "MorphMLP.slowfast.datasets.ava_dataset",
        "peekOfCode": "logger = logging.getLogger(__name__)\n@DATASET_REGISTRY.register()\nclass Ava(torch.utils.data.Dataset):\n    \"\"\"\n    AVA Dataset\n    \"\"\"\n    def __init__(self, cfg, split):\n        self.cfg = cfg\n        self._split = split\n        self._sample_rate = cfg.DATA.SAMPLING_RATE",
        "detail": "MorphMLP.slowfast.datasets.ava_dataset",
        "documentation": {}
    },
    {
        "label": "load_image_lists",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.ava_helper",
        "description": "MorphMLP.slowfast.datasets.ava_helper",
        "peekOfCode": "def load_image_lists(cfg, is_train):\n    \"\"\"\n    Loading image paths from corresponding files.\n    Args:\n        cfg (CfgNode): config.\n        is_train (bool): if it is training dataset or not.\n    Returns:\n        image_paths (list[list]): a list of items. Each item (also a list)\n            corresponds to one video and contains the paths of images for\n            this video.",
        "detail": "MorphMLP.slowfast.datasets.ava_helper",
        "documentation": {}
    },
    {
        "label": "load_boxes_and_labels",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.ava_helper",
        "description": "MorphMLP.slowfast.datasets.ava_helper",
        "peekOfCode": "def load_boxes_and_labels(cfg, mode):\n    \"\"\"\n    Loading boxes and labels from csv files.\n    Args:\n        cfg (CfgNode): config.\n        mode (str): 'train', 'val', or 'test' mode.\n    Returns:\n        all_boxes (dict): a dict which maps from `video_name` and\n            `frame_sec` to a list of `box`. Each `box` is a\n            [`box_coord`, `box_labels`] where `box_coord` is the",
        "detail": "MorphMLP.slowfast.datasets.ava_helper",
        "documentation": {}
    },
    {
        "label": "get_keyframe_data",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.ava_helper",
        "description": "MorphMLP.slowfast.datasets.ava_helper",
        "peekOfCode": "def get_keyframe_data(boxes_and_labels):\n    \"\"\"\n    Getting keyframe indices, boxes and labels in the dataset.\n    Args:\n        boxes_and_labels (list[dict]): a list which maps from video_idx to a dict.\n            Each dict `frame_sec` to a list of boxes and corresponding labels.\n    Returns:\n        keyframe_indices (list): a list of indices of the keyframes.\n        keyframe_boxes_and_labels (list[list[list]]): a list of list which maps from\n            video_idx and sec_idx to a list of boxes and corresponding labels.",
        "detail": "MorphMLP.slowfast.datasets.ava_helper",
        "documentation": {}
    },
    {
        "label": "get_num_boxes_used",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.ava_helper",
        "description": "MorphMLP.slowfast.datasets.ava_helper",
        "peekOfCode": "def get_num_boxes_used(keyframe_indices, keyframe_boxes_and_labels):\n    \"\"\"\n    Get total number of used boxes.\n    Args:\n        keyframe_indices (list): a list of indices of the keyframes.\n        keyframe_boxes_and_labels (list[list[list]]): a list of list which maps from\n            video_idx and sec_idx to a list of boxes and corresponding labels.\n    Returns:\n        count (int): total number of used boxes.\n    \"\"\"",
        "detail": "MorphMLP.slowfast.datasets.ava_helper",
        "documentation": {}
    },
    {
        "label": "parse_bboxes_file",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.ava_helper",
        "description": "MorphMLP.slowfast.datasets.ava_helper",
        "peekOfCode": "def parse_bboxes_file(\n    ann_filenames, ann_is_gt_box, detect_thresh, boxes_sample_rate=1\n):\n    \"\"\"\n    Parse AVA bounding boxes files.\n    Args:\n        ann_filenames (list of str(s)): a list of AVA bounding boxes annotation files.\n        ann_is_gt_box (list of bools): a list of boolean to indicate whether the corresponding\n            ann_file is ground-truth. `ann_is_gt_box[i]` correspond to `ann_filenames[i]`.\n        detect_thresh (float): threshold for accepting predicted boxes, range [0, 1].",
        "detail": "MorphMLP.slowfast.datasets.ava_helper",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.ava_helper",
        "description": "MorphMLP.slowfast.datasets.ava_helper",
        "peekOfCode": "logger = logging.getLogger(__name__)\nFPS = 30\nAVA_VALID_FRAMES = range(902, 1799)\ndef load_image_lists(cfg, is_train):\n    \"\"\"\n    Loading image paths from corresponding files.\n    Args:\n        cfg (CfgNode): config.\n        is_train (bool): if it is training dataset or not.\n    Returns:",
        "detail": "MorphMLP.slowfast.datasets.ava_helper",
        "documentation": {}
    },
    {
        "label": "FPS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.ava_helper",
        "description": "MorphMLP.slowfast.datasets.ava_helper",
        "peekOfCode": "FPS = 30\nAVA_VALID_FRAMES = range(902, 1799)\ndef load_image_lists(cfg, is_train):\n    \"\"\"\n    Loading image paths from corresponding files.\n    Args:\n        cfg (CfgNode): config.\n        is_train (bool): if it is training dataset or not.\n    Returns:\n        image_paths (list[list]): a list of items. Each item (also a list)",
        "detail": "MorphMLP.slowfast.datasets.ava_helper",
        "documentation": {}
    },
    {
        "label": "AVA_VALID_FRAMES",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.ava_helper",
        "description": "MorphMLP.slowfast.datasets.ava_helper",
        "peekOfCode": "AVA_VALID_FRAMES = range(902, 1799)\ndef load_image_lists(cfg, is_train):\n    \"\"\"\n    Loading image paths from corresponding files.\n    Args:\n        cfg (CfgNode): config.\n        is_train (bool): if it is training dataset or not.\n    Returns:\n        image_paths (list[list]): a list of items. Each item (also a list)\n            corresponds to one video and contains the paths of images for",
        "detail": "MorphMLP.slowfast.datasets.ava_helper",
        "documentation": {}
    },
    {
        "label": "build_dataset",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.build",
        "description": "MorphMLP.slowfast.datasets.build",
        "peekOfCode": "def build_dataset(dataset_name, cfg, split):\n    \"\"\"\n    Build a dataset, defined by `dataset_name`.\n    Args:\n        dataset_name (str): the name of the dataset to be constructed.\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        split (str): the split of the data loader. Options include `train`,\n            `val`, and `test`.\n    Returns:",
        "detail": "MorphMLP.slowfast.datasets.build",
        "documentation": {}
    },
    {
        "label": "DATASET_REGISTRY",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.build",
        "description": "MorphMLP.slowfast.datasets.build",
        "peekOfCode": "DATASET_REGISTRY = Registry(\"DATASET\")\nDATASET_REGISTRY.__doc__ = \"\"\"\nRegistry for dataset.\nThe registered object will be called with `obj(cfg, split)`.\nThe call should return a `torch.utils.data.Dataset` object.\n\"\"\"\ndef build_dataset(dataset_name, cfg, split):\n    \"\"\"\n    Build a dataset, defined by `dataset_name`.\n    Args:",
        "detail": "MorphMLP.slowfast.datasets.build",
        "documentation": {}
    },
    {
        "label": "DATASET_REGISTRY.__doc__",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.build",
        "description": "MorphMLP.slowfast.datasets.build",
        "peekOfCode": "DATASET_REGISTRY.__doc__ = \"\"\"\nRegistry for dataset.\nThe registered object will be called with `obj(cfg, split)`.\nThe call should return a `torch.utils.data.Dataset` object.\n\"\"\"\ndef build_dataset(dataset_name, cfg, split):\n    \"\"\"\n    Build a dataset, defined by `dataset_name`.\n    Args:\n        dataset_name (str): the name of the dataset to be constructed.",
        "detail": "MorphMLP.slowfast.datasets.build",
        "documentation": {}
    },
    {
        "label": "Charades",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.datasets.charades",
        "description": "MorphMLP.slowfast.datasets.charades",
        "peekOfCode": "class Charades(torch.utils.data.Dataset):\n    \"\"\"\n    Charades video loader. Construct the Charades video loader, then sample\n    clips from the videos. For training and validation, a single clip is randomly\n    sampled from every video with random cropping, scaling, and flipping. For\n    testing, multiple clips are uniformaly sampled from every video with uniform\n    cropping. For uniform cropping, we take the left, center, and right crop if\n    the width is larger than height, or take top, center, and bottom crop if the\n    height is larger than the width.\n    \"\"\"",
        "detail": "MorphMLP.slowfast.datasets.charades",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.charades",
        "description": "MorphMLP.slowfast.datasets.charades",
        "peekOfCode": "logger = logging.get_logger(__name__)\n@DATASET_REGISTRY.register()\nclass Charades(torch.utils.data.Dataset):\n    \"\"\"\n    Charades video loader. Construct the Charades video loader, then sample\n    clips from the videos. For training and validation, a single clip is randomly\n    sampled from every video with random cropping, scaling, and flipping. For\n    testing, multiple clips are uniformaly sampled from every video with uniform\n    cropping. For uniform cropping, we take the left, center, and right crop if\n    the width is larger than height, or take top, center, and bottom crop if the",
        "detail": "MorphMLP.slowfast.datasets.charades",
        "documentation": {}
    },
    {
        "label": "clip_boxes_to_image",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def clip_boxes_to_image(boxes, height, width):\n    \"\"\"\n    Clip the boxes with the height and width of the image size.\n    Args:\n        boxes (ndarray): bounding boxes to peform crop. The dimension is\n        `num boxes` x 4.\n        height (int): the height of the image.\n        width (int): the width of the image.\n    Returns:\n        boxes (ndarray): cropped bounding boxes.",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "random_short_side_scale_jitter_list",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def random_short_side_scale_jitter_list(images, min_size, max_size, boxes=None):\n    \"\"\"\n    Perform a spatial short scale jittering on the given images and\n    corresponding boxes.\n    Args:\n        images (list): list of images to perform scale jitter. Dimension is\n            `height` x `width` x `channel`.\n        min_size (int): the minimal size to scale the frames.\n        max_size (int): the maximal size to scale the frames.\n        boxes (list): optional. Corresponding boxes to images. Dimension is",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "scale",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def scale(size, image):\n    \"\"\"\n    Scale the short side of the image to size.\n    Args:\n        size (int): size to scale the image.\n        image (array): image to perform short side scale. Dimension is\n            `height` x `width` x `channel`.\n    Returns:\n        (ndarray): the scaled image with dimension of\n            `height` x `width` x `channel`.",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "scale_boxes",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def scale_boxes(size, boxes, height, width):\n    \"\"\"\n    Scale the short side of the box to size.\n    Args:\n        size (int): size to scale the image.\n        boxes (ndarray): bounding boxes to peform scale. The dimension is\n        `num boxes` x 4.\n        height (int): the height of the image.\n        width (int): the width of the image.\n    Returns:",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "horizontal_flip_list",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def horizontal_flip_list(prob, images, order=\"CHW\", boxes=None):\n    \"\"\"\n    Horizontally flip the list of image and optional boxes.\n    Args:\n        prob (float): probability to flip.\n        image (list): ilist of images to perform short side scale. Dimension is\n            `height` x `width` x `channel` or `channel` x `height` x `width`.\n        order (str): order of the `height`, `channel` and `width`.\n        boxes (list): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "spatial_shift_crop_list",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def spatial_shift_crop_list(size, images, spatial_shift_pos, boxes=None):\n    \"\"\"\n    Perform left, center, or right crop of the given list of images.\n    Args:\n        size (int): size to crop.\n        image (list): ilist of images to perform short side scale. Dimension is\n            `height` x `width` x `channel` or `channel` x `height` x `width`.\n        spatial_shift_pos (int): option includes 0 (left), 1 (middle), and\n            2 (right) crop.\n        boxes (list): optional. Corresponding boxes to images.",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "CHW2HWC",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def CHW2HWC(image):\n    \"\"\"\n    Transpose the dimension from `channel` x `height` x `width` to\n        `height` x `width` x `channel`.\n    Args:\n        image (array): image to transpose.\n    Returns\n        (array): transposed image.\n    \"\"\"\n    return image.transpose([1, 2, 0])",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "HWC2CHW",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def HWC2CHW(image):\n    \"\"\"\n    Transpose the dimension from `height` x `width` x `channel` to\n        `channel` x `height` x `width`.\n    Args:\n        image (array): image to transpose.\n    Returns\n        (array): transposed image.\n    \"\"\"\n    return image.transpose([2, 0, 1])",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "color_jitter_list",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def color_jitter_list(\n    images, img_brightness=0, img_contrast=0, img_saturation=0\n):\n    \"\"\"\n    Perform color jitter on the list of images.\n    Args:\n        images (list): list of images to perform color jitter.\n        img_brightness (float): jitter ratio for brightness.\n        img_contrast (float): jitter ratio for contrast.\n        img_saturation (float): jitter ratio for saturation.",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "lighting_list",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def lighting_list(imgs, alphastd, eigval, eigvec, alpha=None):\n    \"\"\"\n    Perform AlexNet-style PCA jitter on the given list of images.\n    Args:\n        images (list): list of images to perform lighting jitter.\n        alphastd (float): jitter ratio for PCA jitter.\n        eigval (list): eigenvalues for PCA jitter.\n        eigvec (list[list]): eigenvectors for PCA jitter.\n    Returns:\n        out_images (list): the list of jittered images.",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "color_normalization",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def color_normalization(image, mean, stddev):\n    \"\"\"\n    Perform color normalization on the image with the given mean and stddev.\n    Args:\n        image (array): image to perform color normalization.\n        mean (float): mean value to subtract.\n        stddev (float): stddev to devide.\n    \"\"\"\n    # Input image should in format of CHW\n    assert len(mean) == image.shape[0], \"channel mean not computed properly\"",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "pad_image",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def pad_image(image, pad_size, order=\"CHW\"):\n    \"\"\"\n    Pad the given image with the size of pad_size.\n    Args:\n        image (array): image to pad.\n        pad_size (int): size to pad.\n        order (str): order of the `height`, `channel` and `width`.\n    Returns:\n        img (array): padded image.\n    \"\"\"",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "horizontal_flip",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def horizontal_flip(prob, image, order=\"CHW\"):\n    \"\"\"\n    Horizontally flip the image.\n    Args:\n        prob (float): probability to flip.\n        image (array): image to pad.\n        order (str): order of the `height`, `channel` and `width`.\n    Returns:\n        img (array): flipped image.\n    \"\"\"",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "flip_boxes",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def flip_boxes(boxes, im_width):\n    \"\"\"\n    Horizontally flip the boxes.\n    Args:\n        boxes (array): box to flip.\n        im_width (int): width of the image.\n    Returns:\n        boxes_flipped (array): flipped box.\n    \"\"\"\n    boxes_flipped = boxes.copy()",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "crop_boxes",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def crop_boxes(boxes, x_offset, y_offset):\n    \"\"\"\n    Crop the boxes given the offsets.\n    Args:\n        boxes (array): boxes to crop.\n        x_offset (int): offset on x.\n        y_offset (int): offset on y.\n    \"\"\"\n    boxes[:, [0, 2]] = boxes[:, [0, 2]] - x_offset\n    boxes[:, [1, 3]] = boxes[:, [1, 3]] - y_offset",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "random_crop_list",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def random_crop_list(images, size, pad_size=0, order=\"CHW\", boxes=None):\n    \"\"\"\n    Perform random crop on a list of images.\n    Args:\n        images (list): list of images to perform random crop.\n        size (int): size to crop.\n        pad_size (int): padding size.\n        order (str): order of the `height`, `channel` and `width`.\n        boxes (list): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "center_crop",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def center_crop(size, image):\n    \"\"\"\n    Perform center crop on input images.\n    Args:\n        size (int): size of the cropped height and width.\n        image (array): the image to perform center crop.\n    \"\"\"\n    height = image.shape[0]\n    width = image.shape[1]\n    y_offset = int(math.ceil((height - size) / 2))",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "random_scale_jitter",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def random_scale_jitter(image, min_size, max_size):\n    \"\"\"\n    Perform ResNet style random scale jittering: randomly select the scale from\n        [1/max_size, 1/min_size].\n    Args:\n        image (array): image to perform random scale.\n        min_size (int): min size to scale.\n        max_size (int) max size to scale.\n    Returns:\n        image (array): scaled image.",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "random_scale_jitter_list",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def random_scale_jitter_list(images, min_size, max_size):\n    \"\"\"\n    Perform ResNet style random scale jittering on a list of image: randomly\n        select the scale from [1/max_size, 1/min_size]. Note that all the image\n        will share the same scale.\n    Args:\n        images (list): list of images to perform random scale.\n        min_size (int): min size to scale.\n        max_size (int) max size to scale.\n    Returns:",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "random_sized_crop",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def random_sized_crop(image, size, area_frac=0.08):\n    \"\"\"\n    Perform random sized cropping on the given image. Random crop with size\n        8% - 100% image area and aspect ratio in [3/4, 4/3].\n    Args:\n        image (array): image to crop.\n        size (int): size to crop.\n        area_frac (float): area of fraction.\n    Returns:\n        (array): cropped image.",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "lighting",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def lighting(img, alphastd, eigval, eigvec):\n    \"\"\"\n    Perform AlexNet-style PCA jitter on the given image.\n    Args:\n        image (array): list of images to perform lighting jitter.\n        alphastd (float): jitter ratio for PCA jitter.\n        eigval (array): eigenvalues for PCA jitter.\n        eigvec (list): eigenvectors for PCA jitter.\n    Returns:\n        img (tensor): the jittered image.",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "random_sized_crop_list",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def random_sized_crop_list(images, size, crop_area_fraction=0.08):\n    \"\"\"\n    Perform random sized cropping on the given list of images. Random crop with\n        size 8% - 100% image area and aspect ratio in [3/4, 4/3].\n    Args:\n        images (list): image to crop.\n        size (int): size to crop.\n        area_frac (float): area of fraction.\n    Returns:\n        (list): list of cropped image.",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "blend",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def blend(image1, image2, alpha):\n    return image1 * alpha + image2 * (1 - alpha)\ndef grayscale(image):\n    \"\"\"\n    Convert the image to gray scale.\n    Args:\n        image (tensor): image to convert to gray scale. Dimension is\n            `channel` x `height` x `width`.\n    Returns:\n        img_gray (tensor): image in gray scale.",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "grayscale",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def grayscale(image):\n    \"\"\"\n    Convert the image to gray scale.\n    Args:\n        image (tensor): image to convert to gray scale. Dimension is\n            `channel` x `height` x `width`.\n    Returns:\n        img_gray (tensor): image in gray scale.\n    \"\"\"\n    # R -> 0.299, G -> 0.587, B -> 0.114.",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "saturation",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def saturation(var, image):\n    \"\"\"\n    Perform color saturation on the given image.\n    Args:\n        var (float): variance.\n        image (array): image to perform color saturation.\n    Returns:\n        (array): image that performed color saturation.\n    \"\"\"\n    img_gray = grayscale(image)",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "brightness",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def brightness(var, image):\n    \"\"\"\n    Perform color brightness on the given image.\n    Args:\n        var (float): variance.\n        image (array): image to perform color brightness.\n    Returns:\n        (array): image that performed color brightness.\n    \"\"\"\n    img_bright = np.zeros(image.shape).astype(image.dtype)",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "contrast",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def contrast(var, image):\n    \"\"\"\n    Perform color contrast on the given image.\n    Args:\n        var (float): variance.\n        image (array): image to perform color contrast.\n    Returns:\n        (array): image that performed color contrast.\n    \"\"\"\n    img_gray = grayscale(image)",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "saturation_list",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def saturation_list(var, images):\n    \"\"\"\n    Perform color saturation on the list of given images.\n    Args:\n        var (float): variance.\n        images (list): list of images to perform color saturation.\n    Returns:\n        (list): list of images that performed color saturation.\n    \"\"\"\n    alpha = 1.0 + np.random.uniform(-var, var)",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "brightness_list",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def brightness_list(var, images):\n    \"\"\"\n    Perform color brightness on the given list of images.\n    Args:\n        var (float): variance.\n        images (list): list of images to perform color brightness.\n    Returns:\n        (array): list of images that performed color brightness.\n    \"\"\"\n    alpha = 1.0 + np.random.uniform(-var, var)",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "contrast_list",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def contrast_list(var, images):\n    \"\"\"\n    Perform color contrast on the given list of images.\n    Args:\n        var (float): variance.\n        images (list): list of images to perform color contrast.\n    Returns:\n        (array): image that performed color contrast.\n    \"\"\"\n    alpha = 1.0 + np.random.uniform(-var, var)",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "color_jitter",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def color_jitter(image, img_brightness=0, img_contrast=0, img_saturation=0):\n    \"\"\"\n    Perform color jitter on the given image.\n    Args:\n        image (array): image to perform color jitter.\n        img_brightness (float): jitter ratio for brightness.\n        img_contrast (float): jitter ratio for contrast.\n        img_saturation (float): jitter ratio for saturation.\n    Returns:\n        image (array): the jittered image.",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "revert_scaled_boxes",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.cv2_transform",
        "description": "MorphMLP.slowfast.datasets.cv2_transform",
        "peekOfCode": "def revert_scaled_boxes(size, boxes, img_height, img_width):\n    \"\"\"\n    Revert scaled input boxes to match the original image size.\n    Args:\n        size (int): size of the cropped image.\n        boxes (array): shape (num_boxes, 4).\n        img_height (int): height of original image.\n        img_width (int): width of original image.\n    Returns:\n        reverted_boxes (array): boxes scaled back to the original image size.",
        "detail": "MorphMLP.slowfast.datasets.cv2_transform",
        "documentation": {}
    },
    {
        "label": "temporal_sampling",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.decoder",
        "description": "MorphMLP.slowfast.datasets.decoder",
        "peekOfCode": "def temporal_sampling(frames, start_idx, end_idx, num_samples):\n    \"\"\"\n    Given the start and end frame index, sample num_samples frames between\n    the start and end with equal interval.\n    Args:\n        frames (tensor): a tensor of video frames, dimension is\n            `num video frames` x `channel` x `height` x `width`.\n        start_idx (int): the index of the start frame.\n        end_idx (int): the index of the end frame.\n        num_samples (int): number of frames to sample.",
        "detail": "MorphMLP.slowfast.datasets.decoder",
        "documentation": {}
    },
    {
        "label": "get_start_end_idx",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.decoder",
        "description": "MorphMLP.slowfast.datasets.decoder",
        "peekOfCode": "def get_start_end_idx(\n    video_size, clip_size, clip_idx, num_clips, use_offset=False\n):\n    \"\"\"\n    Sample a clip of size clip_size from a video of size video_size and\n    return the indices of the first and last frame of the clip. If clip_idx is\n    -1, the clip is randomly sampled, otherwise uniformly split the video to\n    num_clips clips, and select the start and end index of clip_idx-th video\n    clip.\n    Args:",
        "detail": "MorphMLP.slowfast.datasets.decoder",
        "documentation": {}
    },
    {
        "label": "pyav_decode_stream",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.decoder",
        "description": "MorphMLP.slowfast.datasets.decoder",
        "peekOfCode": "def pyav_decode_stream(\n    container, start_pts, end_pts, stream, stream_name, buffer_size=0\n):\n    \"\"\"\n    Decode the video with PyAV decoder.\n    Args:\n        container (container): PyAV container.\n        start_pts (int): the starting Presentation TimeStamp to fetch the\n            video frames.\n        end_pts (int): the ending Presentation TimeStamp of the decoded frames.",
        "detail": "MorphMLP.slowfast.datasets.decoder",
        "documentation": {}
    },
    {
        "label": "torchvision_decode",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.decoder",
        "description": "MorphMLP.slowfast.datasets.decoder",
        "peekOfCode": "def torchvision_decode(\n    video_handle,\n    sampling_rate,\n    num_frames,\n    clip_idx,\n    video_meta,\n    num_clips=10,\n    target_fps=30,\n    modalities=(\"visual\",),\n    max_spatial_scale=0,",
        "detail": "MorphMLP.slowfast.datasets.decoder",
        "documentation": {}
    },
    {
        "label": "pyav_decode",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.decoder",
        "description": "MorphMLP.slowfast.datasets.decoder",
        "peekOfCode": "def pyav_decode(\n    container,\n    sampling_rate,\n    num_frames,\n    clip_idx,\n    num_clips=10,\n    target_fps=30,\n    use_offset=False,\n):\n    \"\"\"",
        "detail": "MorphMLP.slowfast.datasets.decoder",
        "documentation": {}
    },
    {
        "label": "decode",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.decoder",
        "description": "MorphMLP.slowfast.datasets.decoder",
        "peekOfCode": "def decode(\n    container,\n    sampling_rate,\n    num_frames,\n    clip_idx=-1,\n    num_clips=10,\n    video_meta=None,\n    target_fps=30,\n    backend=\"pyav\",\n    max_spatial_scale=0,",
        "detail": "MorphMLP.slowfast.datasets.decoder",
        "documentation": {}
    },
    {
        "label": "Imagenet",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.datasets.imagenet",
        "description": "MorphMLP.slowfast.datasets.imagenet",
        "peekOfCode": "class Imagenet(torch.utils.data.Dataset):\n    \"\"\"ImageNet dataset.\"\"\"\n    def __init__(self, cfg, mode, num_retries=10):\n        self.num_retries = num_retries\n        self.cfg = cfg\n        self.mode = mode\n        self.data_path = cfg.DATA.PATH_TO_DATA_DIR\n        assert mode in [\n            \"train\",\n            \"val\",",
        "detail": "MorphMLP.slowfast.datasets.imagenet",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.imagenet",
        "description": "MorphMLP.slowfast.datasets.imagenet",
        "peekOfCode": "logger = logging.get_logger(__name__)\n@DATASET_REGISTRY.register()\nclass Imagenet(torch.utils.data.Dataset):\n    \"\"\"ImageNet dataset.\"\"\"\n    def __init__(self, cfg, mode, num_retries=10):\n        self.num_retries = num_retries\n        self.cfg = cfg\n        self.mode = mode\n        self.data_path = cfg.DATA.PATH_TO_DATA_DIR\n        assert mode in [",
        "detail": "MorphMLP.slowfast.datasets.imagenet",
        "documentation": {}
    },
    {
        "label": "Kinetics",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.datasets.kinetics",
        "description": "MorphMLP.slowfast.datasets.kinetics",
        "peekOfCode": "class Kinetics(torch.utils.data.Dataset):\n    \"\"\"\n    Kinetics video loader. Construct the Kinetics video loader, then sample\n    clips from the videos. For training and validation, a single clip is\n    randomly sampled from every video with random cropping, scaling, and\n    flipping. For testing, multiple clips are uniformaly sampled from every\n    video with uniform cropping. For uniform cropping, we take the left, center,\n    and right crop if the width is larger than height, or take top, center, and\n    bottom crop if the height is larger than the width.\n    \"\"\"",
        "detail": "MorphMLP.slowfast.datasets.kinetics",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.kinetics",
        "description": "MorphMLP.slowfast.datasets.kinetics",
        "peekOfCode": "logger = logging.get_logger(__name__)\n@DATASET_REGISTRY.register()\nclass Kinetics(torch.utils.data.Dataset):\n    \"\"\"\n    Kinetics video loader. Construct the Kinetics video loader, then sample\n    clips from the videos. For training and validation, a single clip is\n    randomly sampled from every video with random cropping, scaling, and\n    flipping. For testing, multiple clips are uniformaly sampled from every\n    video with uniform cropping. For uniform cropping, we take the left, center,\n    and right crop if the width is larger than height, or take top, center, and",
        "detail": "MorphMLP.slowfast.datasets.kinetics",
        "documentation": {}
    },
    {
        "label": "multiple_samples_collate",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.loader",
        "description": "MorphMLP.slowfast.datasets.loader",
        "peekOfCode": "def multiple_samples_collate(batch, fold=False):\n    \"\"\"\n    Collate function for repeated augmentation. Each instance in the batch has\n    more than one sample.\n    Args:\n        batch (tuple or list): data batch to collate.\n    Returns:\n        (tuple): collated data batch.\n    \"\"\"\n    inputs, labels, video_idx, extra_data = zip(*batch)",
        "detail": "MorphMLP.slowfast.datasets.loader",
        "documentation": {}
    },
    {
        "label": "detection_collate",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.loader",
        "description": "MorphMLP.slowfast.datasets.loader",
        "peekOfCode": "def detection_collate(batch):\n    \"\"\"\n    Collate function for detection task. Concatanate bboxes, labels and\n    metadata from different samples in the first dimension instead of\n    stacking them to have a batch-size dimension.\n    Args:\n        batch (tuple or list): data batch to collate.\n    Returns:\n        (tuple): collated detection data batch.\n    \"\"\"",
        "detail": "MorphMLP.slowfast.datasets.loader",
        "documentation": {}
    },
    {
        "label": "construct_loader",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.loader",
        "description": "MorphMLP.slowfast.datasets.loader",
        "peekOfCode": "def construct_loader(cfg, split, is_precise_bn=False):\n    \"\"\"\n    Constructs the data loader for the given dataset.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        split (str): the split of the data loader. Options include `train`,\n            `val`, and `test`.\n    \"\"\"\n    assert split in [\"train\", \"val\", \"test\"]",
        "detail": "MorphMLP.slowfast.datasets.loader",
        "documentation": {}
    },
    {
        "label": "shuffle_dataset",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.loader",
        "description": "MorphMLP.slowfast.datasets.loader",
        "peekOfCode": "def shuffle_dataset(loader, cur_epoch):\n    \"\"\" \"\n    Shuffles the data.\n    Args:\n        loader (loader): data loader to perform shuffle.\n        cur_epoch (int): number of the current epoch.\n    \"\"\"\n    if (\n        loader._dataset_kind\n        == torch.utils.data.dataloader._DatasetKind.Iterable",
        "detail": "MorphMLP.slowfast.datasets.loader",
        "documentation": {}
    },
    {
        "label": "MixUp",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.datasets.mixup",
        "description": "MorphMLP.slowfast.datasets.mixup",
        "peekOfCode": "class MixUp:\n    \"\"\"\n    Apply mixup and/or cutmix for videos at batch level.\n    mixup: Beyond Empirical Risk Minimization (https://arxiv.org/abs/1710.09412)\n    CutMix: Regularization Strategy to Train Strong Classifiers with Localizable\n        Features (https://arxiv.org/abs/1905.04899)\n    \"\"\"\n    def __init__(\n        self,\n        mixup_alpha=1.0,",
        "detail": "MorphMLP.slowfast.datasets.mixup",
        "documentation": {}
    },
    {
        "label": "convert_to_one_hot",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.mixup",
        "description": "MorphMLP.slowfast.datasets.mixup",
        "peekOfCode": "def convert_to_one_hot(targets, num_classes, on_value=1.0, off_value=0.0):\n    \"\"\"\n    This function converts target class indices to one-hot vectors, given the\n    number of classes.\n    Args:\n        targets (loader): Class labels.\n        num_classes (int): Total number of classes.\n        on_value (float): Target Value for ground truth class.\n        off_value (float): Target Value for other classes.This value is used for\n            label smoothing.",
        "detail": "MorphMLP.slowfast.datasets.mixup",
        "documentation": {}
    },
    {
        "label": "mixup_target",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.mixup",
        "description": "MorphMLP.slowfast.datasets.mixup",
        "peekOfCode": "def mixup_target(target, num_classes, lam=1.0, smoothing=0.0):\n    \"\"\"\n    This function converts target class indices to one-hot vectors, given the\n    number of classes.\n    Args:\n        targets (loader): Class labels.\n        num_classes (int): Total number of classes.\n        lam (float): lamba value for mixup/cutmix.\n        smoothing (float): Label smoothing value.\n    \"\"\"",
        "detail": "MorphMLP.slowfast.datasets.mixup",
        "documentation": {}
    },
    {
        "label": "rand_bbox",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.mixup",
        "description": "MorphMLP.slowfast.datasets.mixup",
        "peekOfCode": "def rand_bbox(img_shape, lam, margin=0.0, count=None):\n    \"\"\"\n    Generates a random square bbox based on lambda value.\n    Args:\n        img_shape (tuple): Image shape as tuple\n        lam (float): Cutmix lambda value\n        margin (float): Percentage of bbox dimension to enforce as margin (reduce amount of box outside image)\n        count (int): Number of bbox to generate\n    \"\"\"\n    ratio = np.sqrt(1 - lam)",
        "detail": "MorphMLP.slowfast.datasets.mixup",
        "documentation": {}
    },
    {
        "label": "get_cutmix_bbox",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.mixup",
        "description": "MorphMLP.slowfast.datasets.mixup",
        "peekOfCode": "def get_cutmix_bbox(img_shape, lam, correct_lam=True, count=None):\n    \"\"\"\n    Generates the box coordinates for cutmix.\n    Args:\n        img_shape (tuple): Image shape as tuple\n        lam (float): Cutmix lambda value\n        correct_lam (bool): Apply lambda correction when cutmix bbox clipped by\n            image borders.\n        count (int): Number of bbox to generate\n    \"\"\"",
        "detail": "MorphMLP.slowfast.datasets.mixup",
        "documentation": {}
    },
    {
        "label": "ShortCycleBatchSampler",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.datasets.multigrid_helper",
        "description": "MorphMLP.slowfast.datasets.multigrid_helper",
        "peekOfCode": "class ShortCycleBatchSampler(Sampler):\n    \"\"\"\n    Extend Sampler to support \"short cycle\" sampling.\n    See paper \"A Multigrid Method for Efficiently Training Video Models\",\n    Wu et al., 2019 (https://arxiv.org/abs/1912.00998) for details.\n    \"\"\"\n    def __init__(self, sampler, batch_size, drop_last, cfg):\n        if not isinstance(sampler, Sampler):\n            raise ValueError(\n                \"sampler should be an instance of \"",
        "detail": "MorphMLP.slowfast.datasets.multigrid_helper",
        "documentation": {}
    },
    {
        "label": "TORCH_MAJOR",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.multigrid_helper",
        "description": "MorphMLP.slowfast.datasets.multigrid_helper",
        "peekOfCode": "TORCH_MAJOR = int(torch.__version__.split(\".\")[0])\nTORCH_MINOR = int(torch.__version__.split(\".\")[1])\nif TORCH_MAJOR >= 1 and TORCH_MINOR >= 8:\n    _int_classes = int\nelse:\n    from torch._six import int_classes as _int_classes\nclass ShortCycleBatchSampler(Sampler):\n    \"\"\"\n    Extend Sampler to support \"short cycle\" sampling.\n    See paper \"A Multigrid Method for Efficiently Training Video Models\",",
        "detail": "MorphMLP.slowfast.datasets.multigrid_helper",
        "documentation": {}
    },
    {
        "label": "TORCH_MINOR",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.multigrid_helper",
        "description": "MorphMLP.slowfast.datasets.multigrid_helper",
        "peekOfCode": "TORCH_MINOR = int(torch.__version__.split(\".\")[1])\nif TORCH_MAJOR >= 1 and TORCH_MINOR >= 8:\n    _int_classes = int\nelse:\n    from torch._six import int_classes as _int_classes\nclass ShortCycleBatchSampler(Sampler):\n    \"\"\"\n    Extend Sampler to support \"short cycle\" sampling.\n    See paper \"A Multigrid Method for Efficiently Training Video Models\",\n    Wu et al., 2019 (https://arxiv.org/abs/1912.00998) for details.",
        "detail": "MorphMLP.slowfast.datasets.multigrid_helper",
        "documentation": {}
    },
    {
        "label": "PTVDatasetWrapper",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.datasets.ptv_datasets",
        "description": "MorphMLP.slowfast.datasets.ptv_datasets",
        "peekOfCode": "class PTVDatasetWrapper(torch.utils.data.IterableDataset):\n    \"\"\"\n    Wrapper for PyTorchVideo datasets.\n    \"\"\"\n    def __init__(self, num_videos, clips_per_video, crops_per_clip, dataset):\n        \"\"\"\n        Construct the dataset.\n        Args:\n            num_vidoes (int): number of videos in the dataset.\n            clips_per_video (int): number of clips per video in the dataset.",
        "detail": "MorphMLP.slowfast.datasets.ptv_datasets",
        "documentation": {}
    },
    {
        "label": "PackPathway",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.datasets.ptv_datasets",
        "description": "MorphMLP.slowfast.datasets.ptv_datasets",
        "peekOfCode": "class PackPathway(torch.nn.Module):\n    \"\"\"\n    Transform for converting video frames as a list of tensors. Each tensor\n    corresponding to a unique pathway.\n    \"\"\"\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n    def forward(self, x: torch.Tensor):\n        return utils.pack_pathway_output(self.cfg, x)",
        "detail": "MorphMLP.slowfast.datasets.ptv_datasets",
        "documentation": {}
    },
    {
        "label": "DictToTuple",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.datasets.ptv_datasets",
        "description": "MorphMLP.slowfast.datasets.ptv_datasets",
        "peekOfCode": "class DictToTuple(torch.nn.Module):\n    \"\"\"\n    Transform for converting output from dict to a tuple following PySlowFast\n    dataset output format.\n    \"\"\"\n    def __init__(self, num_clips, num_crops):\n        super().__init__()\n        self._num_clips = num_clips\n        self._num_crops = num_crops\n    def forward(self, x: Dict[str, torch.Tensor]):",
        "detail": "MorphMLP.slowfast.datasets.ptv_datasets",
        "documentation": {}
    },
    {
        "label": "div255",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.ptv_datasets",
        "description": "MorphMLP.slowfast.datasets.ptv_datasets",
        "peekOfCode": "def div255(x):\n    \"\"\"\n    Scale clip frames from [0, 255] to [0, 1].\n    Args:\n        x (Tensor): A tensor of the clip's RGB frames with shape:\n            (channel, time, height, width).\n    Returns:\n        x (Tensor): Scaled tensor by divide 255.\n    \"\"\"\n    return x / 255.0",
        "detail": "MorphMLP.slowfast.datasets.ptv_datasets",
        "documentation": {}
    },
    {
        "label": "Ptvkinetics",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.ptv_datasets",
        "description": "MorphMLP.slowfast.datasets.ptv_datasets",
        "peekOfCode": "def Ptvkinetics(cfg, mode):\n    \"\"\"\n    Construct the Kinetics video loader with a given csv file. The format of\n    the csv file is:\n    ```\n    path_to_video_1 label_1\n    path_to_video_2 label_2\n    ...\n    path_to_video_N label_N\n    ```",
        "detail": "MorphMLP.slowfast.datasets.ptv_datasets",
        "documentation": {}
    },
    {
        "label": "process_charades_label",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.ptv_datasets",
        "description": "MorphMLP.slowfast.datasets.ptv_datasets",
        "peekOfCode": "def process_charades_label(x, mode, num_classes):\n    \"\"\"\n    Process the video label for Charades dataset. Use video-level label for\n    training mode, otherwise use clip-level label. Then convert the label into\n    a binary vector.\n    Args:\n        x (dict): a video clip including label index.\n        mode (string): Options includes `train`, `val`, or `test` mode.\n        num_classes (int): Number of classes in the dataset.\n    Returns:",
        "detail": "MorphMLP.slowfast.datasets.ptv_datasets",
        "documentation": {}
    },
    {
        "label": "rgb2bgr",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.ptv_datasets",
        "description": "MorphMLP.slowfast.datasets.ptv_datasets",
        "peekOfCode": "def rgb2bgr(x):\n    \"\"\"\n    Convert clip frames from RGB mode to BRG mode.\n    Args:\n        x (Tensor): A tensor of the clip's RGB frames with shape:\n            (channel, time, height, width).\n    Returns:\n        x (Tensor): Converted tensor\n    \"\"\"\n    return x[[2, 1, 0], ...]",
        "detail": "MorphMLP.slowfast.datasets.ptv_datasets",
        "documentation": {}
    },
    {
        "label": "Ptvcharades",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.ptv_datasets",
        "description": "MorphMLP.slowfast.datasets.ptv_datasets",
        "peekOfCode": "def Ptvcharades(cfg, mode):\n    \"\"\"\n    Construct PyTorchVideo Charades video loader.\n    Load Charades data (frame paths, labels, etc. ) to Charades Dataset object.\n    The dataset could be downloaded from Chrades official website\n    (https://allenai.org/plato/charades/).\n    Please see datasets/DATASET.md for more information about the data format.\n    For `train` and `val` mode, a single clip is randomly sampled from every video\n    with random cropping, scaling, and flipping. For `test` mode, multiple clips are\n    uniformaly sampled from every video with center cropping.",
        "detail": "MorphMLP.slowfast.datasets.ptv_datasets",
        "documentation": {}
    },
    {
        "label": "Ptvssv2",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.ptv_datasets",
        "description": "MorphMLP.slowfast.datasets.ptv_datasets",
        "peekOfCode": "def Ptvssv2(cfg, mode):\n    \"\"\"\n    Construct PyTorchVideo Something-Something v2 SSv2 video loader.\n    Load SSv2 data (frame paths, labels, etc. ) to SSv2 Dataset object.\n    The dataset could be downloaded from Chrades official website\n    (https://20bn.com/datasets/something-something).\n    Please see datasets/DATASET.md for more information about the data format.\n    For training and validation, a single  clip is randomly sampled from every\n    video with random cropping and scaling. For testing, multiple clips are\n    uniformaly sampled from every video with uniform cropping. For uniform cropping,",
        "detail": "MorphMLP.slowfast.datasets.ptv_datasets",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.ptv_datasets",
        "description": "MorphMLP.slowfast.datasets.ptv_datasets",
        "peekOfCode": "logger = logging.get_logger(__name__)\nclass PTVDatasetWrapper(torch.utils.data.IterableDataset):\n    \"\"\"\n    Wrapper for PyTorchVideo datasets.\n    \"\"\"\n    def __init__(self, num_videos, clips_per_video, crops_per_clip, dataset):\n        \"\"\"\n        Construct the dataset.\n        Args:\n            num_vidoes (int): number of videos in the dataset.",
        "detail": "MorphMLP.slowfast.datasets.ptv_datasets",
        "documentation": {}
    },
    {
        "label": "RandomErasing",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.datasets.random_erasing",
        "description": "MorphMLP.slowfast.datasets.random_erasing",
        "peekOfCode": "class RandomErasing:\n    \"\"\"Randomly selects a rectangle region in an image and erases its pixels.\n        'Random Erasing Data Augmentation' by Zhong et al.\n        See https://arxiv.org/pdf/1708.04896.pdf\n        This variant of RandomErasing is intended to be applied to either a batch\n        or single image tensor after it has been normalized by dataset mean and std.\n    Args:\n         probability: Probability that the Random Erasing operation will be performed.\n         min_area: Minimum percentage of erased area wrt input image area.\n         max_area: Maximum percentage of erased area wrt input image area.",
        "detail": "MorphMLP.slowfast.datasets.random_erasing",
        "documentation": {}
    },
    {
        "label": "AugmentOp",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "class AugmentOp:\n    \"\"\"\n    Apply for video.\n    \"\"\"\n    def __init__(self, name, prob=0.5, magnitude=10, hparams=None):\n        hparams = hparams or _HPARAMS_DEFAULT\n        self.aug_fn = NAME_TO_OP[name]\n        self.level_fn = LEVEL_TO_ARG[name]\n        self.prob = prob\n        self.magnitude = magnitude",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "RandAugment",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "class RandAugment:\n    def __init__(self, ops, num_layers=2, choice_weights=None):\n        self.ops = ops\n        self.num_layers = num_layers\n        self.choice_weights = choice_weights\n    def __call__(self, img):\n        # no replacement when using weighted choice\n        ops = np.random.choice(\n            self.ops,\n            self.num_layers,",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "shear_x",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "def shear_x(img, factor, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(\n        img.size, Image.AFFINE, (1, factor, 0, 0, 1, 0), **kwargs\n    )\ndef shear_y(img, factor, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(\n        img.size, Image.AFFINE, (1, 0, 0, factor, 1, 0), **kwargs\n    )",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "shear_y",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "def shear_y(img, factor, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(\n        img.size, Image.AFFINE, (1, 0, 0, factor, 1, 0), **kwargs\n    )\ndef translate_x_rel(img, pct, **kwargs):\n    pixels = pct * img.size[0]\n    _check_args_tf(kwargs)\n    return img.transform(\n        img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "translate_x_rel",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "def translate_x_rel(img, pct, **kwargs):\n    pixels = pct * img.size[0]\n    _check_args_tf(kwargs)\n    return img.transform(\n        img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs\n    )\ndef translate_y_rel(img, pct, **kwargs):\n    pixels = pct * img.size[1]\n    _check_args_tf(kwargs)\n    return img.transform(",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "translate_y_rel",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "def translate_y_rel(img, pct, **kwargs):\n    pixels = pct * img.size[1]\n    _check_args_tf(kwargs)\n    return img.transform(\n        img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels), **kwargs\n    )\ndef translate_x_abs(img, pixels, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(\n        img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "translate_x_abs",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "def translate_x_abs(img, pixels, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(\n        img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs\n    )\ndef translate_y_abs(img, pixels, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(\n        img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels), **kwargs\n    )",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "translate_y_abs",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "def translate_y_abs(img, pixels, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(\n        img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels), **kwargs\n    )\ndef rotate(img, degrees, **kwargs):\n    _check_args_tf(kwargs)\n    if _PIL_VER >= (5, 2):\n        return img.rotate(degrees, **kwargs)\n    elif _PIL_VER >= (5, 0):",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "rotate",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "def rotate(img, degrees, **kwargs):\n    _check_args_tf(kwargs)\n    if _PIL_VER >= (5, 2):\n        return img.rotate(degrees, **kwargs)\n    elif _PIL_VER >= (5, 0):\n        w, h = img.size\n        post_trans = (0, 0)\n        rotn_center = (w / 2.0, h / 2.0)\n        angle = -math.radians(degrees)\n        matrix = [",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "auto_contrast",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "def auto_contrast(img, **__):\n    return ImageOps.autocontrast(img)\ndef invert(img, **__):\n    return ImageOps.invert(img)\ndef equalize(img, **__):\n    return ImageOps.equalize(img)\ndef solarize(img, thresh, **__):\n    return ImageOps.solarize(img, thresh)\ndef solarize_add(img, add, thresh=128, **__):\n    lut = []",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "invert",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "def invert(img, **__):\n    return ImageOps.invert(img)\ndef equalize(img, **__):\n    return ImageOps.equalize(img)\ndef solarize(img, thresh, **__):\n    return ImageOps.solarize(img, thresh)\ndef solarize_add(img, add, thresh=128, **__):\n    lut = []\n    for i in range(256):\n        if i < thresh:",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "equalize",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "def equalize(img, **__):\n    return ImageOps.equalize(img)\ndef solarize(img, thresh, **__):\n    return ImageOps.solarize(img, thresh)\ndef solarize_add(img, add, thresh=128, **__):\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "solarize",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "def solarize(img, thresh, **__):\n    return ImageOps.solarize(img, thresh)\ndef solarize_add(img, add, thresh=128, **__):\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in (\"L\", \"RGB\"):",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "solarize_add",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "def solarize_add(img, add, thresh=128, **__):\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in (\"L\", \"RGB\"):\n        if img.mode == \"RGB\" and len(lut) == 256:\n            lut = lut + lut + lut",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "posterize",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "def posterize(img, bits_to_keep, **__):\n    if bits_to_keep >= 8:\n        return img\n    return ImageOps.posterize(img, bits_to_keep)\ndef contrast(img, factor, **__):\n    return ImageEnhance.Contrast(img).enhance(factor)\ndef color(img, factor, **__):\n    return ImageEnhance.Color(img).enhance(factor)\ndef brightness(img, factor, **__):\n    return ImageEnhance.Brightness(img).enhance(factor)",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "contrast",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "def contrast(img, factor, **__):\n    return ImageEnhance.Contrast(img).enhance(factor)\ndef color(img, factor, **__):\n    return ImageEnhance.Color(img).enhance(factor)\ndef brightness(img, factor, **__):\n    return ImageEnhance.Brightness(img).enhance(factor)\ndef sharpness(img, factor, **__):\n    return ImageEnhance.Sharpness(img).enhance(factor)\ndef _randomly_negate(v):\n    \"\"\"With 50% prob, negate the value\"\"\"",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "color",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "def color(img, factor, **__):\n    return ImageEnhance.Color(img).enhance(factor)\ndef brightness(img, factor, **__):\n    return ImageEnhance.Brightness(img).enhance(factor)\ndef sharpness(img, factor, **__):\n    return ImageEnhance.Sharpness(img).enhance(factor)\ndef _randomly_negate(v):\n    \"\"\"With 50% prob, negate the value\"\"\"\n    return -v if random.random() > 0.5 else v\ndef _rotate_level_to_arg(level, _hparams):",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "brightness",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "def brightness(img, factor, **__):\n    return ImageEnhance.Brightness(img).enhance(factor)\ndef sharpness(img, factor, **__):\n    return ImageEnhance.Sharpness(img).enhance(factor)\ndef _randomly_negate(v):\n    \"\"\"With 50% prob, negate the value\"\"\"\n    return -v if random.random() > 0.5 else v\ndef _rotate_level_to_arg(level, _hparams):\n    # range [-30, 30]\n    level = (level / _MAX_LEVEL) * 30.0",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "sharpness",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "def sharpness(img, factor, **__):\n    return ImageEnhance.Sharpness(img).enhance(factor)\ndef _randomly_negate(v):\n    \"\"\"With 50% prob, negate the value\"\"\"\n    return -v if random.random() > 0.5 else v\ndef _rotate_level_to_arg(level, _hparams):\n    # range [-30, 30]\n    level = (level / _MAX_LEVEL) * 30.0\n    level = _randomly_negate(level)\n    return (level,)",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "rand_augment_ops",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "def rand_augment_ops(magnitude=10, hparams=None, transforms=None):\n    hparams = hparams or _HPARAMS_DEFAULT\n    transforms = transforms or _RAND_TRANSFORMS\n    return [\n        AugmentOp(name, prob=0.5, magnitude=magnitude, hparams=hparams)\n        for name in transforms\n    ]\nclass RandAugment:\n    def __init__(self, ops, num_layers=2, choice_weights=None):\n        self.ops = ops",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "rand_augment_transform",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "def rand_augment_transform(config_str, hparams):\n    \"\"\"\n    RandAugment: Practical automated data augmentation... - https://arxiv.org/abs/1909.13719\n    Create a RandAugment transform\n    :param config_str: String defining configuration of random augmentation. Consists of multiple sections separated by\n    dashes ('-'). The first section defines the specific variant of rand augment (currently only 'rand'). The remaining\n    sections, not order sepecific determine\n        'm' - integer magnitude of rand augment\n        'n' - integer num layers (number of transform ops selected per image)\n        'w' - integer probabiliy weight index (index of a set of weights to influence choice of op)",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "_PIL_VER",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "_PIL_VER = tuple([int(x) for x in PIL.__version__.split(\".\")[:2]])\n_FILL = (128, 128, 128)\n# This signifies the max integer that the controller RNN could predict for the\n# augmentation scheme.\n_MAX_LEVEL = 10.0\n_HPARAMS_DEFAULT = {\n    \"translate_const\": 250,\n    \"img_mean\": _FILL,\n}\n_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "_FILL",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "_FILL = (128, 128, 128)\n# This signifies the max integer that the controller RNN could predict for the\n# augmentation scheme.\n_MAX_LEVEL = 10.0\n_HPARAMS_DEFAULT = {\n    \"translate_const\": 250,\n    \"img_mean\": _FILL,\n}\n_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\ndef _interpolation(kwargs):",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "_MAX_LEVEL",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "_MAX_LEVEL = 10.0\n_HPARAMS_DEFAULT = {\n    \"translate_const\": 250,\n    \"img_mean\": _FILL,\n}\n_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\ndef _interpolation(kwargs):\n    interpolation = kwargs.pop(\"resample\", Image.BILINEAR)\n    if isinstance(interpolation, (list, tuple)):\n        return random.choice(interpolation)",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "_HPARAMS_DEFAULT",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "_HPARAMS_DEFAULT = {\n    \"translate_const\": 250,\n    \"img_mean\": _FILL,\n}\n_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\ndef _interpolation(kwargs):\n    interpolation = kwargs.pop(\"resample\", Image.BILINEAR)\n    if isinstance(interpolation, (list, tuple)):\n        return random.choice(interpolation)\n    else:",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "_RANDOM_INTERPOLATION",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\ndef _interpolation(kwargs):\n    interpolation = kwargs.pop(\"resample\", Image.BILINEAR)\n    if isinstance(interpolation, (list, tuple)):\n        return random.choice(interpolation)\n    else:\n        return interpolation\ndef _check_args_tf(kwargs):\n    if \"fillcolor\" in kwargs and _PIL_VER < (5, 0):\n        kwargs.pop(\"fillcolor\")",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "LEVEL_TO_ARG",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "LEVEL_TO_ARG = {\n    \"AutoContrast\": None,\n    \"Equalize\": None,\n    \"Invert\": None,\n    \"Rotate\": _rotate_level_to_arg,\n    # There are several variations of the posterize level scaling in various Tensorflow/Google repositories/papers\n    \"Posterize\": _posterize_level_to_arg,\n    \"PosterizeIncreasing\": _posterize_increasing_level_to_arg,\n    \"PosterizeOriginal\": _posterize_original_level_to_arg,\n    \"Solarize\": _solarize_level_to_arg,",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "NAME_TO_OP",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "NAME_TO_OP = {\n    \"AutoContrast\": auto_contrast,\n    \"Equalize\": equalize,\n    \"Invert\": invert,\n    \"Rotate\": rotate,\n    \"Posterize\": posterize,\n    \"PosterizeIncreasing\": posterize,\n    \"PosterizeOriginal\": posterize,\n    \"Solarize\": solarize,\n    \"SolarizeIncreasing\": solarize,",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "_RAND_TRANSFORMS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "_RAND_TRANSFORMS = [\n    \"AutoContrast\",\n    \"Equalize\",\n    \"Invert\",\n    \"Rotate\",\n    \"Posterize\",\n    \"Solarize\",\n    \"SolarizeAdd\",\n    \"Color\",\n    \"Contrast\",",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "_RAND_INCREASING_TRANSFORMS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "_RAND_INCREASING_TRANSFORMS = [\n    \"AutoContrast\",\n    \"Equalize\",\n    \"Invert\",\n    \"Rotate\",\n    \"PosterizeIncreasing\",\n    \"SolarizeIncreasing\",\n    \"SolarizeAdd\",\n    \"ColorIncreasing\",\n    \"ContrastIncreasing\",",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "_RAND_CHOICE_WEIGHTS_0",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.rand_augment",
        "description": "MorphMLP.slowfast.datasets.rand_augment",
        "peekOfCode": "_RAND_CHOICE_WEIGHTS_0 = {\n    \"Rotate\": 0.3,\n    \"ShearX\": 0.2,\n    \"ShearY\": 0.2,\n    \"TranslateXRel\": 0.1,\n    \"TranslateYRel\": 0.1,\n    \"Color\": 0.025,\n    \"Sharpness\": 0.025,\n    \"AutoContrast\": 0.025,\n    \"Solarize\": 0.005,",
        "detail": "MorphMLP.slowfast.datasets.rand_augment",
        "documentation": {}
    },
    {
        "label": "Ssv2",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.datasets.ssv2",
        "description": "MorphMLP.slowfast.datasets.ssv2",
        "peekOfCode": "class Ssv2(torch.utils.data.Dataset):\n    \"\"\"\n    Something-Something v2 (SSV2) video loader. Construct the SSV2 video loader,\n    then sample clips from the videos. For training and validation, a single\n    clip is randomly sampled from every video with random cropping, scaling, and\n    flipping. For testing, multiple clips are uniformaly sampled from every\n    video with uniform cropping. For uniform cropping, we take the left, center,\n    and right crop if the width is larger than height, or take top, center, and\n    bottom crop if the height is larger than the width.\n    \"\"\"",
        "detail": "MorphMLP.slowfast.datasets.ssv2",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.ssv2",
        "description": "MorphMLP.slowfast.datasets.ssv2",
        "peekOfCode": "logger = logging.get_logger(__name__)\n@DATASET_REGISTRY.register()\nclass Ssv2(torch.utils.data.Dataset):\n    \"\"\"\n    Something-Something v2 (SSV2) video loader. Construct the SSV2 video loader,\n    then sample clips from the videos. For training and validation, a single\n    clip is randomly sampled from every video with random cropping, scaling, and\n    flipping. For testing, multiple clips are uniformaly sampled from every\n    video with uniform cropping. For uniform cropping, we take the left, center,\n    and right crop if the width is larger than height, or take top, center, and",
        "detail": "MorphMLP.slowfast.datasets.ssv2",
        "documentation": {}
    },
    {
        "label": "Sth",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.datasets.sth",
        "description": "MorphMLP.slowfast.datasets.sth",
        "peekOfCode": "class Sth(torch.utils.data.Dataset):\n    \"\"\"\n    Something-Something video loader. Construct the Sth video loader,\n    then sample clips from the videos. For training and validation, a single\n    clip is randomly sampled from every video with random cropping, scaling, and\n    flipping. For testing, multiple clips are uniformaly sampled from every\n    video with uniform cropping. For uniform cropping, we take the left, center,\n    and right crop if the width is larger than height, or take top, center, and\n    bottom crop if the height is larger than the width.\n    \"\"\"",
        "detail": "MorphMLP.slowfast.datasets.sth",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.sth",
        "description": "MorphMLP.slowfast.datasets.sth",
        "peekOfCode": "logger = logging.get_logger(__name__)\n@DATASET_REGISTRY.register()\nclass Sth(torch.utils.data.Dataset):\n    \"\"\"\n    Something-Something video loader. Construct the Sth video loader,\n    then sample clips from the videos. For training and validation, a single\n    clip is randomly sampled from every video with random cropping, scaling, and\n    flipping. For testing, multiple clips are uniformaly sampled from every\n    video with uniform cropping. For uniform cropping, we take the left, center,\n    and right crop if the width is larger than height, or take top, center, and",
        "detail": "MorphMLP.slowfast.datasets.sth",
        "documentation": {}
    },
    {
        "label": "RandomResizedCropAndInterpolation",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "class RandomResizedCropAndInterpolation:\n    \"\"\"Crop the given PIL Image to random size and aspect ratio with random interpolation.\n    A crop of random size (default: of 0.08 to 1.0) of the original size and a random\n    aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop\n    is finally resized to given size.\n    This is popularly used to train the Inception networks.\n    Args:\n        size: expected output size of each edge\n        scale: range of size of the origin size cropped\n        ratio: range of aspect ratio of the origin aspect ratio cropped",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "GaussianBlur",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "class GaussianBlur(object):\n    \"\"\"Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709\"\"\"\n    def __init__(self, sigma=[0.1, 2.0]):\n        self.sigma = sigma\n    def __call__(self, x):\n        if len(self.sigma) == 2:\n            sigma = random.uniform(self.sigma[0], self.sigma[1])\n        elif len(self.sigma) == 1:\n            sigma = self.sigma[0]\n        x = x.filter(ImageFilter.GaussianBlur(radius=sigma))",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "GaussianBlurVideo",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "class GaussianBlurVideo(object):\n    def __init__(\n        self, sigma_min=[0.0, 0.1], sigma_max=[0.0, 2.0], use_PIL=False\n    ):\n        self.sigma_min = sigma_min\n        self.sigma_max = sigma_max\n    def __call__(self, frames):\n        sigma_y = sigma_x = random.uniform(self.sigma_min[1], self.sigma_max[1])\n        sigma_t = random.uniform(self.sigma_min[0], self.sigma_max[0])\n        frames = gaussian_filter(frames, sigma=(0.0, sigma_t, sigma_y, sigma_x))",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "random_short_side_scale_jitter",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "def random_short_side_scale_jitter(\n    images, min_size, max_size, boxes=None, inverse_uniform_sampling=False\n):\n    \"\"\"\n    Perform a spatial short scale jittering on the given images and\n    corresponding boxes.\n    Args:\n        images (tensor): images to perform scale jitter. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n        min_size (int): the minimal size to scale the frames.",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "crop_boxes",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "def crop_boxes(boxes, x_offset, y_offset):\n    \"\"\"\n    Peform crop on the bounding boxes given the offsets.\n    Args:\n        boxes (ndarray or None): bounding boxes to peform crop. The dimension\n            is `num boxes` x 4.\n        x_offset (int): cropping offset in the x axis.\n        y_offset (int): cropping offset in the y axis.\n    Returns:\n        cropped_boxes (ndarray or None): the cropped boxes with dimension of",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "random_crop",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "def random_crop(images, size, boxes=None):\n    \"\"\"\n    Perform random spatial crop on the given images and corresponding boxes.\n    Args:\n        images (tensor): images to perform random crop. The dimension is\n            `num frames` x `channel` x `height` x `width`.\n        size (int): the size of height and width to crop on the image.\n        boxes (ndarray or None): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.\n    Returns:",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "horizontal_flip",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "def horizontal_flip(prob, images, boxes=None):\n    \"\"\"\n    Perform horizontal flip on the given images and corresponding boxes.\n    Args:\n        prob (float): probility to flip the images.\n        images (tensor): images to perform horizontal flip, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n        boxes (ndarray or None): optional. Corresponding boxes to images.\n            Dimension is `num boxes` x 4.\n    Returns:",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "uniform_crop",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "def uniform_crop(images, size, spatial_idx, boxes=None, scale_size=None):\n    \"\"\"\n    Perform uniform spatial sampling on the images and corresponding boxes.\n    Args:\n        images (tensor): images to perform uniform crop. The dimension is\n            `num frames` x `channel` x `height` x `width`.\n        size (int): size of height and weight to crop the images.\n        spatial_idx (int): 0, 1, or 2 for left, center, and right crop if width\n            is larger than height. Or 0, 1, or 2 for top, center, and bottom\n            crop if height is larger than width.",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "clip_boxes_to_image",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "def clip_boxes_to_image(boxes, height, width):\n    \"\"\"\n    Clip an array of boxes to an image with the given height and width.\n    Args:\n        boxes (ndarray): bounding boxes to perform clipping.\n            Dimension is `num boxes` x 4.\n        height (int): given image height.\n        width (int): given image width.\n    Returns:\n        clipped_boxes (ndarray): the clipped boxes with dimension of",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "blend",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "def blend(images1, images2, alpha):\n    \"\"\"\n    Blend two images with a given weight alpha.\n    Args:\n        images1 (tensor): the first images to be blended, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n        images2 (tensor): the second images to be blended, the dimension is\n            `num frames` x `channel` x `height` x `width`.\n        alpha (float): the blending weight.\n    Returns:",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "grayscale",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "def grayscale(images):\n    \"\"\"\n    Get the grayscale for the input images. The channels of images should be\n    in order BGR.\n    Args:\n        images (tensor): the input images for getting grayscale. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n    Returns:\n        img_gray (tensor): blended images, the dimension is\n            `num frames` x `channel` x `height` x `width`.",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "color_jitter",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "def color_jitter(images, img_brightness=0, img_contrast=0, img_saturation=0):\n    \"\"\"\n    Perfrom a color jittering on the input images. The channels of images\n    should be in order BGR.\n    Args:\n        images (tensor): images to perform color jitter. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n        img_brightness (float): jitter ratio for brightness.\n        img_contrast (float): jitter ratio for contrast.\n        img_saturation (float): jitter ratio for saturation.",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "brightness_jitter",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "def brightness_jitter(var, images):\n    \"\"\"\n    Perfrom brightness jittering on the input images. The channels of images\n    should be in order BGR.\n    Args:\n        var (float): jitter ratio for brightness.\n        images (tensor): images to perform color jitter. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n    Returns:\n        images (tensor): the jittered images, the dimension is",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "contrast_jitter",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "def contrast_jitter(var, images):\n    \"\"\"\n    Perfrom contrast jittering on the input images. The channels of images\n    should be in order BGR.\n    Args:\n        var (float): jitter ratio for contrast.\n        images (tensor): images to perform color jitter. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n    Returns:\n        images (tensor): the jittered images, the dimension is",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "saturation_jitter",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "def saturation_jitter(var, images):\n    \"\"\"\n    Perfrom saturation jittering on the input images. The channels of images\n    should be in order BGR.\n    Args:\n        var (float): jitter ratio for saturation.\n        images (tensor): images to perform color jitter. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n    Returns:\n        images (tensor): the jittered images, the dimension is",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "lighting_jitter",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "def lighting_jitter(images, alphastd, eigval, eigvec):\n    \"\"\"\n    Perform AlexNet-style PCA jitter on the given images.\n    Args:\n        images (tensor): images to perform lighting jitter. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n        alphastd (float): jitter ratio for PCA jitter.\n        eigval (list): eigenvalues for PCA jitter.\n        eigvec (list[list]): eigenvectors for PCA jitter.\n    Returns:",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "color_normalization",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "def color_normalization(images, mean, stddev):\n    \"\"\"\n    Perform color nomration on the given images.\n    Args:\n        images (tensor): images to perform color normalization. Dimension is\n            `num frames` x `channel` x `height` x `width`.\n        mean (list): mean values for normalization.\n        stddev (list): standard deviations for normalization.\n    Returns:\n        out_images (tensor): the noramlized images, the dimension is",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "random_resized_crop",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "def random_resized_crop(\n    images,\n    target_height,\n    target_width,\n    scale=(0.8, 1.0),\n    ratio=(3.0 / 4.0, 4.0 / 3.0),\n):\n    \"\"\"\n    Crop the given images to random size and aspect ratio. A crop of random\n    size (default: of 0.08 to 1.0) of the original size and a random aspect",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "random_resized_crop_with_shift",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "def random_resized_crop_with_shift(\n    images,\n    target_height,\n    target_width,\n    scale=(0.8, 1.0),\n    ratio=(3.0 / 4.0, 4.0 / 3.0),\n):\n    \"\"\"\n    This is similar to random_resized_crop. However, it samples two different\n    boxes (for cropping) for the first and last frame. It then linearly",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "create_random_augment",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "def create_random_augment(\n    input_size,\n    auto_augment=None,\n    interpolation=\"bilinear\",\n):\n    \"\"\"\n    Get video randaug transform.\n    Args:\n        input_size: The size of the input video in tuple.\n        auto_augment: Parameters for randaug. An example:",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "random_sized_crop_img",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "def random_sized_crop_img(\n    im,\n    size,\n    jitter_scale=(0.08, 1.0),\n    jitter_aspect=(3.0 / 4.0, 4.0 / 3.0),\n    max_iter=10,\n):\n    \"\"\"\n    Performs Inception-style cropping (used for training).\n    \"\"\"",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "transforms_imagenet_train",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "def transforms_imagenet_train(\n    img_size=224,\n    scale=None,\n    ratio=None,\n    hflip=0.5,\n    vflip=0.0,\n    color_jitter=0.4,\n    auto_augment=None,\n    interpolation=\"random\",\n    use_prefetcher=False,",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "temporal_difference",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "def temporal_difference(\n    frames,\n    use_grayscale=False,\n    absolute=False,\n):\n    if use_grayscale:\n        gray_channel = (\n            0.299 * frames[2, :] + 0.587 * frames[1, :] + 0.114 * frames[0, :]\n        )\n        frames[0, :] = gray_channel",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "color_jitter_video_ssl",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "def color_jitter_video_ssl(\n    frames,\n    bri_con_sat=[0.4] * 3,\n    hue=0.1,\n    p_convert_gray=0.0,\n    moco_v2_aug=False,\n    gaussan_sigma_min=[0.0, 0.1],\n    gaussan_sigma_max=[0.0, 2.0],\n):\n    # T H W C -> C T H W.",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "augment_raw_frames",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "def augment_raw_frames(frames, time_diff_prob=0.0, gaussian_prob=0.0):\n    frames = frames.float()\n    if gaussian_prob > 0.0:\n        blur_trans = tv.transforms.RandomApply(\n            [GaussianBlurVideo()], p=gaussian_prob\n        )\n        frames = blur_trans(frames)\n    time_diff_out = False\n    if time_diff_prob > 0.0 and random.random() < time_diff_prob:\n        # T H W C -> C T H W.",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "_pil_interpolation_to_str",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "_pil_interpolation_to_str = {\n    Image.NEAREST: \"PIL.Image.NEAREST\",\n    Image.BILINEAR: \"PIL.Image.BILINEAR\",\n    Image.BICUBIC: \"PIL.Image.BICUBIC\",\n    Image.LANCZOS: \"PIL.Image.LANCZOS\",\n    Image.HAMMING: \"PIL.Image.HAMMING\",\n    Image.BOX: \"PIL.Image.BOX\",\n}\n_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\ndef _pil_interp(method):",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "_RANDOM_INTERPOLATION",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\ndef _pil_interp(method):\n    if method == \"bicubic\":\n        return Image.BICUBIC\n    elif method == \"lanczos\":\n        return Image.LANCZOS\n    elif method == \"hamming\":\n        return Image.HAMMING\n    else:\n        return Image.BILINEAR",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.transform",
        "description": "MorphMLP.slowfast.datasets.transform",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef random_short_side_scale_jitter(\n    images, min_size, max_size, boxes=None, inverse_uniform_sampling=False\n):\n    \"\"\"\n    Perform a spatial short scale jittering on the given images and\n    corresponding boxes.\n    Args:\n        images (tensor): images to perform scale jitter. Dimension is\n            `num frames` x `channel` x `height` x `width`.",
        "detail": "MorphMLP.slowfast.datasets.transform",
        "documentation": {}
    },
    {
        "label": "retry_load_images",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.utils",
        "description": "MorphMLP.slowfast.datasets.utils",
        "peekOfCode": "def retry_load_images(image_paths, retry=10, backend=\"pytorch\"):\n    \"\"\"\n    This function is to load images with support of retrying for failed load.\n    Args:\n        image_paths (list): paths of images needed to be loaded.\n        retry (int, optional): maximum time of loading retrying. Defaults to 10.\n        backend (str): `pytorch` or `cv2`.\n    Returns:\n        imgs (list): list of loaded images.\n    \"\"\"",
        "detail": "MorphMLP.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "get_sequence",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.utils",
        "description": "MorphMLP.slowfast.datasets.utils",
        "peekOfCode": "def get_sequence(center_idx, half_len, sample_rate, num_frames):\n    \"\"\"\n    Sample frames among the corresponding clip.\n    Args:\n        center_idx (int): center frame idx for current clip\n        half_len (int): half of the clip length\n        sample_rate (int): sampling rate for sampling frames inside of the clip\n        num_frames (int): number of expected sampled frames\n    Returns:\n        seq (list): list of indexes of sampled frames in this clip.",
        "detail": "MorphMLP.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "pack_pathway_output",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.utils",
        "description": "MorphMLP.slowfast.datasets.utils",
        "peekOfCode": "def pack_pathway_output(cfg, frames):\n    \"\"\"\n    Prepare output as a list of tensors. Each tensor corresponding to a\n    unique pathway.\n    Args:\n        frames (tensor): frames of images sampled from the video. The\n            dimension is `channel` x `num frames` x `height` x `width`.\n    Returns:\n        frame_list (list): list of tensors with the dimension of\n            `channel` x `num frames` x `height` x `width`.",
        "detail": "MorphMLP.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "spatial_sampling",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.utils",
        "description": "MorphMLP.slowfast.datasets.utils",
        "peekOfCode": "def spatial_sampling(\n    frames,\n    spatial_idx=-1,\n    min_scale=256,\n    max_scale=320,\n    crop_size=224,\n    random_horizontal_flip=True,\n    inverse_uniform_sampling=False,\n    aspect_ratio=None,\n    scale=None,",
        "detail": "MorphMLP.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "as_binary_vector",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.utils",
        "description": "MorphMLP.slowfast.datasets.utils",
        "peekOfCode": "def as_binary_vector(labels, num_classes):\n    \"\"\"\n    Construct binary label vector given a list of label indices.\n    Args:\n        labels (list): The input label list.\n        num_classes (int): Number of classes of the label vector.\n    Returns:\n        labels (numpy array): the resulting binary vector.\n    \"\"\"\n    label_arr = np.zeros((num_classes,))",
        "detail": "MorphMLP.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "aggregate_labels",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.utils",
        "description": "MorphMLP.slowfast.datasets.utils",
        "peekOfCode": "def aggregate_labels(label_list):\n    \"\"\"\n    Join a list of label list.\n    Args:\n        labels (list): The input label list.\n    Returns:\n        labels (list): The joint list of all lists in input.\n    \"\"\"\n    all_labels = []\n    for labels in label_list:",
        "detail": "MorphMLP.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "convert_to_video_level_labels",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.utils",
        "description": "MorphMLP.slowfast.datasets.utils",
        "peekOfCode": "def convert_to_video_level_labels(labels):\n    \"\"\"\n    Aggregate annotations from all frames of a video to form video-level labels.\n    Args:\n        labels (list): The input label list.\n    Returns:\n        labels (list): Same as input, but with each label replaced by\n        a video-level one.\n    \"\"\"\n    for video_id in range(len(labels)):",
        "detail": "MorphMLP.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "load_image_lists",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.utils",
        "description": "MorphMLP.slowfast.datasets.utils",
        "peekOfCode": "def load_image_lists(frame_list_file, prefix=\"\", return_list=False):\n    \"\"\"\n    Load image paths and labels from a \"frame list\".\n    Each line of the frame list contains:\n    `original_vido_id video_id frame_id path labels`\n    Args:\n        frame_list_file (string): path to the frame list.\n        prefix (str): the prefix for the path.\n        return_list (bool): if True, return a list. If False, return a dict.\n    Returns:",
        "detail": "MorphMLP.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "tensor_normalize",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.utils",
        "description": "MorphMLP.slowfast.datasets.utils",
        "peekOfCode": "def tensor_normalize(tensor, mean, std, func=None):\n    \"\"\"\n    Normalize a given tensor by subtracting the mean and dividing the std.\n    Args:\n        tensor (tensor): tensor to normalize.\n        mean (tensor or list): mean value to subtract.\n        std (tensor or list): std to divide.\n    \"\"\"\n    if tensor.dtype == torch.uint8:\n        tensor = tensor.float()",
        "detail": "MorphMLP.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "get_random_sampling_rate",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.utils",
        "description": "MorphMLP.slowfast.datasets.utils",
        "peekOfCode": "def get_random_sampling_rate(long_cycle_sampling_rate, sampling_rate):\n    \"\"\"\n    When multigrid training uses a fewer number of frames, we randomly\n    increase the sampling rate so that some clips cover the original span.\n    \"\"\"\n    if long_cycle_sampling_rate > 0:\n        assert long_cycle_sampling_rate >= sampling_rate\n        return random.randint(sampling_rate, long_cycle_sampling_rate)\n    else:\n        return sampling_rate",
        "detail": "MorphMLP.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "revert_tensor_normalize",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.utils",
        "description": "MorphMLP.slowfast.datasets.utils",
        "peekOfCode": "def revert_tensor_normalize(tensor, mean, std):\n    \"\"\"\n    Revert normalization for a given tensor by multiplying by the std and adding the mean.\n    Args:\n        tensor (tensor): tensor to revert normalization.\n        mean (tensor or list): mean value to add.\n        std (tensor or list): std to multiply.\n    \"\"\"\n    if type(mean) == list:\n        mean = torch.tensor(mean)",
        "detail": "MorphMLP.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "create_sampler",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.utils",
        "description": "MorphMLP.slowfast.datasets.utils",
        "peekOfCode": "def create_sampler(dataset, shuffle, cfg):\n    \"\"\"\n    Create sampler for the given dataset.\n    Args:\n        dataset (torch.utils.data.Dataset): the given dataset.\n        shuffle (bool): set to ``True`` to have the data reshuffled\n            at every epoch.\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n    Returns:",
        "detail": "MorphMLP.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "loader_worker_init_fn",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.utils",
        "description": "MorphMLP.slowfast.datasets.utils",
        "peekOfCode": "def loader_worker_init_fn(dataset):\n    \"\"\"\n    Create init function passed to pytorch data loader.\n    Args:\n        dataset (torch.utils.data.Dataset): the given dataset.\n    \"\"\"\n    return None",
        "detail": "MorphMLP.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.datasets.utils",
        "description": "MorphMLP.slowfast.datasets.utils",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef retry_load_images(image_paths, retry=10, backend=\"pytorch\"):\n    \"\"\"\n    This function is to load images with support of retrying for failed load.\n    Args:\n        image_paths (list): paths of images needed to be loaded.\n        retry (int, optional): maximum time of loading retrying. Defaults to 10.\n        backend (str): `pytorch` or `cv2`.\n    Returns:\n        imgs (list): list of loaded images.",
        "detail": "MorphMLP.slowfast.datasets.utils",
        "documentation": {}
    },
    {
        "label": "get_video_container",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.datasets.video_container",
        "description": "MorphMLP.slowfast.datasets.video_container",
        "peekOfCode": "def get_video_container(path_to_vid, multi_thread_decode=False, backend=\"torchvision\"):\n    \"\"\"\n    Given the path to the video, return the pyav video container.\n    Args:\n        path_to_vid (str): path to the video.\n        multi_thread_decode (bool): if True, perform multi-thread decoding.\n        backend (str): decoder backend, options include `pyav` and\n            `torchvision`, default is `pyav`.\n    Returns:\n        container (container): video container.",
        "detail": "MorphMLP.slowfast.datasets.video_container",
        "documentation": {}
    },
    {
        "label": "MultiScaleAttention",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.attention",
        "description": "MorphMLP.slowfast.models.attention",
        "peekOfCode": "class MultiScaleAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        input_size,\n        num_heads=8,\n        qkv_bias=False,\n        drop_rate=0.0,\n        kernel_q=(1, 1, 1),",
        "detail": "MorphMLP.slowfast.models.attention",
        "documentation": {}
    },
    {
        "label": "MultiScaleBlock",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.attention",
        "description": "MorphMLP.slowfast.models.attention",
        "peekOfCode": "class MultiScaleBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        num_heads,\n        input_size,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,",
        "detail": "MorphMLP.slowfast.models.attention",
        "documentation": {}
    },
    {
        "label": "attention_pool",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.models.attention",
        "description": "MorphMLP.slowfast.models.attention",
        "peekOfCode": "def attention_pool(tensor, pool, thw_shape, has_cls_embed=True, norm=None):\n    if pool is None:\n        return tensor, thw_shape\n    tensor_dim = tensor.ndim\n    if tensor_dim == 4:\n        pass\n    elif tensor_dim == 3:\n        tensor = tensor.unsqueeze(1)\n    else:\n        raise NotImplementedError(f\"Unsupported input dimension {tensor.shape}\")",
        "detail": "MorphMLP.slowfast.models.attention",
        "documentation": {}
    },
    {
        "label": "get_rel_pos",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.models.attention",
        "description": "MorphMLP.slowfast.models.attention",
        "peekOfCode": "def get_rel_pos(rel_pos, d):\n    if isinstance(d, int):\n        ori_d = rel_pos.shape[0]\n        if ori_d == d:\n            return rel_pos\n        else:\n            # Interpolate rel pos.\n            new_pos_embed = F.interpolate(\n                rel_pos.reshape(1, ori_d, -1).permute(0, 2, 1),\n                size=d,",
        "detail": "MorphMLP.slowfast.models.attention",
        "documentation": {}
    },
    {
        "label": "cal_rel_pos_spatial",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.models.attention",
        "description": "MorphMLP.slowfast.models.attention",
        "peekOfCode": "def cal_rel_pos_spatial(\n    attn, q, k, has_cls_embed, q_shape, k_shape, rel_pos_h, rel_pos_w\n):\n    \"\"\"\n    Decomposed Spatial Relative Positional Embeddings.\n    \"\"\"\n    sp_idx = 1 if has_cls_embed else 0\n    q_t, q_h, q_w = q_shape\n    k_t, k_h, k_w = k_shape\n    dh = int(2 * max(q_h, k_h) - 1)",
        "detail": "MorphMLP.slowfast.models.attention",
        "documentation": {}
    },
    {
        "label": "cal_rel_pos_temporal",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.models.attention",
        "description": "MorphMLP.slowfast.models.attention",
        "peekOfCode": "def cal_rel_pos_temporal(attn, q, has_cls_embed, q_shape, k_shape, rel_pos_t):\n    \"\"\"\n    Temporal Relative Positional Embeddings.\n    \"\"\"\n    sp_idx = 1 if has_cls_embed else 0\n    q_t, q_h, q_w = q_shape\n    k_t, k_h, k_w = k_shape\n    dt = int(2 * max(q_t, k_t) - 1)\n    # Intepolate rel pos if needed.\n    rel_pos_t = get_rel_pos(rel_pos_t, dt)",
        "detail": "MorphMLP.slowfast.models.attention",
        "documentation": {}
    },
    {
        "label": "SubBatchNorm3d",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.batchnorm_helper",
        "description": "MorphMLP.slowfast.models.batchnorm_helper",
        "peekOfCode": "class SubBatchNorm3d(nn.Module):\n    \"\"\"\n    The standard BN layer computes stats across all examples in a GPU. In some\n    cases it is desirable to compute stats across only a subset of examples\n    (e.g., in multigrid training https://arxiv.org/abs/1912.00998).\n    SubBatchNorm3d splits the batch dimension into N splits, and run BN on\n    each of them separately (so that the stats are computed on each subset of\n    examples (1/N of batch) independently. During evaluation, it aggregates\n    the stats from all splits into one BN.\n    \"\"\"",
        "detail": "MorphMLP.slowfast.models.batchnorm_helper",
        "documentation": {}
    },
    {
        "label": "get_norm",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.models.batchnorm_helper",
        "description": "MorphMLP.slowfast.models.batchnorm_helper",
        "peekOfCode": "def get_norm(cfg):\n    \"\"\"\n    Args:\n        cfg (CfgNode): model building configs, details are in the comments of\n            the config file.\n    Returns:\n        nn.Module: the normalization layer.\n    \"\"\"\n    if cfg.BN.NORM_TYPE in {\"batchnorm\", \"sync_batchnorm_apex\"}:\n        return nn.BatchNorm3d",
        "detail": "MorphMLP.slowfast.models.batchnorm_helper",
        "documentation": {}
    },
    {
        "label": "build_model",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.models.build",
        "description": "MorphMLP.slowfast.models.build",
        "peekOfCode": "def build_model(cfg, gpu_id=None):\n    \"\"\"\n    Builds the video model.\n    Args:\n        cfg (configs): configs that contains the hyper-parameters to build the\n        backbone. Details can be seen in slowfast/config/defaults.py.\n        gpu_id (Optional[int]): specify the gpu index to build model.\n    \"\"\"\n    if torch.cuda.is_available():\n        assert (",
        "detail": "MorphMLP.slowfast.models.build",
        "documentation": {}
    },
    {
        "label": "MODEL_REGISTRY",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.models.build",
        "description": "MorphMLP.slowfast.models.build",
        "peekOfCode": "MODEL_REGISTRY = Registry(\"MODEL\")\nMODEL_REGISTRY.__doc__ = \"\"\"\nRegistry for video model.\nThe registered object will be called with `obj(cfg)`.\nThe call should return a `torch.nn.Module` object.\n\"\"\"\ndef build_model(cfg, gpu_id=None):\n    \"\"\"\n    Builds the video model.\n    Args:",
        "detail": "MorphMLP.slowfast.models.build",
        "documentation": {}
    },
    {
        "label": "MODEL_REGISTRY.__doc__",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.models.build",
        "description": "MorphMLP.slowfast.models.build",
        "peekOfCode": "MODEL_REGISTRY.__doc__ = \"\"\"\nRegistry for video model.\nThe registered object will be called with `obj(cfg)`.\nThe call should return a `torch.nn.Module` object.\n\"\"\"\ndef build_model(cfg, gpu_id=None):\n    \"\"\"\n    Builds the video model.\n    Args:\n        cfg (configs): configs that contains the hyper-parameters to build the",
        "detail": "MorphMLP.slowfast.models.build",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.common",
        "description": "MorphMLP.slowfast.models.common",
        "peekOfCode": "class Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop_rate=0.0,\n    ):\n        super().__init__()",
        "detail": "MorphMLP.slowfast.models.common",
        "documentation": {}
    },
    {
        "label": "Permute",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.common",
        "description": "MorphMLP.slowfast.models.common",
        "peekOfCode": "class Permute(nn.Module):\n    def __init__(self, dims):\n        super().__init__()\n        self.dims = dims\n    def forward(self, x):\n        return x.permute(*self.dims)\ndef drop_path(x, drop_prob: float = 0.0, training: bool = False):\n    \"\"\"\n    Stochastic Depth per sample.\n    \"\"\"",
        "detail": "MorphMLP.slowfast.models.common",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.common",
        "description": "MorphMLP.slowfast.models.common",
        "peekOfCode": "class DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)",
        "detail": "MorphMLP.slowfast.models.common",
        "documentation": {}
    },
    {
        "label": "drop_path",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.models.common",
        "description": "MorphMLP.slowfast.models.common",
        "peekOfCode": "def drop_path(x, drop_prob: float = 0.0, training: bool = False):\n    \"\"\"\n    Stochastic Depth per sample.\n    \"\"\"\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (\n        x.ndim - 1\n    )  # work with diff dim tensors, not just 2D ConvNets",
        "detail": "MorphMLP.slowfast.models.common",
        "documentation": {}
    },
    {
        "label": "ContrastiveModel",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.contrastive",
        "description": "MorphMLP.slowfast.models.contrastive",
        "peekOfCode": "class ContrastiveModel(nn.Module):\n    \"\"\"\n    Contrastive Model, currently mainly focused on memory bank and CSC.\n    \"\"\"\n    def __init__(self, cfg):\n        super(ContrastiveModel, self).__init__()\n        # Construct the model.\n        self.backbone = _MODEL_TYPES[cfg.MODEL.ARCH](cfg)\n        self.type = cfg.CONTRASTIVE.TYPE\n        self.T = cfg.CONTRASTIVE.T",
        "detail": "MorphMLP.slowfast.models.contrastive",
        "documentation": {}
    },
    {
        "label": "Normalize",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.contrastive",
        "description": "MorphMLP.slowfast.models.contrastive",
        "peekOfCode": "class Normalize(nn.Module):\n    def __init__(self, power=2, dim=1):\n        super(Normalize, self).__init__()\n        self.dim = dim\n        self.power = power\n    def forward(self, x):\n        norm = (\n            x.pow(self.power).sum(self.dim, keepdim=True).pow(1.0 / self.power)\n        )\n        out = x.div(norm)",
        "detail": "MorphMLP.slowfast.models.contrastive",
        "documentation": {}
    },
    {
        "label": "Memory",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.contrastive",
        "description": "MorphMLP.slowfast.models.contrastive",
        "peekOfCode": "class Memory(nn.Module):\n    def __init__(self, length, duration, dim, cfg):\n        super(Memory, self).__init__()\n        self.length = length\n        self.duration = duration\n        self.dim = dim\n        stdv = 1.0 / math.sqrt(dim / 3)\n        self.register_buffer(\n            \"memory\",\n            torch.rand(length, duration, dim).mul_(2 * stdv).add_(-stdv),",
        "detail": "MorphMLP.slowfast.models.contrastive",
        "documentation": {}
    },
    {
        "label": "Memory1D",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.contrastive",
        "description": "MorphMLP.slowfast.models.contrastive",
        "peekOfCode": "class Memory1D(nn.Module):\n    def __init__(self, length, duration, dim, cfg):\n        super(Memory1D, self).__init__()\n        assert duration == 1\n        self.length = length\n        self.duration = duration\n        self.dim = dim\n        stdv = 1.0 / math.sqrt(dim / 3)\n        self.register_buffer(\n            \"memory\", torch.rand(length, dim).mul_(2 * stdv).add_(-stdv)",
        "detail": "MorphMLP.slowfast.models.contrastive",
        "documentation": {}
    },
    {
        "label": "l2_loss",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.models.contrastive",
        "description": "MorphMLP.slowfast.models.contrastive",
        "peekOfCode": "def l2_loss(x, y):\n    return 2 - 2 * (x * y).sum(dim=-1)\nclass Normalize(nn.Module):\n    def __init__(self, power=2, dim=1):\n        super(Normalize, self).__init__()\n        self.dim = dim\n        self.power = power\n    def forward(self, x):\n        norm = (\n            x.pow(self.power).sum(self.dim, keepdim=True).pow(1.0 / self.power)",
        "detail": "MorphMLP.slowfast.models.contrastive",
        "documentation": {}
    },
    {
        "label": "cancel_swav_gradients",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.models.contrastive",
        "description": "MorphMLP.slowfast.models.contrastive",
        "peekOfCode": "def cancel_swav_gradients(model, cfg, epoch_exact):\n    # cancel some gradients in first epoch of SwAV\n    if (\n        cfg.MODEL.MODEL_NAME == \"ContrastiveModel\"\n        and cfg.CONTRASTIVE.TYPE == \"swav\"\n        and epoch_exact <= 1.0\n    ):\n        for name, p in model.named_parameters():\n            if \"swav_prototypes\" in name:\n                p.grad = None",
        "detail": "MorphMLP.slowfast.models.contrastive",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.models.contrastive",
        "description": "MorphMLP.slowfast.models.contrastive",
        "peekOfCode": "logger = logging.get_logger(__name__)\n# Supported model types\n_MODEL_TYPES = {\n    \"slowfast\": SlowFast,\n    \"slow\": ResNet,\n    \"c2d\": ResNet,\n    \"i3d\": ResNet,\n    \"slow_c2d\": ResNet,\n    \"x3d\": X3D,\n    \"mvit\": MViT,",
        "detail": "MorphMLP.slowfast.models.contrastive",
        "documentation": {}
    },
    {
        "label": "_MODEL_TYPES",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.models.contrastive",
        "description": "MorphMLP.slowfast.models.contrastive",
        "peekOfCode": "_MODEL_TYPES = {\n    \"slowfast\": SlowFast,\n    \"slow\": ResNet,\n    \"c2d\": ResNet,\n    \"i3d\": ResNet,\n    \"slow_c2d\": ResNet,\n    \"x3d\": X3D,\n    \"mvit\": MViT,\n}\n@MODEL_REGISTRY.register()",
        "detail": "MorphMLP.slowfast.models.contrastive",
        "documentation": {}
    },
    {
        "label": "ResNetRoIHead",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.head_helper",
        "description": "MorphMLP.slowfast.models.head_helper",
        "peekOfCode": "class ResNetRoIHead(nn.Module):\n    \"\"\"\n    ResNe(X)t RoI head.\n    \"\"\"\n    def __init__(\n        self,\n        dim_in,\n        num_classes,\n        pool_size,\n        resolution,",
        "detail": "MorphMLP.slowfast.models.head_helper",
        "documentation": {}
    },
    {
        "label": "MLPHead",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.head_helper",
        "description": "MorphMLP.slowfast.models.head_helper",
        "peekOfCode": "class MLPHead(nn.Module):\n    def __init__(\n        self,\n        dim_in,\n        dim_out,\n        mlp_dim,\n        num_layers,\n        bn_on=False,\n        bias=True,\n        flatten=False,",
        "detail": "MorphMLP.slowfast.models.head_helper",
        "documentation": {}
    },
    {
        "label": "ResNetBasicHead",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.head_helper",
        "description": "MorphMLP.slowfast.models.head_helper",
        "peekOfCode": "class ResNetBasicHead(nn.Module):\n    \"\"\"\n    ResNe(X)t 3D head.\n    This layer performs a fully-connected projection during training, when the\n    input size is 1x1x1. It performs a convolutional projection during testing\n    when the input size is larger than 1x1x1. If the inputs are from multiple\n    different pathways, the inputs will be concatenated after pooling.\n    \"\"\"\n    def __init__(\n        self,",
        "detail": "MorphMLP.slowfast.models.head_helper",
        "documentation": {}
    },
    {
        "label": "X3DHead",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.head_helper",
        "description": "MorphMLP.slowfast.models.head_helper",
        "peekOfCode": "class X3DHead(nn.Module):\n    \"\"\"\n    X3D head.\n    This layer performs a fully-connected projection during training, when the\n    input size is 1x1x1. It performs a convolutional projection during testing\n    when the input size is larger than 1x1x1. If the inputs are from multiple\n    different pathways, the inputs will be concatenated after pooling.\n    \"\"\"\n    def __init__(\n        self,",
        "detail": "MorphMLP.slowfast.models.head_helper",
        "documentation": {}
    },
    {
        "label": "TransformerBasicHead",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.head_helper",
        "description": "MorphMLP.slowfast.models.head_helper",
        "peekOfCode": "class TransformerBasicHead(nn.Module):\n    \"\"\"\n    BasicHead. No pool.\n    \"\"\"\n    def __init__(\n        self,\n        dim_in,\n        num_classes,\n        dropout_rate=0.0,\n        act_func=\"softmax\",",
        "detail": "MorphMLP.slowfast.models.head_helper",
        "documentation": {}
    },
    {
        "label": "ContrastiveLoss",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.losses",
        "description": "MorphMLP.slowfast.models.losses",
        "peekOfCode": "class ContrastiveLoss(nn.Module):\n    def __init__(self, reduction=\"mean\"):\n        super(ContrastiveLoss, self).__init__()\n        self.reduction = reduction\n    def forward(self, inputs, dummy_labels=None):\n        targets = torch.zeros(inputs.shape[0], dtype=torch.long).cuda()\n        loss = nn.CrossEntropyLoss(reduction=self.reduction).cuda()(\n            inputs, targets\n        )\n        return loss",
        "detail": "MorphMLP.slowfast.models.losses",
        "documentation": {}
    },
    {
        "label": "get_loss_func",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.models.losses",
        "description": "MorphMLP.slowfast.models.losses",
        "peekOfCode": "def get_loss_func(loss_name):\n    \"\"\"\n    Retrieve the loss given the loss name.\n    Args (int):\n        loss_name: the name of the loss to use.\n    \"\"\"\n    if loss_name not in _LOSSES.keys():\n        raise NotImplementedError(\"Loss {} is not supported\".format(loss_name))\n    return _LOSSES[loss_name]",
        "detail": "MorphMLP.slowfast.models.losses",
        "documentation": {}
    },
    {
        "label": "_LOSSES",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.models.losses",
        "description": "MorphMLP.slowfast.models.losses",
        "peekOfCode": "_LOSSES = {\n    \"cross_entropy\": nn.CrossEntropyLoss,\n    \"bce\": nn.BCELoss,\n    \"bce_logit\": nn.BCEWithLogitsLoss,\n    \"soft_cross_entropy\": partial(SoftTargetCrossEntropyLoss, normalize_targets=False),\n    \"contrastive_loss\": ContrastiveLoss,\n}\ndef get_loss_func(loss_name):\n    \"\"\"\n    Retrieve the loss given the loss name.",
        "detail": "MorphMLP.slowfast.models.losses",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.morphmlp",
        "description": "MorphMLP.slowfast.models.morphmlp",
        "peekOfCode": "class Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n    def forward(self, x):",
        "detail": "MorphMLP.slowfast.models.morphmlp",
        "documentation": {}
    },
    {
        "label": "MorphFC_S2",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.morphmlp",
        "description": "MorphMLP.slowfast.models.morphmlp",
        "peekOfCode": "class MorphFC_S2(nn.Module):\n    def __init__(self, dim, segment_dim=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.segment_dim = segment_dim\n        self.mlp_c = nn.Linear(dim, dim, bias=qkv_bias)\n        self.mlp_h = nn.Linear(dim, dim, bias=qkv_bias)\n        self.reweight = Mlp(dim, dim // 4, dim * 2)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n    def forward(self, x):",
        "detail": "MorphMLP.slowfast.models.morphmlp",
        "documentation": {}
    },
    {
        "label": "MorphFC_S",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.morphmlp",
        "description": "MorphMLP.slowfast.models.morphmlp",
        "peekOfCode": "class MorphFC_S(nn.Module):\n    def __init__(self, dim, segment_dim=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.segment_dim = segment_dim\n        self.mlp_h = nn.Linear(dim, dim, bias=qkv_bias)\n        self.mlp_w = nn.Linear(dim, dim, bias=qkv_bias)\n        self.mlp_c = nn.Linear(dim, dim, bias=qkv_bias)\n        # init weight problem\n        self.reweight = Mlp(dim, dim // 4, dim * 3)\n        self.proj = nn.Linear(dim, dim)",
        "detail": "MorphMLP.slowfast.models.morphmlp",
        "documentation": {}
    },
    {
        "label": "MorphFC_T",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.morphmlp",
        "description": "MorphMLP.slowfast.models.morphmlp",
        "peekOfCode": "class MorphFC_T(nn.Module):\n    def __init__(self, dim, segment_dim=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        global t_stride\n        self.segment_dim = 8\n        dim2 = dim\n        self.mlp_t = nn.Linear(dim2, dim2, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n    def forward(self, x):",
        "detail": "MorphMLP.slowfast.models.morphmlp",
        "documentation": {}
    },
    {
        "label": "PermutatorBlock",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.morphmlp",
        "description": "MorphMLP.slowfast.models.morphmlp",
        "peekOfCode": "class PermutatorBlock(nn.Module):\n    def __init__(self, dim, segment_dim, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, skip_lam=1.0, mlp_fn=MorphFC_S):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.t_norm1 = norm_layer(dim)\n        self.t_fc = MorphFC_T(dim, segment_dim=segment_dim, qkv_bias=qkv_bias, qk_scale=None,\n                                        attn_drop=attn_drop)\n        self.fc = mlp_fn(dim, segment_dim=segment_dim, qkv_bias=qkv_bias, qk_scale=None, attn_drop=attn_drop)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here",
        "detail": "MorphMLP.slowfast.models.morphmlp",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.morphmlp",
        "description": "MorphMLP.slowfast.models.morphmlp",
        "peekOfCode": "class PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        self.proj1 = conv_3xnxn(in_chans, embed_dim//2, kernel_size=3, stride=2)\n        self.norm1= nn.BatchNorm3d(embed_dim//2)\n        self.act=nn.GELU()\n        self.proj2 = conv_1xnxn(embed_dim//2, embed_dim, kernel_size=3, stride=2)\n        self.norm2 = nn.BatchNorm3d(embed_dim)",
        "detail": "MorphMLP.slowfast.models.morphmlp",
        "documentation": {}
    },
    {
        "label": "Downsample",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.morphmlp",
        "description": "MorphMLP.slowfast.models.morphmlp",
        "peekOfCode": "class Downsample(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n    def __init__(self, in_embed_dim, out_embed_dim, patch_size):\n        super().__init__()\n        self.proj = conv_1xnxn(in_embed_dim, out_embed_dim, kernel_size=3, stride=2)\n        self.norm=nn.LayerNorm(out_embed_dim)\n    def forward(self, x):\n        x = x.permute(0, 4, 1, 2, 3)\n        x = self.proj(x)  # B, C, T, H, W",
        "detail": "MorphMLP.slowfast.models.morphmlp",
        "documentation": {}
    },
    {
        "label": "MorphMLP",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.morphmlp",
        "description": "MorphMLP.slowfast.models.morphmlp",
        "peekOfCode": "class MorphMLP(nn.Module):\n    \"\"\" MorphMLP\n    \"\"\"\n    def __init__(self, cfg):\n        super().__init__()\n        global t_stride\n        num_classes = cfg.MODEL.NUM_CLASSES\n        img_size = cfg.DATA.TRAIN_CROP_SIZE\n        in_chans = cfg.DATA.INPUT_CHANNEL_NUM[0]\n        layers = cfg.MORPH.LAYERS",
        "detail": "MorphMLP.slowfast.models.morphmlp",
        "documentation": {}
    },
    {
        "label": "conv_3xnxn",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.models.morphmlp",
        "description": "MorphMLP.slowfast.models.morphmlp",
        "peekOfCode": "def conv_3xnxn(inp, oup, kernel_size=3, stride=3):\n    return nn.Conv3d(inp, oup, (3, kernel_size, kernel_size), (2, stride, stride), (1, 1, 1))\ndef conv_1xnxn(inp, oup, kernel_size=3, stride=3):\n    return nn.Conv3d(inp, oup, (1, kernel_size, kernel_size), (1, stride, stride), (0, 1, 1))\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)",
        "detail": "MorphMLP.slowfast.models.morphmlp",
        "documentation": {}
    },
    {
        "label": "conv_1xnxn",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.models.morphmlp",
        "description": "MorphMLP.slowfast.models.morphmlp",
        "peekOfCode": "def conv_1xnxn(inp, oup, kernel_size=3, stride=3):\n    return nn.Conv3d(inp, oup, (1, kernel_size, kernel_size), (1, stride, stride), (0, 1, 1))\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)",
        "detail": "MorphMLP.slowfast.models.morphmlp",
        "documentation": {}
    },
    {
        "label": "t_stride",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.models.morphmlp",
        "description": "MorphMLP.slowfast.models.morphmlp",
        "peekOfCode": "t_stride = 1\nmodel_path = {\n    's_sip_nods_s4': '/mnt/WXRC0020/users/junhao.zhang/tmp/slowfast/tools/s_sip_nods.pth',\n}\ndef conv_3xnxn(inp, oup, kernel_size=3, stride=3):\n    return nn.Conv3d(inp, oup, (3, kernel_size, kernel_size), (2, stride, stride), (1, 1, 1))\ndef conv_1xnxn(inp, oup, kernel_size=3, stride=3):\n    return nn.Conv3d(inp, oup, (1, kernel_size, kernel_size), (1, stride, stride), (0, 1, 1))\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):",
        "detail": "MorphMLP.slowfast.models.morphmlp",
        "documentation": {}
    },
    {
        "label": "model_path",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.models.morphmlp",
        "description": "MorphMLP.slowfast.models.morphmlp",
        "peekOfCode": "model_path = {\n    's_sip_nods_s4': '/mnt/WXRC0020/users/junhao.zhang/tmp/slowfast/tools/s_sip_nods.pth',\n}\ndef conv_3xnxn(inp, oup, kernel_size=3, stride=3):\n    return nn.Conv3d(inp, oup, (3, kernel_size, kernel_size), (2, stride, stride), (1, 1, 1))\ndef conv_1xnxn(inp, oup, kernel_size=3, stride=3):\n    return nn.Conv3d(inp, oup, (1, kernel_size, kernel_size), (1, stride, stride), (0, 1, 1))\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()",
        "detail": "MorphMLP.slowfast.models.morphmlp",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.morphmlp_32",
        "description": "MorphMLP.slowfast.models.morphmlp_32",
        "peekOfCode": "class Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n    def forward(self, x):",
        "detail": "MorphMLP.slowfast.models.morphmlp_32",
        "documentation": {}
    },
    {
        "label": "MorphFC_S2",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.morphmlp_32",
        "description": "MorphMLP.slowfast.models.morphmlp_32",
        "peekOfCode": "class MorphFC_S2(nn.Module):\n    def __init__(self, dim, segment_dim=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.segment_dim = segment_dim\n        self.mlp_c = nn.Linear(dim, dim, bias=qkv_bias)\n        self.mlp_h = nn.Linear(dim, dim, bias=qkv_bias)\n        self.reweight = Mlp(dim, dim // 4, dim * 2)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n    def forward(self, x):",
        "detail": "MorphMLP.slowfast.models.morphmlp_32",
        "documentation": {}
    },
    {
        "label": "MorphFC_S",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.morphmlp_32",
        "description": "MorphMLP.slowfast.models.morphmlp_32",
        "peekOfCode": "class MorphFC_S(nn.Module):\n    def __init__(self, dim, segment_dim=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.segment_dim = segment_dim\n        self.mlp_h = nn.Linear(dim, dim, bias=qkv_bias)\n        self.mlp_w = nn.Linear(dim, dim, bias=qkv_bias)\n        self.mlp_c = nn.Linear(dim, dim, bias=qkv_bias)\n        # init weight problem\n        self.reweight = Mlp(dim, dim // 4, dim * 3)\n        self.proj = nn.Linear(dim, dim)",
        "detail": "MorphMLP.slowfast.models.morphmlp_32",
        "documentation": {}
    },
    {
        "label": "MorphFC_T",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.morphmlp_32",
        "description": "MorphMLP.slowfast.models.morphmlp_32",
        "peekOfCode": "class MorphFC_T(nn.Module):\n    def __init__(self, dim, segment_dim=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        global t_stride\n        self.segment_dim = 16\n        dim2 = dim\n        if dim == 392:\n            dim2 = 392 // 14 * 16\n            self.segment_dim=14\n        self.mlp_t = nn.Linear(dim2, dim2, bias=qkv_bias)",
        "detail": "MorphMLP.slowfast.models.morphmlp_32",
        "documentation": {}
    },
    {
        "label": "PermutatorBlock",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.morphmlp_32",
        "description": "MorphMLP.slowfast.models.morphmlp_32",
        "peekOfCode": "class PermutatorBlock(nn.Module):\n    def __init__(self, dim, segment_dim, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, skip_lam=1.0, mlp_fn=MorphFC_S):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.t_norm1 = norm_layer(dim)\n        self.t_fc = MorphFC_T(dim, segment_dim=segment_dim, qkv_bias=qkv_bias, qk_scale=None,\n                                        attn_drop=attn_drop)\n        self.fc = mlp_fn(dim, segment_dim=segment_dim, qkv_bias=qkv_bias, qk_scale=None, attn_drop=attn_drop)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here",
        "detail": "MorphMLP.slowfast.models.morphmlp_32",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.morphmlp_32",
        "description": "MorphMLP.slowfast.models.morphmlp_32",
        "peekOfCode": "class PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        self.proj1 = conv_3xnxn(in_chans, embed_dim//2, kernel_size=3, stride=2)\n        self.norm1= nn.BatchNorm3d(embed_dim//2)\n        self.act=nn.GELU()\n        self.proj2 = conv_1xnxn(embed_dim//2, embed_dim, kernel_size=3, stride=2)\n        self.norm2 = nn.BatchNorm3d(embed_dim)",
        "detail": "MorphMLP.slowfast.models.morphmlp_32",
        "documentation": {}
    },
    {
        "label": "Downsample",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.morphmlp_32",
        "description": "MorphMLP.slowfast.models.morphmlp_32",
        "peekOfCode": "class Downsample(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n    def __init__(self, in_embed_dim, out_embed_dim, patch_size):\n        super().__init__()\n        self.proj = conv_1xnxn(in_embed_dim, out_embed_dim, kernel_size=3, stride=2)\n        self.norm=nn.LayerNorm(out_embed_dim)\n    def forward(self, x):\n        x = x.permute(0, 4, 1, 2, 3)\n        x = self.proj(x)  # B, C, T, H, W",
        "detail": "MorphMLP.slowfast.models.morphmlp_32",
        "documentation": {}
    },
    {
        "label": "MorphMLP_32",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.morphmlp_32",
        "description": "MorphMLP.slowfast.models.morphmlp_32",
        "peekOfCode": "class MorphMLP_32(nn.Module):\n    \"\"\" MorphMLP\n    \"\"\"\n    def __init__(self, cfg):\n        super().__init__()\n        global t_stride\n        num_classes = cfg.MODEL.NUM_CLASSES\n        img_size = cfg.DATA.TRAIN_CROP_SIZE\n        in_chans = cfg.DATA.INPUT_CHANNEL_NUM[0]\n        layers = cfg.MORPH.LAYERS",
        "detail": "MorphMLP.slowfast.models.morphmlp_32",
        "documentation": {}
    },
    {
        "label": "conv_3xnxn",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.models.morphmlp_32",
        "description": "MorphMLP.slowfast.models.morphmlp_32",
        "peekOfCode": "def conv_3xnxn(inp, oup, kernel_size=3, stride=3):\n    return nn.Conv3d(inp, oup, (3, kernel_size, kernel_size), (2, stride, stride), (1, 1, 1))\ndef conv_1xnxn(inp, oup, kernel_size=3, stride=3):\n    return nn.Conv3d(inp, oup, (1, kernel_size, kernel_size), (1, stride, stride), (0, 1, 1))\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)",
        "detail": "MorphMLP.slowfast.models.morphmlp_32",
        "documentation": {}
    },
    {
        "label": "conv_1xnxn",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.models.morphmlp_32",
        "description": "MorphMLP.slowfast.models.morphmlp_32",
        "peekOfCode": "def conv_1xnxn(inp, oup, kernel_size=3, stride=3):\n    return nn.Conv3d(inp, oup, (1, kernel_size, kernel_size), (1, stride, stride), (0, 1, 1))\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)",
        "detail": "MorphMLP.slowfast.models.morphmlp_32",
        "documentation": {}
    },
    {
        "label": "t_stride",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.models.morphmlp_32",
        "description": "MorphMLP.slowfast.models.morphmlp_32",
        "peekOfCode": "t_stride = 1\ndef conv_3xnxn(inp, oup, kernel_size=3, stride=3):\n    return nn.Conv3d(inp, oup, (3, kernel_size, kernel_size), (2, stride, stride), (1, 1, 1))\ndef conv_1xnxn(inp, oup, kernel_size=3, stride=3):\n    return nn.Conv3d(inp, oup, (1, kernel_size, kernel_size), (1, stride, stride), (0, 1, 1))\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features",
        "detail": "MorphMLP.slowfast.models.morphmlp_32",
        "documentation": {}
    },
    {
        "label": "Nonlocal",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.nonlocal_helper",
        "description": "MorphMLP.slowfast.models.nonlocal_helper",
        "peekOfCode": "class Nonlocal(nn.Module):\n    \"\"\"\n    Builds Non-local Neural Networks as a generic family of building\n    blocks for capturing long-range dependencies. Non-local Network\n    computes the response at a position as a weighted sum of the\n    features at all positions. This building block can be plugged into\n    many computer vision architectures.\n    More details in the paper: https://arxiv.org/pdf/1711.07971.pdf\n    \"\"\"\n    def __init__(",
        "detail": "MorphMLP.slowfast.models.nonlocal_helper",
        "documentation": {}
    },
    {
        "label": "SE",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.operators",
        "description": "MorphMLP.slowfast.models.operators",
        "peekOfCode": "class SE(nn.Module):\n    \"\"\"Squeeze-and-Excitation (SE) block w/ Swish: AvgPool, FC, Swish, FC, Sigmoid.\"\"\"\n    def _round_width(self, width, multiplier, min_width=8, divisor=8):\n        \"\"\"\n        Round width of filters based on width multiplier\n        Args:\n            width (int): the channel dimensions of the input.\n            multiplier (float): the multiplication factor.\n            min_width (int): the minimum width after multiplication.\n            divisor (int): the new width should be dividable by divisor.",
        "detail": "MorphMLP.slowfast.models.operators",
        "documentation": {}
    },
    {
        "label": "construct_optimizer",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.models.optimizer",
        "description": "MorphMLP.slowfast.models.optimizer",
        "peekOfCode": "def construct_optimizer(model, cfg):\n    \"\"\"\n    Construct a stochastic gradient descent or ADAM optimizer with momentum.\n    Details can be found in:\n    Herbert Robbins, and Sutton Monro. \"A stochastic approximation method.\"\n    and\n    Diederik P.Kingma, and Jimmy Ba.\n    \"Adam: A Method for Stochastic Optimization.\"\n    Args:\n        model (model): model to perform stochastic gradient descent",
        "detail": "MorphMLP.slowfast.models.optimizer",
        "documentation": {}
    },
    {
        "label": "get_epoch_lr",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.models.optimizer",
        "description": "MorphMLP.slowfast.models.optimizer",
        "peekOfCode": "def get_epoch_lr(cur_epoch, cfg):\n    \"\"\"\n    Retrieves the lr for the given epoch (as specified by the lr policy).\n    Args:\n        cfg (config): configs of hyper-parameters of ADAM, includes base\n        learning rate, betas, and weight decay.\n        cur_epoch (float): the number of epoch of the current training stage.\n    \"\"\"\n    return lr_policy.get_lr_at_epoch(cfg, cur_epoch)\ndef set_lr(optimizer, new_lr):",
        "detail": "MorphMLP.slowfast.models.optimizer",
        "documentation": {}
    },
    {
        "label": "set_lr",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.models.optimizer",
        "description": "MorphMLP.slowfast.models.optimizer",
        "peekOfCode": "def set_lr(optimizer, new_lr):\n    \"\"\"\n    Sets the optimizer lr to the specified value.\n    Args:\n        optimizer (optim): the optimizer using to optimize the current network.\n        new_lr (float): the new learning rate to set.\n    \"\"\"\n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = new_lr",
        "detail": "MorphMLP.slowfast.models.optimizer",
        "documentation": {}
    },
    {
        "label": "PTVResNet",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.ptv_model_builder",
        "description": "MorphMLP.slowfast.models.ptv_model_builder",
        "peekOfCode": "class PTVResNet(nn.Module):\n    \"\"\"\n    ResNet models using PyTorchVideo model builder.\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        The `__init__` method of any subclass should also contain these\n            arguments.\n        Args:\n            cfg (CfgNode): model building configs, details are in the",
        "detail": "MorphMLP.slowfast.models.ptv_model_builder",
        "documentation": {}
    },
    {
        "label": "PTVSlowFast",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.ptv_model_builder",
        "description": "MorphMLP.slowfast.models.ptv_model_builder",
        "peekOfCode": "class PTVSlowFast(nn.Module):\n    def __init__(self, cfg):\n        \"\"\"\n        The `__init__` method of any subclass should also contain these\n            arguments.\n        Args:\n            cfg (CfgNode): model building configs, details are in the\n                comments of the config file.\n        \"\"\"\n        super(PTVSlowFast, self).__init__()",
        "detail": "MorphMLP.slowfast.models.ptv_model_builder",
        "documentation": {}
    },
    {
        "label": "PTVX3D",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.ptv_model_builder",
        "description": "MorphMLP.slowfast.models.ptv_model_builder",
        "peekOfCode": "class PTVX3D(nn.Module):\n    def __init__(self, cfg):\n        \"\"\"\n        The `__init__` method of any subclass should also contain these\n            arguments.\n        Args:\n            cfg (CfgNode): model building configs, details are in the\n                comments of the config file.\n        \"\"\"\n        super(PTVX3D, self).__init__()",
        "detail": "MorphMLP.slowfast.models.ptv_model_builder",
        "documentation": {}
    },
    {
        "label": "PTVCSN",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.ptv_model_builder",
        "description": "MorphMLP.slowfast.models.ptv_model_builder",
        "peekOfCode": "class PTVCSN(nn.Module):\n    \"\"\"\n    CSN models using PyTorchVideo model builder.\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        The `__init__` method of any subclass should also contain these\n            arguments.\n        Args:\n            cfg (CfgNode): model building configs, details are in the",
        "detail": "MorphMLP.slowfast.models.ptv_model_builder",
        "documentation": {}
    },
    {
        "label": "PTVR2plus1D",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.ptv_model_builder",
        "description": "MorphMLP.slowfast.models.ptv_model_builder",
        "peekOfCode": "class PTVR2plus1D(nn.Module):\n    \"\"\"\n    R(2+1)D models using PyTorchVideo model builder.\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        The `__init__` method of any subclass should also contain these\n            arguments.\n        Args:\n            cfg (CfgNode): model building configs, details are in the",
        "detail": "MorphMLP.slowfast.models.ptv_model_builder",
        "documentation": {}
    },
    {
        "label": "PTVMViT",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.ptv_model_builder",
        "description": "MorphMLP.slowfast.models.ptv_model_builder",
        "peekOfCode": "class PTVMViT(nn.Module):\n    \"\"\"\n    MViT models using PyTorchVideo model builder.\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        The `__init__` method of any subclass should also contain these\n            arguments.\n        Args:\n            cfg (CfgNode): model building configs, details are in the",
        "detail": "MorphMLP.slowfast.models.ptv_model_builder",
        "documentation": {}
    },
    {
        "label": "get_head_act",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.models.ptv_model_builder",
        "description": "MorphMLP.slowfast.models.ptv_model_builder",
        "peekOfCode": "def get_head_act(act_func):\n    \"\"\"\n    Return the actual head activation function given the activation fucntion name.\n    Args:\n        act_func (string): activation function to use. 'softmax': applies\n        softmax on the output. 'sigmoid': applies sigmoid on the output.\n    Returns:\n        nn.Module: the activation layer.\n    \"\"\"\n    if act_func == \"softmax\":",
        "detail": "MorphMLP.slowfast.models.ptv_model_builder",
        "documentation": {}
    },
    {
        "label": "BasicTransform",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.resnet_helper",
        "description": "MorphMLP.slowfast.models.resnet_helper",
        "peekOfCode": "class BasicTransform(nn.Module):\n    \"\"\"\n    Basic transformation: Tx3x3, 1x3x3, where T is the size of temporal kernel.\n    \"\"\"\n    def __init__(\n        self,\n        dim_in,\n        dim_out,\n        temp_kernel_size,\n        stride,",
        "detail": "MorphMLP.slowfast.models.resnet_helper",
        "documentation": {}
    },
    {
        "label": "X3DTransform",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.resnet_helper",
        "description": "MorphMLP.slowfast.models.resnet_helper",
        "peekOfCode": "class X3DTransform(nn.Module):\n    \"\"\"\n    X3D transformation: 1x1x1, Tx3x3 (channelwise, num_groups=dim_in), 1x1x1,\n        augmented with (optional) SE (squeeze-excitation) on the 3x3x3 output.\n        T is the temporal kernel size (defaulting to 3)\n    \"\"\"\n    def __init__(\n        self,\n        dim_in,\n        dim_out,",
        "detail": "MorphMLP.slowfast.models.resnet_helper",
        "documentation": {}
    },
    {
        "label": "BottleneckTransform",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.resnet_helper",
        "description": "MorphMLP.slowfast.models.resnet_helper",
        "peekOfCode": "class BottleneckTransform(nn.Module):\n    \"\"\"\n    Bottleneck transformation: Tx1x1, 1x3x3, 1x1x1, where T is the size of\n        temporal kernel.\n    \"\"\"\n    def __init__(\n        self,\n        dim_in,\n        dim_out,\n        temp_kernel_size,",
        "detail": "MorphMLP.slowfast.models.resnet_helper",
        "documentation": {}
    },
    {
        "label": "ResBlock",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.resnet_helper",
        "description": "MorphMLP.slowfast.models.resnet_helper",
        "peekOfCode": "class ResBlock(nn.Module):\n    \"\"\"\n    Residual block.\n    \"\"\"\n    def __init__(\n        self,\n        dim_in,\n        dim_out,\n        temp_kernel_size,\n        stride,",
        "detail": "MorphMLP.slowfast.models.resnet_helper",
        "documentation": {}
    },
    {
        "label": "ResStage",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.resnet_helper",
        "description": "MorphMLP.slowfast.models.resnet_helper",
        "peekOfCode": "class ResStage(nn.Module):\n    \"\"\"\n    Stage of 3D ResNet. It expects to have one or more tensors as input for\n        single pathway (C2D, I3D, Slow), and multi-pathway (SlowFast) cases.\n        More details can be found here:\n        Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He.\n        \"SlowFast networks for video recognition.\"\n        https://arxiv.org/pdf/1812.03982.pdf\n    \"\"\"\n    def __init__(",
        "detail": "MorphMLP.slowfast.models.resnet_helper",
        "documentation": {}
    },
    {
        "label": "get_trans_func",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.models.resnet_helper",
        "description": "MorphMLP.slowfast.models.resnet_helper",
        "peekOfCode": "def get_trans_func(name):\n    \"\"\"\n    Retrieves the transformation module by name.\n    \"\"\"\n    trans_funcs = {\n        \"bottleneck_transform\": BottleneckTransform,\n        \"basic_transform\": BasicTransform,\n        \"x3d_transform\": X3DTransform,\n    }\n    assert (",
        "detail": "MorphMLP.slowfast.models.resnet_helper",
        "documentation": {}
    },
    {
        "label": "VideoModelStem",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.stem_helper",
        "description": "MorphMLP.slowfast.models.stem_helper",
        "peekOfCode": "class VideoModelStem(nn.Module):\n    \"\"\"\n    Video 3D stem module. Provides stem operations of Conv, BN, ReLU, MaxPool\n    on input data tensor for one or multiple pathways.\n    \"\"\"\n    def __init__(\n        self,\n        dim_in,\n        dim_out,\n        kernel,",
        "detail": "MorphMLP.slowfast.models.stem_helper",
        "documentation": {}
    },
    {
        "label": "ResNetBasicStem",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.stem_helper",
        "description": "MorphMLP.slowfast.models.stem_helper",
        "peekOfCode": "class ResNetBasicStem(nn.Module):\n    \"\"\"\n    ResNe(X)t 3D stem module.\n    Performs spatiotemporal Convolution, BN, and Relu following by a\n        spatiotemporal pooling.\n    \"\"\"\n    def __init__(\n        self,\n        dim_in,\n        dim_out,",
        "detail": "MorphMLP.slowfast.models.stem_helper",
        "documentation": {}
    },
    {
        "label": "X3DStem",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.stem_helper",
        "description": "MorphMLP.slowfast.models.stem_helper",
        "peekOfCode": "class X3DStem(nn.Module):\n    \"\"\"\n    X3D's 3D stem module.\n    Performs a spatial followed by a depthwise temporal Convolution, BN, and Relu following by a\n        spatiotemporal pooling.\n    \"\"\"\n    def __init__(\n        self,\n        dim_in,\n        dim_out,",
        "detail": "MorphMLP.slowfast.models.stem_helper",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.stem_helper",
        "description": "MorphMLP.slowfast.models.stem_helper",
        "peekOfCode": "class PatchEmbed(nn.Module):\n    \"\"\"\n    PatchEmbed.\n    \"\"\"\n    def __init__(\n        self,\n        dim_in=3,\n        dim_out=768,\n        kernel=(1, 16, 16),\n        stride=(1, 4, 4),",
        "detail": "MorphMLP.slowfast.models.stem_helper",
        "documentation": {}
    },
    {
        "label": "get_stem_func",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.models.stem_helper",
        "description": "MorphMLP.slowfast.models.stem_helper",
        "peekOfCode": "def get_stem_func(name):\n    \"\"\"\n    Retrieves the stem module by name.\n    \"\"\"\n    trans_funcs = {\"x3d_stem\": X3DStem, \"basic_stem\": ResNetBasicStem}\n    assert (\n        name in trans_funcs.keys()\n    ), \"Transformation function '{}' not supported\".format(name)\n    return trans_funcs[name]\nclass VideoModelStem(nn.Module):",
        "detail": "MorphMLP.slowfast.models.stem_helper",
        "documentation": {}
    },
    {
        "label": "round_width",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.models.utils",
        "description": "MorphMLP.slowfast.models.utils",
        "peekOfCode": "def round_width(width, multiplier, min_width=1, divisor=1, verbose=False):\n    if not multiplier:\n        return width\n    width *= multiplier\n    min_width = min_width or divisor\n    if verbose:\n        logger.info(f\"min width {min_width}\")\n        logger.info(f\"width {width} divisor {divisor}\")\n        logger.info(f\"other {int(width + divisor / 2) // divisor * divisor}\")\n    width_out = max(min_width, int(width + divisor / 2) // divisor * divisor)",
        "detail": "MorphMLP.slowfast.models.utils",
        "documentation": {}
    },
    {
        "label": "validate_checkpoint_wrapper_import",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.models.utils",
        "description": "MorphMLP.slowfast.models.utils",
        "peekOfCode": "def validate_checkpoint_wrapper_import(checkpoint_wrapper):\n    \"\"\"\n    Check if checkpoint_wrapper is imported.\n    \"\"\"\n    if checkpoint_wrapper is None:\n        raise ImportError(\"Please install fairscale.\")",
        "detail": "MorphMLP.slowfast.models.utils",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.models.utils",
        "description": "MorphMLP.slowfast.models.utils",
        "peekOfCode": "logger = logging.get_logger(__name__)\ndef round_width(width, multiplier, min_width=1, divisor=1, verbose=False):\n    if not multiplier:\n        return width\n    width *= multiplier\n    min_width = min_width or divisor\n    if verbose:\n        logger.info(f\"min width {min_width}\")\n        logger.info(f\"width {width} divisor {divisor}\")\n        logger.info(f\"other {int(width + divisor / 2) // divisor * divisor}\")",
        "detail": "MorphMLP.slowfast.models.utils",
        "documentation": {}
    },
    {
        "label": "FuseFastToSlow",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.video_model_builder",
        "description": "MorphMLP.slowfast.models.video_model_builder",
        "peekOfCode": "class FuseFastToSlow(nn.Module):\n    \"\"\"\n    Fuses the information from the Fast pathway to the Slow pathway. Given the\n    tensors from Slow pathway and Fast pathway, fuse information from Fast to\n    Slow, then return the fused tensors from Slow and Fast pathway in order.\n    \"\"\"\n    def __init__(\n        self,\n        dim_in,\n        fusion_conv_channel_ratio,",
        "detail": "MorphMLP.slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "SlowFast",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.video_model_builder",
        "description": "MorphMLP.slowfast.models.video_model_builder",
        "peekOfCode": "class SlowFast(nn.Module):\n    \"\"\"\n    SlowFast model builder for SlowFast network.\n    Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He.\n    \"SlowFast networks for video recognition.\"\n    https://arxiv.org/pdf/1812.03982.pdf\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        The `__init__` method of any subclass should also contain these",
        "detail": "MorphMLP.slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "ResNet",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.video_model_builder",
        "description": "MorphMLP.slowfast.models.video_model_builder",
        "peekOfCode": "class ResNet(nn.Module):\n    \"\"\"\n    ResNet model builder. It builds a ResNet like network backbone without\n    lateral connection (C2D, I3D, Slow).\n    Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He.\n    \"SlowFast networks for video recognition.\"\n    https://arxiv.org/pdf/1812.03982.pdf\n    Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He.\n    \"Non-local neural networks.\"\n    https://arxiv.org/pdf/1711.07971.pdf",
        "detail": "MorphMLP.slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "X3D",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.video_model_builder",
        "description": "MorphMLP.slowfast.models.video_model_builder",
        "peekOfCode": "class X3D(nn.Module):\n    \"\"\"\n    X3D model builder. It builds a X3D network backbone, which is a ResNet.\n    Christoph Feichtenhofer.\n    \"X3D: Expanding Architectures for Efficient Video Recognition.\"\n    https://arxiv.org/abs/2004.04730\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        The `__init__` method of any subclass should also contain these",
        "detail": "MorphMLP.slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "MViT",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.models.video_model_builder",
        "description": "MorphMLP.slowfast.models.video_model_builder",
        "peekOfCode": "class MViT(nn.Module):\n    \"\"\"\n    Model builder for MViTv1 and MViTv2.\n    \"MViTv2: Improved Multiscale Vision Transformers for Classification and Detection\"\n    Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, Christoph Feichtenhofer\n    https://arxiv.org/abs/2112.01526\n    \"Multiscale Vision Transformers\"\n    Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, Christoph Feichtenhofer\n    https://arxiv.org/abs/2104.11227\n    \"\"\"",
        "detail": "MorphMLP.slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "_MODEL_STAGE_DEPTH",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.models.video_model_builder",
        "description": "MorphMLP.slowfast.models.video_model_builder",
        "peekOfCode": "_MODEL_STAGE_DEPTH = {18: (2, 2, 2, 2), 50: (3, 4, 6, 3), 101: (3, 4, 23, 3)}\n# Basis of temporal kernel sizes for each of the stage.\n_TEMPORAL_KERNEL_BASIS = {\n    \"2d\": [\n        [[1]],  # conv1 temporal kernel.\n        [[1]],  # res2 temporal kernel.\n        [[1]],  # res3 temporal kernel.\n        [[1]],  # res4 temporal kernel.\n        [[1]],  # res5 temporal kernel.\n    ],",
        "detail": "MorphMLP.slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "_TEMPORAL_KERNEL_BASIS",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.models.video_model_builder",
        "description": "MorphMLP.slowfast.models.video_model_builder",
        "peekOfCode": "_TEMPORAL_KERNEL_BASIS = {\n    \"2d\": [\n        [[1]],  # conv1 temporal kernel.\n        [[1]],  # res2 temporal kernel.\n        [[1]],  # res3 temporal kernel.\n        [[1]],  # res4 temporal kernel.\n        [[1]],  # res5 temporal kernel.\n    ],\n    \"c2d\": [\n        [[1]],  # conv1 temporal kernel.",
        "detail": "MorphMLP.slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "_POOL1",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.models.video_model_builder",
        "description": "MorphMLP.slowfast.models.video_model_builder",
        "peekOfCode": "_POOL1 = {\n    \"2d\": [[1, 1, 1]],\n    \"c2d\": [[2, 1, 1]],\n    \"slow_c2d\": [[1, 1, 1]],\n    \"i3d\": [[2, 1, 1]],\n    \"slow_i3d\": [[1, 1, 1]],\n    \"slow\": [[1, 1, 1]],\n    \"slowfast\": [[1, 1, 1], [1, 1, 1]],\n    \"x3d\": [[1, 1, 1]],\n}",
        "detail": "MorphMLP.slowfast.models.video_model_builder",
        "documentation": {}
    },
    {
        "label": "create_category_index",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.label_map_util",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.label_map_util",
        "peekOfCode": "def create_category_index(categories):\n    \"\"\"Creates dictionary of COCO compatible categories keyed by category id.\n    Args:\n      categories: a list of dicts, each of which has the following keys:\n        'id': (required) an integer id uniquely identifying this category.\n        'name': (required) string representing category name\n          e.g., 'cat', 'dog', 'pizza'.\n    Returns:\n      category_index: a dict containing the same entries as categories, but keyed\n        by the 'id' field of each category.",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.label_map_util",
        "documentation": {}
    },
    {
        "label": "get_max_label_map_index",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.label_map_util",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.label_map_util",
        "peekOfCode": "def get_max_label_map_index(label_map):\n    \"\"\"Get maximum index in label map.\n    Args:\n      label_map: a StringIntLabelMapProto\n    Returns:\n      an integer\n    \"\"\"\n    return max([item.id for item in label_map.item])\ndef convert_label_map_to_categories(\n    label_map, max_num_classes, use_display_name=True",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.label_map_util",
        "documentation": {}
    },
    {
        "label": "convert_label_map_to_categories",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.label_map_util",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.label_map_util",
        "peekOfCode": "def convert_label_map_to_categories(\n    label_map, max_num_classes, use_display_name=True\n):\n    \"\"\"Loads label map proto and returns categories list compatible with eval.\n    This function loads a label map and returns a list of dicts, each of which\n    has the following keys:\n      'id': (required) an integer id uniquely identifying this category.\n      'name': (required) string representing category name\n        e.g., 'cat', 'dog', 'pizza'.\n    We only allow class into the list if its id-label_id_offset is",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.label_map_util",
        "documentation": {}
    },
    {
        "label": "load_labelmap",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.label_map_util",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.label_map_util",
        "peekOfCode": "def load_labelmap(path):\n    \"\"\"Loads label map proto.\n    Args:\n      path: path to StringIntLabelMap proto text file.\n    Returns:\n      a StringIntLabelMapProto\n    \"\"\"\n    with open(path, \"r\") as fid:\n        label_map_string = fid.read()\n        label_map = string_int_label_map_pb2.StringIntLabelMap()",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.label_map_util",
        "documentation": {}
    },
    {
        "label": "get_label_map_dict",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.label_map_util",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.label_map_util",
        "peekOfCode": "def get_label_map_dict(label_map_path, use_display_name=False):\n    \"\"\"Reads a label map and returns a dictionary of label names to id.\n    Args:\n      label_map_path: path to label_map.\n      use_display_name: whether to use the label map items' display names as keys.\n    Returns:\n      A dictionary mapping label names to id.\n    \"\"\"\n    label_map = load_labelmap(label_map_path)\n    label_map_dict = {}",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.label_map_util",
        "documentation": {}
    },
    {
        "label": "create_category_index_from_labelmap",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.label_map_util",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.label_map_util",
        "peekOfCode": "def create_category_index_from_labelmap(label_map_path):\n    \"\"\"Reads a label map and returns a category index.\n    Args:\n      label_map_path: Path to `StringIntLabelMap` proto text file.\n    Returns:\n      A category index, which is a dictionary that maps integer ids to dicts\n      containing categories, e.g.\n      {1: {'id': 1, 'name': 'dog'}, 2: {'id': 2, 'name': 'cat'}, ...}\n    \"\"\"\n    label_map = load_labelmap(label_map_path)",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.label_map_util",
        "documentation": {}
    },
    {
        "label": "create_class_agnostic_category_index",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.label_map_util",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.label_map_util",
        "peekOfCode": "def create_class_agnostic_category_index():\n    \"\"\"Creates a category index with a single `object` class.\"\"\"\n    return {1: {\"id\": 1, \"name\": \"object\"}}",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.label_map_util",
        "documentation": {}
    },
    {
        "label": "compute_precision_recall",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.metrics",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.metrics",
        "peekOfCode": "def compute_precision_recall(scores, labels, num_gt):\n    \"\"\"Compute precision and recall.\n    Args:\n      scores: A float numpy array representing detection score\n      labels: A boolean numpy array representing true/false positive labels\n      num_gt: Number of ground truth instances\n    Raises:\n      ValueError: if the input is not of the correct format\n    Returns:\n      precision: Fraction of positive instances over detected ones. This value is",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.metrics",
        "documentation": {}
    },
    {
        "label": "compute_average_precision",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.metrics",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.metrics",
        "peekOfCode": "def compute_average_precision(precision, recall):\n    \"\"\"Compute Average Precision according to the definition in VOCdevkit.\n    Precision is modified to ensure that it does not decrease as recall\n    decrease.\n    Args:\n      precision: A float [N, 1] numpy array of precisions\n      recall: A float [N, 1] numpy array of recalls\n    Raises:\n      ValueError: if the input is not of the correct format\n    Returns:",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.metrics",
        "documentation": {}
    },
    {
        "label": "compute_cor_loc",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.metrics",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.metrics",
        "peekOfCode": "def compute_cor_loc(\n    num_gt_imgs_per_class, num_images_correctly_detected_per_class\n):\n    \"\"\"Compute CorLoc according to the definition in the following paper.\n    https://www.robots.ox.ac.uk/~vgg/rg/papers/deselaers-eccv10.pdf\n    Returns nans if there are no ground truth images for a class.\n    Args:\n      num_gt_imgs_per_class: 1D array, representing number of images containing\n          at least one object instance of a particular class\n      num_images_correctly_detected_per_class: 1D array, representing number of",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.metrics",
        "documentation": {}
    },
    {
        "label": "BoxList",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list",
        "peekOfCode": "class BoxList(object):\n    \"\"\"Box collection.\n    BoxList represents a list of bounding boxes as numpy array, where each\n    bounding box is represented as a row of 4 numbers,\n    [y_min, x_min, y_max, x_max].  It is assumed that all bounding boxes within a\n    given list correspond to a single image.\n    Optionally, users can add additional related fields (such as\n    objectness/classification scores).\n    \"\"\"\n    def __init__(self, data):",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list",
        "documentation": {}
    },
    {
        "label": "SortOrder",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "class SortOrder(object):\n    \"\"\"Enum class for sort order.\n    Attributes:\n      ascend: ascend order.\n      descend: descend order.\n    \"\"\"\n    ASCEND = 1\n    DESCEND = 2\ndef area(boxlist):\n    \"\"\"Computes area of boxes.",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "area",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def area(boxlist):\n    \"\"\"Computes area of boxes.\n    Args:\n      boxlist: BoxList holding N boxes\n    Returns:\n      a numpy array with shape [N*1] representing box areas\n    \"\"\"\n    y_min, x_min, y_max, x_max = boxlist.get_coordinates()\n    return (y_max - y_min) * (x_max - x_min)\ndef intersection(boxlist1, boxlist2):",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "intersection",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def intersection(boxlist1, boxlist2):\n    \"\"\"Compute pairwise intersection areas between boxes.\n    Args:\n      boxlist1: BoxList holding N boxes\n      boxlist2: BoxList holding M boxes\n    Returns:\n      a numpy array with shape [N*M] representing pairwise intersection area\n    \"\"\"\n    return np_box_ops.intersection(boxlist1.get(), boxlist2.get())\ndef iou(boxlist1, boxlist2):",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "iou",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def iou(boxlist1, boxlist2):\n    \"\"\"Computes pairwise intersection-over-union between box collections.\n    Args:\n      boxlist1: BoxList holding N boxes\n      boxlist2: BoxList holding M boxes\n    Returns:\n      a numpy array with shape [N, M] representing pairwise iou scores.\n    \"\"\"\n    return np_box_ops.iou(boxlist1.get(), boxlist2.get())\ndef ioa(boxlist1, boxlist2):",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "ioa",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def ioa(boxlist1, boxlist2):\n    \"\"\"Computes pairwise intersection-over-area between box collections.\n    Intersection-over-area (ioa) between two boxes box1 and box2 is defined as\n    their intersection area over box2's area. Note that ioa is not symmetric,\n    that is, IOA(box1, box2) != IOA(box2, box1).\n    Args:\n      boxlist1: BoxList holding N boxes\n      boxlist2: BoxList holding M boxes\n    Returns:\n      a numpy array with shape [N, M] representing pairwise ioa scores.",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "gather",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def gather(boxlist, indices, fields=None):\n    \"\"\"Gather boxes from BoxList according to indices and return new BoxList.\n    By default, gather returns boxes corresponding to the input index list, as\n    well as all additional fields stored in the boxlist (indexing into the\n    first dimension).  However one can optionally only gather from a\n    subset of fields.\n    Args:\n      boxlist: BoxList holding N boxes\n      indices: a 1-d numpy array of type int_\n      fields: (optional) list of fields to also gather from.  If None (default),",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "sort_by_field",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def sort_by_field(boxlist, field, order=SortOrder.DESCEND):\n    \"\"\"Sort boxes and associated fields according to a scalar field.\n    A common use case is reordering the boxes according to descending scores.\n    Args:\n      boxlist: BoxList holding N boxes.\n      field: A BoxList field for sorting and reordering the BoxList.\n      order: (Optional) 'descend' or 'ascend'. Default is descend.\n    Returns:\n      sorted_boxlist: A sorted BoxList with the field in the specified order.\n    Raises:",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "non_max_suppression",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def non_max_suppression(\n    boxlist, max_output_size=10000, iou_threshold=1.0, score_threshold=-10.0\n):\n    \"\"\"Non maximum suppression.\n    This op greedily selects a subset of detection bounding boxes, pruning\n    away boxes that have high IOU (intersection over union) overlap (> thresh)\n    with already selected boxes. In each iteration, the detected bounding box with\n    highest score in the available pool is selected.\n    Args:\n      boxlist: BoxList holding N boxes.  Must contain a 'scores' field",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "multi_class_non_max_suppression",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def multi_class_non_max_suppression(\n    boxlist, score_thresh, iou_thresh, max_output_size\n):\n    \"\"\"Multi-class version of non maximum suppression.\n    This op greedily selects a subset of detection bounding boxes, pruning\n    away boxes that have high IOU (intersection over union) overlap (> thresh)\n    with already selected boxes.  It operates independently for each class for\n    which scores are provided (via the scores field of the input box_list),\n    pruning boxes with score less than a provided threshold prior to\n    applying NMS.",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "scale",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def scale(boxlist, y_scale, x_scale):\n    \"\"\"Scale box coordinates in x and y dimensions.\n    Args:\n      boxlist: BoxList holding N boxes\n      y_scale: float\n      x_scale: float\n    Returns:\n      boxlist: BoxList holding N boxes\n    \"\"\"\n    y_min, x_min, y_max, x_max = np.array_split(boxlist.get(), 4, axis=1)",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "clip_to_window",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def clip_to_window(boxlist, window):\n    \"\"\"Clip bounding boxes to a window.\n    This op clips input bounding boxes (represented by bounding box\n    corners) to a window, optionally filtering out boxes that do not\n    overlap at all with the window.\n    Args:\n      boxlist: BoxList holding M_in boxes\n      window: a numpy array of shape [4] representing the\n              [y_min, x_min, y_max, x_max] window to which the op\n              should clip boxes.",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "prune_non_overlapping_boxes",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def prune_non_overlapping_boxes(boxlist1, boxlist2, minoverlap=0.0):\n    \"\"\"Prunes the boxes in boxlist1 that overlap less than thresh with boxlist2.\n    For each box in boxlist1, we want its IOA to be more than minoverlap with\n    at least one of the boxes in boxlist2. If it does not, we remove it.\n    Args:\n      boxlist1: BoxList holding N boxes.\n      boxlist2: BoxList holding M boxes.\n      minoverlap: Minimum required overlap between boxes, to count them as\n                  overlapping.\n    Returns:",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "prune_outside_window",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def prune_outside_window(boxlist, window):\n    \"\"\"Prunes bounding boxes that fall outside a given window.\n    This function prunes bounding boxes that even partially fall outside the given\n    window. See also ClipToWindow which only prunes bounding boxes that fall\n    completely outside the window, and clips any bounding boxes that partially\n    overflow.\n    Args:\n      boxlist: a BoxList holding M_in boxes.\n      window: a numpy array of size 4, representing [ymin, xmin, ymax, xmax]\n              of the window.",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "concatenate",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def concatenate(boxlists, fields=None):\n    \"\"\"Concatenate list of BoxLists.\n    This op concatenates a list of input BoxLists into a larger BoxList.  It also\n    handles concatenation of BoxList fields as long as the field tensor shapes\n    are equal except for the first dimension.\n    Args:\n      boxlists: list of BoxList objects\n      fields: optional list of fields to also concatenate.  By default, all\n        fields from the first BoxList in the list are included in the\n        concatenation.",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "filter_scores_greater_than",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def filter_scores_greater_than(boxlist, thresh):\n    \"\"\"Filter to keep only boxes with score exceeding a given threshold.\n    This op keeps the collection of boxes whose corresponding scores are\n    greater than the input threshold.\n    Args:\n      boxlist: BoxList holding N boxes.  Must contain a 'scores' field\n        representing detection scores.\n      thresh: scalar threshold\n    Returns:\n      a BoxList holding M boxes where M <= N",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "change_coordinate_frame",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "peekOfCode": "def change_coordinate_frame(boxlist, window):\n    \"\"\"Change coordinate frame of the boxlist to be relative to window's frame.\n    Given a window of the form [ymin, xmin, ymax, xmax],\n    changes bounding box coordinates from boxlist to be relative to this window\n    (e.g., the min corner maps to (0,0) and the max corner maps to (1,1)).\n    An example use case is data augmentation: where we are given groundtruth\n    boxes (boxlist) and would like to randomly crop the image to some\n    window (window). In this case we need to change the coordinate frame of\n    each groundtruth box to be relative to this new window.\n    Args:",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_list_ops",
        "documentation": {}
    },
    {
        "label": "BoxMaskList",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list",
        "peekOfCode": "class BoxMaskList(np_box_list.BoxList):\n    \"\"\"Convenience wrapper for BoxList with masks.\n    BoxMaskList extends the np_box_list.BoxList to contain masks as well.\n    In particular, its constructor receives both boxes and masks. Note that the\n    masks correspond to the full image.\n    \"\"\"\n    def __init__(self, box_data, mask_data):\n        \"\"\"Constructs box collection.\n        Args:\n          box_data: a numpy array of shape [N, 4] representing box coordinates",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list",
        "documentation": {}
    },
    {
        "label": "box_list_to_box_mask_list",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "peekOfCode": "def box_list_to_box_mask_list(boxlist):\n    \"\"\"Converts a BoxList containing 'masks' into a BoxMaskList.\n    Args:\n      boxlist: An np_box_list.BoxList object.\n    Returns:\n      An np_box_mask_list.BoxMaskList object.\n    Raises:\n      ValueError: If boxlist does not contain `masks` as a field.\n    \"\"\"\n    if not boxlist.has_field(\"masks\"):",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "documentation": {}
    },
    {
        "label": "area",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "peekOfCode": "def area(box_mask_list):\n    \"\"\"Computes area of masks.\n    Args:\n      box_mask_list: np_box_mask_list.BoxMaskList holding N boxes and masks\n    Returns:\n      a numpy array with shape [N*1] representing mask areas\n    \"\"\"\n    return np_mask_ops.area(box_mask_list.get_masks())\ndef intersection(box_mask_list1, box_mask_list2):\n    \"\"\"Compute pairwise intersection areas between masks.",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "documentation": {}
    },
    {
        "label": "intersection",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "peekOfCode": "def intersection(box_mask_list1, box_mask_list2):\n    \"\"\"Compute pairwise intersection areas between masks.\n    Args:\n      box_mask_list1: BoxMaskList holding N boxes and masks\n      box_mask_list2: BoxMaskList holding M boxes and masks\n    Returns:\n      a numpy array with shape [N*M] representing pairwise intersection area\n    \"\"\"\n    return np_mask_ops.intersection(\n        box_mask_list1.get_masks(), box_mask_list2.get_masks()",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "documentation": {}
    },
    {
        "label": "iou",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "peekOfCode": "def iou(box_mask_list1, box_mask_list2):\n    \"\"\"Computes pairwise intersection-over-union between box and mask collections.\n    Args:\n      box_mask_list1: BoxMaskList holding N boxes and masks\n      box_mask_list2: BoxMaskList holding M boxes and masks\n    Returns:\n      a numpy array with shape [N, M] representing pairwise iou scores.\n    \"\"\"\n    return np_mask_ops.iou(\n        box_mask_list1.get_masks(), box_mask_list2.get_masks()",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "documentation": {}
    },
    {
        "label": "ioa",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "peekOfCode": "def ioa(box_mask_list1, box_mask_list2):\n    \"\"\"Computes pairwise intersection-over-area between box and mask collections.\n    Intersection-over-area (ioa) between two masks mask1 and mask2 is defined as\n    their intersection area over mask2's area. Note that ioa is not symmetric,\n    that is, IOA(mask1, mask2) != IOA(mask2, mask1).\n    Args:\n      box_mask_list1: np_box_mask_list.BoxMaskList holding N boxes and masks\n      box_mask_list2: np_box_mask_list.BoxMaskList holding M boxes and masks\n    Returns:\n      a numpy array with shape [N, M] representing pairwise ioa scores.",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "documentation": {}
    },
    {
        "label": "gather",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "peekOfCode": "def gather(box_mask_list, indices, fields=None):\n    \"\"\"Gather boxes from np_box_mask_list.BoxMaskList according to indices.\n    By default, gather returns boxes corresponding to the input index list, as\n    well as all additional fields stored in the box_mask_list (indexing into the\n    first dimension).  However one can optionally only gather from a\n    subset of fields.\n    Args:\n      box_mask_list: np_box_mask_list.BoxMaskList holding N boxes\n      indices: a 1-d numpy array of type int_\n      fields: (optional) list of fields to also gather from.  If None (default),",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "documentation": {}
    },
    {
        "label": "sort_by_field",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "peekOfCode": "def sort_by_field(\n    box_mask_list, field, order=np_box_list_ops.SortOrder.DESCEND\n):\n    \"\"\"Sort boxes and associated fields according to a scalar field.\n    A common use case is reordering the boxes according to descending scores.\n    Args:\n      box_mask_list: BoxMaskList holding N boxes.\n      field: A BoxMaskList field for sorting and reordering the BoxMaskList.\n      order: (Optional) 'descend' or 'ascend'. Default is descend.\n    Returns:",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "documentation": {}
    },
    {
        "label": "non_max_suppression",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "peekOfCode": "def non_max_suppression(\n    box_mask_list,\n    max_output_size=10000,\n    iou_threshold=1.0,\n    score_threshold=-10.0,\n):\n    \"\"\"Non maximum suppression.\n    This op greedily selects a subset of detection bounding boxes, pruning\n    away boxes that have high IOU (intersection over union) overlap (> thresh)\n    with already selected boxes. In each iteration, the detected bounding box with",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "documentation": {}
    },
    {
        "label": "multi_class_non_max_suppression",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "peekOfCode": "def multi_class_non_max_suppression(\n    box_mask_list, score_thresh, iou_thresh, max_output_size\n):\n    \"\"\"Multi-class version of non maximum suppression.\n    This op greedily selects a subset of detection bounding boxes, pruning\n    away boxes that have high IOU (intersection over union) overlap (> thresh)\n    with already selected boxes.  It operates independently for each class for\n    which scores are provided (via the scores field of the input box_list),\n    pruning boxes with score less than a provided threshold prior to\n    applying NMS.",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "documentation": {}
    },
    {
        "label": "prune_non_overlapping_masks",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "peekOfCode": "def prune_non_overlapping_masks(box_mask_list1, box_mask_list2, minoverlap=0.0):\n    \"\"\"Prunes the boxes in list1 that overlap less than thresh with list2.\n    For each mask in box_mask_list1, we want its IOA to be more than minoverlap\n    with at least one of the masks in box_mask_list2. If it does not, we remove\n    it. If the masks are not full size image, we do the pruning based on boxes.\n    Args:\n      box_mask_list1: np_box_mask_list.BoxMaskList holding N boxes and masks.\n      box_mask_list2: np_box_mask_list.BoxMaskList holding M boxes and masks.\n      minoverlap: Minimum required overlap between boxes, to count them as\n                  overlapping.",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "documentation": {}
    },
    {
        "label": "concatenate",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "peekOfCode": "def concatenate(box_mask_lists, fields=None):\n    \"\"\"Concatenate list of box_mask_lists.\n    This op concatenates a list of input box_mask_lists into a larger\n    box_mask_list.  It also\n    handles concatenation of box_mask_list fields as long as the field tensor\n    shapes are equal except for the first dimension.\n    Args:\n      box_mask_lists: list of np_box_mask_list.BoxMaskList objects\n      fields: optional list of fields to also concatenate.  By default, all\n        fields from the first BoxMaskList in the list are included in the",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "documentation": {}
    },
    {
        "label": "filter_scores_greater_than",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "peekOfCode": "def filter_scores_greater_than(box_mask_list, thresh):\n    \"\"\"Filter to keep only boxes and masks with score exceeding a given threshold.\n    This op keeps the collection of boxes and masks whose corresponding scores are\n    greater than the input threshold.\n    Args:\n      box_mask_list: BoxMaskList holding N boxes and masks.  Must contain a\n        'scores' field representing detection scores.\n      thresh: scalar threshold\n    Returns:\n      a BoxMaskList holding M boxes and masks where M <= N",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_mask_list_ops",
        "documentation": {}
    },
    {
        "label": "area",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_ops",
        "peekOfCode": "def area(boxes):\n    \"\"\"Computes area of boxes.\n    Args:\n      boxes: Numpy array with shape [N, 4] holding N boxes\n    Returns:\n      a numpy array with shape [N*1] representing box areas\n    \"\"\"\n    return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\ndef intersection(boxes1, boxes2):\n    \"\"\"Compute pairwise intersection areas between boxes.",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_ops",
        "documentation": {}
    },
    {
        "label": "intersection",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_ops",
        "peekOfCode": "def intersection(boxes1, boxes2):\n    \"\"\"Compute pairwise intersection areas between boxes.\n    Args:\n      boxes1: a numpy array with shape [N, 4] holding N boxes\n      boxes2: a numpy array with shape [M, 4] holding M boxes\n    Returns:\n      a numpy array with shape [N*M] representing pairwise intersection area\n    \"\"\"\n    [y_min1, x_min1, y_max1, x_max1] = np.split(boxes1, 4, axis=1)\n    [y_min2, x_min2, y_max2, x_max2] = np.split(boxes2, 4, axis=1)",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_ops",
        "documentation": {}
    },
    {
        "label": "iou",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_ops",
        "peekOfCode": "def iou(boxes1, boxes2):\n    \"\"\"Computes pairwise intersection-over-union between box collections.\n    Args:\n      boxes1: a numpy array with shape [N, 4] holding N boxes.\n      boxes2: a numpy array with shape [M, 4] holding N boxes.\n    Returns:\n      a numpy array with shape [N, M] representing pairwise iou scores.\n    \"\"\"\n    intersect = intersection(boxes1, boxes2)\n    area1 = area(boxes1)",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_ops",
        "documentation": {}
    },
    {
        "label": "ioa",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_box_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_box_ops",
        "peekOfCode": "def ioa(boxes1, boxes2):\n    \"\"\"Computes pairwise intersection-over-area between box collections.\n    Intersection-over-area (ioa) between two boxes box1 and box2 is defined as\n    their intersection area over box2's area. Note that ioa is not symmetric,\n    that is, IOA(box1, box2) != IOA(box2, box1).\n    Args:\n      boxes1: a numpy array with shape [N, 4] holding N boxes.\n      boxes2: a numpy array with shape [M, 4] holding N boxes.\n    Returns:\n      a numpy array with shape [N, M] representing pairwise ioa scores.",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_box_ops",
        "documentation": {}
    },
    {
        "label": "area",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_mask_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_mask_ops",
        "peekOfCode": "def area(masks):\n    \"\"\"Computes area of masks.\n    Args:\n      masks: Numpy array with shape [N, height, width] holding N masks. Masks\n        values are of type np.uint8 and values are in {0,1}.\n    Returns:\n      a numpy array with shape [N*1] representing mask areas.\n    Raises:\n      ValueError: If masks.dtype is not np.uint8\n    \"\"\"",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_mask_ops",
        "documentation": {}
    },
    {
        "label": "intersection",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_mask_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_mask_ops",
        "peekOfCode": "def intersection(masks1, masks2):\n    \"\"\"Compute pairwise intersection areas between masks.\n    Args:\n      masks1: a numpy array with shape [N, height, width] holding N masks. Masks\n        values are of type np.uint8 and values are in {0,1}.\n      masks2: a numpy array with shape [M, height, width] holding M masks. Masks\n        values are of type np.uint8 and values are in {0,1}.\n    Returns:\n      a numpy array with shape [N*M] representing pairwise intersection area.\n    Raises:",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_mask_ops",
        "documentation": {}
    },
    {
        "label": "iou",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_mask_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_mask_ops",
        "peekOfCode": "def iou(masks1, masks2):\n    \"\"\"Computes pairwise intersection-over-union between mask collections.\n    Args:\n      masks1: a numpy array with shape [N, height, width] holding N masks. Masks\n        values are of type np.uint8 and values are in {0,1}.\n      masks2: a numpy array with shape [M, height, width] holding N masks. Masks\n        values are of type np.uint8 and values are in {0,1}.\n    Returns:\n      a numpy array with shape [N, M] representing pairwise iou scores.\n    Raises:",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_mask_ops",
        "documentation": {}
    },
    {
        "label": "ioa",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_mask_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_mask_ops",
        "peekOfCode": "def ioa(masks1, masks2):\n    \"\"\"Computes pairwise intersection-over-area between box collections.\n    Intersection-over-area (ioa) between two masks, mask1 and mask2 is defined as\n    their intersection area over mask2's area. Note that ioa is not symmetric,\n    that is, IOA(mask1, mask2) != IOA(mask2, mask1).\n    Args:\n      masks1: a numpy array with shape [N, height, width] holding N masks. Masks\n        values are of type np.uint8 and values are in {0,1}.\n      masks2: a numpy array with shape [M, height, width] holding N masks. Masks\n        values are of type np.uint8 and values are in {0,1}.",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_mask_ops",
        "documentation": {}
    },
    {
        "label": "EPSILON",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.np_mask_ops",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.np_mask_ops",
        "peekOfCode": "EPSILON = 1e-7\ndef area(masks):\n    \"\"\"Computes area of masks.\n    Args:\n      masks: Numpy array with shape [N, height, width] holding N masks. Masks\n        values are of type np.uint8 and values are in {0,1}.\n    Returns:\n      a numpy array with shape [N*1] representing mask areas.\n    Raises:\n      ValueError: If masks.dtype is not np.uint8",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.np_mask_ops",
        "documentation": {}
    },
    {
        "label": "DetectionEvaluator",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "peekOfCode": "class DetectionEvaluator(object):\n    \"\"\"Interface for object detection evalution classes.\n    Example usage of the Evaluator:\n    ------------------------------\n    evaluator = DetectionEvaluator(categories)\n    # Detections and groundtruth for image 1.\n    evaluator.add_single_groundtruth_image_info(...)\n    evaluator.add_single_detected_image_info(...)\n    # Detections and groundtruth for image 2.\n    evaluator.add_single_groundtruth_image_info(...)",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "documentation": {}
    },
    {
        "label": "ObjectDetectionEvaluator",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "peekOfCode": "class ObjectDetectionEvaluator(DetectionEvaluator):\n    \"\"\"A class to evaluate detections.\"\"\"\n    def __init__(\n        self,\n        categories,\n        matching_iou_threshold=0.5,\n        evaluate_corlocs=False,\n        metric_prefix=None,\n        use_weighted_mean_ap=False,\n        evaluate_masks=False,",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "documentation": {}
    },
    {
        "label": "PascalDetectionEvaluator",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "peekOfCode": "class PascalDetectionEvaluator(ObjectDetectionEvaluator):\n    \"\"\"A class to evaluate detections using PASCAL metrics.\"\"\"\n    def __init__(self, categories, matching_iou_threshold=0.5):\n        super(PascalDetectionEvaluator, self).__init__(\n            categories,\n            matching_iou_threshold=matching_iou_threshold,\n            evaluate_corlocs=False,\n            metric_prefix=\"PascalBoxes\",\n            use_weighted_mean_ap=False,\n        )",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "documentation": {}
    },
    {
        "label": "WeightedPascalDetectionEvaluator",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "peekOfCode": "class WeightedPascalDetectionEvaluator(ObjectDetectionEvaluator):\n    \"\"\"A class to evaluate detections using weighted PASCAL metrics.\n    Weighted PASCAL metrics computes the mean average precision as the average\n    precision given the scores and tp_fp_labels of all classes. In comparison,\n    PASCAL metrics computes the mean average precision as the mean of the\n    per-class average precisions.\n    This definition is very similar to the mean of the per-class average\n    precisions weighted by class frequency. However, they are typically not the\n    same as the average precision is not a linear function of the scores and\n    tp_fp_labels.",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "documentation": {}
    },
    {
        "label": "PascalInstanceSegmentationEvaluator",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "peekOfCode": "class PascalInstanceSegmentationEvaluator(ObjectDetectionEvaluator):\n    \"\"\"A class to evaluate instance masks using PASCAL metrics.\"\"\"\n    def __init__(self, categories, matching_iou_threshold=0.5):\n        super(PascalInstanceSegmentationEvaluator, self).__init__(\n            categories,\n            matching_iou_threshold=matching_iou_threshold,\n            evaluate_corlocs=False,\n            metric_prefix=\"PascalMasks\",\n            use_weighted_mean_ap=False,\n            evaluate_masks=True,",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "documentation": {}
    },
    {
        "label": "WeightedPascalInstanceSegmentationEvaluator",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "peekOfCode": "class WeightedPascalInstanceSegmentationEvaluator(ObjectDetectionEvaluator):\n    \"\"\"A class to evaluate instance masks using weighted PASCAL metrics.\n    Weighted PASCAL metrics computes the mean average precision as the average\n    precision given the scores and tp_fp_labels of all classes. In comparison,\n    PASCAL metrics computes the mean average precision as the mean of the\n    per-class average precisions.\n    This definition is very similar to the mean of the per-class average\n    precisions weighted by class frequency. However, they are typically not the\n    same as the average precision is not a linear function of the scores and\n    tp_fp_labels.",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "documentation": {}
    },
    {
        "label": "OpenImagesDetectionEvaluator",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "peekOfCode": "class OpenImagesDetectionEvaluator(ObjectDetectionEvaluator):\n    \"\"\"A class to evaluate detections using Open Images V2 metrics.\n    Open Images V2 introduce group_of type of bounding boxes and this metric\n    handles those boxes appropriately.\n    \"\"\"\n    def __init__(\n        self, categories, matching_iou_threshold=0.5, evaluate_corlocs=False\n    ):\n        \"\"\"Constructor.\n        Args:",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "documentation": {}
    },
    {
        "label": "ObjectDetectionEvaluation",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "peekOfCode": "class ObjectDetectionEvaluation(object):\n    \"\"\"Internal implementation of Pascal object detection metrics.\"\"\"\n    def __init__(\n        self,\n        num_groundtruth_classes,\n        matching_iou_threshold=0.5,\n        nms_iou_threshold=1.0,\n        nms_max_output_boxes=10000,\n        use_weighted_mean_ap=False,\n        label_id_offset=0,",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "documentation": {}
    },
    {
        "label": "ObjectDetectionEvalMetrics",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "peekOfCode": "ObjectDetectionEvalMetrics = collections.namedtuple(\n    \"ObjectDetectionEvalMetrics\",\n    [\n        \"average_precisions\",\n        \"mean_ap\",\n        \"precisions\",\n        \"recalls\",\n        \"corlocs\",\n        \"mean_corloc\",\n    ],",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.object_detection_evaluation",
        "documentation": {}
    },
    {
        "label": "PerImageEvaluation",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.per_image_evaluation",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.per_image_evaluation",
        "peekOfCode": "class PerImageEvaluation(object):\n    \"\"\"Evaluate detection result of a single image.\"\"\"\n    def __init__(self, num_groundtruth_classes, matching_iou_threshold=0.5):\n        \"\"\"Initialized PerImageEvaluation by evaluation parameters.\n        Args:\n          num_groundtruth_classes: Number of ground truth object classes\n          matching_iou_threshold: A ratio of area intersection to union, which is\n              the threshold to consider whether a detection is true positive or not\n        \"\"\"\n        self.matching_iou_threshold = matching_iou_threshold",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.per_image_evaluation",
        "documentation": {}
    },
    {
        "label": "InputDataFields",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.standard_fields",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.standard_fields",
        "peekOfCode": "class InputDataFields(object):\n    \"\"\"Names for the input tensors.\n    Holds the standard data field names to use for identifying input tensors. This\n    should be used by the decoder to identify keys for the returned tensor_dict\n    containing input tensors. And it should be used by the model to identify the\n    tensors it needs.\n    Attributes:\n      image: image.\n      original_image: image in the original input size.\n      key: unique key corresponding to image.",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.standard_fields",
        "documentation": {}
    },
    {
        "label": "DetectionResultFields",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.standard_fields",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.standard_fields",
        "peekOfCode": "class DetectionResultFields(object):\n    \"\"\"Naming conventions for storing the output of the detector.\n    Attributes:\n      source_id: source of the original image.\n      key: unique key corresponding to image.\n      detection_boxes: coordinates of the detection boxes in the image.\n      detection_scores: detection scores for the detection boxes in the image.\n      detection_classes: detection-level class labels.\n      detection_masks: contains a segmentation mask for each detection box.\n      detection_boundaries: contains an object boundary for each detection box.",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.standard_fields",
        "documentation": {}
    },
    {
        "label": "BoxListFields",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.standard_fields",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.standard_fields",
        "peekOfCode": "class BoxListFields(object):\n    \"\"\"Naming conventions for BoxLists.\n    Attributes:\n      boxes: bounding box coordinates.\n      classes: classes per bounding box.\n      scores: scores per bounding box.\n      weights: sample weights per bounding box.\n      objectness: objectness score per bounding box.\n      masks: masks per bounding box.\n      boundaries: boundaries per bounding box.",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.standard_fields",
        "documentation": {}
    },
    {
        "label": "TfExampleFields",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.utils.ava_evaluation.standard_fields",
        "description": "MorphMLP.slowfast.utils.ava_evaluation.standard_fields",
        "peekOfCode": "class TfExampleFields(object):\n    \"\"\"TF-example proto feature names for object detection.\n    Holds the standard feature names to load from an Example proto for object\n    detection.\n    Attributes:\n      image_encoded: JPEG encoded string\n      image_format: image format, e.g. \"JPEG\"\n      filename: filename\n      channels: number of channels of image\n      colorspace: colorspace, e.g. \"RGB\"",
        "detail": "MorphMLP.slowfast.utils.ava_evaluation.standard_fields",
        "documentation": {}
    },
    {
        "label": "make_image_key",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_eval_helper",
        "description": "MorphMLP.slowfast.utils.ava_eval_helper",
        "peekOfCode": "def make_image_key(video_id, timestamp):\n    \"\"\"Returns a unique identifier for a video id & timestamp.\"\"\"\n    return \"%s,%04d\" % (video_id, int(timestamp))\ndef read_csv(csv_file, class_whitelist=None, load_score=False):\n    \"\"\"Loads boxes and class labels from a CSV file in the AVA format.\n    CSV file format described at https://research.google.com/ava/download.html.\n    Args:\n      csv_file: A file object.\n      class_whitelist: If provided, boxes corresponding to (integer) class labels\n        not in this set are skipped.",
        "detail": "MorphMLP.slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "read_csv",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_eval_helper",
        "description": "MorphMLP.slowfast.utils.ava_eval_helper",
        "peekOfCode": "def read_csv(csv_file, class_whitelist=None, load_score=False):\n    \"\"\"Loads boxes and class labels from a CSV file in the AVA format.\n    CSV file format described at https://research.google.com/ava/download.html.\n    Args:\n      csv_file: A file object.\n      class_whitelist: If provided, boxes corresponding to (integer) class labels\n        not in this set are skipped.\n    Returns:\n      boxes: A dictionary mapping each unique image key (string) to a list of\n        boxes, given as coordinates [y1, x1, y2, x2].",
        "detail": "MorphMLP.slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "read_exclusions",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_eval_helper",
        "description": "MorphMLP.slowfast.utils.ava_eval_helper",
        "peekOfCode": "def read_exclusions(exclusions_file):\n    \"\"\"Reads a CSV file of excluded timestamps.\n    Args:\n      exclusions_file: A file object containing a csv of video-id,timestamp.\n    Returns:\n      A set of strings containing excluded image keys, e.g. \"aaaaaaaaaaa,0904\",\n      or an empty set if exclusions file is None.\n    \"\"\"\n    excluded = set()\n    if exclusions_file:",
        "detail": "MorphMLP.slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "read_labelmap",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_eval_helper",
        "description": "MorphMLP.slowfast.utils.ava_eval_helper",
        "peekOfCode": "def read_labelmap(labelmap_file):\n    \"\"\"Read label map and class ids.\"\"\"\n    labelmap = []\n    class_ids = set()\n    name = \"\"\n    class_id = \"\"\n    with pathmgr.open(labelmap_file, \"r\") as f:\n        for line in f:\n            if line.startswith(\"  name:\"):\n                name = line.split('\"')[1]",
        "detail": "MorphMLP.slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "evaluate_ava_from_files",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_eval_helper",
        "description": "MorphMLP.slowfast.utils.ava_eval_helper",
        "peekOfCode": "def evaluate_ava_from_files(labelmap, groundtruth, detections, exclusions):\n    \"\"\"Run AVA evaluation given annotation/prediction files.\"\"\"\n    categories, class_whitelist = read_labelmap(labelmap)\n    excluded_keys = read_exclusions(exclusions)\n    groundtruth = read_csv(groundtruth, class_whitelist, load_score=False)\n    detections = read_csv(detections, class_whitelist, load_score=True)\n    run_evaluation(categories, groundtruth, detections, excluded_keys)\ndef evaluate_ava(\n    preds,\n    original_boxes,",
        "detail": "MorphMLP.slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "evaluate_ava",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_eval_helper",
        "description": "MorphMLP.slowfast.utils.ava_eval_helper",
        "peekOfCode": "def evaluate_ava(\n    preds,\n    original_boxes,\n    metadata,\n    excluded_keys,\n    class_whitelist,\n    categories,\n    groundtruth=None,\n    video_idx_to_name=None,\n    name=\"latest\",",
        "detail": "MorphMLP.slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "run_evaluation",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_eval_helper",
        "description": "MorphMLP.slowfast.utils.ava_eval_helper",
        "peekOfCode": "def run_evaluation(\n    categories, groundtruth, detections, excluded_keys, verbose=True\n):\n    \"\"\"AVA evaluation main logic.\"\"\"\n    pascal_evaluator = object_detection_evaluation.PascalDetectionEvaluator(\n        categories\n    )\n    boxes, labels, _ = groundtruth\n    gt_keys = []\n    pred_keys = []",
        "detail": "MorphMLP.slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "get_ava_eval_data",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_eval_helper",
        "description": "MorphMLP.slowfast.utils.ava_eval_helper",
        "peekOfCode": "def get_ava_eval_data(\n    scores,\n    boxes,\n    metadata,\n    class_whitelist,\n    verbose=False,\n    video_idx_to_name=None,\n):\n    \"\"\"\n    Convert our data format into the data format used in official AVA",
        "detail": "MorphMLP.slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "write_results",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.ava_eval_helper",
        "description": "MorphMLP.slowfast.utils.ava_eval_helper",
        "peekOfCode": "def write_results(detections, filename):\n    \"\"\"Write prediction results into official formats.\"\"\"\n    start = time.time()\n    boxes, labels, scores = detections\n    with pathmgr.open(filename, \"w\") as f:\n        for key in boxes.keys():\n            for box, label, score in zip(boxes[key], labels[key], scores[key]):\n                f.write(\n                    \"%s,%.03f,%.03f,%.03f,%.03f,%d,%.04f\\n\"\n                    % (key, box[1], box[0], box[3], box[2], label, score)",
        "detail": "MorphMLP.slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.utils.ava_eval_helper",
        "description": "MorphMLP.slowfast.utils.ava_eval_helper",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef make_image_key(video_id, timestamp):\n    \"\"\"Returns a unique identifier for a video id & timestamp.\"\"\"\n    return \"%s,%04d\" % (video_id, int(timestamp))\ndef read_csv(csv_file, class_whitelist=None, load_score=False):\n    \"\"\"Loads boxes and class labels from a CSV file in the AVA format.\n    CSV file format described at https://research.google.com/ava/download.html.\n    Args:\n      csv_file: A file object.\n      class_whitelist: If provided, boxes corresponding to (integer) class labels",
        "detail": "MorphMLP.slowfast.utils.ava_eval_helper",
        "documentation": {}
    },
    {
        "label": "benchmark_data_loading",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.benchmark",
        "description": "MorphMLP.slowfast.utils.benchmark",
        "peekOfCode": "def benchmark_data_loading(cfg):\n    \"\"\"\n    Benchmark the speed of data loading in PySlowFast.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n    \"\"\"\n    # Set up environment.\n    setup_environment()\n    # Set random seed from configs.",
        "detail": "MorphMLP.slowfast.utils.benchmark",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.utils.benchmark",
        "description": "MorphMLP.slowfast.utils.benchmark",
        "peekOfCode": "logger = logging.get_logger(__name__)\ndef benchmark_data_loading(cfg):\n    \"\"\"\n    Benchmark the speed of data loading in PySlowFast.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n    \"\"\"\n    # Set up environment.\n    setup_environment()",
        "detail": "MorphMLP.slowfast.utils.benchmark",
        "documentation": {}
    },
    {
        "label": "compute_and_update_bn_stats",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.bn_helper",
        "description": "MorphMLP.slowfast.utils.bn_helper",
        "peekOfCode": "def compute_and_update_bn_stats(model, data_loader, num_batches=200):\n    \"\"\"\n    Compute and update the batch norm stats to make it more precise. During\n    training both bn stats and the weight are changing after every iteration,\n    so the bn can not precisely reflect the latest stats of the current model.\n    Here the bn stats is recomputed without change of weights, to make the\n    running mean and running var more precise.\n    Args:\n        model (model): the model using to compute and update the bn stats.\n        data_loader (dataloader): dataloader using to provide inputs.",
        "detail": "MorphMLP.slowfast.utils.bn_helper",
        "documentation": {}
    },
    {
        "label": "get_name_convert_func",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.c2_model_loading",
        "description": "MorphMLP.slowfast.utils.c2_model_loading",
        "peekOfCode": "def get_name_convert_func():\n    \"\"\"\n    Get the function to convert Caffe2 layer names to PyTorch layer names.\n    Returns:\n        (func): function to convert parameter name from Caffe2 format to PyTorch\n        format.\n    \"\"\"\n    pairs = [\n        # ------------------------------------------------------------\n        # 'nonlocal_conv3_1_theta_w' -> 's3.pathway0_nonlocal3.conv_g.weight'",
        "detail": "MorphMLP.slowfast.utils.c2_model_loading",
        "documentation": {}
    },
    {
        "label": "make_checkpoint_dir",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint",
        "description": "MorphMLP.slowfast.utils.checkpoint",
        "peekOfCode": "def make_checkpoint_dir(path_to_job):\n    \"\"\"\n    Creates the checkpoint directory (if not present already).\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n    \"\"\"\n    checkpoint_dir = os.path.join(path_to_job, \"checkpoints\")\n    # Create the checkpoint dir from the master process\n    if du.is_master_proc() and not g_pathmgr.exists(checkpoint_dir):\n        try:",
        "detail": "MorphMLP.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "get_checkpoint_dir",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint",
        "description": "MorphMLP.slowfast.utils.checkpoint",
        "peekOfCode": "def get_checkpoint_dir(path_to_job):\n    \"\"\"\n    Get path for storing checkpoints.\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n    \"\"\"\n    return os.path.join(path_to_job, \"checkpoints\")\ndef get_path_to_checkpoint(path_to_job, epoch):\n    \"\"\"\n    Get the full path to a checkpoint file.",
        "detail": "MorphMLP.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "get_path_to_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint",
        "description": "MorphMLP.slowfast.utils.checkpoint",
        "peekOfCode": "def get_path_to_checkpoint(path_to_job, epoch):\n    \"\"\"\n    Get the full path to a checkpoint file.\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n        epoch (int): the number of epoch for the checkpoint.\n    \"\"\"\n    name = \"checkpoint_epoch_{:05d}.pyth\".format(epoch)\n    return os.path.join(get_checkpoint_dir(path_to_job), name)\ndef get_last_checkpoint(path_to_job):",
        "detail": "MorphMLP.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint",
        "description": "MorphMLP.slowfast.utils.checkpoint",
        "peekOfCode": "def get_last_checkpoint(path_to_job):\n    \"\"\"\n    Get the last checkpoint from the checkpointing folder.\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n    \"\"\"\n    d = get_checkpoint_dir(path_to_job)\n    names = g_pathmgr.ls(d) if g_pathmgr.exists(d) else []\n    names = [f for f in names if \"checkpoint\" in f]\n    assert len(names), \"No checkpoints found in '{}'.\".format(d)",
        "detail": "MorphMLP.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "has_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint",
        "description": "MorphMLP.slowfast.utils.checkpoint",
        "peekOfCode": "def has_checkpoint(path_to_job):\n    \"\"\"\n    Determines if the given directory contains a checkpoint.\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n    \"\"\"\n    d = get_checkpoint_dir(path_to_job)\n    files = g_pathmgr.ls(d) if g_pathmgr.exists(d) else []\n    return any(\"checkpoint\" in f for f in files)\ndef is_checkpoint_epoch(cfg, cur_epoch, multigrid_schedule=None):",
        "detail": "MorphMLP.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "is_checkpoint_epoch",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint",
        "description": "MorphMLP.slowfast.utils.checkpoint",
        "peekOfCode": "def is_checkpoint_epoch(cfg, cur_epoch, multigrid_schedule=None):\n    \"\"\"\n    Determine if a checkpoint should be saved on current epoch.\n    Args:\n        cfg (CfgNode): configs to save.\n        cur_epoch (int): current number of epoch of the model.\n        multigrid_schedule (List): schedule for multigrid training.\n    \"\"\"\n    if cur_epoch + 1 == cfg.SOLVER.MAX_EPOCH:\n        return True",
        "detail": "MorphMLP.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "save_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint",
        "description": "MorphMLP.slowfast.utils.checkpoint",
        "peekOfCode": "def save_checkpoint(path_to_job, model, optimizer, loss_scaler, epoch, cfg):\n    \"\"\"\n    Save a checkpoint.\n    Args:\n        model (model): model to save the weight to the checkpoint.\n        optimizer (optim): optimizer to save the historical state.\n        loss_scaler (scaler): scaler for loss.\n        epoch (int): current number of epoch of the model.\n        cfg (CfgNode): configs to save.\n    \"\"\"",
        "detail": "MorphMLP.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "inflate_weight",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint",
        "description": "MorphMLP.slowfast.utils.checkpoint",
        "peekOfCode": "def inflate_weight(state_dict_2d, state_dict_3d):\n    \"\"\"\n    Inflate 2D model weights in state_dict_2d to the 3D model weights in\n    state_dict_3d. The details can be found in:\n    Joao Carreira, and Andrew Zisserman.\n    \"Quo vadis, action recognition? a new model and the kinetics dataset.\"\n    Args:\n        state_dict_2d (OrderedDict): a dict of parameters from a 2D model.\n        state_dict_3d (OrderedDict): a dict of parameters from a 3D model.\n    Returns:",
        "detail": "MorphMLP.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint",
        "description": "MorphMLP.slowfast.utils.checkpoint",
        "peekOfCode": "def load_checkpoint(\n    path_to_checkpoint,\n    model,\n    loss_scaler=None,\n    data_parallel=True,\n    optimizer=None,\n    inflation=False,\n    convert_from_caffe2=False,\n    epoch_reset=False,\n    clear_name_pattern=(),",
        "detail": "MorphMLP.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "sub_to_normal_bn",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint",
        "description": "MorphMLP.slowfast.utils.checkpoint",
        "peekOfCode": "def sub_to_normal_bn(sd):\n    \"\"\"\n    Convert the Sub-BN paprameters to normal BN parameters in a state dict.\n    There are two copies of BN layers in a Sub-BN implementation: `bn.bn` and\n    `bn.split_bn`. `bn.split_bn` is used during training and\n    \"compute_precise_bn\". Before saving or evaluation, its stats are copied to\n    `bn.bn`. We rename `bn.bn` to `bn` and store it to be consistent with normal\n    BN layers.\n    Args:\n        sd (OrderedDict): a dict of parameters whitch might contain Sub-BN",
        "detail": "MorphMLP.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "c2_normal_to_sub_bn",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint",
        "description": "MorphMLP.slowfast.utils.checkpoint",
        "peekOfCode": "def c2_normal_to_sub_bn(key, model_keys):\n    \"\"\"\n    Convert BN parameters to Sub-BN parameters if model contains Sub-BNs.\n    Args:\n        key (OrderedDict): source dict of parameters.\n        mdoel_key (OrderedDict): target dict of parameters.\n    Returns:\n        new_sd (OrderedDict): converted dict of parameters.\n    \"\"\"\n    if \"bn.running_\" in key:",
        "detail": "MorphMLP.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "normal_to_sub_bn",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint",
        "description": "MorphMLP.slowfast.utils.checkpoint",
        "peekOfCode": "def normal_to_sub_bn(checkpoint_sd, model_sd):\n    \"\"\"\n    Convert BN parameters to Sub-BN parameters if model contains Sub-BNs.\n    Args:\n        checkpoint_sd (OrderedDict): source dict of parameters.\n        model_sd (OrderedDict): target dict of parameters.\n    Returns:\n        new_sd (OrderedDict): converted dict of parameters.\n    \"\"\"\n    for key in model_sd:",
        "detail": "MorphMLP.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "load_test_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint",
        "description": "MorphMLP.slowfast.utils.checkpoint",
        "peekOfCode": "def load_test_checkpoint(cfg, model):\n    \"\"\"\n    Loading checkpoint logic for testing.\n    \"\"\"\n    # Load a checkpoint to test if applicable.\n    if cfg.TEST.CHECKPOINT_FILE_PATH != \"\":\n        # If no checkpoint found in MODEL_VIS.CHECKPOINT_FILE_PATH or in the current\n        # checkpoint folder, try to load checkpoint from\n        # TEST.CHECKPOINT_FILE_PATH and test it.\n        load_checkpoint(",
        "detail": "MorphMLP.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "load_train_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint",
        "description": "MorphMLP.slowfast.utils.checkpoint",
        "peekOfCode": "def load_train_checkpoint(cfg, model, optimizer, loss_scaler):\n    \"\"\"\n    Loading checkpoint logic for training.\n    \"\"\"\n    if cfg.TRAIN.AUTO_RESUME and has_checkpoint(cfg.OUTPUT_DIR):\n        last_checkpoint = get_last_checkpoint(cfg.OUTPUT_DIR)\n        logger.info(\"Load from last checkpoint, {}.\".format(last_checkpoint))\n        checkpoint_epoch = load_checkpoint(\n            last_checkpoint, model, loss_scaler, cfg.NUM_GPUS > 1, optimizer\n        )",
        "detail": "MorphMLP.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.utils.checkpoint",
        "description": "MorphMLP.slowfast.utils.checkpoint",
        "peekOfCode": "logger = logging.get_logger(__name__)\ndef make_checkpoint_dir(path_to_job):\n    \"\"\"\n    Creates the checkpoint directory (if not present already).\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n    \"\"\"\n    checkpoint_dir = os.path.join(path_to_job, \"checkpoints\")\n    # Create the checkpoint dir from the master process\n    if du.is_master_proc() and not g_pathmgr.exists(checkpoint_dir):",
        "detail": "MorphMLP.slowfast.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "make_checkpoint_dir",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def make_checkpoint_dir(path_to_job):\n    \"\"\"\n    Creates the checkpoint directory (if not present already).\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n    \"\"\"\n    checkpoint_dir = os.path.join(path_to_job, \"checkpoints\")\n    # Create the checkpoint dir from the master process\n    if du.is_master_proc() and not g_pathmgr.exists(checkpoint_dir):\n        try:",
        "detail": "MorphMLP.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "get_checkpoint_dir",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def get_checkpoint_dir(path_to_job):\n    \"\"\"\n    Get path for storing checkpoints.\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n    \"\"\"\n    return os.path.join(path_to_job, \"checkpoints\")\ndef get_path_to_checkpoint(path_to_job, epoch):\n    \"\"\"\n    Get the full path to a checkpoint file.",
        "detail": "MorphMLP.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "get_path_to_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def get_path_to_checkpoint(path_to_job, epoch):\n    \"\"\"\n    Get the full path to a checkpoint file.\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n        epoch (int): the number of epoch for the checkpoint.\n    \"\"\"\n    name = \"checkpoint_epoch_{:05d}.pyth\".format(epoch)\n    return os.path.join(get_checkpoint_dir(path_to_job), name)\ndef get_last_checkpoint(path_to_job):",
        "detail": "MorphMLP.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def get_last_checkpoint(path_to_job):\n    \"\"\"\n    Get the last checkpoint from the checkpointing folder.\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n    \"\"\"\n    d = get_checkpoint_dir(path_to_job)\n    names = g_pathmgr.ls(d) if g_pathmgr.exists(d) else []\n    names = [f for f in names if \"checkpoint\" in f]\n    assert len(names), \"No checkpoints found in '{}'.\".format(d)",
        "detail": "MorphMLP.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "has_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def has_checkpoint(path_to_job):\n    \"\"\"\n    Determines if the given directory contains a checkpoint.\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n    \"\"\"\n    d = get_checkpoint_dir(path_to_job)\n    files = g_pathmgr.ls(d) if g_pathmgr.exists(d) else []\n    return any(\"checkpoint\" in f for f in files)\ndef is_checkpoint_epoch(cfg, cur_epoch, multigrid_schedule=None):",
        "detail": "MorphMLP.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "is_checkpoint_epoch",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def is_checkpoint_epoch(cfg, cur_epoch, multigrid_schedule=None):\n    \"\"\"\n    Determine if a checkpoint should be saved on current epoch.\n    Args:\n        cfg (CfgNode): configs to save.\n        cur_epoch (int): current number of epoch of the model.\n        multigrid_schedule (List): schedule for multigrid training.\n    \"\"\"\n    if cur_epoch + 1 == cfg.SOLVER.MAX_EPOCH:\n        return True",
        "detail": "MorphMLP.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "save_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def save_checkpoint(path_to_job, model, optimizer, loss_scaler, epoch, cfg):\n    \"\"\"\n    Save a checkpoint.\n    Args:\n        model (model): model to save the weight to the checkpoint.\n        optimizer (optim): optimizer to save the historical state.\n        loss_scaler (scaler): scaler for loss.\n        epoch (int): current number of epoch of the model.\n        cfg (CfgNode): configs to save.\n    \"\"\"",
        "detail": "MorphMLP.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "inflate_weight",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def inflate_weight(state_dict_2d, state_dict_3d):\n    \"\"\"\n    Inflate 2D model weights in state_dict_2d to the 3D model weights in\n    state_dict_3d. The details can be found in:\n    Joao Carreira, and Andrew Zisserman.\n    \"Quo vadis, action recognition? a new model and the kinetics dataset.\"\n    Args:\n        state_dict_2d (OrderedDict): a dict of parameters from a 2D model.\n        state_dict_3d (OrderedDict): a dict of parameters from a 3D model.\n    Returns:",
        "detail": "MorphMLP.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def load_checkpoint(\n    path_to_checkpoint,\n    model,\n    loss_scaler=None,\n    data_parallel=True,\n    optimizer=None,\n    inflation=False,\n    convert_from_caffe2=False,\n    epoch_reset=False,\n    clear_name_pattern=(),",
        "detail": "MorphMLP.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "sub_to_normal_bn",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def sub_to_normal_bn(sd):\n    \"\"\"\n    Convert the Sub-BN paprameters to normal BN parameters in a state dict.\n    There are two copies of BN layers in a Sub-BN implementation: `bn.bn` and\n    `bn.split_bn`. `bn.split_bn` is used during training and\n    \"compute_precise_bn\". Before saving or evaluation, its stats are copied to\n    `bn.bn`. We rename `bn.bn` to `bn` and store it to be consistent with normal\n    BN layers.\n    Args:\n        sd (OrderedDict): a dict of parameters whitch might contain Sub-BN",
        "detail": "MorphMLP.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "c2_normal_to_sub_bn",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def c2_normal_to_sub_bn(key, model_keys):\n    \"\"\"\n    Convert BN parameters to Sub-BN parameters if model contains Sub-BNs.\n    Args:\n        key (OrderedDict): source dict of parameters.\n        mdoel_key (OrderedDict): target dict of parameters.\n    Returns:\n        new_sd (OrderedDict): converted dict of parameters.\n    \"\"\"\n    if \"bn.running_\" in key:",
        "detail": "MorphMLP.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "normal_to_sub_bn",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def normal_to_sub_bn(checkpoint_sd, model_sd):\n    \"\"\"\n    Convert BN parameters to Sub-BN parameters if model contains Sub-BNs.\n    Args:\n        checkpoint_sd (OrderedDict): source dict of parameters.\n        model_sd (OrderedDict): target dict of parameters.\n    Returns:\n        new_sd (OrderedDict): converted dict of parameters.\n    \"\"\"\n    for key in model_sd:",
        "detail": "MorphMLP.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "load_test_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def load_test_checkpoint(cfg, model):\n    \"\"\"\n    Loading checkpoint logic for testing.\n    \"\"\"\n    # Load a checkpoint to test if applicable.\n    if cfg.TEST.CHECKPOINT_FILE_PATH != \"\":\n        # If no checkpoint found in MODEL_VIS.CHECKPOINT_FILE_PATH or in the current\n        # checkpoint folder, try to load checkpoint from\n        # TEST.CHECKPOINT_FILE_PATH and test it.\n        load_checkpoint(",
        "detail": "MorphMLP.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "load_train_checkpoint",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.slowfast.utils.checkpoint_amp",
        "peekOfCode": "def load_train_checkpoint(cfg, model, optimizer, loss_scaler):\n    \"\"\"\n    Loading checkpoint logic for training.\n    \"\"\"\n    if cfg.TRAIN.AUTO_RESUME and has_checkpoint(cfg.OUTPUT_DIR):\n        last_checkpoint = get_last_checkpoint(cfg.OUTPUT_DIR)\n        logger.info(\"Load from last checkpoint, {}.\".format(last_checkpoint))\n        checkpoint_epoch = load_checkpoint(\n            last_checkpoint, model, loss_scaler, cfg.NUM_GPUS > 1, optimizer\n        )",
        "detail": "MorphMLP.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.utils.checkpoint_amp",
        "description": "MorphMLP.slowfast.utils.checkpoint_amp",
        "peekOfCode": "logger = logging.get_logger(__name__)\ndef make_checkpoint_dir(path_to_job):\n    \"\"\"\n    Creates the checkpoint directory (if not present already).\n    Args:\n        path_to_job (string): the path to the folder of the current job.\n    \"\"\"\n    checkpoint_dir = os.path.join(path_to_job, \"checkpoints\")\n    # Create the checkpoint dir from the master process\n    if du.is_master_proc() and not g_pathmgr.exists(checkpoint_dir):",
        "detail": "MorphMLP.slowfast.utils.checkpoint_amp",
        "documentation": {}
    },
    {
        "label": "GatherLayer",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.utils.distributed",
        "description": "MorphMLP.slowfast.utils.distributed",
        "peekOfCode": "class GatherLayer(torch.autograd.Function):\n    \"\"\"Gather tensors from all process, supporting backward propagation.\"\"\"\n    @staticmethod\n    def forward(ctx, input):\n        ctx.save_for_backward(input)\n        output = [torch.zeros_like(input) for _ in range(dist.get_world_size())]\n        dist.all_gather(output, input)\n        return tuple(output)\n    @staticmethod\n    def backward(ctx, *grads):",
        "detail": "MorphMLP.slowfast.utils.distributed",
        "documentation": {}
    },
    {
        "label": "AllGatherWithGradient",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.utils.distributed",
        "description": "MorphMLP.slowfast.utils.distributed",
        "peekOfCode": "class AllGatherWithGradient(torch.autograd.Function):\n    \"\"\"AllGatherWithGradient\"\"\"\n    @staticmethod\n    def forward(ctx, input):\n        world_size = dist.get_world_size()\n        x_gather = [torch.ones_like(input) for _ in range(world_size)]\n        torch.distributed.all_gather(x_gather, input, async_op=False)\n        x_gather = torch.cat(x_gather, dim=0)\n        return x_gather\n    @staticmethod",
        "detail": "MorphMLP.slowfast.utils.distributed",
        "documentation": {}
    },
    {
        "label": "all_gather",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.distributed",
        "description": "MorphMLP.slowfast.utils.distributed",
        "peekOfCode": "def all_gather(tensors):\n    \"\"\"\n    All gathers the provided tensors from all processes across machines.\n    Args:\n        tensors (list): tensors to perform all gather across all processes in\n        all machines.\n    \"\"\"\n    gather_list = []\n    output_tensor = []\n    world_size = dist.get_world_size()",
        "detail": "MorphMLP.slowfast.utils.distributed",
        "documentation": {}
    },
    {
        "label": "all_reduce",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.distributed",
        "description": "MorphMLP.slowfast.utils.distributed",
        "peekOfCode": "def all_reduce(tensors, average=True):\n    \"\"\"\n    All reduce the provided tensors from all processes across machines.\n    Args:\n        tensors (list): tensors to perform all reduce across all processes in\n        all machines.\n        average (bool): scales the reduced tensor by the number of overall\n        processes across all machines.\n    \"\"\"\n    for tensor in tensors:",
        "detail": "MorphMLP.slowfast.utils.distributed",
        "documentation": {}
    },
    {
        "label": "init_process_group",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.distributed",
        "description": "MorphMLP.slowfast.utils.distributed",
        "peekOfCode": "def init_process_group(\n    local_rank,\n    local_world_size,\n    shard_id,\n    num_shards,\n    init_method,\n    dist_backend=\"nccl\",\n):\n    \"\"\"\n    Initializes the default process group.",
        "detail": "MorphMLP.slowfast.utils.distributed",
        "documentation": {}
    },
    {
        "label": "is_master_proc",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.distributed",
        "description": "MorphMLP.slowfast.utils.distributed",
        "peekOfCode": "def is_master_proc(num_gpus=8):\n    \"\"\"\n    Determines if the current process is the master process.\n    \"\"\"\n    if torch.distributed.is_initialized():\n        return dist.get_rank() % num_gpus == 0\n    else:\n        return True\ndef is_root_proc():\n    \"\"\"",
        "detail": "MorphMLP.slowfast.utils.distributed",
        "documentation": {}
    },
    {
        "label": "is_root_proc",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.distributed",
        "description": "MorphMLP.slowfast.utils.distributed",
        "peekOfCode": "def is_root_proc():\n    \"\"\"\n    Determines if the current process is the root process.\n    \"\"\"\n    if torch.distributed.is_initialized():\n        return dist.get_rank() == 0\n    else:\n        return True\ndef get_rank():\n    \"\"\"",
        "detail": "MorphMLP.slowfast.utils.distributed",
        "documentation": {}
    },
    {
        "label": "get_rank",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.distributed",
        "description": "MorphMLP.slowfast.utils.distributed",
        "peekOfCode": "def get_rank():\n    \"\"\"\n    Get the rank of the current process.\n    \"\"\"\n    if not dist.is_available():\n        return 0\n    if not dist.is_initialized():\n        return 0\n    return dist.get_rank()\ndef synchronize():",
        "detail": "MorphMLP.slowfast.utils.distributed",
        "documentation": {}
    },
    {
        "label": "synchronize",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.distributed",
        "description": "MorphMLP.slowfast.utils.distributed",
        "peekOfCode": "def synchronize():\n    \"\"\"\n    Helper function to synchronize (barrier) among all processes when\n    using distributed training\n    \"\"\"\n    if not dist.is_available():\n        return\n    if not dist.is_initialized():\n        return\n    world_size = dist.get_world_size()",
        "detail": "MorphMLP.slowfast.utils.distributed",
        "documentation": {}
    },
    {
        "label": "all_gather_unaligned",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.distributed",
        "description": "MorphMLP.slowfast.utils.distributed",
        "peekOfCode": "def all_gather_unaligned(data, group=None):\n    \"\"\"\n    Run all_gather on arbitrary picklable data (not necessarily tensors).\n    Args:\n        data: any picklable object\n        group: a torch process group. By default, will use a group which\n            contains all ranks on gloo backend.\n    Returns:\n        list[data]: list of data gathered from each rank\n    \"\"\"",
        "detail": "MorphMLP.slowfast.utils.distributed",
        "documentation": {}
    },
    {
        "label": "setup_environment",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.env",
        "description": "MorphMLP.slowfast.utils.env",
        "peekOfCode": "def setup_environment():\n    global _ENV_SETUP_DONE\n    if _ENV_SETUP_DONE:\n        return\n    _ENV_SETUP_DONE = True",
        "detail": "MorphMLP.slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "_ENV_SETUP_DONE",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.utils.env",
        "description": "MorphMLP.slowfast.utils.env",
        "peekOfCode": "_ENV_SETUP_DONE = False\npathmgr = PathManagerFactory.get(key=\"pyslowfast\")\ncheckpoint_pathmgr = PathManagerFactory.get(key=\"pyslowfast_checkpoint\")\ndef setup_environment():\n    global _ENV_SETUP_DONE\n    if _ENV_SETUP_DONE:\n        return\n    _ENV_SETUP_DONE = True",
        "detail": "MorphMLP.slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "pathmgr",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.utils.env",
        "description": "MorphMLP.slowfast.utils.env",
        "peekOfCode": "pathmgr = PathManagerFactory.get(key=\"pyslowfast\")\ncheckpoint_pathmgr = PathManagerFactory.get(key=\"pyslowfast_checkpoint\")\ndef setup_environment():\n    global _ENV_SETUP_DONE\n    if _ENV_SETUP_DONE:\n        return\n    _ENV_SETUP_DONE = True",
        "detail": "MorphMLP.slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "checkpoint_pathmgr",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.utils.env",
        "description": "MorphMLP.slowfast.utils.env",
        "peekOfCode": "checkpoint_pathmgr = PathManagerFactory.get(key=\"pyslowfast_checkpoint\")\ndef setup_environment():\n    global _ENV_SETUP_DONE\n    if _ENV_SETUP_DONE:\n        return\n    _ENV_SETUP_DONE = True",
        "detail": "MorphMLP.slowfast.utils.env",
        "documentation": {}
    },
    {
        "label": "setup_logging",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.logging",
        "description": "MorphMLP.slowfast.utils.logging",
        "peekOfCode": "def setup_logging(output_dir=None):\n    \"\"\"\n    Sets up the logging for multiple processes. Only enable the logging for the\n    master process, and suppress logging for the non-master processes.\n    \"\"\"\n    # Set up logging format.\n    _FORMAT = \"[%(levelname)s: %(filename)s: %(lineno)4d]: %(message)s\"\n    if du.is_master_proc():\n        # Enable logging for the master process.\n        logging.root.handlers = []",
        "detail": "MorphMLP.slowfast.utils.logging",
        "documentation": {}
    },
    {
        "label": "get_logger",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.logging",
        "description": "MorphMLP.slowfast.utils.logging",
        "peekOfCode": "def get_logger(name):\n    \"\"\"\n    Retrieve the logger with the specified name or, if name is None, return a\n    logger which is the root logger of the hierarchy.\n    Args:\n        name (string): name of the logger.\n    \"\"\"\n    return logging.getLogger(name)\ndef log_json_stats(stats, output_dir=None):\n    \"\"\"",
        "detail": "MorphMLP.slowfast.utils.logging",
        "documentation": {}
    },
    {
        "label": "log_json_stats",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.logging",
        "description": "MorphMLP.slowfast.utils.logging",
        "peekOfCode": "def log_json_stats(stats, output_dir=None):\n    \"\"\"\n    Logs json stats.\n    Args:\n        stats (dict): a dictionary of statistical information to log.\n    \"\"\"\n    stats = {\n        k: decimal.Decimal(\"{:.3f}\".format(v)) if isinstance(v, float) else v\n        for k, v in stats.items()\n    }",
        "detail": "MorphMLP.slowfast.utils.logging",
        "documentation": {}
    },
    {
        "label": "get_lr_at_epoch",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.lr_policy",
        "description": "MorphMLP.slowfast.utils.lr_policy",
        "peekOfCode": "def get_lr_at_epoch(cfg, cur_epoch):\n    \"\"\"\n    Retrieve the learning rate of the current epoch with the option to perform\n    warm up in the beginning of the training stage.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        cur_epoch (float): the number of epoch of the current training stage.\n    \"\"\"\n    lr = get_lr_func(cfg.SOLVER.LR_POLICY)(cfg, cur_epoch)",
        "detail": "MorphMLP.slowfast.utils.lr_policy",
        "documentation": {}
    },
    {
        "label": "lr_func_cosine",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.lr_policy",
        "description": "MorphMLP.slowfast.utils.lr_policy",
        "peekOfCode": "def lr_func_cosine(cfg, cur_epoch):\n    \"\"\"\n    Retrieve the learning rate to specified values at specified epoch with the\n    cosine learning rate schedule. Details can be found in:\n    Ilya Loshchilov, and  Frank Hutter\n    SGDR: Stochastic Gradient Descent With Warm Restarts.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        cur_epoch (float): the number of epoch of the current training stage.",
        "detail": "MorphMLP.slowfast.utils.lr_policy",
        "documentation": {}
    },
    {
        "label": "lr_func_steps_with_relative_lrs",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.lr_policy",
        "description": "MorphMLP.slowfast.utils.lr_policy",
        "peekOfCode": "def lr_func_steps_with_relative_lrs(cfg, cur_epoch):\n    \"\"\"\n    Retrieve the learning rate to specified values at specified epoch with the\n    steps with relative learning rate schedule.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        cur_epoch (float): the number of epoch of the current training stage.\n    \"\"\"\n    ind = get_step_index(cfg, cur_epoch)",
        "detail": "MorphMLP.slowfast.utils.lr_policy",
        "documentation": {}
    },
    {
        "label": "get_step_index",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.lr_policy",
        "description": "MorphMLP.slowfast.utils.lr_policy",
        "peekOfCode": "def get_step_index(cfg, cur_epoch):\n    \"\"\"\n    Retrieves the lr step index for the given epoch.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        cur_epoch (float): the number of epoch of the current training stage.\n    \"\"\"\n    steps = cfg.SOLVER.STEPS + [cfg.SOLVER.MAX_EPOCH]\n    for ind, step in enumerate(steps):  # NoQA",
        "detail": "MorphMLP.slowfast.utils.lr_policy",
        "documentation": {}
    },
    {
        "label": "get_lr_func",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.lr_policy",
        "description": "MorphMLP.slowfast.utils.lr_policy",
        "peekOfCode": "def get_lr_func(lr_policy):\n    \"\"\"\n    Given the configs, retrieve the specified lr policy function.\n    Args:\n        lr_policy (string): the learning rate policy to use for the job.\n    \"\"\"\n    policy = \"lr_func_\" + lr_policy\n    if policy not in globals():\n        raise NotImplementedError(\"Unknown LR policy: {}\".format(lr_policy))\n    else:",
        "detail": "MorphMLP.slowfast.utils.lr_policy",
        "documentation": {}
    },
    {
        "label": "AVAMeter",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.utils.meters",
        "description": "MorphMLP.slowfast.utils.meters",
        "peekOfCode": "class AVAMeter(object):\n    \"\"\"\n    Measure the AVA train, val, and test stats.\n    \"\"\"\n    def __init__(self, overall_iters, cfg, mode):\n        \"\"\"\n        overall_iters (int): the overall number of iterations of one epoch.\n        cfg (CfgNode): configs.\n        mode (str): `train`, `val`, or `test` mode.\n        \"\"\"",
        "detail": "MorphMLP.slowfast.utils.meters",
        "documentation": {}
    },
    {
        "label": "TestMeter",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.utils.meters",
        "description": "MorphMLP.slowfast.utils.meters",
        "peekOfCode": "class TestMeter(object):\n    \"\"\"\n    Perform the multi-view ensemble for testing: each video with an unique index\n    will be sampled with multiple clips, and the predictions of the clips will\n    be aggregated to produce the final prediction for the video.\n    The accuracy is calculated with the given ground truth labels.\n    \"\"\"\n    def __init__(\n        self,\n        num_videos,",
        "detail": "MorphMLP.slowfast.utils.meters",
        "documentation": {}
    },
    {
        "label": "ScalarMeter",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.utils.meters",
        "description": "MorphMLP.slowfast.utils.meters",
        "peekOfCode": "class ScalarMeter(object):\n    \"\"\"\n    A scalar meter uses a deque to track a series of scaler values with a given\n    window size. It supports calculating the median and average values of the\n    window, and also supports calculating the global average.\n    \"\"\"\n    def __init__(self, window_size):\n        \"\"\"\n        Args:\n            window_size (int): size of the max length of the deque.",
        "detail": "MorphMLP.slowfast.utils.meters",
        "documentation": {}
    },
    {
        "label": "TrainMeter",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.utils.meters",
        "description": "MorphMLP.slowfast.utils.meters",
        "peekOfCode": "class TrainMeter(object):\n    \"\"\"\n    Measure training stats.\n    \"\"\"\n    def __init__(self, epoch_iters, cfg):\n        \"\"\"\n        Args:\n            epoch_iters (int): the overall number of iterations of one epoch.\n            cfg (CfgNode): configs.\n        \"\"\"",
        "detail": "MorphMLP.slowfast.utils.meters",
        "documentation": {}
    },
    {
        "label": "ValMeter",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.utils.meters",
        "description": "MorphMLP.slowfast.utils.meters",
        "peekOfCode": "class ValMeter(object):\n    \"\"\"\n    Measures validation stats.\n    \"\"\"\n    def __init__(self, max_iter, cfg):\n        \"\"\"\n        Args:\n            max_iter (int): the max number of iteration of the current epoch.\n            cfg (CfgNode): configs.\n        \"\"\"",
        "detail": "MorphMLP.slowfast.utils.meters",
        "documentation": {}
    },
    {
        "label": "EpochTimer",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.utils.meters",
        "description": "MorphMLP.slowfast.utils.meters",
        "peekOfCode": "class EpochTimer:\n    \"\"\"\n    A timer which computes the epoch time.\n    \"\"\"\n    def __init__(self) -> None:\n        self.timer = Timer()\n        self.timer.reset()\n        self.epoch_times = []\n    def reset(self) -> None:\n        \"\"\"",
        "detail": "MorphMLP.slowfast.utils.meters",
        "documentation": {}
    },
    {
        "label": "get_ava_mini_groundtruth",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.meters",
        "description": "MorphMLP.slowfast.utils.meters",
        "peekOfCode": "def get_ava_mini_groundtruth(full_groundtruth):\n    \"\"\"\n    Get the groundtruth annotations corresponding the \"subset\" of AVA val set.\n    We define the subset to be the frames such that (second % 4 == 0).\n    We optionally use subset for faster evaluation during training\n    (in order to track training progress).\n    Args:\n        full_groundtruth(dict): list of groundtruth.\n    \"\"\"\n    ret = [defaultdict(list), defaultdict(list), defaultdict(list)]",
        "detail": "MorphMLP.slowfast.utils.meters",
        "documentation": {}
    },
    {
        "label": "get_map",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.meters",
        "description": "MorphMLP.slowfast.utils.meters",
        "peekOfCode": "def get_map(preds, labels):\n    \"\"\"\n    Compute mAP for multi-label case.\n    Args:\n        preds (numpy tensor): num_examples x num_classes.\n        labels (numpy tensor): num_examples x num_classes.\n    Returns:\n        mean_ap (int): final mAP score.\n    \"\"\"\n    logger.info(\"Getting mAP for {} examples\".format(preds.shape[0]))",
        "detail": "MorphMLP.slowfast.utils.meters",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.utils.meters",
        "description": "MorphMLP.slowfast.utils.meters",
        "peekOfCode": "logger = logging.get_logger(__name__)\ndef get_ava_mini_groundtruth(full_groundtruth):\n    \"\"\"\n    Get the groundtruth annotations corresponding the \"subset\" of AVA val set.\n    We define the subset to be the frames such that (second % 4 == 0).\n    We optionally use subset for faster evaluation during training\n    (in order to track training progress).\n    Args:\n        full_groundtruth(dict): list of groundtruth.\n    \"\"\"",
        "detail": "MorphMLP.slowfast.utils.meters",
        "documentation": {}
    },
    {
        "label": "topks_correct",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.metrics",
        "description": "MorphMLP.slowfast.utils.metrics",
        "peekOfCode": "def topks_correct(preds, labels, ks):\n    \"\"\"\n    Given the predictions, labels, and a list of top-k values, compute the\n    number of correct predictions for each top-k value.\n    Args:\n        preds (array): array of predictions. Dimension is batchsize\n            N x ClassNum.\n        labels (array): array of labels. Dimension is batchsize N.\n        ks (list): list of top-k values. For example, ks = [1, 5] correspods\n            to top-1 and top-5.",
        "detail": "MorphMLP.slowfast.utils.metrics",
        "documentation": {}
    },
    {
        "label": "topk_errors",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.metrics",
        "description": "MorphMLP.slowfast.utils.metrics",
        "peekOfCode": "def topk_errors(preds, labels, ks):\n    \"\"\"\n    Computes the top-k error for each k.\n    Args:\n        preds (array): array of predictions. Dimension is N.\n        labels (array): array of labels. Dimension is N.\n        ks (list): list of ks to calculate the top accuracies.\n    \"\"\"\n    num_topks_correct = topks_correct(preds, labels, ks)\n    return [(1.0 - x / preds.size(0)) * 100.0 for x in num_topks_correct]",
        "detail": "MorphMLP.slowfast.utils.metrics",
        "documentation": {}
    },
    {
        "label": "topk_accuracies",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.metrics",
        "description": "MorphMLP.slowfast.utils.metrics",
        "peekOfCode": "def topk_accuracies(preds, labels, ks):\n    \"\"\"\n    Computes the top-k accuracy for each k.\n    Args:\n        preds (array): array of predictions. Dimension is N.\n        labels (array): array of labels. Dimension is N.\n        ks (list): list of ks to calculate the top accuracies.\n    \"\"\"\n    num_topks_correct = topks_correct(preds, labels, ks)\n    return [(x / preds.size(0)) * 100.0 for x in num_topks_correct]",
        "detail": "MorphMLP.slowfast.utils.metrics",
        "documentation": {}
    },
    {
        "label": "check_nan_losses",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.misc",
        "description": "MorphMLP.slowfast.utils.misc",
        "peekOfCode": "def check_nan_losses(loss):\n    \"\"\"\n    Determine whether the loss is NaN (not a number).\n    Args:\n        loss (loss): loss to check whether is NaN.\n    \"\"\"\n    if math.isnan(loss):\n        raise RuntimeError(\"ERROR: Got NaN losses {}\".format(datetime.now()))\ndef params_count(model, ignore_bn=False):\n    \"\"\"",
        "detail": "MorphMLP.slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "params_count",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.misc",
        "description": "MorphMLP.slowfast.utils.misc",
        "peekOfCode": "def params_count(model, ignore_bn=False):\n    \"\"\"\n    Compute the number of parameters.\n    Args:\n        model (model): model to count the number of parameters.\n    \"\"\"\n    if not ignore_bn:\n        return np.sum([p.numel() for p in model.parameters()]).item()\n    else:\n        count = 0",
        "detail": "MorphMLP.slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "gpu_mem_usage",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.misc",
        "description": "MorphMLP.slowfast.utils.misc",
        "peekOfCode": "def gpu_mem_usage():\n    \"\"\"\n    Compute the GPU memory usage for the current device (GB).\n    \"\"\"\n    if torch.cuda.is_available():\n        mem_usage_bytes = torch.cuda.max_memory_allocated()\n    else:\n        mem_usage_bytes = 0\n    return mem_usage_bytes / 1024**3\ndef cpu_mem_usage():",
        "detail": "MorphMLP.slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "cpu_mem_usage",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.misc",
        "description": "MorphMLP.slowfast.utils.misc",
        "peekOfCode": "def cpu_mem_usage():\n    \"\"\"\n    Compute the system memory (RAM) usage for the current device (GB).\n    Returns:\n        usage (float): used memory (GB).\n        total (float): total memory (GB).\n    \"\"\"\n    vram = psutil.virtual_memory()\n    usage = (vram.total - vram.available) / 1024**3\n    total = vram.total / 1024**3",
        "detail": "MorphMLP.slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "get_model_stats",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.misc",
        "description": "MorphMLP.slowfast.utils.misc",
        "peekOfCode": "def get_model_stats(model, cfg, mode, use_train_input):\n    \"\"\"\n    Compute statistics for the current model given the config.\n    Args:\n        model (model): model to perform analysis.\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        mode (str): Options include `flop` or `activation`. Compute either flop\n            (gflops) or activation count (mega).\n        use_train_input (bool): if True, compute statistics for training. Otherwise,",
        "detail": "MorphMLP.slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "log_model_info",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.misc",
        "description": "MorphMLP.slowfast.utils.misc",
        "peekOfCode": "def log_model_info(model, cfg, use_train_input=True):\n    \"\"\"\n    Log info, includes number of parameters, gpu usage, gflops and activation count.\n        The model info is computed when the model is in validation mode.\n    Args:\n        model (model): model to log the info.\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        use_train_input (bool): if True, log info for training. Otherwise,\n            log info for testing.",
        "detail": "MorphMLP.slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "is_eval_epoch",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.misc",
        "description": "MorphMLP.slowfast.utils.misc",
        "peekOfCode": "def is_eval_epoch(cfg, cur_epoch, multigrid_schedule):\n    \"\"\"\n    Determine if the model should be evaluated at the current epoch.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        cur_epoch (int): current epoch.\n        multigrid_schedule (List): schedule for multigrid training.\n    \"\"\"\n    if cur_epoch + 1 == cfg.SOLVER.MAX_EPOCH:",
        "detail": "MorphMLP.slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "plot_input",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.misc",
        "description": "MorphMLP.slowfast.utils.misc",
        "peekOfCode": "def plot_input(tensor, bboxes=(), texts=(), path=\"./tmp_vis.png\"):\n    \"\"\"\n    Plot the input tensor with the optional bounding box and save it to disk.\n    Args:\n        tensor (tensor): a tensor with shape of `NxCxHxW`.\n        bboxes (tuple): bounding boxes with format of [[x, y, h, w]].\n        texts (tuple): a tuple of string to plot.\n        path (str): path to the image to save to.\n    \"\"\"\n    tensor = tensor.float()",
        "detail": "MorphMLP.slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "frozen_bn_stats",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.misc",
        "description": "MorphMLP.slowfast.utils.misc",
        "peekOfCode": "def frozen_bn_stats(model):\n    \"\"\"\n    Set all the bn layers to eval mode.\n    Args:\n        model (model): model to set bn layers to eval mode.\n    \"\"\"\n    for m in model.modules():\n        if isinstance(m, nn.BatchNorm3d):\n            m.eval()\ndef aggregate_sub_bn_stats(module):",
        "detail": "MorphMLP.slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "aggregate_sub_bn_stats",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.misc",
        "description": "MorphMLP.slowfast.utils.misc",
        "peekOfCode": "def aggregate_sub_bn_stats(module):\n    \"\"\"\n    Recursively find all SubBN modules and aggregate sub-BN stats.\n    Args:\n        module (nn.Module)\n    Returns:\n        count (int): number of SubBN module found.\n    \"\"\"\n    count = 0\n    for child in module.children():",
        "detail": "MorphMLP.slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "launch_job",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.misc",
        "description": "MorphMLP.slowfast.utils.misc",
        "peekOfCode": "def launch_job(cfg, init_method, func, daemon=False):\n    \"\"\"\n    Run 'func' on one or more GPUs, specified in cfg\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        init_method (str): initialization method to launch the job with multiple\n            devices.\n        func (function): job to run on GPU(s)\n        daemon (bool): The spawned processes’ daemon flag. If set to True,",
        "detail": "MorphMLP.slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "get_class_names",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.misc",
        "description": "MorphMLP.slowfast.utils.misc",
        "peekOfCode": "def get_class_names(path, parent_path=None, subset_path=None):\n    \"\"\"\n    Read json file with entries {classname: index} and return\n    an array of class names in order.\n    If parent_path is provided, load and map all children to their ids.\n    Args:\n        path (str): path to class ids json file.\n            File must be in the format {\"class1\": id1, \"class2\": id2, ...}\n        parent_path (Optional[str]): path to parent-child json file.\n            File must be in the format {\"parent1\": [\"child1\", \"child2\", ...], ...}",
        "detail": "MorphMLP.slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.utils.misc",
        "description": "MorphMLP.slowfast.utils.misc",
        "peekOfCode": "logger = logging.get_logger(__name__)\ndef check_nan_losses(loss):\n    \"\"\"\n    Determine whether the loss is NaN (not a number).\n    Args:\n        loss (loss): loss to check whether is NaN.\n    \"\"\"\n    if math.isnan(loss):\n        raise RuntimeError(\"ERROR: Got NaN losses {}\".format(datetime.now()))\ndef params_count(model, ignore_bn=False):",
        "detail": "MorphMLP.slowfast.utils.misc",
        "documentation": {}
    },
    {
        "label": "MultigridSchedule",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.utils.multigrid",
        "description": "MorphMLP.slowfast.utils.multigrid",
        "peekOfCode": "class MultigridSchedule(object):\n    \"\"\"\n    This class defines multigrid training schedule and update cfg accordingly.\n    \"\"\"\n    def init_multigrid(self, cfg):\n        \"\"\"\n        Update cfg based on multigrid settings.\n        Args:\n            cfg (configs): configs that contains training and multigrid specific\n                hyperparameters. Details can be seen in",
        "detail": "MorphMLP.slowfast.utils.multigrid",
        "documentation": {}
    },
    {
        "label": "print_schedule",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.multigrid",
        "description": "MorphMLP.slowfast.utils.multigrid",
        "peekOfCode": "def print_schedule(schedule):\n    \"\"\"\n    Log schedule.\n    \"\"\"\n    logger.info(\"Long cycle index\\tBase shape\\tEpochs\")\n    for s in schedule:\n        logger.info(\"{}\\t{}\\t{}\".format(s[0], s[1], s[2]))\ndef get_current_long_cycle_shape(schedule, epoch):\n    \"\"\"\n    Given a schedule and epoch index, return the long cycle base shape.",
        "detail": "MorphMLP.slowfast.utils.multigrid",
        "documentation": {}
    },
    {
        "label": "get_current_long_cycle_shape",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.multigrid",
        "description": "MorphMLP.slowfast.utils.multigrid",
        "peekOfCode": "def get_current_long_cycle_shape(schedule, epoch):\n    \"\"\"\n    Given a schedule and epoch index, return the long cycle base shape.\n    Args:\n        schedule (configs): configs that contains training and multigrid specific\n            hyperparameters. Details can be seen in\n            slowfast/config/defaults.py.\n        cur_epoch (int): current epoch index.\n    Returns:\n        shapes (list): A list describing the base shape in a long cycle:",
        "detail": "MorphMLP.slowfast.utils.multigrid",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.utils.multigrid",
        "description": "MorphMLP.slowfast.utils.multigrid",
        "peekOfCode": "logger = logging.get_logger(__name__)\nclass MultigridSchedule(object):\n    \"\"\"\n    This class defines multigrid training schedule and update cfg accordingly.\n    \"\"\"\n    def init_multigrid(self, cfg):\n        \"\"\"\n        Update cfg based on multigrid settings.\n        Args:\n            cfg (configs): configs that contains training and multigrid specific",
        "detail": "MorphMLP.slowfast.utils.multigrid",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.multiprocessing",
        "description": "MorphMLP.slowfast.utils.multiprocessing",
        "peekOfCode": "def run(\n    local_rank,\n    num_proc,\n    func,\n    init_method,\n    shard_id,\n    num_shards,\n    backend,\n    cfg,\n    output_queue=None,",
        "detail": "MorphMLP.slowfast.utils.multiprocessing",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.parser",
        "description": "MorphMLP.slowfast.utils.parser",
        "peekOfCode": "def parse_args():\n    \"\"\"\n    Parse the following arguments for a default parser for PySlowFast users.\n    Args:\n        shard_id (int): shard id for the current machine. Starts from 0 to\n            num_shards - 1. If single machine is used, then set shard id to 0.\n        num_shards (int): number of shards using by the job.\n        init_method (str): initialization method to launch the job with multiple\n            devices. Options includes TCP or shared file-system for\n            initialization. details can be find in",
        "detail": "MorphMLP.slowfast.utils.parser",
        "documentation": {}
    },
    {
        "label": "load_config",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.parser",
        "description": "MorphMLP.slowfast.utils.parser",
        "peekOfCode": "def load_config(args):\n    \"\"\"\n    Given the arguemnts, load and initialize the configs.\n    Args:\n        args (argument): arguments includes `shard_id`, `num_shards`,\n            `init_method`, `cfg_file`, and `opts`.\n    \"\"\"\n    # Setup cfg.\n    cfg = get_cfg()\n    # Load config from cfg.",
        "detail": "MorphMLP.slowfast.utils.parser",
        "documentation": {}
    },
    {
        "label": "init_weights",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.utils.weight_init_helper",
        "description": "MorphMLP.slowfast.utils.weight_init_helper",
        "peekOfCode": "def init_weights(\n    model, fc_init_std=0.01, zero_init_final_bn=True, zero_init_final_conv=False\n):\n    \"\"\"\n    Performs ResNet style weight initialization.\n    Args:\n        fc_init_std (float): the expected standard deviation for fc layer.\n        zero_init_final_bn (bool): if True, zero initialize the final bn for\n            every bottleneck.\n    \"\"\"",
        "detail": "MorphMLP.slowfast.utils.weight_init_helper",
        "documentation": {}
    },
    {
        "label": "AsycnActionPredictor",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.visualization.async_predictor",
        "description": "MorphMLP.slowfast.visualization.async_predictor",
        "peekOfCode": "class AsycnActionPredictor:\n    class _Predictor(mp.Process):\n        def __init__(self, cfg, task_queue, result_queue, gpu_id=None):\n            \"\"\"\n            Predict Worker for Detectron2.\n            Args:\n                cfg (CfgNode): configs. Details can be found in\n                    slowfast/config/defaults.py\n                task_queue (mp.Queue): a shared queue for incoming task.\n                result_queue (mp.Queue): a shared queue for predicted results.",
        "detail": "MorphMLP.slowfast.visualization.async_predictor",
        "documentation": {}
    },
    {
        "label": "AsyncVis",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.visualization.async_predictor",
        "description": "MorphMLP.slowfast.visualization.async_predictor",
        "peekOfCode": "class AsyncVis:\n    class _VisWorker(mp.Process):\n        def __init__(self, video_vis, task_queue, result_queue):\n            \"\"\"\n            Visualization Worker for AsyncVis.\n            Args:\n                video_vis (VideoVisualizer object): object with tools for visualization.\n                task_queue (mp.Queue): a shared queue for incoming task for visualization.\n                result_queue (mp.Queue): a shared queue for visualized results.\n            \"\"\"",
        "detail": "MorphMLP.slowfast.visualization.async_predictor",
        "documentation": {}
    },
    {
        "label": "_StopToken",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.visualization.async_predictor",
        "description": "MorphMLP.slowfast.visualization.async_predictor",
        "peekOfCode": "class _StopToken:\n    pass\nclass AsyncDemo:\n    \"\"\"\n    Asynchronous Action Prediction and Visualization pipeline with AsyncVis.\n    \"\"\"\n    def __init__(self, cfg, async_vis):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in",
        "detail": "MorphMLP.slowfast.visualization.async_predictor",
        "documentation": {}
    },
    {
        "label": "AsyncDemo",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.visualization.async_predictor",
        "description": "MorphMLP.slowfast.visualization.async_predictor",
        "peekOfCode": "class AsyncDemo:\n    \"\"\"\n    Asynchronous Action Prediction and Visualization pipeline with AsyncVis.\n    \"\"\"\n    def __init__(self, cfg, async_vis):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in\n                slowfast/config/defaults.py\n            async_vis (AsyncVis object): asynchronous visualizer.",
        "detail": "MorphMLP.slowfast.visualization.async_predictor",
        "documentation": {}
    },
    {
        "label": "draw_predictions",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.visualization.async_predictor",
        "description": "MorphMLP.slowfast.visualization.async_predictor",
        "peekOfCode": "def draw_predictions(task, video_vis):\n    \"\"\"\n    Draw prediction for the given task.\n    Args:\n        task (TaskInfo object): task object that contain\n            the necessary information for visualization. (e.g. frames, preds)\n            All attributes must lie on CPU devices.\n        video_vis (VideoVisualizer object): the video visualizer object.\n    \"\"\"\n    boxes = task.bboxes",
        "detail": "MorphMLP.slowfast.visualization.async_predictor",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.visualization.async_predictor",
        "description": "MorphMLP.slowfast.visualization.async_predictor",
        "peekOfCode": "logger = logging.get_logger(__name__)\nclass AsycnActionPredictor:\n    class _Predictor(mp.Process):\n        def __init__(self, cfg, task_queue, result_queue, gpu_id=None):\n            \"\"\"\n            Predict Worker for Detectron2.\n            Args:\n                cfg (CfgNode): configs. Details can be found in\n                    slowfast/config/defaults.py\n                task_queue (mp.Queue): a shared queue for incoming task.",
        "detail": "MorphMLP.slowfast.visualization.async_predictor",
        "documentation": {}
    },
    {
        "label": "AVAVisualizerWithPrecomputedBox",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.visualization.ava_demo_precomputed_boxes",
        "description": "MorphMLP.slowfast.visualization.ava_demo_precomputed_boxes",
        "peekOfCode": "class AVAVisualizerWithPrecomputedBox:\n    \"\"\"\n    Visualize action predictions for videos or folder of images with precomputed\n    and ground-truth boxes in AVA format.\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in\n                slowfast/config/defaults.py",
        "detail": "MorphMLP.slowfast.visualization.ava_demo_precomputed_boxes",
        "documentation": {}
    },
    {
        "label": "merge_pred_gt_boxes",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.visualization.ava_demo_precomputed_boxes",
        "description": "MorphMLP.slowfast.visualization.ava_demo_precomputed_boxes",
        "peekOfCode": "def merge_pred_gt_boxes(pred_dict, gt_dict=None):\n    \"\"\"\n    Merge data from precomputed and ground-truth boxes dictionaries.\n    Args:\n        pred_dict (dict): a dict which maps from `frame_idx` to a list of `boxes`\n            and `labels`. Each `box` is a list of 4 box coordinates. `labels[i]` is\n            a list of labels for `boxes[i]`.\n        gt_dict (Optional[dict]): a dict which maps from `frame_idx` to a list of `boxes`\n            and `labels`. Each `box` is a list of 4 box coordinates. `labels[i]` is\n            a list of labels for `boxes[i]`. Note that label is -1 for predicted boxes.",
        "detail": "MorphMLP.slowfast.visualization.ava_demo_precomputed_boxes",
        "documentation": {}
    },
    {
        "label": "load_boxes_labels",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.visualization.ava_demo_precomputed_boxes",
        "description": "MorphMLP.slowfast.visualization.ava_demo_precomputed_boxes",
        "peekOfCode": "def load_boxes_labels(cfg, video_name, fps, img_width, img_height):\n    \"\"\"\n    Loading boxes and labels from AVA bounding boxes csv files.\n    Args:\n        cfg (CfgNode): config.\n        video_name (str): name of the given video.\n        fps (int or float): frames per second of the input video/images folder.\n        img_width (int): width of images in input video/images folder.\n        img_height (int): height of images in input video/images folder.\n    Returns:",
        "detail": "MorphMLP.slowfast.visualization.ava_demo_precomputed_boxes",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.visualization.ava_demo_precomputed_boxes",
        "description": "MorphMLP.slowfast.visualization.ava_demo_precomputed_boxes",
        "peekOfCode": "logger = logging.get_logger(__name__)\nclass AVAVisualizerWithPrecomputedBox:\n    \"\"\"\n    Visualize action predictions for videos or folder of images with precomputed\n    and ground-truth boxes in AVA format.\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in",
        "detail": "MorphMLP.slowfast.visualization.ava_demo_precomputed_boxes",
        "documentation": {}
    },
    {
        "label": "VideoManager",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.visualization.demo_loader",
        "description": "MorphMLP.slowfast.visualization.demo_loader",
        "peekOfCode": "class VideoManager:\n    \"\"\"\n    VideoManager object for getting frames from video source for inference.\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        \"\"\"",
        "detail": "MorphMLP.slowfast.visualization.demo_loader",
        "documentation": {}
    },
    {
        "label": "ThreadVideoManager",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.visualization.demo_loader",
        "description": "MorphMLP.slowfast.visualization.demo_loader",
        "peekOfCode": "class ThreadVideoManager:\n    \"\"\"\n    VideoManager object for getting frames from video source for inference\n    using multithreading for read and write frames.\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py",
        "detail": "MorphMLP.slowfast.visualization.demo_loader",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.visualization.demo_loader",
        "description": "MorphMLP.slowfast.visualization.demo_loader",
        "peekOfCode": "logger = logging.get_logger(__name__)\nclass VideoManager:\n    \"\"\"\n    VideoManager object for getting frames from video source for inference.\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py",
        "detail": "MorphMLP.slowfast.visualization.demo_loader",
        "documentation": {}
    },
    {
        "label": "GradCAM",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.visualization.gradcam_utils",
        "description": "MorphMLP.slowfast.visualization.gradcam_utils",
        "peekOfCode": "class GradCAM:\n    \"\"\"\n    GradCAM class helps create localization maps using the Grad-CAM method for input videos\n    and overlap the maps over the input videos as heatmaps.\n    https://arxiv.org/pdf/1610.02391.pdf\n    \"\"\"\n    def __init__(\n        self, model, target_layers, data_mean, data_std, colormap=\"viridis\"\n    ):\n        \"\"\"",
        "detail": "MorphMLP.slowfast.visualization.gradcam_utils",
        "documentation": {}
    },
    {
        "label": "WrongPredictionVis",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.visualization.prediction_vis",
        "description": "MorphMLP.slowfast.visualization.prediction_vis",
        "peekOfCode": "class WrongPredictionVis:\n    \"\"\"\n    WrongPredictionVis class for visualizing video inputs to Tensorboard\n    for instances that the model makes wrong predictions.\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in\n                slowfast/config/defaults.py",
        "detail": "MorphMLP.slowfast.visualization.prediction_vis",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.visualization.prediction_vis",
        "description": "MorphMLP.slowfast.visualization.prediction_vis",
        "peekOfCode": "logger = logging.get_logger(__name__)\nclass WrongPredictionVis:\n    \"\"\"\n    WrongPredictionVis class for visualizing video inputs to Tensorboard\n    for instances that the model makes wrong predictions.\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in",
        "detail": "MorphMLP.slowfast.visualization.prediction_vis",
        "documentation": {}
    },
    {
        "label": "Predictor",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.visualization.predictor",
        "description": "MorphMLP.slowfast.visualization.predictor",
        "peekOfCode": "class Predictor:\n    \"\"\"\n    Action Predictor for action recognition.\n    \"\"\"\n    def __init__(self, cfg, gpu_id=None):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in\n                slowfast/config/defaults.py\n            gpu_id (Optional[int]): GPU id.",
        "detail": "MorphMLP.slowfast.visualization.predictor",
        "documentation": {}
    },
    {
        "label": "ActionPredictor",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.visualization.predictor",
        "description": "MorphMLP.slowfast.visualization.predictor",
        "peekOfCode": "class ActionPredictor:\n    \"\"\"\n    Synchronous Action Prediction and Visualization pipeline with AsyncVis.\n    \"\"\"\n    def __init__(self, cfg, async_vis=None, gpu_id=None):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in\n                slowfast/config/defaults.py\n            async_vis (AsyncVis object): asynchronous visualizer.",
        "detail": "MorphMLP.slowfast.visualization.predictor",
        "documentation": {}
    },
    {
        "label": "Detectron2Predictor",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.visualization.predictor",
        "description": "MorphMLP.slowfast.visualization.predictor",
        "peekOfCode": "class Detectron2Predictor:\n    \"\"\"\n    Wrapper around Detectron2 to return the required predicted bounding boxes\n    as a ndarray.\n    \"\"\"\n    def __init__(self, cfg, gpu_id=None):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in\n                slowfast/config/defaults.py",
        "detail": "MorphMLP.slowfast.visualization.predictor",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.visualization.predictor",
        "description": "MorphMLP.slowfast.visualization.predictor",
        "peekOfCode": "logger = logging.get_logger(__name__)\nclass Predictor:\n    \"\"\"\n    Action Predictor for action recognition.\n    \"\"\"\n    def __init__(self, cfg, gpu_id=None):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in\n                slowfast/config/defaults.py",
        "detail": "MorphMLP.slowfast.visualization.predictor",
        "documentation": {}
    },
    {
        "label": "TensorboardWriter",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.visualization.tensorboard_vis",
        "description": "MorphMLP.slowfast.visualization.tensorboard_vis",
        "peekOfCode": "class TensorboardWriter(object):\n    \"\"\"\n    Helper class to log information to Tensorboard.\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in\n                slowfast/config/defaults.py\n        \"\"\"",
        "detail": "MorphMLP.slowfast.visualization.tensorboard_vis",
        "documentation": {}
    },
    {
        "label": "add_confusion_matrix",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.visualization.tensorboard_vis",
        "description": "MorphMLP.slowfast.visualization.tensorboard_vis",
        "peekOfCode": "def add_confusion_matrix(\n    writer,\n    cmtx,\n    num_classes,\n    global_step=None,\n    subset_ids=None,\n    class_names=None,\n    tag=\"Confusion Matrix\",\n    figsize=None,\n):",
        "detail": "MorphMLP.slowfast.visualization.tensorboard_vis",
        "documentation": {}
    },
    {
        "label": "plot_hist",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.visualization.tensorboard_vis",
        "description": "MorphMLP.slowfast.visualization.tensorboard_vis",
        "peekOfCode": "def plot_hist(\n    writer,\n    cmtx,\n    num_classes,\n    k=10,\n    global_step=None,\n    subset_ids=None,\n    class_names=None,\n    figsize=None,\n):",
        "detail": "MorphMLP.slowfast.visualization.tensorboard_vis",
        "documentation": {}
    },
    {
        "label": "add_ndim_array",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.visualization.tensorboard_vis",
        "description": "MorphMLP.slowfast.visualization.tensorboard_vis",
        "peekOfCode": "def add_ndim_array(\n    writer,\n    array,\n    name,\n    nrow=None,\n    normalize=False,\n    global_step=None,\n    heat_map=True,\n):\n    \"\"\"",
        "detail": "MorphMLP.slowfast.visualization.tensorboard_vis",
        "documentation": {}
    },
    {
        "label": "add_heatmap",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.visualization.tensorboard_vis",
        "description": "MorphMLP.slowfast.visualization.tensorboard_vis",
        "peekOfCode": "def add_heatmap(tensor):\n    \"\"\"\n    Add heatmap to 2D tensor.\n    Args:\n        tensor (tensor): a 2D tensor. Tensor value must be in [0..1] range.\n    Returns:\n        heatmap (tensor): a 3D tensor. Result of applying heatmap to the 2D tensor.\n    \"\"\"\n    assert tensor.ndim == 2, \"Only support 2D tensors.\"\n    # Move tensor to cpu if necessary.",
        "detail": "MorphMLP.slowfast.visualization.tensorboard_vis",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.visualization.tensorboard_vis",
        "description": "MorphMLP.slowfast.visualization.tensorboard_vis",
        "peekOfCode": "logger = logging.get_logger(__name__)\nlog.getLogger(\"matplotlib\").setLevel(log.ERROR)\nclass TensorboardWriter(object):\n    \"\"\"\n    Helper class to log information to Tensorboard.\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        Args:\n            cfg (CfgNode): configs. Details can be found in",
        "detail": "MorphMLP.slowfast.visualization.tensorboard_vis",
        "documentation": {}
    },
    {
        "label": "GetWeightAndActivation",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.visualization.utils",
        "description": "MorphMLP.slowfast.visualization.utils",
        "peekOfCode": "class GetWeightAndActivation:\n    \"\"\"\n    A class used to get weights and activations from specified layers from a Pytorch model.\n    \"\"\"\n    def __init__(self, model, layers):\n        \"\"\"\n        Args:\n            model (nn.Module): the model containing layers to obtain weights and activations from.\n            layers (list of strings): a list of layer names to obtain weights and activations from.\n                Names are hierarchical, separated by /. For example, If a layer follow a path",
        "detail": "MorphMLP.slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "TaskInfo",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.visualization.utils",
        "description": "MorphMLP.slowfast.visualization.utils",
        "peekOfCode": "class TaskInfo:\n    def __init__(self):\n        self.frames = None\n        self.id = -1\n        self.bboxes = None\n        self.action_preds = None\n        self.num_buffer_frames = 0\n        self.img_height = -1\n        self.img_width = -1\n        self.crop_size = -1",
        "detail": "MorphMLP.slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "get_confusion_matrix",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.visualization.utils",
        "description": "MorphMLP.slowfast.visualization.utils",
        "peekOfCode": "def get_confusion_matrix(preds, labels, num_classes, normalize=\"true\"):\n    \"\"\"\n    Calculate confusion matrix on the provided preds and labels.\n    Args:\n        preds (tensor or lists of tensors): predictions. Each tensor is in\n            in the shape of (n_batch, num_classes). Tensor(s) must be on CPU.\n        labels (tensor or lists of tensors): corresponding labels. Each tensor is\n            in the shape of either (n_batch,) or (n_batch, num_classes).\n        num_classes (int): number of classes. Tensor(s) must be on CPU.\n        normalize (Optional[str]) : {‘true’, ‘pred’, ‘all’}, default=\"true\"",
        "detail": "MorphMLP.slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "plot_confusion_matrix",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.visualization.utils",
        "description": "MorphMLP.slowfast.visualization.utils",
        "peekOfCode": "def plot_confusion_matrix(cmtx, num_classes, class_names=None, figsize=None):\n    \"\"\"\n    A function to create a colored and labeled confusion matrix matplotlib figure\n    given true labels and preds.\n    Args:\n        cmtx (ndarray): confusion matrix.\n        num_classes (int): total number of classes.\n        class_names (Optional[list of strs]): a list of class names.\n        figsize (Optional[float, float]): the figure size of the confusion matrix.\n            If None, default to [6.4, 4.8].",
        "detail": "MorphMLP.slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "plot_topk_histogram",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.visualization.utils",
        "description": "MorphMLP.slowfast.visualization.utils",
        "peekOfCode": "def plot_topk_histogram(tag, array, k=10, class_names=None, figsize=None):\n    \"\"\"\n    Plot histogram of top-k value from the given array.\n    Args:\n        tag (str): histogram title.\n        array (tensor): a tensor to draw top k value from.\n        k (int): number of top values to draw from array.\n            Defaut to 10.\n        class_names (list of strings, optional):\n            a list of names for values in array.",
        "detail": "MorphMLP.slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "get_indexing",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.visualization.utils",
        "description": "MorphMLP.slowfast.visualization.utils",
        "peekOfCode": "def get_indexing(string):\n    \"\"\"\n    Parse numpy-like fancy indexing from a string.\n    Args:\n        string (str): string represent the indices to take\n            a subset of from array. Indices for each dimension\n            are separated by `,`; indices for different dimensions\n            are separated by `;`.\n            e.g.: For a numpy array `arr` of shape (3,3,3), the string \"1,2;1,2\"\n            means taking the sub-array `arr[[1,2], [1,2]]",
        "detail": "MorphMLP.slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "process_layer_index_data",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.visualization.utils",
        "description": "MorphMLP.slowfast.visualization.utils",
        "peekOfCode": "def process_layer_index_data(layer_ls, layer_name_prefix=\"\"):\n    \"\"\"\n    Extract layer names and numpy-like fancy indexing from a string.\n    Args:\n        layer_ls (list of strs): list of strings containing data about layer names\n            and their indexing. For each string, layer name and indexing is separated by whitespaces.\n            e.g.: [layer1 1,2;2, layer2, layer3 150;3,4]\n        layer_name_prefix (Optional[str]): prefix to be added to each layer name.\n    Returns:\n        layer_name (list of strings): a list of layer names.",
        "detail": "MorphMLP.slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "process_cv2_inputs",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.visualization.utils",
        "description": "MorphMLP.slowfast.visualization.utils",
        "peekOfCode": "def process_cv2_inputs(frames, cfg):\n    \"\"\"\n    Normalize and prepare inputs as a list of tensors. Each tensor\n    correspond to a unique pathway.\n    Args:\n        frames (list of array): list of input images (correspond to one clip) in range [0, 255].\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n    \"\"\"\n    inputs = torch.from_numpy(np.array(frames)).float() / 255",
        "detail": "MorphMLP.slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "get_layer",
        "kind": 2,
        "importPath": "MorphMLP.slowfast.visualization.utils",
        "description": "MorphMLP.slowfast.visualization.utils",
        "peekOfCode": "def get_layer(model, layer_name):\n    \"\"\"\n    Return the targeted layer (nn.Module Object) given a hierarchical layer name,\n    separated by /.\n    Args:\n        model (model): model to get layers from.\n        layer_name (str): name of the layer.\n    Returns:\n        prev_module (nn.Module): the layer from the model with `layer_name` name.\n    \"\"\"",
        "detail": "MorphMLP.slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.visualization.utils",
        "description": "MorphMLP.slowfast.visualization.utils",
        "peekOfCode": "logger = logging.get_logger(__name__)\ndef get_confusion_matrix(preds, labels, num_classes, normalize=\"true\"):\n    \"\"\"\n    Calculate confusion matrix on the provided preds and labels.\n    Args:\n        preds (tensor or lists of tensors): predictions. Each tensor is in\n            in the shape of (n_batch, num_classes). Tensor(s) must be on CPU.\n        labels (tensor or lists of tensors): corresponding labels. Each tensor is\n            in the shape of either (n_batch,) or (n_batch, num_classes).\n        num_classes (int): number of classes. Tensor(s) must be on CPU.",
        "detail": "MorphMLP.slowfast.visualization.utils",
        "documentation": {}
    },
    {
        "label": "ImgVisualizer",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.visualization.video_visualizer",
        "description": "MorphMLP.slowfast.visualization.video_visualizer",
        "peekOfCode": "class ImgVisualizer(Visualizer):\n    def __init__(self, img_rgb, meta, **kwargs):\n        \"\"\"\n        See https://github.com/facebookresearch/detectron2/blob/master/detectron2/utils/visualizer.py\n        for more details.\n        Args:\n            img_rgb: a tensor or numpy array of shape (H, W, C), where H and W correspond to\n                the height and width of the image respectively. C is the number of\n                color channels. The image is required to be in RGB format since that\n                is a requirement of the Matplotlib library. The image is also expected",
        "detail": "MorphMLP.slowfast.visualization.video_visualizer",
        "documentation": {}
    },
    {
        "label": "VideoVisualizer",
        "kind": 6,
        "importPath": "MorphMLP.slowfast.visualization.video_visualizer",
        "description": "MorphMLP.slowfast.visualization.video_visualizer",
        "peekOfCode": "class VideoVisualizer:\n    def __init__(\n        self,\n        num_classes,\n        class_names_path,\n        top_k=1,\n        colormap=\"rainbow\",\n        thres=0.7,\n        lower_thres=0.3,\n        common_class_names=None,",
        "detail": "MorphMLP.slowfast.visualization.video_visualizer",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.slowfast.visualization.video_visualizer",
        "description": "MorphMLP.slowfast.visualization.video_visualizer",
        "peekOfCode": "logger = logging.get_logger(__name__)\nlog.getLogger(\"matplotlib\").setLevel(log.ERROR)\ndef _create_text_labels(classes, scores, class_names, ground_truth=False):\n    \"\"\"\n    Create text labels.\n    Args:\n        classes (list[int]): a list of class ids for each example.\n        scores (list[float] or None): list of scores for each example.\n        class_names (list[str]): a list of class names, ordered by their ids.\n        ground_truth (bool): whether the labels are ground truth.",
        "detail": "MorphMLP.slowfast.visualization.video_visualizer",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "MorphMLP.tools.benchmark",
        "description": "MorphMLP.tools.benchmark",
        "peekOfCode": "def main():\n    args = parse_args()\n    cfg = load_config(args)\n    launch_job(\n        cfg=cfg, init_method=args.init_method, func=benchmark_data_loading\n    )\nif __name__ == \"__main__\":\n    main()",
        "detail": "MorphMLP.tools.benchmark",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.tools.benchmark",
        "description": "MorphMLP.tools.benchmark",
        "peekOfCode": "logger = logging.get_logger(__name__)\ndef main():\n    args = parse_args()\n    cfg = load_config(args)\n    launch_job(\n        cfg=cfg, init_method=args.init_method, func=benchmark_data_loading\n    )\nif __name__ == \"__main__\":\n    main()",
        "detail": "MorphMLP.tools.benchmark",
        "documentation": {}
    },
    {
        "label": "run_demo",
        "kind": 2,
        "importPath": "MorphMLP.tools.demo_net",
        "description": "MorphMLP.tools.demo_net",
        "peekOfCode": "def run_demo(cfg, frame_provider):\n    \"\"\"\n    Run demo visualization.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        frame_provider (iterator): Python iterator that return task objects that are filled\n            with necessary information such as `frames`, `id` and `num_buffer_frames` for the\n            prediction and visualization pipeline.\n    \"\"\"",
        "detail": "MorphMLP.tools.demo_net",
        "documentation": {}
    },
    {
        "label": "demo",
        "kind": 2,
        "importPath": "MorphMLP.tools.demo_net",
        "description": "MorphMLP.tools.demo_net",
        "peekOfCode": "def demo(cfg):\n    \"\"\"\n    Run inference on an input video or stream from webcam.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n    \"\"\"\n    # AVA format-specific visualization with precomputed boxes.\n    if cfg.DETECTION.ENABLE and cfg.DEMO.PREDS_BOXES != \"\":\n        precomputed_box_vis = AVAVisualizerWithPrecomputedBox(cfg)",
        "detail": "MorphMLP.tools.demo_net",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.tools.demo_net",
        "description": "MorphMLP.tools.demo_net",
        "peekOfCode": "logger = logging.get_logger(__name__)\ndef run_demo(cfg, frame_provider):\n    \"\"\"\n    Run demo visualization.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        frame_provider (iterator): Python iterator that return task objects that are filled\n            with necessary information such as `frames`, `id` and `num_buffer_frames` for the\n            prediction and visualization pipeline.",
        "detail": "MorphMLP.tools.demo_net",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "MorphMLP.tools.run_net",
        "description": "MorphMLP.tools.run_net",
        "peekOfCode": "def main():\n    \"\"\"\n    Main function to spawn the train and test process.\n    \"\"\"\n    args = parse_args()\n    cfg = load_config(args)\n    cfg = assert_and_infer_cfg(cfg)\n    # Perform training.\n    if cfg.TRAIN.ENABLE:\n        launch_job(cfg=cfg, init_method=args.init_method, func=train)",
        "detail": "MorphMLP.tools.run_net",
        "documentation": {}
    },
    {
        "label": "perform_test",
        "kind": 2,
        "importPath": "MorphMLP.tools.test_net",
        "description": "MorphMLP.tools.test_net",
        "peekOfCode": "def perform_test(test_loader, model, test_meter, cfg, writer=None):\n    \"\"\"\n    For classification:\n    Perform mutli-view testing that uniformly samples N clips from a video along\n    its temporal axis. For each clip, it takes 3 crops to cover the spatial\n    dimension, followed by averaging the softmax scores across all Nx3 views to\n    form a video-level prediction. All video predictions are compared to\n    ground-truth labels and the final testing performance is logged.\n    For detection:\n    Perform fully-convolutional testing on the full frames without crop.",
        "detail": "MorphMLP.tools.test_net",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "MorphMLP.tools.test_net",
        "description": "MorphMLP.tools.test_net",
        "peekOfCode": "def test(cfg):\n    \"\"\"\n    Perform multi-view testing on the pretrained video model.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n    \"\"\"\n    # Set up environment.\n    du.init_distributed_training(cfg)\n    # Set random seed from configs.",
        "detail": "MorphMLP.tools.test_net",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.tools.test_net",
        "description": "MorphMLP.tools.test_net",
        "peekOfCode": "logger = logging.get_logger(__name__)\n@torch.no_grad()\ndef perform_test(test_loader, model, test_meter, cfg, writer=None):\n    \"\"\"\n    For classification:\n    Perform mutli-view testing that uniformly samples N clips from a video along\n    its temporal axis. For each clip, it takes 3 crops to cover the spatial\n    dimension, followed by averaging the softmax scores across all Nx3 views to\n    form a video-level prediction. All video predictions are compared to\n    ground-truth labels and the final testing performance is logged.",
        "detail": "MorphMLP.tools.test_net",
        "documentation": {}
    },
    {
        "label": "train_epoch",
        "kind": 2,
        "importPath": "MorphMLP.tools.train_net",
        "description": "MorphMLP.tools.train_net",
        "peekOfCode": "def train_epoch(\n    train_loader, model, optimizer, loss_scaler, train_meter, cur_epoch, cfg, writer=None\n):\n    \"\"\"\n    Perform the video training for one epoch.\n    Args:\n        train_loader (loader): video training loader.\n        model (model): the video model to train.\n        optimizer (optim): the optimizer to perform optimization on the model's\n            parameters.",
        "detail": "MorphMLP.tools.train_net",
        "documentation": {}
    },
    {
        "label": "eval_epoch",
        "kind": 2,
        "importPath": "MorphMLP.tools.train_net",
        "description": "MorphMLP.tools.train_net",
        "peekOfCode": "def eval_epoch(val_loader, model, val_meter, loss_scaler, cur_epoch, cfg, writer=None):\n    \"\"\"\n    Evaluate the model on the val set.\n    Args:\n        val_loader (loader): data loader to provide validation data.\n        model (model): model to evaluate the performance.\n        loss_scaler (scaler): scaler for loss.\n        val_meter (ValMeter): meter instance to record and calculate the metrics.\n        cur_epoch (int): number of the current epoch of training.\n        cfg (CfgNode): configs. Details can be found in",
        "detail": "MorphMLP.tools.train_net",
        "documentation": {}
    },
    {
        "label": "calculate_and_update_precise_bn",
        "kind": 2,
        "importPath": "MorphMLP.tools.train_net",
        "description": "MorphMLP.tools.train_net",
        "peekOfCode": "def calculate_and_update_precise_bn(loader, model, num_iters=200, use_gpu=True):\n    \"\"\"\n    Update the stats in bn layers by calculate the precise stats.\n    Args:\n        loader (loader): data loader to provide training data.\n        model (model): model to update the bn stats.\n        num_iters (int): number of iterations to compute and update the bn stats.\n        use_gpu (bool): whether to use GPU or not.\n    \"\"\"\n    def _gen_loader():",
        "detail": "MorphMLP.tools.train_net",
        "documentation": {}
    },
    {
        "label": "build_trainer",
        "kind": 2,
        "importPath": "MorphMLP.tools.train_net",
        "description": "MorphMLP.tools.train_net",
        "peekOfCode": "def build_trainer(cfg):\n    \"\"\"\n    Build training model and its associated tools, including optimizer,\n    dataloaders and meters.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n    Returns:\n        model (nn.Module): training model.\n        optimizer (Optimizer): optimizer.",
        "detail": "MorphMLP.tools.train_net",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "MorphMLP.tools.train_net",
        "description": "MorphMLP.tools.train_net",
        "peekOfCode": "def train(cfg):\n    \"\"\"\n    Train a video model for many epochs on train set and evaluate it on val set.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n    \"\"\"\n    # Set up environment.\n    du.init_distributed_training(cfg)\n    # Set random seed from configs.",
        "detail": "MorphMLP.tools.train_net",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.tools.train_net",
        "description": "MorphMLP.tools.train_net",
        "peekOfCode": "logger = logging.get_logger(__name__)\ndef train_epoch(\n    train_loader, model, optimizer, loss_scaler, train_meter, cur_epoch, cfg, writer=None\n):\n    \"\"\"\n    Perform the video training for one epoch.\n    Args:\n        train_loader (loader): video training loader.\n        model (model): the video model to train.\n        optimizer (optim): the optimizer to perform optimization on the model's",
        "detail": "MorphMLP.tools.train_net",
        "documentation": {}
    },
    {
        "label": "run_visualization",
        "kind": 2,
        "importPath": "MorphMLP.tools.visualization",
        "description": "MorphMLP.tools.visualization",
        "peekOfCode": "def run_visualization(vis_loader, model, cfg, writer=None):\n    \"\"\"\n    Run model visualization (weights, activations and model inputs) and visualize\n    them on Tensorboard.\n    Args:\n        vis_loader (loader): video visualization loader.\n        model (model): the video model to visualize.\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n        writer (TensorboardWriter, optional): TensorboardWriter object",
        "detail": "MorphMLP.tools.visualization",
        "documentation": {}
    },
    {
        "label": "perform_wrong_prediction_vis",
        "kind": 2,
        "importPath": "MorphMLP.tools.visualization",
        "description": "MorphMLP.tools.visualization",
        "peekOfCode": "def perform_wrong_prediction_vis(vis_loader, model, cfg):\n    \"\"\"\n    Visualize video inputs with wrong predictions on Tensorboard.\n    Args:\n        vis_loader (loader): video visualization loader.\n        model (model): the video model to visualize.\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n    \"\"\"\n    wrong_prediction_visualizer = WrongPredictionVis(cfg=cfg)",
        "detail": "MorphMLP.tools.visualization",
        "documentation": {}
    },
    {
        "label": "visualize",
        "kind": 2,
        "importPath": "MorphMLP.tools.visualization",
        "description": "MorphMLP.tools.visualization",
        "peekOfCode": "def visualize(cfg):\n    \"\"\"\n    Perform layer weights and activations visualization on the model.\n    Args:\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py\n    \"\"\"\n    if cfg.TENSORBOARD.ENABLE and (\n        cfg.TENSORBOARD.MODEL_VIS.ENABLE\n        or cfg.TENSORBOARD.WRONG_PRED_VIS.ENABLE",
        "detail": "MorphMLP.tools.visualization",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "MorphMLP.tools.visualization",
        "description": "MorphMLP.tools.visualization",
        "peekOfCode": "logger = logging.get_logger(__name__)\ndef run_visualization(vis_loader, model, cfg, writer=None):\n    \"\"\"\n    Run model visualization (weights, activations and model inputs) and visualize\n    them on Tensorboard.\n    Args:\n        vis_loader (loader): video visualization loader.\n        model (model): the video model to visualize.\n        cfg (CfgNode): configs. Details can be found in\n            slowfast/config/defaults.py",
        "detail": "MorphMLP.tools.visualization",
        "documentation": {}
    },
    {
        "label": "BatchGenerator",
        "kind": 6,
        "importPath": "SSTDA.batch_gen",
        "description": "SSTDA.batch_gen",
        "peekOfCode": "class BatchGenerator(object):\n    def __init__(self, num_classes, actions_dict, gt_path, features_path, sample_rate):\n        self.list_of_examples = list()\n        self.num_examples = 0\n        self.index = 0\n        self.num_classes = num_classes\n        self.actions_dict = actions_dict\n        self.gt_path = gt_path\n        self.features_path = features_path\n        self.sample_rate = sample_rate",
        "detail": "SSTDA.batch_gen",
        "documentation": {}
    },
    {
        "label": "Centroid",
        "kind": 6,
        "importPath": "SSTDA.centroid",
        "description": "SSTDA.centroid",
        "peekOfCode": "class Centroid(nn.Module):\n    def __init__(self, num_f_maps, num_classes):\n        super(Centroid, self).__init__()\n        self.dim_feat = num_f_maps\n        self.num_classes = num_classes\n        self.register_buffer('centroid_s', torch.zeros(num_classes, num_f_maps))  # easier to convert devices\n        self.register_buffer('centroid_t', torch.zeros(num_classes, num_f_maps))\n    def update_centroids(self, feat_s, feat_t, y_s, y_t, method_centroid, ratio_ma):\n        # get labels (source: ground truth / target: select highest probability)\n        label_source = y_s.detach()",
        "detail": "SSTDA.centroid",
        "documentation": {}
    },
    {
        "label": "read_file",
        "kind": 2,
        "importPath": "SSTDA.eval",
        "description": "SSTDA.eval",
        "peekOfCode": "def read_file(path):\n    with open(path, 'r') as f:\n        content = f.read()\n        f.close()\n    return content\ndef get_labels_start_end_time(frame_wise_labels, bg_class=[\"background\"]):\n    labels = []\n    starts = []\n    ends = []\n    last_label = frame_wise_labels[0]",
        "detail": "SSTDA.eval",
        "documentation": {}
    },
    {
        "label": "get_labels_start_end_time",
        "kind": 2,
        "importPath": "SSTDA.eval",
        "description": "SSTDA.eval",
        "peekOfCode": "def get_labels_start_end_time(frame_wise_labels, bg_class=[\"background\"]):\n    labels = []\n    starts = []\n    ends = []\n    last_label = frame_wise_labels[0]\n    if frame_wise_labels[0] not in bg_class:\n        labels.append(frame_wise_labels[0])\n        starts.append(0)\n    for i in range(len(frame_wise_labels)):\n        if frame_wise_labels[i] != last_label:",
        "detail": "SSTDA.eval",
        "documentation": {}
    },
    {
        "label": "levenstein",
        "kind": 2,
        "importPath": "SSTDA.eval",
        "description": "SSTDA.eval",
        "peekOfCode": "def levenstein(p, y, norm=False):\n    m_row = len(p)    \n    n_col = len(y)\n    D = np.zeros([m_row+1, n_col+1], np.float)\n    for i in range(m_row+1):\n        D[i, 0] = i\n    for i in range(n_col+1):\n        D[0, i] = i\n    for j in range(1, n_col+1):\n        for i in range(1, m_row+1):",
        "detail": "SSTDA.eval",
        "documentation": {}
    },
    {
        "label": "edit_score",
        "kind": 2,
        "importPath": "SSTDA.eval",
        "description": "SSTDA.eval",
        "peekOfCode": "def edit_score(recognized, ground_truth, norm=True, bg_class=[\"background\"]):\n    P, _, _ = get_labels_start_end_time(recognized, bg_class)\n    Y, _, _ = get_labels_start_end_time(ground_truth, bg_class)\n    return levenstein(P, Y, norm)\ndef f_score(recognized, ground_truth, overlap, bg_class=[\"background\"]):\n    p_label, p_start, p_end = get_labels_start_end_time(recognized, bg_class)\n    y_label, y_start, y_end = get_labels_start_end_time(ground_truth, bg_class)\n    tp = 0\n    fp = 0\n    hits = np.zeros(len(y_label))",
        "detail": "SSTDA.eval",
        "documentation": {}
    },
    {
        "label": "f_score",
        "kind": 2,
        "importPath": "SSTDA.eval",
        "description": "SSTDA.eval",
        "peekOfCode": "def f_score(recognized, ground_truth, overlap, bg_class=[\"background\"]):\n    p_label, p_start, p_end = get_labels_start_end_time(recognized, bg_class)\n    y_label, y_start, y_end = get_labels_start_end_time(ground_truth, bg_class)\n    tp = 0\n    fp = 0\n    hits = np.zeros(len(y_label))\n    for j in range(len(p_label)):\n        intersection = np.minimum(p_end[j], y_end) - np.maximum(p_start[j], y_start)\n        union = np.maximum(p_end[j], y_end) - np.minimum(p_start[j], y_start)\n        IoU = (1.0*intersection / union)*([p_label[j] == y_label[x] for x in range(len(y_label))])",
        "detail": "SSTDA.eval",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "SSTDA.eval",
        "description": "SSTDA.eval",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--path_data', default='data/')\n    parser.add_argument('--path_result', default='results/')\n    parser.add_argument('--dataset', default=\"gtea\")\n    parser.add_argument('--split', default='1')\n    parser.add_argument('--split_target', default='0', help='split for target data (0: no additional split for target)')\n    args = parser.parse_args()\n    ground_truth_path = args.path_data+args.dataset+\"/groundTruth/\"\n    recog_path = args.path_result+args.dataset+\"/split_\"+args.split+\"/\"",
        "detail": "SSTDA.eval",
        "documentation": {}
    },
    {
        "label": "cross_entropy_soft",
        "kind": 2,
        "importPath": "SSTDA.loss",
        "description": "SSTDA.loss",
        "peekOfCode": "def cross_entropy_soft(pred):\n    softmax = nn.Softmax(dim=1)\n    logsoftmax = nn.LogSoftmax(dim=1)\n    loss = torch.mean(torch.sum(-softmax(pred) * logsoftmax(pred), 1))\n    return loss\n# attentive entropy loss (source + target)\ndef attentive_entropy(pred, pred_domain):\n    softmax = nn.Softmax(dim=1)\n    logsoftmax = nn.LogSoftmax(dim=1)\n    # attention weight",
        "detail": "SSTDA.loss",
        "documentation": {}
    },
    {
        "label": "attentive_entropy",
        "kind": 2,
        "importPath": "SSTDA.loss",
        "description": "SSTDA.loss",
        "peekOfCode": "def attentive_entropy(pred, pred_domain):\n    softmax = nn.Softmax(dim=1)\n    logsoftmax = nn.LogSoftmax(dim=1)\n    # attention weight\n    entropy = torch.sum(-softmax(pred_domain) * logsoftmax(pred_domain), 1)\n    weights = 1 + entropy\n    # attentive entropy\n    loss = weights * torch.sum(-softmax(pred) * logsoftmax(pred), 1)\n    return loss\n# discrepancy for ensemble loss",
        "detail": "SSTDA.loss",
        "documentation": {}
    },
    {
        "label": "dis_mcd",
        "kind": 2,
        "importPath": "SSTDA.loss",
        "description": "SSTDA.loss",
        "peekOfCode": "def dis_mcd(out1, out2):\n    return torch.mean(torch.abs(out1 - out2))\ndef dis_swd(p1, p2, dim_proj=128):\n    s = p1.shape\n    if s[1] > 1:\n        proj = torch.randn(s[1], dim_proj)\n        if p1.get_device() >= 0:\n            proj = proj.to(p1.get_device())\n        proj *= torch.rsqrt(torch.sum(torch.mul(proj, proj), 0, keepdim=True))\n        p1 = torch.matmul(p1, proj)",
        "detail": "SSTDA.loss",
        "documentation": {}
    },
    {
        "label": "dis_swd",
        "kind": 2,
        "importPath": "SSTDA.loss",
        "description": "SSTDA.loss",
        "peekOfCode": "def dis_swd(p1, p2, dim_proj=128):\n    s = p1.shape\n    if s[1] > 1:\n        proj = torch.randn(s[1], dim_proj)\n        if p1.get_device() >= 0:\n            proj = proj.to(p1.get_device())\n        proj *= torch.rsqrt(torch.sum(torch.mul(proj, proj), 0, keepdim=True))\n        p1 = torch.matmul(p1, proj)\n        p2 = torch.matmul(p2, proj)\n    p1 = torch.topk(p1, s[0], dim=0)[0]",
        "detail": "SSTDA.loss",
        "documentation": {}
    },
    {
        "label": "gaussian_kernel",
        "kind": 2,
        "importPath": "SSTDA.loss",
        "description": "SSTDA.loss",
        "peekOfCode": "def gaussian_kernel(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n    n_samples = int(source.size()[0])+int(target.size()[0])\n    total = torch.cat([source, target], dim=0)\n    total0 = total.unsqueeze(0).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n    total1 = total.unsqueeze(1).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n    dist_l2 = ((total0-total1)**2).sum(2)\n    if fix_sigma:\n        bandwidth = fix_sigma\n    else:\n        bandwidth = torch.sum(dist_l2.detach()) / (n_samples**2-n_samples)",
        "detail": "SSTDA.loss",
        "documentation": {}
    },
    {
        "label": "loss_jan",
        "kind": 2,
        "importPath": "SSTDA.loss",
        "description": "SSTDA.loss",
        "peekOfCode": "def loss_jan(source_list, target_list, kernel_muls=[2.0, 2.0], kernel_nums=[5, 1], fix_sigma_list=[None, 1.68], ver=2):\n    batch_size = int(source_list[0].size()[0])\n    layer_num = len(source_list)\n    joint_kernels = None\n    for i in range(layer_num):\n        source = source_list[i]\n        target = target_list[i]\n        kernel_mul = kernel_muls[i]\n        kernel_num = kernel_nums[i]\n        fix_sigma = fix_sigma_list[i]",
        "detail": "SSTDA.loss",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "SSTDA.main",
        "description": "SSTDA.main",
        "peekOfCode": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nseed = 1538574472\nrandom.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.deterministic = True\nparser = argparse.ArgumentParser()\n# architecture\nparser.add_argument('--num_stages', default=4, type=int, help='stage number')\nparser.add_argument('--num_layers', default=10, type=int, help='layer number in each stage')",
        "detail": "SSTDA.main",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "SSTDA.main",
        "description": "SSTDA.main",
        "peekOfCode": "seed = 1538574472\nrandom.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.deterministic = True\nparser = argparse.ArgumentParser()\n# architecture\nparser.add_argument('--num_stages', default=4, type=int, help='stage number')\nparser.add_argument('--num_layers', default=10, type=int, help='layer number in each stage')\nparser.add_argument('--num_f_maps', default=64, type=int, help='embedded feat. dim.')",
        "detail": "SSTDA.main",
        "documentation": {}
    },
    {
        "label": "torch.backends.cudnn.deterministic",
        "kind": 5,
        "importPath": "SSTDA.main",
        "description": "SSTDA.main",
        "peekOfCode": "torch.backends.cudnn.deterministic = True\nparser = argparse.ArgumentParser()\n# architecture\nparser.add_argument('--num_stages', default=4, type=int, help='stage number')\nparser.add_argument('--num_layers', default=10, type=int, help='layer number in each stage')\nparser.add_argument('--num_f_maps', default=64, type=int, help='embedded feat. dim.')\nparser.add_argument('--features_dim', default=2048, type=int, help='input feat. dim.')\nparser.add_argument('--DA_adv', default='none', type=str, help='adversarial loss (none | rev_grad)')\nparser.add_argument('--DA_adv_video', default='none', type=str, help='video-level adversarial loss (none | rev_grad | rev_grad_ssl | rev_grad_ssl_2)')\nparser.add_argument('--pair_ssl', default='all', type=str, help='pair-feature methods for SSL-DA (all | adjacent)')",
        "detail": "SSTDA.main",
        "documentation": {}
    },
    {
        "label": "parser",
        "kind": 5,
        "importPath": "SSTDA.main",
        "description": "SSTDA.main",
        "peekOfCode": "parser = argparse.ArgumentParser()\n# architecture\nparser.add_argument('--num_stages', default=4, type=int, help='stage number')\nparser.add_argument('--num_layers', default=10, type=int, help='layer number in each stage')\nparser.add_argument('--num_f_maps', default=64, type=int, help='embedded feat. dim.')\nparser.add_argument('--features_dim', default=2048, type=int, help='input feat. dim.')\nparser.add_argument('--DA_adv', default='none', type=str, help='adversarial loss (none | rev_grad)')\nparser.add_argument('--DA_adv_video', default='none', type=str, help='video-level adversarial loss (none | rev_grad | rev_grad_ssl | rev_grad_ssl_2)')\nparser.add_argument('--pair_ssl', default='all', type=str, help='pair-feature methods for SSL-DA (all | adjacent)')\nparser.add_argument('--num_seg', default=10, type=int, help='segment number for each video')",
        "detail": "SSTDA.main",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "SSTDA.main",
        "description": "SSTDA.main",
        "peekOfCode": "args = parser.parse_args()\n# check whether place_adv & place_sem are valid\nif len(args.place_adv) != args.num_stages:\n    raise ValueError('len(place_dis) should be equal to num_stages')\nif len(args.place_sem) != args.num_stages:\n    raise ValueError('len(place_sem) should be equal to num_stages')\nif len(args.place_ent) != args.num_stages:\n    raise ValueError('len(place_ent) should be equal to num_stages')\nif len(args.place_dis) != args.num_stages:\n    raise ValueError('len(place_dis) should be equal to num_stages')",
        "detail": "SSTDA.main",
        "documentation": {}
    },
    {
        "label": "sample_rate",
        "kind": 5,
        "importPath": "SSTDA.main",
        "description": "SSTDA.main",
        "peekOfCode": "sample_rate = 1\n# sample input features @ 15fps instead of 30 fps\n# for 50salads, and up-sample the output to 30 fps\nif args.dataset == \"50salads\":\n    sample_rate = 2\n# ====== Load files ====== #\nvid_list_file = args.path_data+args.dataset+\"/splits/train.split\"+args.split+\".bundle\"\nvid_list_file_target = args.path_data+args.dataset+\"/splits/test.split\"+args.split+\".bundle\"\nvid_list_file_test = vid_list_file_target\nif args.split_target != '0':",
        "detail": "SSTDA.main",
        "documentation": {}
    },
    {
        "label": "vid_list_file",
        "kind": 5,
        "importPath": "SSTDA.main",
        "description": "SSTDA.main",
        "peekOfCode": "vid_list_file = args.path_data+args.dataset+\"/splits/train.split\"+args.split+\".bundle\"\nvid_list_file_target = args.path_data+args.dataset+\"/splits/test.split\"+args.split+\".bundle\"\nvid_list_file_test = vid_list_file_target\nif args.split_target != '0':\n    vid_list_file_target = args.path_data + args.dataset + \"/splits/test_train_\" + args.split_target + \".split\" + args.split + \".bundle\"\n    vid_list_file_test = args.path_data + args.dataset + \"/splits/test_test_\" + args.split_target + \".split\" + args.split + \".bundle\"\nfeatures_path = args.path_data+args.dataset+\"/features/\"\ngt_path = args.path_data+args.dataset+\"/groundTruth/\"\nmapping_file = args.path_data+args.dataset+\"/mapping.txt\"  # mapping between classes & indices\nmodel_dir = args.path_model+args.dataset+\"/split_\"+args.split",
        "detail": "SSTDA.main",
        "documentation": {}
    },
    {
        "label": "vid_list_file_target",
        "kind": 5,
        "importPath": "SSTDA.main",
        "description": "SSTDA.main",
        "peekOfCode": "vid_list_file_target = args.path_data+args.dataset+\"/splits/test.split\"+args.split+\".bundle\"\nvid_list_file_test = vid_list_file_target\nif args.split_target != '0':\n    vid_list_file_target = args.path_data + args.dataset + \"/splits/test_train_\" + args.split_target + \".split\" + args.split + \".bundle\"\n    vid_list_file_test = args.path_data + args.dataset + \"/splits/test_test_\" + args.split_target + \".split\" + args.split + \".bundle\"\nfeatures_path = args.path_data+args.dataset+\"/features/\"\ngt_path = args.path_data+args.dataset+\"/groundTruth/\"\nmapping_file = args.path_data+args.dataset+\"/mapping.txt\"  # mapping between classes & indices\nmodel_dir = args.path_model+args.dataset+\"/split_\"+args.split\nresults_dir = args.path_result+args.dataset+\"/split_\"+args.split",
        "detail": "SSTDA.main",
        "documentation": {}
    },
    {
        "label": "vid_list_file_test",
        "kind": 5,
        "importPath": "SSTDA.main",
        "description": "SSTDA.main",
        "peekOfCode": "vid_list_file_test = vid_list_file_target\nif args.split_target != '0':\n    vid_list_file_target = args.path_data + args.dataset + \"/splits/test_train_\" + args.split_target + \".split\" + args.split + \".bundle\"\n    vid_list_file_test = args.path_data + args.dataset + \"/splits/test_test_\" + args.split_target + \".split\" + args.split + \".bundle\"\nfeatures_path = args.path_data+args.dataset+\"/features/\"\ngt_path = args.path_data+args.dataset+\"/groundTruth/\"\nmapping_file = args.path_data+args.dataset+\"/mapping.txt\"  # mapping between classes & indices\nmodel_dir = args.path_model+args.dataset+\"/split_\"+args.split\nresults_dir = args.path_result+args.dataset+\"/split_\"+args.split\nif not os.path.exists(model_dir):",
        "detail": "SSTDA.main",
        "documentation": {}
    },
    {
        "label": "features_path",
        "kind": 5,
        "importPath": "SSTDA.main",
        "description": "SSTDA.main",
        "peekOfCode": "features_path = args.path_data+args.dataset+\"/features/\"\ngt_path = args.path_data+args.dataset+\"/groundTruth/\"\nmapping_file = args.path_data+args.dataset+\"/mapping.txt\"  # mapping between classes & indices\nmodel_dir = args.path_model+args.dataset+\"/split_\"+args.split\nresults_dir = args.path_result+args.dataset+\"/split_\"+args.split\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\nif not os.path.exists(results_dir):\n    os.makedirs(results_dir)\nfile_ptr = open(mapping_file, 'r')",
        "detail": "SSTDA.main",
        "documentation": {}
    },
    {
        "label": "gt_path",
        "kind": 5,
        "importPath": "SSTDA.main",
        "description": "SSTDA.main",
        "peekOfCode": "gt_path = args.path_data+args.dataset+\"/groundTruth/\"\nmapping_file = args.path_data+args.dataset+\"/mapping.txt\"  # mapping between classes & indices\nmodel_dir = args.path_model+args.dataset+\"/split_\"+args.split\nresults_dir = args.path_result+args.dataset+\"/split_\"+args.split\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\nif not os.path.exists(results_dir):\n    os.makedirs(results_dir)\nfile_ptr = open(mapping_file, 'r')\nactions = file_ptr.read().split('\\n')[:-1]  # list of classes",
        "detail": "SSTDA.main",
        "documentation": {}
    },
    {
        "label": "mapping_file",
        "kind": 5,
        "importPath": "SSTDA.main",
        "description": "SSTDA.main",
        "peekOfCode": "mapping_file = args.path_data+args.dataset+\"/mapping.txt\"  # mapping between classes & indices\nmodel_dir = args.path_model+args.dataset+\"/split_\"+args.split\nresults_dir = args.path_result+args.dataset+\"/split_\"+args.split\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\nif not os.path.exists(results_dir):\n    os.makedirs(results_dir)\nfile_ptr = open(mapping_file, 'r')\nactions = file_ptr.read().split('\\n')[:-1]  # list of classes\nfile_ptr.close()",
        "detail": "SSTDA.main",
        "documentation": {}
    },
    {
        "label": "model_dir",
        "kind": 5,
        "importPath": "SSTDA.main",
        "description": "SSTDA.main",
        "peekOfCode": "model_dir = args.path_model+args.dataset+\"/split_\"+args.split\nresults_dir = args.path_result+args.dataset+\"/split_\"+args.split\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\nif not os.path.exists(results_dir):\n    os.makedirs(results_dir)\nfile_ptr = open(mapping_file, 'r')\nactions = file_ptr.read().split('\\n')[:-1]  # list of classes\nfile_ptr.close()\nactions_dict = dict()",
        "detail": "SSTDA.main",
        "documentation": {}
    },
    {
        "label": "results_dir",
        "kind": 5,
        "importPath": "SSTDA.main",
        "description": "SSTDA.main",
        "peekOfCode": "results_dir = args.path_result+args.dataset+\"/split_\"+args.split\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\nif not os.path.exists(results_dir):\n    os.makedirs(results_dir)\nfile_ptr = open(mapping_file, 'r')\nactions = file_ptr.read().split('\\n')[:-1]  # list of classes\nfile_ptr.close()\nactions_dict = dict()\nfor a in actions:",
        "detail": "SSTDA.main",
        "documentation": {}
    },
    {
        "label": "file_ptr",
        "kind": 5,
        "importPath": "SSTDA.main",
        "description": "SSTDA.main",
        "peekOfCode": "file_ptr = open(mapping_file, 'r')\nactions = file_ptr.read().split('\\n')[:-1]  # list of classes\nfile_ptr.close()\nactions_dict = dict()\nfor a in actions:\n    actions_dict[a.split()[1]] = int(a.split()[0])\nnum_classes = len(actions_dict)\n# initialize model & trainer\nmodel = MultiStageModel(args, num_classes)\ntrainer = Trainer(num_classes)",
        "detail": "SSTDA.main",
        "documentation": {}
    },
    {
        "label": "actions",
        "kind": 5,
        "importPath": "SSTDA.main",
        "description": "SSTDA.main",
        "peekOfCode": "actions = file_ptr.read().split('\\n')[:-1]  # list of classes\nfile_ptr.close()\nactions_dict = dict()\nfor a in actions:\n    actions_dict[a.split()[1]] = int(a.split()[0])\nnum_classes = len(actions_dict)\n# initialize model & trainer\nmodel = MultiStageModel(args, num_classes)\ntrainer = Trainer(num_classes)\n# ====== Main Program ====== #",
        "detail": "SSTDA.main",
        "documentation": {}
    },
    {
        "label": "actions_dict",
        "kind": 5,
        "importPath": "SSTDA.main",
        "description": "SSTDA.main",
        "peekOfCode": "actions_dict = dict()\nfor a in actions:\n    actions_dict[a.split()[1]] = int(a.split()[0])\nnum_classes = len(actions_dict)\n# initialize model & trainer\nmodel = MultiStageModel(args, num_classes)\ntrainer = Trainer(num_classes)\n# ====== Main Program ====== #\nstart_time = time.time()\nif args.action == \"train\":",
        "detail": "SSTDA.main",
        "documentation": {}
    },
    {
        "label": "num_classes",
        "kind": 5,
        "importPath": "SSTDA.main",
        "description": "SSTDA.main",
        "peekOfCode": "num_classes = len(actions_dict)\n# initialize model & trainer\nmodel = MultiStageModel(args, num_classes)\ntrainer = Trainer(num_classes)\n# ====== Main Program ====== #\nstart_time = time.time()\nif args.action == \"train\":\n    batch_gen_source = BatchGenerator(num_classes, actions_dict, gt_path, features_path, sample_rate)\n    batch_gen_target = BatchGenerator(num_classes, actions_dict, gt_path, features_path, sample_rate)\n    batch_gen_source.read_data(vid_list_file)  # read & shuffle the source training list",
        "detail": "SSTDA.main",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "SSTDA.main",
        "description": "SSTDA.main",
        "peekOfCode": "model = MultiStageModel(args, num_classes)\ntrainer = Trainer(num_classes)\n# ====== Main Program ====== #\nstart_time = time.time()\nif args.action == \"train\":\n    batch_gen_source = BatchGenerator(num_classes, actions_dict, gt_path, features_path, sample_rate)\n    batch_gen_target = BatchGenerator(num_classes, actions_dict, gt_path, features_path, sample_rate)\n    batch_gen_source.read_data(vid_list_file)  # read & shuffle the source training list\n    batch_gen_target.read_data(vid_list_file_target)  # read & shuffle the target training list\n    trainer.train(model, model_dir, results_dir, batch_gen_source, batch_gen_target, device, args)",
        "detail": "SSTDA.main",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "SSTDA.main",
        "description": "SSTDA.main",
        "peekOfCode": "trainer = Trainer(num_classes)\n# ====== Main Program ====== #\nstart_time = time.time()\nif args.action == \"train\":\n    batch_gen_source = BatchGenerator(num_classes, actions_dict, gt_path, features_path, sample_rate)\n    batch_gen_target = BatchGenerator(num_classes, actions_dict, gt_path, features_path, sample_rate)\n    batch_gen_source.read_data(vid_list_file)  # read & shuffle the source training list\n    batch_gen_target.read_data(vid_list_file_target)  # read & shuffle the target training list\n    trainer.train(model, model_dir, results_dir, batch_gen_source, batch_gen_target, device, args)\nif args.action == \"predict\":",
        "detail": "SSTDA.main",
        "documentation": {}
    },
    {
        "label": "start_time",
        "kind": 5,
        "importPath": "SSTDA.main",
        "description": "SSTDA.main",
        "peekOfCode": "start_time = time.time()\nif args.action == \"train\":\n    batch_gen_source = BatchGenerator(num_classes, actions_dict, gt_path, features_path, sample_rate)\n    batch_gen_target = BatchGenerator(num_classes, actions_dict, gt_path, features_path, sample_rate)\n    batch_gen_source.read_data(vid_list_file)  # read & shuffle the source training list\n    batch_gen_target.read_data(vid_list_file_target)  # read & shuffle the target training list\n    trainer.train(model, model_dir, results_dir, batch_gen_source, batch_gen_target, device, args)\nif args.action == \"predict\":\n    predict(model, model_dir, results_dir, features_path, vid_list_file_test, args.num_epochs, actions_dict,\n            device, sample_rate, args)",
        "detail": "SSTDA.main",
        "documentation": {}
    },
    {
        "label": "end_time",
        "kind": 5,
        "importPath": "SSTDA.main",
        "description": "SSTDA.main",
        "peekOfCode": "end_time = time.time()\nif args.verbose:\n    print('')\n    print('total running time:', end_time - start_time)",
        "detail": "SSTDA.main",
        "documentation": {}
    },
    {
        "label": "GradRevLayer",
        "kind": 6,
        "importPath": "SSTDA.model",
        "description": "SSTDA.model",
        "peekOfCode": "class GradRevLayer(Function):\n    @staticmethod\n    def forward(ctx, x, beta):\n        ctx.beta = beta\n        return x.view_as(x)\n    @staticmethod\n    def backward(ctx, grad_output):\n        grad_input = grad_output.neg() * ctx.beta\n        return grad_input, None\n# definition of Adversarial Domain Classifier (base part)",
        "detail": "SSTDA.model",
        "documentation": {}
    },
    {
        "label": "AdvDomainClsBase",
        "kind": 6,
        "importPath": "SSTDA.model",
        "description": "SSTDA.model",
        "peekOfCode": "class AdvDomainClsBase(nn.Module):\n    def __init__(self, in_feat, hidden_size, type_adv, args):\n        super(AdvDomainClsBase, self).__init__()\n        # ====== collect arguments ====== #\n        self.num_f_maps = args.num_f_maps\n        self.DA_adv_video = args.DA_adv_video\n        self.pair_ssl = args.pair_ssl\n        self.type_adv = type_adv\n        # ====== main architecture ====== #\n        if self.type_adv == 'video' and self.DA_adv_video == 'rev_grad_ssl_2':",
        "detail": "SSTDA.model",
        "documentation": {}
    },
    {
        "label": "MultiStageModel",
        "kind": 6,
        "importPath": "SSTDA.model",
        "description": "SSTDA.model",
        "peekOfCode": "class MultiStageModel(nn.Module):\n    def __init__(self, args, num_classes):\n        super(MultiStageModel, self).__init__()\n        # ====== collect arguments ====== #\n        # this function only\n        num_stages = args.num_stages\n        num_layers = args.num_layers\n        num_f_maps = args.num_f_maps\n        dim_in = args.features_dim\n        method_centroid = args.method_centroid",
        "detail": "SSTDA.model",
        "documentation": {}
    },
    {
        "label": "SingleStageModel",
        "kind": 6,
        "importPath": "SSTDA.model",
        "description": "SSTDA.model",
        "peekOfCode": "class SingleStageModel(nn.Module):\n    def __init__(self, num_layers, num_f_maps, dim_in, num_classes, DA_ens):\n        super(SingleStageModel, self).__init__()\n        self.conv_1x1 = nn.Conv1d(dim_in, num_f_maps, 1)\n        self.layers = nn.ModuleList([copy.deepcopy(DilatedResidualLayer(2 ** i, num_f_maps, num_f_maps)) for i in range(num_layers)])\n        self.conv_out = nn.Conv1d(num_f_maps, num_classes, 1)\n        # for ensemble methods\n        if DA_ens != 'none':\n            self.conv_out_2 = nn.Conv1d(num_f_maps, num_classes, 1)\n    def forward(self, x):",
        "detail": "SSTDA.model",
        "documentation": {}
    },
    {
        "label": "DilatedResidualLayer",
        "kind": 6,
        "importPath": "SSTDA.model",
        "description": "SSTDA.model",
        "peekOfCode": "class DilatedResidualLayer(nn.Module):\n    def __init__(self, dilation, in_channels, out_channels):\n        super(DilatedResidualLayer, self).__init__()\n        self.conv_dilated = nn.Conv1d(in_channels, out_channels, 3, padding=dilation, dilation=dilation)\n        self.conv_1x1 = nn.Conv1d(out_channels, out_channels, 1)\n        self.dropout = nn.Dropout()\n    def forward(self, x):\n        out = F.relu(self.conv_dilated(x))\n        out = self.conv_1x1(out)\n        out = self.dropout(out)  ",
        "detail": "SSTDA.model",
        "documentation": {}
    },
    {
        "label": "predict",
        "kind": 2,
        "importPath": "SSTDA.predict",
        "description": "SSTDA.predict",
        "peekOfCode": "def predict(model, model_dir, results_dir, features_path, vid_list_file, epoch, actions_dict, device, sample_rate, args):\n    # collect arguments\n    verbose = args.verbose\n    use_best_model = args.use_best_model\n    # multi-GPU\n    if args.multi_gpu and torch.cuda.device_count() > 1:\n        model = nn.DataParallel(model)\n    model.eval()\n    with torch.no_grad():\n        model.to(device)",
        "detail": "SSTDA.predict",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "SSTDA.train",
        "description": "SSTDA.train",
        "peekOfCode": "class Trainer:\n    def __init__(self, num_classes):\n        self.ce = nn.CrossEntropyLoss(ignore_index=-100)\n        self.ce_d = nn.CrossEntropyLoss(reduction='none')\n        self.mse = nn.MSELoss(reduction='none')\n        self.num_classes = num_classes\n    def adapt_weight(self, iter_now, iter_max_default, iter_max_input, weight_loss, weight_value=10.0, high_value=1.0, low_value=0.0):\n        # affect adaptive weight value\n        iter_max = iter_max_default\n        if weight_loss < -1:",
        "detail": "SSTDA.train",
        "documentation": {}
    },
    {
        "label": "_base_",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "peekOfCode": "_base_ = [\n    '../../_base_/collater/stream_compose.py'\n]\nsample_rate = 8\nignore_index = -100\nsliding_window = 1\nclip_seg_num = 8\nMODEL = dict(\n    architecture = \"ActionCLIP\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",",
        "detail": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "sample_rate",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "peekOfCode": "sample_rate = 8\nignore_index = -100\nsliding_window = 1\nclip_seg_num = 8\nMODEL = dict(\n    architecture = \"ActionCLIP\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",\n    is_feature_extract = True,\n    image_prompt = dict(\n        name = \"CLIP\",",
        "detail": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "ignore_index",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "peekOfCode": "ignore_index = -100\nsliding_window = 1\nclip_seg_num = 8\nMODEL = dict(\n    architecture = \"ActionCLIP\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",\n    is_feature_extract = True,\n    image_prompt = dict(\n        name = \"CLIP\",\n        # pretrained = \"./data/checkpoint/ViT-B-16.pt\",",
        "detail": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "sliding_window",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "peekOfCode": "sliding_window = 1\nclip_seg_num = 8\nMODEL = dict(\n    architecture = \"ActionCLIP\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",\n    is_feature_extract = True,\n    image_prompt = dict(\n        name = \"CLIP\",\n        # pretrained = \"./data/checkpoint/ViT-B-16.pt\",\n        embed_dim = 512,",
        "detail": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "clip_seg_num",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "peekOfCode": "clip_seg_num = 8\nMODEL = dict(\n    architecture = \"ActionCLIP\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",\n    is_feature_extract = True,\n    image_prompt = dict(\n        name = \"CLIP\",\n        # pretrained = \"./data/checkpoint/ViT-B-16.pt\",\n        embed_dim = 512,\n        image_resolution = 224,",
        "detail": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "peekOfCode": "MODEL = dict(\n    architecture = \"ActionCLIP\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",\n    is_feature_extract = True,\n    image_prompt = dict(\n        name = \"CLIP\",\n        # pretrained = \"./data/checkpoint/ViT-B-16.pt\",\n        embed_dim = 512,\n        image_resolution = 224,\n        vision_layers = 12,",
        "detail": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "PRETRAINED",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "peekOfCode": "PRETRAINED = None\nPOSTPRECESSING = dict(\n    name = \"StreamFeaturePostProcessing\",\n    sliding_window = sliding_window,\n    ignore_index = ignore_index\n)\nDATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = 4,\n    num_workers = 2,",
        "detail": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "POSTPRECESSING",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "peekOfCode": "POSTPRECESSING = dict(\n    name = \"StreamFeaturePostProcessing\",\n    sliding_window = sliding_window,\n    ignore_index = ignore_index\n)\nDATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = 4,\n    num_workers = 2,\n    config = dict(",
        "detail": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = 4,\n    num_workers = 2,\n    config = dict(\n        name = \"RawFrameStreamSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/gtea/splits/all_files.txt\",\n        videos_path = \"./data/gtea/Videos\",\n        gt_path = \"./data/gtea/groundTruth\",",
        "detail": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "PIPELINE",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "peekOfCode": "PIPELINE = dict(\n    name = \"BasePipline\",\n    decode = dict(\n        name = \"VideoDecoder\",\n        backend = \"decord\"\n    ),\n    sample = dict(\n        name = \"VideoStreamSampler\",\n        is_train = False,\n        sample_rate = sample_rate,",
        "detail": "SVTAS.config.extract.extract_feature.action_clip_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "_base_",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "peekOfCode": "_base_ = [\n    '../../_base_/collater/stream_compose.py'\n]\nsample_rate = 1\nignore_index = -100\nsliding_window = 1\nclip_seg_num = 16\nMODEL = dict(\n    architecture = \"ActionCLIP\",\n    pretrained = \"./data/checkpoint/last_model.pt\",",
        "detail": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "sample_rate",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "peekOfCode": "sample_rate = 1\nignore_index = -100\nsliding_window = 1\nclip_seg_num = 16\nMODEL = dict(\n    architecture = \"ActionCLIP\",\n    pretrained = \"./data/checkpoint/last_model.pt\",\n    is_feature_extract = True,\n    image_prompt = dict(\n        name = \"CLIP\",",
        "detail": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "ignore_index",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "peekOfCode": "ignore_index = -100\nsliding_window = 1\nclip_seg_num = 16\nMODEL = dict(\n    architecture = \"ActionCLIP\",\n    pretrained = \"./data/checkpoint/last_model.pt\",\n    is_feature_extract = True,\n    image_prompt = dict(\n        name = \"CLIP\",\n        # pretrained = \"./data/ViT-B-16.pt\",",
        "detail": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "sliding_window",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "peekOfCode": "sliding_window = 1\nclip_seg_num = 16\nMODEL = dict(\n    architecture = \"ActionCLIP\",\n    pretrained = \"./data/checkpoint/last_model.pt\",\n    is_feature_extract = True,\n    image_prompt = dict(\n        name = \"CLIP\",\n        # pretrained = \"./data/ViT-B-16.pt\",\n        embed_dim = 512,",
        "detail": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "clip_seg_num",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "peekOfCode": "clip_seg_num = 16\nMODEL = dict(\n    architecture = \"ActionCLIP\",\n    pretrained = \"./data/checkpoint/last_model.pt\",\n    is_feature_extract = True,\n    image_prompt = dict(\n        name = \"CLIP\",\n        # pretrained = \"./data/ViT-B-16.pt\",\n        embed_dim = 512,\n        image_resolution = 224,",
        "detail": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "peekOfCode": "MODEL = dict(\n    architecture = \"ActionCLIP\",\n    pretrained = \"./data/checkpoint/last_model.pt\",\n    is_feature_extract = True,\n    image_prompt = dict(\n        name = \"CLIP\",\n        # pretrained = \"./data/ViT-B-16.pt\",\n        embed_dim = 512,\n        image_resolution = 224,\n        vision_layers = 12,",
        "detail": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "PRETRAINED",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "peekOfCode": "PRETRAINED = None\nPOSTPRECESSING = dict(\n    name = \"StreamFeaturePostProcessing\",\n    sliding_window = sliding_window,\n    ignore_index = ignore_index\n)\nDATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = 4,\n    num_workers = 2,",
        "detail": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "POSTPRECESSING",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "peekOfCode": "POSTPRECESSING = dict(\n    name = \"StreamFeaturePostProcessing\",\n    sliding_window = sliding_window,\n    ignore_index = ignore_index\n)\nDATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = 4,\n    num_workers = 2,\n    config = dict(",
        "detail": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = 4,\n    num_workers = 2,\n    config = dict(\n        name = \"RawFrameStreamSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/gtea/splits/all_files.txt\",\n        videos_path = \"./data/gtea/Videos\",\n        gt_path = \"./data/gtea/groundTruth\",",
        "detail": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "PIPELINE",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "peekOfCode": "PIPELINE = dict(\n    name = \"BasePipline\",\n    decode = dict(\n        name = \"VideoDecoder\",\n        backend = \"decord\"\n    ),\n    sample = dict(\n        name = \"VideoStreamSampler\",\n        is_train = False,\n        sample_rate = sample_rate,",
        "detail": "SVTAS.config.extract.extract_feature.bridge_prompt_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "_base_",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "description": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "peekOfCode": "_base_ = [\n    '../../_base_/collater/stream_compose.py', '../../_base_/models/action_recognition/i3d.py',\n    '../../_base_/dataset/gtea/gtea_stream_video.py'\n]\nsample_rate = 1\nignore_index = -100\nsliding_window = 1\nclip_seg_num = 64\nMODEL = dict(\n    backbone = dict(",
        "detail": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "documentation": {}
    },
    {
        "label": "sample_rate",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "description": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "peekOfCode": "sample_rate = 1\nignore_index = -100\nsliding_window = 1\nclip_seg_num = 64\nMODEL = dict(\n    backbone = dict(\n        pretrained = \"./data/i3d_flow.pt\",\n        in_channels = 2\n    ),\n    head = dict(",
        "detail": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "documentation": {}
    },
    {
        "label": "ignore_index",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "description": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "peekOfCode": "ignore_index = -100\nsliding_window = 1\nclip_seg_num = 64\nMODEL = dict(\n    backbone = dict(\n        pretrained = \"./data/i3d_flow.pt\",\n        in_channels = 2\n    ),\n    head = dict(\n        sample_rate = sample_rate,",
        "detail": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "documentation": {}
    },
    {
        "label": "sliding_window",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "description": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "peekOfCode": "sliding_window = 1\nclip_seg_num = 64\nMODEL = dict(\n    backbone = dict(\n        pretrained = \"./data/i3d_flow.pt\",\n        in_channels = 2\n    ),\n    head = dict(\n        sample_rate = sample_rate,\n    ),",
        "detail": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "documentation": {}
    },
    {
        "label": "clip_seg_num",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "description": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "peekOfCode": "clip_seg_num = 64\nMODEL = dict(\n    backbone = dict(\n        pretrained = \"./data/i3d_flow.pt\",\n        in_channels = 2\n    ),\n    head = dict(\n        sample_rate = sample_rate,\n    ),\n    loss = None",
        "detail": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "description": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "peekOfCode": "MODEL = dict(\n    backbone = dict(\n        pretrained = \"./data/i3d_flow.pt\",\n        in_channels = 2\n    ),\n    head = dict(\n        sample_rate = sample_rate,\n    ),\n    loss = None\n)",
        "detail": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "documentation": {}
    },
    {
        "label": "PRETRAINED",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "description": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "peekOfCode": "PRETRAINED = None\nPOSTPRECESSING = dict(\n    name = \"StreamFeaturePostProcessing\",\n    sliding_window = sliding_window,\n    ignore_index = ignore_index\n)\nDATASET = dict(\n    video_batch_size = 1,\n    config = dict(\n        name = \"RawFrameStreamSegmentationDataset\",",
        "detail": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "documentation": {}
    },
    {
        "label": "POSTPRECESSING",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "description": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "peekOfCode": "POSTPRECESSING = dict(\n    name = \"StreamFeaturePostProcessing\",\n    sliding_window = sliding_window,\n    ignore_index = ignore_index\n)\nDATASET = dict(\n    video_batch_size = 1,\n    config = dict(\n        name = \"RawFrameStreamSegmentationDataset\",\n        data_prefix = \"./\",",
        "detail": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "description": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "peekOfCode": "DATASET = dict(\n    video_batch_size = 1,\n    config = dict(\n        name = \"RawFrameStreamSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/gtea/splits/all_files.txt\",\n        videos_path = \"./data/gtea/Videos\",\n        gt_path = \"./data/gtea/groundTruth\",\n        actions_map_file_path = \"./data/gtea/mapping.txt\",\n        dataset_type = \"gtea\",",
        "detail": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "documentation": {}
    },
    {
        "label": "PIPELINE",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "description": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "peekOfCode": "PIPELINE = dict(\n    name = \"BasePipline\",\n    decode = dict(\n        name = \"FlowVideoDecoder\",\n        backend = \"numpy\"\n    ),\n    sample = dict(\n        name = \"VideoStreamSampler\",\n        is_train = False,\n        sample_rate = sample_rate,",
        "detail": "SVTAS.config.extract.extract_feature.i3d_flow_gtea",
        "documentation": {}
    },
    {
        "label": "_base_",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "peekOfCode": "_base_ = [\n    '../../_base_/collater/stream_compose.py', '../../_base_/models/action_recognition/i3d.py',\n    '../../_base_/dataset/gtea/gtea_stream_video.py'\n]\nsample_rate = 1\nignore_index = -100\nsliding_window = 1\nclip_seg_num = 64\nMODEL = dict(\n    head = dict(",
        "detail": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "sample_rate",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "peekOfCode": "sample_rate = 1\nignore_index = -100\nsliding_window = 1\nclip_seg_num = 64\nMODEL = dict(\n    head = dict(\n        sample_rate = sample_rate\n    )\n)\nPRETRAINED = None",
        "detail": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "ignore_index",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "peekOfCode": "ignore_index = -100\nsliding_window = 1\nclip_seg_num = 64\nMODEL = dict(\n    head = dict(\n        sample_rate = sample_rate\n    )\n)\nPRETRAINED = None\nPOSTPRECESSING = dict(",
        "detail": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "sliding_window",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "peekOfCode": "sliding_window = 1\nclip_seg_num = 64\nMODEL = dict(\n    head = dict(\n        sample_rate = sample_rate\n    )\n)\nPRETRAINED = None\nPOSTPRECESSING = dict(\n    name = \"StreamFeaturePostProcessing\",",
        "detail": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "clip_seg_num",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "peekOfCode": "clip_seg_num = 64\nMODEL = dict(\n    head = dict(\n        sample_rate = sample_rate\n    )\n)\nPRETRAINED = None\nPOSTPRECESSING = dict(\n    name = \"StreamFeaturePostProcessing\",\n    sliding_window = sliding_window,",
        "detail": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "peekOfCode": "MODEL = dict(\n    head = dict(\n        sample_rate = sample_rate\n    )\n)\nPRETRAINED = None\nPOSTPRECESSING = dict(\n    name = \"StreamFeaturePostProcessing\",\n    sliding_window = sliding_window,\n    ignore_index = ignore_index",
        "detail": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "PRETRAINED",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "peekOfCode": "PRETRAINED = None\nPOSTPRECESSING = dict(\n    name = \"StreamFeaturePostProcessing\",\n    sliding_window = sliding_window,\n    ignore_index = ignore_index\n)\nDATASET = dict(\n    video_batch_size = 1,\n    config = dict(\n        name = \"RawFrameStreamSegmentationDataset\",",
        "detail": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "POSTPRECESSING",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "peekOfCode": "POSTPRECESSING = dict(\n    name = \"StreamFeaturePostProcessing\",\n    sliding_window = sliding_window,\n    ignore_index = ignore_index\n)\nDATASET = dict(\n    video_batch_size = 1,\n    config = dict(\n        name = \"RawFrameStreamSegmentationDataset\",\n        data_prefix = \"./\",",
        "detail": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "peekOfCode": "DATASET = dict(\n    video_batch_size = 1,\n    config = dict(\n        name = \"RawFrameStreamSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/gtea/splits/all_files.txt\",\n        videos_path = \"./data/gtea/Videos\",\n        gt_path = \"./data/gtea/groundTruth\",\n        actions_map_file_path = \"./data/gtea/mapping.txt\",\n        dataset_type = \"gtea\",",
        "detail": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "PIPELINE",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "peekOfCode": "PIPELINE = dict(\n    name = \"BasePipline\",\n    decode = dict(\n        name = \"VideoDecoder\",\n        backend = \"decord\"\n    ),\n    sample = dict(\n        name = \"VideoStreamSampler\",\n        is_train = False,\n        sample_rate = sample_rate,",
        "detail": "SVTAS.config.extract.extract_feature.i3d_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "_base_",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "peekOfCode": "_base_ = [\n    '../../_base_/collater/stream_compose.py'\n]\nsample_rate = 8\nignore_index = -100\nsliding_window = 1\nclip_seg_num = 8\nMODEL = dict(\n    architecture = \"Recognition2D\",\n    backbone = dict(",
        "detail": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "sample_rate",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "peekOfCode": "sample_rate = 8\nignore_index = -100\nsliding_window = 1\nclip_seg_num = 8\nMODEL = dict(\n    architecture = \"Recognition2D\",\n    backbone = dict(\n        name = \"MobileNetV2TSM\",\n        pretrained = \"./data/tsm_mobilenetv2_dense_320p_1x1x8_100e_kinetics400_rgb_20210202-61135809.pth\",\n        clip_seg_num = clip_seg_num,",
        "detail": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "ignore_index",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "peekOfCode": "ignore_index = -100\nsliding_window = 1\nclip_seg_num = 8\nMODEL = dict(\n    architecture = \"Recognition2D\",\n    backbone = dict(\n        name = \"MobileNetV2TSM\",\n        pretrained = \"./data/tsm_mobilenetv2_dense_320p_1x1x8_100e_kinetics400_rgb_20210202-61135809.pth\",\n        clip_seg_num = clip_seg_num,\n        shift_div = 8,",
        "detail": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "sliding_window",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "peekOfCode": "sliding_window = 1\nclip_seg_num = 8\nMODEL = dict(\n    architecture = \"Recognition2D\",\n    backbone = dict(\n        name = \"MobileNetV2TSM\",\n        pretrained = \"./data/tsm_mobilenetv2_dense_320p_1x1x8_100e_kinetics400_rgb_20210202-61135809.pth\",\n        clip_seg_num = clip_seg_num,\n        shift_div = 8,\n        out_indices = (7, )",
        "detail": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "clip_seg_num",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "peekOfCode": "clip_seg_num = 8\nMODEL = dict(\n    architecture = \"Recognition2D\",\n    backbone = dict(\n        name = \"MobileNetV2TSM\",\n        pretrained = \"./data/tsm_mobilenetv2_dense_320p_1x1x8_100e_kinetics400_rgb_20210202-61135809.pth\",\n        clip_seg_num = clip_seg_num,\n        shift_div = 8,\n        out_indices = (7, )\n    ),",
        "detail": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "peekOfCode": "MODEL = dict(\n    architecture = \"Recognition2D\",\n    backbone = dict(\n        name = \"MobileNetV2TSM\",\n        pretrained = \"./data/tsm_mobilenetv2_dense_320p_1x1x8_100e_kinetics400_rgb_20210202-61135809.pth\",\n        clip_seg_num = clip_seg_num,\n        shift_div = 8,\n        out_indices = (7, )\n    ),\n    neck = None,",
        "detail": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "PRETRAINED",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "peekOfCode": "PRETRAINED = None\nPOSTPRECESSING = dict(\n    name = \"StreamFeaturePostProcessing\",\n    sliding_window = sliding_window,\n    ignore_index = ignore_index\n)\nDATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = 1,\n    num_workers = 2,",
        "detail": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "POSTPRECESSING",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "peekOfCode": "POSTPRECESSING = dict(\n    name = \"StreamFeaturePostProcessing\",\n    sliding_window = sliding_window,\n    ignore_index = ignore_index\n)\nDATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = 1,\n    num_workers = 2,\n    config = dict(",
        "detail": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = 1,\n    num_workers = 2,\n    config = dict(\n        name = \"RawFrameStreamSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/gtea/splits/all_files.txt\",\n        videos_path = \"./data/gtea/Videos\",\n        gt_path = \"./data/gtea/groundTruth\",",
        "detail": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "PIPELINE",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "peekOfCode": "PIPELINE = dict(\n    name = \"BasePipline\",\n    decode = dict(\n        name = \"VideoDecoder\",\n        backend = \"decord\"\n    ),\n    sample = dict(\n        name = \"VideoStreamSampler\",\n        is_train = False,\n        sample_rate = sample_rate,",
        "detail": "SVTAS.config.extract.extract_feature.mobilev2_tsm_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "_base_",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "peekOfCode": "_base_ = [\n    '../../_base_/collater/stream_compose.py', '../../_base_/models/action_recognition/mvitv2_b.py',\n    '../../_base_/dataset/gtea/gtea_stream_video.py'\n]\nsample_rate = 1\nignore_index = -100\nsliding_window = 1\nclip_seg_num = 16\nMODEL = dict(\n    backbone = dict(",
        "detail": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "sample_rate",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "peekOfCode": "sample_rate = 1\nignore_index = -100\nsliding_window = 1\nclip_seg_num = 16\nMODEL = dict(\n    backbone = dict(\n        name = \"MViT\",\n        clip_seg_num = clip_seg_num,\n        # pretrained = \"./data/checkpoint/MViTv2_B_32x3_k400_f304025456.pyth\",\n        enable_rev = True,",
        "detail": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "ignore_index",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "peekOfCode": "ignore_index = -100\nsliding_window = 1\nclip_seg_num = 16\nMODEL = dict(\n    backbone = dict(\n        name = \"MViT\",\n        clip_seg_num = clip_seg_num,\n        # pretrained = \"./data/checkpoint/MViTv2_B_32x3_k400_f304025456.pyth\",\n        enable_rev = True,\n        cls_embed_on = False",
        "detail": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "sliding_window",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "peekOfCode": "sliding_window = 1\nclip_seg_num = 16\nMODEL = dict(\n    backbone = dict(\n        name = \"MViT\",\n        clip_seg_num = clip_seg_num,\n        # pretrained = \"./data/checkpoint/MViTv2_B_32x3_k400_f304025456.pyth\",\n        enable_rev = True,\n        cls_embed_on = False\n    ),",
        "detail": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "clip_seg_num",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "peekOfCode": "clip_seg_num = 16\nMODEL = dict(\n    backbone = dict(\n        name = \"MViT\",\n        clip_seg_num = clip_seg_num,\n        # pretrained = \"./data/checkpoint/MViTv2_B_32x3_k400_f304025456.pyth\",\n        enable_rev = True,\n        cls_embed_on = False\n    ),\n    head = dict(",
        "detail": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "peekOfCode": "MODEL = dict(\n    backbone = dict(\n        name = \"MViT\",\n        clip_seg_num = clip_seg_num,\n        # pretrained = \"./data/checkpoint/MViTv2_B_32x3_k400_f304025456.pyth\",\n        enable_rev = True,\n        cls_embed_on = False\n    ),\n    head = dict(\n        sample_rate = sample_rate",
        "detail": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "PRETRAINED",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "peekOfCode": "PRETRAINED = None\nPOSTPRECESSING = dict(\n    name = \"StreamFeaturePostProcessing\",\n    sliding_window = sliding_window,\n    ignore_index = ignore_index\n)\nDATASET = dict(\n    video_batch_size = 1,\n    config = dict(\n        name = \"RawFrameStreamSegmentationDataset\",",
        "detail": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "POSTPRECESSING",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "peekOfCode": "POSTPRECESSING = dict(\n    name = \"StreamFeaturePostProcessing\",\n    sliding_window = sliding_window,\n    ignore_index = ignore_index\n)\nDATASET = dict(\n    video_batch_size = 1,\n    config = dict(\n        name = \"RawFrameStreamSegmentationDataset\",\n        data_prefix = \"./\",",
        "detail": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "peekOfCode": "DATASET = dict(\n    video_batch_size = 1,\n    config = dict(\n        name = \"RawFrameStreamSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/gtea/splits/all_files.txt\",\n        videos_path = \"./data/gtea/Videos\",\n        gt_path = \"./data/gtea/groundTruth\",\n        actions_map_file_path = \"./data/gtea/mapping.txt\",\n        dataset_type = \"gtea\",",
        "detail": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "PIPELINE",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "peekOfCode": "PIPELINE = dict(\n    name = \"BasePipline\",\n    decode = dict(\n        name = \"VideoDecoder\",\n        backend = \"decord\"\n    ),\n    sample = dict(\n        name = \"VideoStreamSampler\",\n        is_train = False,\n        sample_rate = 3,",
        "detail": "SVTAS.config.extract.extract_feature.mvitv2_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "_base_",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "peekOfCode": "_base_ = [\n    '../../_base_/collater/stream_compose.py'\n]\nsample_rate = 2\nignore_index = -100\nsliding_window = 1\nclip_seg_num = 32\nMODEL = dict(\n    architecture = \"Recognition3D\",\n    backbone = dict(",
        "detail": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "sample_rate",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "peekOfCode": "sample_rate = 2\nignore_index = -100\nsliding_window = 1\nclip_seg_num = 32\nMODEL = dict(\n    architecture = \"Recognition3D\",\n    backbone = dict(\n        name = \"SwinTransformer3D\",\n        pretrained = \"./data/swin_tiny_patch244_window877_kinetics400_1k.pth\",\n        pretrained2d = False,",
        "detail": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "ignore_index",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "peekOfCode": "ignore_index = -100\nsliding_window = 1\nclip_seg_num = 32\nMODEL = dict(\n    architecture = \"Recognition3D\",\n    backbone = dict(\n        name = \"SwinTransformer3D\",\n        pretrained = \"./data/swin_tiny_patch244_window877_kinetics400_1k.pth\",\n        pretrained2d = False,\n        patch_size = [2, 4, 4],",
        "detail": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "sliding_window",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "peekOfCode": "sliding_window = 1\nclip_seg_num = 32\nMODEL = dict(\n    architecture = \"Recognition3D\",\n    backbone = dict(\n        name = \"SwinTransformer3D\",\n        pretrained = \"./data/swin_tiny_patch244_window877_kinetics400_1k.pth\",\n        pretrained2d = False,\n        patch_size = [2, 4, 4],\n        embed_dim = 96,",
        "detail": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "clip_seg_num",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "peekOfCode": "clip_seg_num = 32\nMODEL = dict(\n    architecture = \"Recognition3D\",\n    backbone = dict(\n        name = \"SwinTransformer3D\",\n        pretrained = \"./data/swin_tiny_patch244_window877_kinetics400_1k.pth\",\n        pretrained2d = False,\n        patch_size = [2, 4, 4],\n        embed_dim = 96,\n        depths = [2, 2, 6, 2],",
        "detail": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "peekOfCode": "MODEL = dict(\n    architecture = \"Recognition3D\",\n    backbone = dict(\n        name = \"SwinTransformer3D\",\n        pretrained = \"./data/swin_tiny_patch244_window877_kinetics400_1k.pth\",\n        pretrained2d = False,\n        patch_size = [2, 4, 4],\n        embed_dim = 96,\n        depths = [2, 2, 6, 2],\n        num_heads = [3, 6, 12, 24],",
        "detail": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "PRETRAINED",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "peekOfCode": "PRETRAINED = None\nPOSTPRECESSING = dict(\n    name = \"StreamFeaturePostProcessing\",\n    sliding_window = sliding_window,\n    ignore_index = ignore_index\n)\nDATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = 4,\n    num_workers = 2,",
        "detail": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "POSTPRECESSING",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "peekOfCode": "POSTPRECESSING = dict(\n    name = \"StreamFeaturePostProcessing\",\n    sliding_window = sliding_window,\n    ignore_index = ignore_index\n)\nDATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = 4,\n    num_workers = 2,\n    config = dict(",
        "detail": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = 4,\n    num_workers = 2,\n    config = dict(\n        name = \"RawFrameStreamSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/gtea/splits/all_files.txt\",\n        videos_path = \"./data/gtea/Videos\",\n        gt_path = \"./data/gtea/groundTruth\",",
        "detail": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "PIPELINE",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "peekOfCode": "PIPELINE = dict(\n    name = \"BasePipline\",\n    decode = dict(\n        name = \"VideoDecoder\",\n        backend = \"decord\"\n    ),\n    sample = dict(\n        name = \"VideoStreamSampler\",\n        is_train = False,\n        sample_rate = sample_rate,",
        "detail": "SVTAS.config.extract.extract_feature.swin_transformer_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "_base_",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "peekOfCode": "_base_ = [\n    '../../_base_/collater/stream_compose.py'\n]\nsample_rate = 8\nignore_index = -100\nsliding_window = 1\nclip_seg_num = 8\nMODEL = dict(\n    architecture = \"Recognition3D\",\n    backbone = dict(",
        "detail": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "sample_rate",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "peekOfCode": "sample_rate = 8\nignore_index = -100\nsliding_window = 1\nclip_seg_num = 8\nMODEL = dict(\n    architecture = \"Recognition3D\",\n    backbone = dict(\n        name = \"TimeSformer\",\n        pretrained = \"./data/timesformer_divST_8x32x1_15e_kinetics400_rgb-3f8e5d03.pth\",\n        num_frames = clip_seg_num,",
        "detail": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "ignore_index",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "peekOfCode": "ignore_index = -100\nsliding_window = 1\nclip_seg_num = 8\nMODEL = dict(\n    architecture = \"Recognition3D\",\n    backbone = dict(\n        name = \"TimeSformer\",\n        pretrained = \"./data/timesformer_divST_8x32x1_15e_kinetics400_rgb-3f8e5d03.pth\",\n        num_frames = clip_seg_num,\n        img_size = 224,",
        "detail": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "sliding_window",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "peekOfCode": "sliding_window = 1\nclip_seg_num = 8\nMODEL = dict(\n    architecture = \"Recognition3D\",\n    backbone = dict(\n        name = \"TimeSformer\",\n        pretrained = \"./data/timesformer_divST_8x32x1_15e_kinetics400_rgb-3f8e5d03.pth\",\n        num_frames = clip_seg_num,\n        img_size = 224,\n        patch_size = 16,",
        "detail": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "clip_seg_num",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "peekOfCode": "clip_seg_num = 8\nMODEL = dict(\n    architecture = \"Recognition3D\",\n    backbone = dict(\n        name = \"TimeSformer\",\n        pretrained = \"./data/timesformer_divST_8x32x1_15e_kinetics400_rgb-3f8e5d03.pth\",\n        num_frames = clip_seg_num,\n        img_size = 224,\n        patch_size = 16,\n        embed_dims = 768",
        "detail": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "peekOfCode": "MODEL = dict(\n    architecture = \"Recognition3D\",\n    backbone = dict(\n        name = \"TimeSformer\",\n        pretrained = \"./data/timesformer_divST_8x32x1_15e_kinetics400_rgb-3f8e5d03.pth\",\n        num_frames = clip_seg_num,\n        img_size = 224,\n        patch_size = 16,\n        embed_dims = 768\n    ),",
        "detail": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "PRETRAINED",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "peekOfCode": "PRETRAINED = None\nPOSTPRECESSING = dict(\n    name = \"StreamFeaturePostProcessing\",\n    sliding_window = sliding_window,\n    ignore_index = ignore_index\n)\nDATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = 4,\n    num_workers = 2,",
        "detail": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "POSTPRECESSING",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "peekOfCode": "POSTPRECESSING = dict(\n    name = \"StreamFeaturePostProcessing\",\n    sliding_window = sliding_window,\n    ignore_index = ignore_index\n)\nDATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = 4,\n    num_workers = 2,\n    config = dict(",
        "detail": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = 4,\n    num_workers = 2,\n    config = dict(\n        name = \"RawFrameStreamSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/gtea/splits/all_files.txt\",\n        videos_path = \"./data/gtea/Videos\",\n        gt_path = \"./data/gtea/groundTruth\",",
        "detail": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "PIPELINE",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "description": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "peekOfCode": "PIPELINE = dict(\n    name = \"BasePipline\",\n    decode = dict(\n        name = \"VideoDecoder\",\n        backend = \"decord\"\n    ),\n    sample = dict(\n        name = \"VideoStreamSampler\",\n        is_train = False,\n        sample_rate = sample_rate,",
        "detail": "SVTAS.config.extract.extract_feature.timesformer_rgb_gtea",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_flow.fastflow_gtea",
        "description": "SVTAS.config.extract.extract_flow.fastflow_gtea",
        "peekOfCode": "MODEL = dict(\n    architecture = \"OpticalFlowEstimation\",\n    model = dict(\n        name = \"FastFlowNet\",\n        pretrained = \"./data/fastflownet_ft_mix.pth\",\n        extract_mode = True,\n        freeze = True\n    )\n)\nDATASET = dict(",
        "detail": "SVTAS.config.extract.extract_flow.fastflow_gtea",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_flow.fastflow_gtea",
        "description": "SVTAS.config.extract.extract_flow.fastflow_gtea",
        "peekOfCode": "DATASET = dict(\n    video_batch_size = 1,\n    config = dict(\n        name = \"RawFrameStreamSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/gtea/splits/all_files.txt\",\n        videos_path = \"./data/gtea/Videos\",\n        gt_path = \"./data/gtea/groundTruth\",\n        actions_map_file_path = \"./data/gtea/mapping.txt\",\n        dataset_type = \"gtea\",",
        "detail": "SVTAS.config.extract.extract_flow.fastflow_gtea",
        "documentation": {}
    },
    {
        "label": "TRANSFORM",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_flow.fastflow_gtea",
        "description": "SVTAS.config.extract.extract_flow.fastflow_gtea",
        "peekOfCode": "TRANSFORM = [\n    dict(PILToTensor = None),\n    dict(ToFloat = None),\n    dict(Normalize = dict(\n        mean = [140.39158961711036, 108.18022223151027, 45.72351736766547],\n        std = [33.94421369129452, 35.93603536756186, 31.508484434367805]\n    ))\n]",
        "detail": "SVTAS.config.extract.extract_flow.fastflow_gtea",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_flow.liteflow_gtea",
        "description": "SVTAS.config.extract.extract_flow.liteflow_gtea",
        "peekOfCode": "MODEL = dict(\n    architecture = \"OpticalFlowEstimation\",\n    model = dict(\n        name = \"LiteFlowNetV3\",\n        pretrained = \"./data/network-sintel.pytorch\",\n        freeze = True\n    )\n)\nDATASET = dict(\n    video_batch_size = 1,",
        "detail": "SVTAS.config.extract.extract_flow.liteflow_gtea",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_flow.liteflow_gtea",
        "description": "SVTAS.config.extract.extract_flow.liteflow_gtea",
        "peekOfCode": "DATASET = dict(\n    video_batch_size = 1,\n    config = dict(\n        name = \"RawFrameStreamSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/gtea/splits/all_files.txt\",\n        videos_path = \"./data/gtea/Videos\",\n        gt_path = \"./data/gtea/groundTruth\",\n        actions_map_file_path = \"./data/gtea/mapping.txt\",\n        dataset_type = \"gtea\",",
        "detail": "SVTAS.config.extract.extract_flow.liteflow_gtea",
        "documentation": {}
    },
    {
        "label": "TRANSFORM",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_flow.liteflow_gtea",
        "description": "SVTAS.config.extract.extract_flow.liteflow_gtea",
        "peekOfCode": "TRANSFORM = [\n    dict(PILToTensor = None),\n    dict(ToFloat = None),\n    dict(NormalizeColorTo1 = None)\n]",
        "detail": "SVTAS.config.extract.extract_flow.liteflow_gtea",
        "documentation": {}
    },
    {
        "label": "_base_",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_flow.raft_gtea",
        "description": "SVTAS.config.extract.extract_flow.raft_gtea",
        "peekOfCode": "_base_ = [\n    '../../_base_/collater/stream_compose.py', '../../_base_/models/optical_flow_estimate/raft.py',\n    '../../_base_/dataset/gtea/gtea_stream_video.py'\n]\nsliding_window = 32\nclip_seg_num = 32\nsample_rate = 1\nDATASET = dict(\n    video_batch_size = 1,\n    config = dict(",
        "detail": "SVTAS.config.extract.extract_flow.raft_gtea",
        "documentation": {}
    },
    {
        "label": "sliding_window",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_flow.raft_gtea",
        "description": "SVTAS.config.extract.extract_flow.raft_gtea",
        "peekOfCode": "sliding_window = 32\nclip_seg_num = 32\nsample_rate = 1\nDATASET = dict(\n    video_batch_size = 1,\n    config = dict(\n        name = \"RawFrameStreamSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/gtea/splits/all_files.txt\",\n        videos_path = \"./data/gtea/Videos\",",
        "detail": "SVTAS.config.extract.extract_flow.raft_gtea",
        "documentation": {}
    },
    {
        "label": "clip_seg_num",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_flow.raft_gtea",
        "description": "SVTAS.config.extract.extract_flow.raft_gtea",
        "peekOfCode": "clip_seg_num = 32\nsample_rate = 1\nDATASET = dict(\n    video_batch_size = 1,\n    config = dict(\n        name = \"RawFrameStreamSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/gtea/splits/all_files.txt\",\n        videos_path = \"./data/gtea/Videos\",\n        gt_path = \"./data/gtea/groundTruth\",",
        "detail": "SVTAS.config.extract.extract_flow.raft_gtea",
        "documentation": {}
    },
    {
        "label": "sample_rate",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_flow.raft_gtea",
        "description": "SVTAS.config.extract.extract_flow.raft_gtea",
        "peekOfCode": "sample_rate = 1\nDATASET = dict(\n    video_batch_size = 1,\n    config = dict(\n        name = \"RawFrameStreamSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/gtea/splits/all_files.txt\",\n        videos_path = \"./data/gtea/Videos\",\n        gt_path = \"./data/gtea/groundTruth\",\n        actions_map_file_path = \"./data/gtea/mapping.txt\",",
        "detail": "SVTAS.config.extract.extract_flow.raft_gtea",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_flow.raft_gtea",
        "description": "SVTAS.config.extract.extract_flow.raft_gtea",
        "peekOfCode": "DATASET = dict(\n    video_batch_size = 1,\n    config = dict(\n        name = \"RawFrameStreamSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/gtea/splits/all_files.txt\",\n        videos_path = \"./data/gtea/Videos\",\n        gt_path = \"./data/gtea/groundTruth\",\n        actions_map_file_path = \"./data/gtea/mapping.txt\",\n        dataset_type = \"gtea\",",
        "detail": "SVTAS.config.extract.extract_flow.raft_gtea",
        "documentation": {}
    },
    {
        "label": "POSTPRECESSING",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_flow.raft_gtea",
        "description": "SVTAS.config.extract.extract_flow.raft_gtea",
        "peekOfCode": "POSTPRECESSING = dict(\n    name = \"OpticalFlowPostProcessing\",\n    fps = 15,\n    need_visualize = False,\n    sliding_window = sliding_window\n)\nPIPELINE = dict(\n    name = \"BasePipline\",\n    decode = dict(\n        name = \"VideoDecoder\",",
        "detail": "SVTAS.config.extract.extract_flow.raft_gtea",
        "documentation": {}
    },
    {
        "label": "PIPELINE",
        "kind": 5,
        "importPath": "SVTAS.config.extract.extract_flow.raft_gtea",
        "description": "SVTAS.config.extract.extract_flow.raft_gtea",
        "peekOfCode": "PIPELINE = dict(\n    name = \"BasePipline\",\n    decode = dict(\n        name = \"VideoDecoder\",\n        backend = \"decord\"\n    ),\n    sample = dict(\n        name = \"VideoStreamSampler\",\n        is_train = False,\n        sample_rate = sample_rate,",
        "detail": "SVTAS.config.extract.extract_flow.raft_gtea",
        "documentation": {}
    },
    {
        "label": "_base_",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "_base_ = [\n    '../../_base_/schedules/optimizer/adamw.py', '../../_base_/schedules/lr/liner_step_50e.py',\n    '../../_base_/default_runtime.py', '../../_base_/collater/stream_compose.py',\n    '../../_base_/dataset/gtea/gtea_stream_video.py'\n]\nsplit = 1\nnum_classes = 11\nsample_rate = 1\nclip_seg_num = 32\nsliding_window = 32",
        "detail": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "split",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "split = 1\nnum_classes = 11\nsample_rate = 1\nclip_seg_num = 32\nsliding_window = 32\nignore_index = -100\nbatch_size = 1\nepochs = 50\ncnt_max = 30\nmodel_name = \"BridgePrompt_gtea_split\" + str(split)",
        "detail": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "num_classes",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "num_classes = 11\nsample_rate = 1\nclip_seg_num = 32\nsliding_window = 32\nignore_index = -100\nbatch_size = 1\nepochs = 50\ncnt_max = 30\nmodel_name = \"BridgePrompt_gtea_split\" + str(split)\nMODEL = dict(",
        "detail": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "sample_rate",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "sample_rate = 1\nclip_seg_num = 32\nsliding_window = 32\nignore_index = -100\nbatch_size = 1\nepochs = 50\ncnt_max = 30\nmodel_name = \"BridgePrompt_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"ActionCLIP\",",
        "detail": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "clip_seg_num",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "clip_seg_num = 32\nsliding_window = 32\nignore_index = -100\nbatch_size = 1\nepochs = 50\ncnt_max = 30\nmodel_name = \"BridgePrompt_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"ActionCLIP\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",",
        "detail": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "sliding_window",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "sliding_window = 32\nignore_index = -100\nbatch_size = 1\nepochs = 50\ncnt_max = 30\nmodel_name = \"BridgePrompt_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"ActionCLIP\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",\n    image_prompt = dict(",
        "detail": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "ignore_index",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "ignore_index = -100\nbatch_size = 1\nepochs = 50\ncnt_max = 30\nmodel_name = \"BridgePrompt_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"ActionCLIP\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",\n    image_prompt = dict(\n        name = \"CLIP\",",
        "detail": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "batch_size = 1\nepochs = 50\ncnt_max = 30\nmodel_name = \"BridgePrompt_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"ActionCLIP\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",\n    image_prompt = dict(\n        name = \"CLIP\",\n        # pretrained = \"./data/checkpoint/ViT-B-16.pt\",",
        "detail": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "epochs",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "epochs = 50\ncnt_max = 30\nmodel_name = \"BridgePrompt_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"ActionCLIP\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",\n    image_prompt = dict(\n        name = \"CLIP\",\n        # pretrained = \"./data/checkpoint/ViT-B-16.pt\",\n        embed_dim = 512,",
        "detail": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "cnt_max",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "cnt_max = 30\nmodel_name = \"BridgePrompt_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"ActionCLIP\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",\n    image_prompt = dict(\n        name = \"CLIP\",\n        # pretrained = \"./data/checkpoint/ViT-B-16.pt\",\n        embed_dim = 512,\n        image_resolution = 224,",
        "detail": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "model_name = \"BridgePrompt_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"ActionCLIP\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",\n    image_prompt = dict(\n        name = \"CLIP\",\n        # pretrained = \"./data/checkpoint/ViT-B-16.pt\",\n        embed_dim = 512,\n        image_resolution = 224,\n        vision_layers = 12,",
        "detail": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "MODEL = dict(\n    architecture = \"ActionCLIP\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",\n    image_prompt = dict(\n        name = \"CLIP\",\n        # pretrained = \"./data/checkpoint/ViT-B-16.pt\",\n        embed_dim = 512,\n        image_resolution = 224,\n        vision_layers = 12,\n        vision_width = 768,",
        "detail": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "POSTPRECESSING",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "POSTPRECESSING = dict(\n    name = \"ScorePostProcessing\",\n    num_classes = num_classes,\n    ignore_index = ignore_index\n)\nLRSCHEDULER = dict(\n    step_size = [epochs]\n)\nDATASET = dict(\n    temporal_clip_batch_size = 3,",
        "detail": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "LRSCHEDULER",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "LRSCHEDULER = dict(\n    step_size = [epochs]\n)\nDATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = batch_size,\n    num_workers = 2,\n    train = dict(\n        file_path = \"./data/gtea/splits/train.split\" + str(split) + \".bundle\",\n        sliding_window = sliding_window",
        "detail": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = batch_size,\n    num_workers = 2,\n    train = dict(\n        file_path = \"./data/gtea/splits/train.split\" + str(split) + \".bundle\",\n        sliding_window = sliding_window\n    ),\n    test = dict(\n        file_path = \"./data/gtea/splits/test.split\" + str(split) + \".bundle\",",
        "detail": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "PIPELINE",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "PIPELINE = dict(\n    train = dict(\n        name = \"BasePipline\",\n        decode = dict(\n            name = \"VideoDecoder\",\n            backend = \"decord\"\n        ),\n        sample = dict(\n            name = \"VideoStreamSampler\",\n            is_train = False,",
        "detail": "SVTAS.config.svtas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "_base_",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.slvit_gtea",
        "description": "SVTAS.config.svtas.rgb.slvit_gtea",
        "peekOfCode": "_base_ = [\n    '../../_base_/schedules/adan_50e.py', '../../_base_/models/image_classification/vit.py',\n    '../../_base_/default_runtime.py', '../../_base_/collater/stream_compose.py',\n    '../../_base_/dataset/gtea/gtea_stream_video.py'\n]\nnum_classes = 11\nsample_rate = 4\nclip_seg_num = 8\nignore_index = -100\nsliding_window = 32",
        "detail": "SVTAS.config.svtas.rgb.slvit_gtea",
        "documentation": {}
    },
    {
        "label": "num_classes",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.slvit_gtea",
        "description": "SVTAS.config.svtas.rgb.slvit_gtea",
        "peekOfCode": "num_classes = 11\nsample_rate = 4\nclip_seg_num = 8\nignore_index = -100\nsliding_window = 32\nsplit = 1\nbatch_size = 2\nmodel_name = \"SLViT_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Recognition2D\",",
        "detail": "SVTAS.config.svtas.rgb.slvit_gtea",
        "documentation": {}
    },
    {
        "label": "sample_rate",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.slvit_gtea",
        "description": "SVTAS.config.svtas.rgb.slvit_gtea",
        "peekOfCode": "sample_rate = 4\nclip_seg_num = 8\nignore_index = -100\nsliding_window = 32\nsplit = 1\nbatch_size = 2\nmodel_name = \"SLViT_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Recognition2D\",\n    backbone = dict(",
        "detail": "SVTAS.config.svtas.rgb.slvit_gtea",
        "documentation": {}
    },
    {
        "label": "clip_seg_num",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.slvit_gtea",
        "description": "SVTAS.config.svtas.rgb.slvit_gtea",
        "peekOfCode": "clip_seg_num = 8\nignore_index = -100\nsliding_window = 32\nsplit = 1\nbatch_size = 2\nmodel_name = \"SLViT_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Recognition2D\",\n    backbone = dict(\n        name = \"SLViT\",",
        "detail": "SVTAS.config.svtas.rgb.slvit_gtea",
        "documentation": {}
    },
    {
        "label": "ignore_index",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.slvit_gtea",
        "description": "SVTAS.config.svtas.rgb.slvit_gtea",
        "peekOfCode": "ignore_index = -100\nsliding_window = 32\nsplit = 1\nbatch_size = 2\nmodel_name = \"SLViT_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Recognition2D\",\n    backbone = dict(\n        name = \"SLViT\",\n        image_size = 224,",
        "detail": "SVTAS.config.svtas.rgb.slvit_gtea",
        "documentation": {}
    },
    {
        "label": "sliding_window",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.slvit_gtea",
        "description": "SVTAS.config.svtas.rgb.slvit_gtea",
        "peekOfCode": "sliding_window = 32\nsplit = 1\nbatch_size = 2\nmodel_name = \"SLViT_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Recognition2D\",\n    backbone = dict(\n        name = \"SLViT\",\n        image_size = 224,\n        patch_size = 32,",
        "detail": "SVTAS.config.svtas.rgb.slvit_gtea",
        "documentation": {}
    },
    {
        "label": "split",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.slvit_gtea",
        "description": "SVTAS.config.svtas.rgb.slvit_gtea",
        "peekOfCode": "split = 1\nbatch_size = 2\nmodel_name = \"SLViT_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Recognition2D\",\n    backbone = dict(\n        name = \"SLViT\",\n        image_size = 224,\n        patch_size = 32,\n        depth = 4,",
        "detail": "SVTAS.config.svtas.rgb.slvit_gtea",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.slvit_gtea",
        "description": "SVTAS.config.svtas.rgb.slvit_gtea",
        "peekOfCode": "batch_size = 2\nmodel_name = \"SLViT_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Recognition2D\",\n    backbone = dict(\n        name = \"SLViT\",\n        image_size = 224,\n        patch_size = 32,\n        depth = 4,\n        heads = 12,",
        "detail": "SVTAS.config.svtas.rgb.slvit_gtea",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.slvit_gtea",
        "description": "SVTAS.config.svtas.rgb.slvit_gtea",
        "peekOfCode": "model_name = \"SLViT_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Recognition2D\",\n    backbone = dict(\n        name = \"SLViT\",\n        image_size = 224,\n        patch_size = 32,\n        depth = 4,\n        heads = 12,\n        mlp_dim = 1024,",
        "detail": "SVTAS.config.svtas.rgb.slvit_gtea",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.slvit_gtea",
        "description": "SVTAS.config.svtas.rgb.slvit_gtea",
        "peekOfCode": "MODEL = dict(\n    architecture = \"Recognition2D\",\n    backbone = dict(\n        name = \"SLViT\",\n        image_size = 224,\n        patch_size = 32,\n        depth = 4,\n        heads = 12,\n        mlp_dim = 1024,\n        dropout = 0.3,",
        "detail": "SVTAS.config.svtas.rgb.slvit_gtea",
        "documentation": {}
    },
    {
        "label": "POSTPRECESSING",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.slvit_gtea",
        "description": "SVTAS.config.svtas.rgb.slvit_gtea",
        "peekOfCode": "POSTPRECESSING = dict(\n    name = \"StreamScorePostProcessing\",\n    sliding_window = sliding_window,\n    ignore_index = ignore_index\n)\nDATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = batch_size,\n    num_workers = 2,\n    train = dict(",
        "detail": "SVTAS.config.svtas.rgb.slvit_gtea",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.slvit_gtea",
        "description": "SVTAS.config.svtas.rgb.slvit_gtea",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = batch_size,\n    num_workers = 2,\n    train = dict(\n        file_path = \"./data/gtea/splits/train.split\" + str(split) + \".bundle\",\n        sliding_window = sliding_window\n    ),\n    test = dict(\n        file_path = \"./data/gtea/splits/test.split\" + str(split) + \".bundle\",",
        "detail": "SVTAS.config.svtas.rgb.slvit_gtea",
        "documentation": {}
    },
    {
        "label": "PIPELINE",
        "kind": 5,
        "importPath": "SVTAS.config.svtas.rgb.slvit_gtea",
        "description": "SVTAS.config.svtas.rgb.slvit_gtea",
        "peekOfCode": "PIPELINE = dict(\n    train = dict(\n        name = \"BasePipline\",\n        decode = dict(\n            name = \"VideoDecoder\",\n            backend = \"decord\"\n        ),\n        sample = dict(\n            name = \"VideoStreamSampler\",\n            is_train = False,",
        "detail": "SVTAS.config.svtas.rgb.slvit_gtea",
        "documentation": {}
    },
    {
        "label": "_base_",
        "kind": 5,
        "importPath": "SVTAS.config.tas.feature.linformer_gtea",
        "description": "SVTAS.config.tas.feature.linformer_gtea",
        "peekOfCode": "_base_ = [\n    '../../_base_/schedules/optimizer/adam.py', '../../_base_/models/temporal_action_segmentation/linformer.py',\n    '../../_base_/default_runtime.py', '../../_base_/collater/batch_compose.py',\n    '../../_base_/dataset/gtea/gtea_feature.py', '../../_base_/schedules/lr/liner_step_50e.py'\n]\nsplit = 1\nnum_classes = 11\nsample_rate = 1\nignore_index = -100\nmodel_name = \"Linformer_gtea_split\" + str(split)",
        "detail": "SVTAS.config.tas.feature.linformer_gtea",
        "documentation": {}
    },
    {
        "label": "split",
        "kind": 5,
        "importPath": "SVTAS.config.tas.feature.linformer_gtea",
        "description": "SVTAS.config.tas.feature.linformer_gtea",
        "peekOfCode": "split = 1\nnum_classes = 11\nsample_rate = 1\nignore_index = -100\nmodel_name = \"Linformer_gtea_split\" + str(split)\nbatch_size = 1\nepochs = 50\nMODEL = dict(\n    head = dict(\n        input_dim = 2048,",
        "detail": "SVTAS.config.tas.feature.linformer_gtea",
        "documentation": {}
    },
    {
        "label": "num_classes",
        "kind": 5,
        "importPath": "SVTAS.config.tas.feature.linformer_gtea",
        "description": "SVTAS.config.tas.feature.linformer_gtea",
        "peekOfCode": "num_classes = 11\nsample_rate = 1\nignore_index = -100\nmodel_name = \"Linformer_gtea_split\" + str(split)\nbatch_size = 1\nepochs = 50\nMODEL = dict(\n    head = dict(\n        input_dim = 2048,\n        num_classes = num_classes,",
        "detail": "SVTAS.config.tas.feature.linformer_gtea",
        "documentation": {}
    },
    {
        "label": "sample_rate",
        "kind": 5,
        "importPath": "SVTAS.config.tas.feature.linformer_gtea",
        "description": "SVTAS.config.tas.feature.linformer_gtea",
        "peekOfCode": "sample_rate = 1\nignore_index = -100\nmodel_name = \"Linformer_gtea_split\" + str(split)\nbatch_size = 1\nepochs = 50\nMODEL = dict(\n    head = dict(\n        input_dim = 2048,\n        num_classes = num_classes,\n        sample_rate = sample_rate",
        "detail": "SVTAS.config.tas.feature.linformer_gtea",
        "documentation": {}
    },
    {
        "label": "ignore_index",
        "kind": 5,
        "importPath": "SVTAS.config.tas.feature.linformer_gtea",
        "description": "SVTAS.config.tas.feature.linformer_gtea",
        "peekOfCode": "ignore_index = -100\nmodel_name = \"Linformer_gtea_split\" + str(split)\nbatch_size = 1\nepochs = 50\nMODEL = dict(\n    head = dict(\n        input_dim = 2048,\n        num_classes = num_classes,\n        sample_rate = sample_rate\n    ),",
        "detail": "SVTAS.config.tas.feature.linformer_gtea",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "SVTAS.config.tas.feature.linformer_gtea",
        "description": "SVTAS.config.tas.feature.linformer_gtea",
        "peekOfCode": "model_name = \"Linformer_gtea_split\" + str(split)\nbatch_size = 1\nepochs = 50\nMODEL = dict(\n    head = dict(\n        input_dim = 2048,\n        num_classes = num_classes,\n        sample_rate = sample_rate\n    ),\n    loss = dict(",
        "detail": "SVTAS.config.tas.feature.linformer_gtea",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "SVTAS.config.tas.feature.linformer_gtea",
        "description": "SVTAS.config.tas.feature.linformer_gtea",
        "peekOfCode": "batch_size = 1\nepochs = 50\nMODEL = dict(\n    head = dict(\n        input_dim = 2048,\n        num_classes = num_classes,\n        sample_rate = sample_rate\n    ),\n    loss = dict(\n        num_classes = num_classes,",
        "detail": "SVTAS.config.tas.feature.linformer_gtea",
        "documentation": {}
    },
    {
        "label": "epochs",
        "kind": 5,
        "importPath": "SVTAS.config.tas.feature.linformer_gtea",
        "description": "SVTAS.config.tas.feature.linformer_gtea",
        "peekOfCode": "epochs = 50\nMODEL = dict(\n    head = dict(\n        input_dim = 2048,\n        num_classes = num_classes,\n        sample_rate = sample_rate\n    ),\n    loss = dict(\n        num_classes = num_classes,\n        sample_rate = sample_rate,",
        "detail": "SVTAS.config.tas.feature.linformer_gtea",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config.tas.feature.linformer_gtea",
        "description": "SVTAS.config.tas.feature.linformer_gtea",
        "peekOfCode": "MODEL = dict(\n    head = dict(\n        input_dim = 2048,\n        num_classes = num_classes,\n        sample_rate = sample_rate\n    ),\n    loss = dict(\n        num_classes = num_classes,\n        sample_rate = sample_rate,\n        ignore_index = ignore_index",
        "detail": "SVTAS.config.tas.feature.linformer_gtea",
        "documentation": {}
    },
    {
        "label": "POSTPRECESSING",
        "kind": 5,
        "importPath": "SVTAS.config.tas.feature.linformer_gtea",
        "description": "SVTAS.config.tas.feature.linformer_gtea",
        "peekOfCode": "POSTPRECESSING = dict(\n    name = \"ScorePostProcessing\",\n    num_classes = num_classes,\n    ignore_index = ignore_index\n)\nLRSCHEDULER = dict(\n    step_size = [epochs]\n)\nOPTIMIZER = dict(\n    learning_rate = 0.0005 * batch_size,",
        "detail": "SVTAS.config.tas.feature.linformer_gtea",
        "documentation": {}
    },
    {
        "label": "LRSCHEDULER",
        "kind": 5,
        "importPath": "SVTAS.config.tas.feature.linformer_gtea",
        "description": "SVTAS.config.tas.feature.linformer_gtea",
        "peekOfCode": "LRSCHEDULER = dict(\n    step_size = [epochs]\n)\nOPTIMIZER = dict(\n    learning_rate = 0.0005 * batch_size,\n    weight_decay = 1e-4,\n    betas = (0.9, 0.999)\n)\nDATASET = dict(\n    temporal_clip_batch_size = batch_size,",
        "detail": "SVTAS.config.tas.feature.linformer_gtea",
        "documentation": {}
    },
    {
        "label": "OPTIMIZER",
        "kind": 5,
        "importPath": "SVTAS.config.tas.feature.linformer_gtea",
        "description": "SVTAS.config.tas.feature.linformer_gtea",
        "peekOfCode": "OPTIMIZER = dict(\n    learning_rate = 0.0005 * batch_size,\n    weight_decay = 1e-4,\n    betas = (0.9, 0.999)\n)\nDATASET = dict(\n    temporal_clip_batch_size = batch_size,\n    video_batch_size = batch_size,\n    train = dict(\n        file_path = \"./data/gtea/splits/train.split\" + str(split) + \".bundle\",",
        "detail": "SVTAS.config.tas.feature.linformer_gtea",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config.tas.feature.linformer_gtea",
        "description": "SVTAS.config.tas.feature.linformer_gtea",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = batch_size,\n    video_batch_size = batch_size,\n    train = dict(\n        file_path = \"./data/gtea/splits/train.split\" + str(split) + \".bundle\",\n        feature_path = \"./data/gtea/raw_features\",\n    ),\n    test = dict(\n        file_path = \"./data/gtea/splits/test.split\" + str(split) + \".bundle\",\n        feature_path = \"./data/gtea/raw_features\",",
        "detail": "SVTAS.config.tas.feature.linformer_gtea",
        "documentation": {}
    },
    {
        "label": "PIPELINE",
        "kind": 5,
        "importPath": "SVTAS.config.tas.feature.linformer_gtea",
        "description": "SVTAS.config.tas.feature.linformer_gtea",
        "peekOfCode": "PIPELINE = dict(\n    train = dict(\n        name = \"BasePipline\",\n        decode = dict(\n            name = \"FeatureDecoder\",\n            backend = \"numpy\"\n        ),\n        sample = dict(\n            name = \"FeatureSampler\",\n            is_train = True,",
        "detail": "SVTAS.config.tas.feature.linformer_gtea",
        "documentation": {}
    },
    {
        "label": "_base_",
        "kind": 5,
        "importPath": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "description": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "peekOfCode": "_base_ = [\n    '../../_base_/schedules/optimizer/adam.py', '../../_base_/schedules/lr/liner_step_50e.py',\n    '../../_base_/models/temporal_action_segmentation/ms_tcn.py',\n    '../../_base_/default_runtime.py', '../../_base_/collater/batch_compose.py',\n    '../../_base_/dataset/gtea/gtea_feature.py'\n]\nsplit = 1\nnum_classes = 11\nsample_rate = 1\nignore_index = -100",
        "detail": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "split",
        "kind": 5,
        "importPath": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "description": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "peekOfCode": "split = 1\nnum_classes = 11\nsample_rate = 1\nignore_index = -100\nepochs = 50\nmodel_name = \"MSTCN_gtea_split\" + str(split)\nMODEL = dict(\n    head = dict(\n        dim = 512,\n        num_classes = num_classes,",
        "detail": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "num_classes",
        "kind": 5,
        "importPath": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "description": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "peekOfCode": "num_classes = 11\nsample_rate = 1\nignore_index = -100\nepochs = 50\nmodel_name = \"MSTCN_gtea_split\" + str(split)\nMODEL = dict(\n    head = dict(\n        dim = 512,\n        num_classes = num_classes,\n        sample_rate = sample_rate",
        "detail": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "sample_rate",
        "kind": 5,
        "importPath": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "description": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "peekOfCode": "sample_rate = 1\nignore_index = -100\nepochs = 50\nmodel_name = \"MSTCN_gtea_split\" + str(split)\nMODEL = dict(\n    head = dict(\n        dim = 512,\n        num_classes = num_classes,\n        sample_rate = sample_rate\n    ),",
        "detail": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "ignore_index",
        "kind": 5,
        "importPath": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "description": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "peekOfCode": "ignore_index = -100\nepochs = 50\nmodel_name = \"MSTCN_gtea_split\" + str(split)\nMODEL = dict(\n    head = dict(\n        dim = 512,\n        num_classes = num_classes,\n        sample_rate = sample_rate\n    ),\n    loss = dict(",
        "detail": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "epochs",
        "kind": 5,
        "importPath": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "description": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "peekOfCode": "epochs = 50\nmodel_name = \"MSTCN_gtea_split\" + str(split)\nMODEL = dict(\n    head = dict(\n        dim = 512,\n        num_classes = num_classes,\n        sample_rate = sample_rate\n    ),\n    loss = dict(\n        num_classes = num_classes,",
        "detail": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "description": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "peekOfCode": "model_name = \"MSTCN_gtea_split\" + str(split)\nMODEL = dict(\n    head = dict(\n        dim = 512,\n        num_classes = num_classes,\n        sample_rate = sample_rate\n    ),\n    loss = dict(\n        num_classes = num_classes,\n        sample_rate = sample_rate,",
        "detail": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "description": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "peekOfCode": "MODEL = dict(\n    head = dict(\n        dim = 512,\n        num_classes = num_classes,\n        sample_rate = sample_rate\n    ),\n    loss = dict(\n        num_classes = num_classes,\n        sample_rate = sample_rate,\n        ignore_index = ignore_index",
        "detail": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "POSTPRECESSING",
        "kind": 5,
        "importPath": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "description": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "peekOfCode": "POSTPRECESSING = dict(\n    name = \"ScorePostProcessing\",\n    num_classes = num_classes,\n    ignore_index = ignore_index\n)\nDATASET = dict(\n    train = dict(\n        file_path = \"./data/gtea/splits/train.split\" + str(split) + \".bundle\",\n        # flow_feature_path = \"./data/gtea/flow_features\"\n    ),",
        "detail": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "description": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "peekOfCode": "DATASET = dict(\n    train = dict(\n        file_path = \"./data/gtea/splits/train.split\" + str(split) + \".bundle\",\n        # flow_feature_path = \"./data/gtea/flow_features\"\n    ),\n    test = dict(\n        file_path = \"./data/gtea/splits/test.split\" + str(split) + \".bundle\",\n        # flow_feature_path = \"./data/gtea/flow_features\"\n    )\n)",
        "detail": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "LRSCHEDULER",
        "kind": 5,
        "importPath": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "description": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "peekOfCode": "LRSCHEDULER = dict(\n    step_size = [epochs]\n)\nPIPELINE = dict(\n    train = dict(\n        name = \"BasePipline\",\n        decode = dict(\n            name = \"FeatureDecoder\",\n            backend = \"numpy\"\n        ),",
        "detail": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "PIPELINE",
        "kind": 5,
        "importPath": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "description": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "peekOfCode": "PIPELINE = dict(\n    train = dict(\n        name = \"BasePipline\",\n        decode = dict(\n            name = \"FeatureDecoder\",\n            backend = \"numpy\"\n        ),\n        sample = dict(\n            name = \"FeatureSampler\",\n            is_train = True,",
        "detail": "SVTAS.config.tas.feature.ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "_base_",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "peekOfCode": "_base_ = [\n    '../../_base_/schedules/optimizer/adamw.py', '../../_base_/schedules/lr/liner_step_50e.py',\n    '../../_base_/default_runtime.py', '../../_base_/collater/batch_compose.py',\n    '../../_base_/dataset/50salads/50salads_video.py'\n]\nsplit = 1\nnum_classes = 19\nsample_rate = 1\nclip_seg_num = 64\nignore_index = -100",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "documentation": {}
    },
    {
        "label": "split",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "peekOfCode": "split = 1\nnum_classes = 19\nsample_rate = 1\nclip_seg_num = 64\nignore_index = -100\nbatch_size = 1\nepochs = 50\ncnt_max = 30\nmodel_name = \"BridgePrompt_MS_TCN_50salads_split\" + str(split)\nMODEL = dict(",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "documentation": {}
    },
    {
        "label": "num_classes",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "peekOfCode": "num_classes = 19\nsample_rate = 1\nclip_seg_num = 64\nignore_index = -100\nbatch_size = 1\nepochs = 50\ncnt_max = 30\nmodel_name = \"BridgePrompt_MS_TCN_50salads_split\" + str(split)\nMODEL = dict(\n    architecture = \"ActionCLIPSegmentation\",",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "documentation": {}
    },
    {
        "label": "sample_rate",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "peekOfCode": "sample_rate = 1\nclip_seg_num = 64\nignore_index = -100\nbatch_size = 1\nepochs = 50\ncnt_max = 30\nmodel_name = \"BridgePrompt_MS_TCN_50salads_split\" + str(split)\nMODEL = dict(\n    architecture = \"ActionCLIPSegmentation\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "documentation": {}
    },
    {
        "label": "clip_seg_num",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "peekOfCode": "clip_seg_num = 64\nignore_index = -100\nbatch_size = 1\nepochs = 50\ncnt_max = 30\nmodel_name = \"BridgePrompt_MS_TCN_50salads_split\" + str(split)\nMODEL = dict(\n    architecture = \"ActionCLIPSegmentation\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",\n    image_prompt = dict(",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "documentation": {}
    },
    {
        "label": "ignore_index",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "peekOfCode": "ignore_index = -100\nbatch_size = 1\nepochs = 50\ncnt_max = 30\nmodel_name = \"BridgePrompt_MS_TCN_50salads_split\" + str(split)\nMODEL = dict(\n    architecture = \"ActionCLIPSegmentation\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",\n    image_prompt = dict(\n        name = \"CLIP\",",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "peekOfCode": "batch_size = 1\nepochs = 50\ncnt_max = 30\nmodel_name = \"BridgePrompt_MS_TCN_50salads_split\" + str(split)\nMODEL = dict(\n    architecture = \"ActionCLIPSegmentation\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",\n    image_prompt = dict(\n        name = \"CLIP\",\n        # pretrained = \"./data/checkpoint/ViT-B-16.pt\",",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "documentation": {}
    },
    {
        "label": "epochs",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "peekOfCode": "epochs = 50\ncnt_max = 30\nmodel_name = \"BridgePrompt_MS_TCN_50salads_split\" + str(split)\nMODEL = dict(\n    architecture = \"ActionCLIPSegmentation\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",\n    image_prompt = dict(\n        name = \"CLIP\",\n        # pretrained = \"./data/checkpoint/ViT-B-16.pt\",\n        embed_dim = 512,",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "documentation": {}
    },
    {
        "label": "cnt_max",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "peekOfCode": "cnt_max = 30\nmodel_name = \"BridgePrompt_MS_TCN_50salads_split\" + str(split)\nMODEL = dict(\n    architecture = \"ActionCLIPSegmentation\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",\n    image_prompt = dict(\n        name = \"CLIP\",\n        # pretrained = \"./data/checkpoint/ViT-B-16.pt\",\n        embed_dim = 512,\n        image_resolution = 224,",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "peekOfCode": "model_name = \"BridgePrompt_MS_TCN_50salads_split\" + str(split)\nMODEL = dict(\n    architecture = \"ActionCLIPSegmentation\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",\n    image_prompt = dict(\n        name = \"CLIP\",\n        # pretrained = \"./data/checkpoint/ViT-B-16.pt\",\n        embed_dim = 512,\n        image_resolution = 224,\n        vision_layers = 12,",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "peekOfCode": "MODEL = dict(\n    architecture = \"ActionCLIPSegmentation\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",\n    image_prompt = dict(\n        name = \"CLIP\",\n        # pretrained = \"./data/checkpoint/ViT-B-16.pt\",\n        embed_dim = 512,\n        image_resolution = 224,\n        vision_layers = 12,\n        vision_width = 768,",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "documentation": {}
    },
    {
        "label": "POSTPRECESSING",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "peekOfCode": "POSTPRECESSING = dict(\n    name = \"ScorePostProcessing\",\n    num_classes = num_classes,\n    ignore_index = ignore_index\n)\nLRSCHEDULER = dict(\n    step_size = [epochs]\n)\nDATASET = dict(\n    temporal_clip_batch_size = batch_size,",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "documentation": {}
    },
    {
        "label": "LRSCHEDULER",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "peekOfCode": "LRSCHEDULER = dict(\n    step_size = [epochs]\n)\nDATASET = dict(\n    temporal_clip_batch_size = batch_size,\n    video_batch_size = batch_size,\n    num_workers = 2,\n    train = dict(\n        file_path = \"./data/50salads/splits/train.split\" + str(split) + \".bundle\"\n    ),",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = batch_size,\n    video_batch_size = batch_size,\n    num_workers = 2,\n    train = dict(\n        file_path = \"./data/50salads/splits/train.split\" + str(split) + \".bundle\"\n    ),\n    test = dict(\n        file_path = \"./data/50salads/splits/test.split\" + str(split) + \".bundle\"\n    )",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "documentation": {}
    },
    {
        "label": "PIPELINE",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "peekOfCode": "PIPELINE = dict(\n    train = dict(\n        name = \"BasePipline\",\n        decode = dict(\n            name = \"VideoDecoder\",\n            backend = \"decord\"\n        ),\n        sample = dict(\n            name = \"VideoSampler\",\n            is_train = True,",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_50salads",
        "documentation": {}
    },
    {
        "label": "_base_",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "_base_ = [\n    '../../_base_/schedules/optimizer/adamw.py', '../../_base_/schedules/lr/liner_step_50e.py',\n    '../../_base_/default_runtime.py', '../../_base_/collater/batch_compose.py',\n    '../../_base_/dataset/gtea/gtea_video.py'\n]\nsplit = 1\nnum_classes = 11\nsample_rate = 1\nclip_seg_num = 64\nignore_index = -100",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "split",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "split = 1\nnum_classes = 11\nsample_rate = 1\nclip_seg_num = 64\nignore_index = -100\nbatch_size = 1\nepochs = 50\ncnt_max = 30\nmodel_name = \"BridgePrompt_MS_TCN_gtea_split\" + str(split)\nMODEL = dict(",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "num_classes",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "num_classes = 11\nsample_rate = 1\nclip_seg_num = 64\nignore_index = -100\nbatch_size = 1\nepochs = 50\ncnt_max = 30\nmodel_name = \"BridgePrompt_MS_TCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"ActionCLIPSegmentation\",",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "sample_rate",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "sample_rate = 1\nclip_seg_num = 64\nignore_index = -100\nbatch_size = 1\nepochs = 50\ncnt_max = 30\nmodel_name = \"BridgePrompt_MS_TCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"ActionCLIPSegmentation\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "clip_seg_num",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "clip_seg_num = 64\nignore_index = -100\nbatch_size = 1\nepochs = 50\ncnt_max = 30\nmodel_name = \"BridgePrompt_MS_TCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"ActionCLIPSegmentation\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",\n    image_prompt = dict(",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "ignore_index",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "ignore_index = -100\nbatch_size = 1\nepochs = 50\ncnt_max = 30\nmodel_name = \"BridgePrompt_MS_TCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"ActionCLIPSegmentation\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",\n    image_prompt = dict(\n        name = \"CLIP\",",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "batch_size = 1\nepochs = 50\ncnt_max = 30\nmodel_name = \"BridgePrompt_MS_TCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"ActionCLIPSegmentation\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",\n    image_prompt = dict(\n        name = \"CLIP\",\n        # pretrained = \"./data/checkpoint/ViT-B-16.pt\",",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "epochs",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "epochs = 50\ncnt_max = 30\nmodel_name = \"BridgePrompt_MS_TCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"ActionCLIPSegmentation\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",\n    image_prompt = dict(\n        name = \"CLIP\",\n        # pretrained = \"./data/checkpoint/ViT-B-16.pt\",\n        embed_dim = 512,",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "cnt_max",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "cnt_max = 30\nmodel_name = \"BridgePrompt_MS_TCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"ActionCLIPSegmentation\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",\n    image_prompt = dict(\n        name = \"CLIP\",\n        # pretrained = \"./data/checkpoint/ViT-B-16.pt\",\n        embed_dim = 512,\n        image_resolution = 224,",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "model_name = \"BridgePrompt_MS_TCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"ActionCLIPSegmentation\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",\n    image_prompt = dict(\n        name = \"CLIP\",\n        # pretrained = \"./data/checkpoint/ViT-B-16.pt\",\n        embed_dim = 512,\n        image_resolution = 224,\n        vision_layers = 12,",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "MODEL = dict(\n    architecture = \"ActionCLIPSegmentation\",\n    pretrained = \"./data/checkpoint/vit-16-32f.pt\",\n    image_prompt = dict(\n        name = \"CLIP\",\n        # pretrained = \"./data/checkpoint/ViT-B-16.pt\",\n        embed_dim = 512,\n        image_resolution = 224,\n        vision_layers = 12,\n        vision_width = 768,",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "POSTPRECESSING",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "POSTPRECESSING = dict(\n    name = \"ScorePostProcessing\",\n    num_classes = num_classes,\n    ignore_index = ignore_index\n)\nLRSCHEDULER = dict(\n    step_size = [epochs]\n)\nDATASET = dict(\n    temporal_clip_batch_size = batch_size,",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "LRSCHEDULER",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "LRSCHEDULER = dict(\n    step_size = [epochs]\n)\nDATASET = dict(\n    temporal_clip_batch_size = batch_size,\n    video_batch_size = batch_size,\n    num_workers = 2,\n    train = dict(\n        file_path = \"./data/gtea/splits/train.split\" + str(split) + \".bundle\"\n    ),",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = batch_size,\n    video_batch_size = batch_size,\n    num_workers = 2,\n    train = dict(\n        file_path = \"./data/gtea/splits/train.split\" + str(split) + \".bundle\"\n    ),\n    test = dict(\n        file_path = \"./data/gtea/splits/test.split\" + str(split) + \".bundle\"\n    )",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "PIPELINE",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "description": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "peekOfCode": "PIPELINE = dict(\n    train = dict(\n        name = \"BasePipline\",\n        decode = dict(\n            name = \"VideoDecoder\",\n            backend = \"decord\"\n        ),\n        sample = dict(\n            name = \"VideoSampler\",\n            is_train = True,",
        "detail": "SVTAS.config.tas.rgb.bridge_prompt_ms_tcn_gtea",
        "documentation": {}
    },
    {
        "label": "_base_",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "peekOfCode": "_base_ = [\n    '../../_base_/schedules/optimizer/adamw.py', '../../_base_/schedules/lr/liner_step_50e.py',\n    '../../_base_/default_runtime.py', '../../_base_/collater/batch_compose.py',\n    '../../_base_/dataset/50salads/50salads_video.py'\n]\nsplit = 1\nnum_classes = 19\nsample_rate = 1\nclip_seg_num = 128\nignore_index = -100",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "documentation": {}
    },
    {
        "label": "split",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "peekOfCode": "split = 1\nnum_classes = 19\nsample_rate = 1\nclip_seg_num = 128\nignore_index = -100\nbatch_size = 1\nepochs = 50\nmodel_name = \"MobileV2_TSM_MS_TCN_50salads_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "documentation": {}
    },
    {
        "label": "num_classes",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "peekOfCode": "num_classes = 19\nsample_rate = 1\nclip_seg_num = 128\nignore_index = -100\nbatch_size = 1\nepochs = 50\nmodel_name = \"MobileV2_TSM_MS_TCN_50salads_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "documentation": {}
    },
    {
        "label": "sample_rate",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "peekOfCode": "sample_rate = 1\nclip_seg_num = 128\nignore_index = -100\nbatch_size = 1\nepochs = 50\nmodel_name = \"MobileV2_TSM_MS_TCN_50salads_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"MobileNetV2TSM\",",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "documentation": {}
    },
    {
        "label": "clip_seg_num",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "peekOfCode": "clip_seg_num = 128\nignore_index = -100\nbatch_size = 1\nepochs = 50\nmodel_name = \"MobileV2_TSM_MS_TCN_50salads_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"MobileNetV2TSM\",\n        # pretrained = \"./data/checkpoint/tsm_mobilenetv2_dense_320p_1x1x8_100e_kinetics400_rgb_20210202-61135809.pth\",",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "documentation": {}
    },
    {
        "label": "ignore_index",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "peekOfCode": "ignore_index = -100\nbatch_size = 1\nepochs = 50\nmodel_name = \"MobileV2_TSM_MS_TCN_50salads_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"MobileNetV2TSM\",\n        # pretrained = \"./data/checkpoint/tsm_mobilenetv2_dense_320p_1x1x8_100e_kinetics400_rgb_20210202-61135809.pth\",\n        clip_seg_num = clip_seg_num,",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "peekOfCode": "batch_size = 1\nepochs = 50\nmodel_name = \"MobileV2_TSM_MS_TCN_50salads_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"MobileNetV2TSM\",\n        # pretrained = \"./data/checkpoint/tsm_mobilenetv2_dense_320p_1x1x8_100e_kinetics400_rgb_20210202-61135809.pth\",\n        clip_seg_num = clip_seg_num,\n        shift_div = 8,",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "documentation": {}
    },
    {
        "label": "epochs",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "peekOfCode": "epochs = 50\nmodel_name = \"MobileV2_TSM_MS_TCN_50salads_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"MobileNetV2TSM\",\n        # pretrained = \"./data/checkpoint/tsm_mobilenetv2_dense_320p_1x1x8_100e_kinetics400_rgb_20210202-61135809.pth\",\n        clip_seg_num = clip_seg_num,\n        shift_div = 8,\n        out_indices = (7, )",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "peekOfCode": "model_name = \"MobileV2_TSM_MS_TCN_50salads_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"MobileNetV2TSM\",\n        # pretrained = \"./data/checkpoint/tsm_mobilenetv2_dense_320p_1x1x8_100e_kinetics400_rgb_20210202-61135809.pth\",\n        clip_seg_num = clip_seg_num,\n        shift_div = 8,\n        out_indices = (7, )\n    ),",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "peekOfCode": "MODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"MobileNetV2TSM\",\n        # pretrained = \"./data/checkpoint/tsm_mobilenetv2_dense_320p_1x1x8_100e_kinetics400_rgb_20210202-61135809.pth\",\n        clip_seg_num = clip_seg_num,\n        shift_div = 8,\n        out_indices = (7, )\n    ),\n    neck = dict(",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "documentation": {}
    },
    {
        "label": "POSTPRECESSING",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "peekOfCode": "POSTPRECESSING = dict(\n    name = \"ScorePostProcessing\",\n    num_classes = num_classes,\n    ignore_index = ignore_index\n)\nDATASET = dict(\n    temporal_clip_batch_size = batch_size,\n    video_batch_size = batch_size,\n    num_workers = 2,\n    train = dict(",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = batch_size,\n    video_batch_size = batch_size,\n    num_workers = 2,\n    train = dict(\n        file_path = \"./data/50salads/splits/train.split\" + str(split) + \".bundle\"\n    ),\n    test = dict(\n        file_path = \"./data/50salads/splits/test.split\" + str(split) + \".bundle\"\n    )",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "documentation": {}
    },
    {
        "label": "LRSCHEDULER",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "peekOfCode": "LRSCHEDULER = dict(\n    step_size = [epochs]\n)\nPIPELINE = dict(\n    train = dict(\n        name = \"BasePipline\",\n        decode = dict(\n            name = \"VideoDecoder\",\n            backend = \"decord\"\n        ),",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "documentation": {}
    },
    {
        "label": "PIPELINE",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "peekOfCode": "PIPELINE = dict(\n    train = dict(\n        name = \"BasePipline\",\n        decode = dict(\n            name = \"VideoDecoder\",\n            backend = \"decord\"\n        ),\n        sample = dict(\n            name = \"VideoSampler\",\n            is_train = True,",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_50salads",
        "documentation": {}
    },
    {
        "label": "_base_",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "peekOfCode": "_base_ = [\n    '../../_base_/schedules/optimizer/adamw.py', '../../_base_/schedules/lr/liner_step_50e.py',\n    '../../_base_/default_runtime.py', '../../_base_/collater/batch_compose.py',\n    '../../_base_/dataset/breakfast/breakfast_video.py'\n]\nsplit = 1\nnum_classes = 48\nsample_rate = 1\nclip_seg_num = 128\nignore_index = -100",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "documentation": {}
    },
    {
        "label": "split",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "peekOfCode": "split = 1\nnum_classes = 48\nsample_rate = 1\nclip_seg_num = 128\nignore_index = -100\nbatch_size = 1\nepochs = 50\nlog_interval = 100\nmodel_name = \"MobileV2_TSM_3DTCN_breakfast_split\" + str(split)\nMODEL = dict(",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "documentation": {}
    },
    {
        "label": "num_classes",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "peekOfCode": "num_classes = 48\nsample_rate = 1\nclip_seg_num = 128\nignore_index = -100\nbatch_size = 1\nepochs = 50\nlog_interval = 100\nmodel_name = \"MobileV2_TSM_3DTCN_breakfast_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "documentation": {}
    },
    {
        "label": "sample_rate",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "peekOfCode": "sample_rate = 1\nclip_seg_num = 128\nignore_index = -100\nbatch_size = 1\nepochs = 50\nlog_interval = 100\nmodel_name = \"MobileV2_TSM_3DTCN_breakfast_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "documentation": {}
    },
    {
        "label": "clip_seg_num",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "peekOfCode": "clip_seg_num = 128\nignore_index = -100\nbatch_size = 1\nepochs = 50\nlog_interval = 100\nmodel_name = \"MobileV2_TSM_3DTCN_breakfast_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"MobileNetV2TSM\",",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "documentation": {}
    },
    {
        "label": "ignore_index",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "peekOfCode": "ignore_index = -100\nbatch_size = 1\nepochs = 50\nlog_interval = 100\nmodel_name = \"MobileV2_TSM_3DTCN_breakfast_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"MobileNetV2TSM\",\n        pretrained = \"./data/checkpoint/tsm_mobilenetv2_dense_320p_1x1x8_100e_kinetics400_rgb_20210202-61135809.pth\",",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "peekOfCode": "batch_size = 1\nepochs = 50\nlog_interval = 100\nmodel_name = \"MobileV2_TSM_3DTCN_breakfast_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"MobileNetV2TSM\",\n        pretrained = \"./data/checkpoint/tsm_mobilenetv2_dense_320p_1x1x8_100e_kinetics400_rgb_20210202-61135809.pth\",\n        clip_seg_num = clip_seg_num,",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "documentation": {}
    },
    {
        "label": "epochs",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "peekOfCode": "epochs = 50\nlog_interval = 100\nmodel_name = \"MobileV2_TSM_3DTCN_breakfast_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"MobileNetV2TSM\",\n        pretrained = \"./data/checkpoint/tsm_mobilenetv2_dense_320p_1x1x8_100e_kinetics400_rgb_20210202-61135809.pth\",\n        clip_seg_num = clip_seg_num,\n        shift_div = 8,",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "documentation": {}
    },
    {
        "label": "log_interval",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "peekOfCode": "log_interval = 100\nmodel_name = \"MobileV2_TSM_3DTCN_breakfast_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"MobileNetV2TSM\",\n        pretrained = \"./data/checkpoint/tsm_mobilenetv2_dense_320p_1x1x8_100e_kinetics400_rgb_20210202-61135809.pth\",\n        clip_seg_num = clip_seg_num,\n        shift_div = 8,\n        out_indices = (7, )",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "peekOfCode": "model_name = \"MobileV2_TSM_3DTCN_breakfast_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"MobileNetV2TSM\",\n        pretrained = \"./data/checkpoint/tsm_mobilenetv2_dense_320p_1x1x8_100e_kinetics400_rgb_20210202-61135809.pth\",\n        clip_seg_num = clip_seg_num,\n        shift_div = 8,\n        out_indices = (7, )\n    ),",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "peekOfCode": "MODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"MobileNetV2TSM\",\n        pretrained = \"./data/checkpoint/tsm_mobilenetv2_dense_320p_1x1x8_100e_kinetics400_rgb_20210202-61135809.pth\",\n        clip_seg_num = clip_seg_num,\n        shift_div = 8,\n        out_indices = (7, )\n    ),\n    neck = dict(",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "documentation": {}
    },
    {
        "label": "POSTPRECESSING",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "peekOfCode": "POSTPRECESSING = dict(\n    name = \"ScorePostProcessing\",\n    num_classes = num_classes,\n    ignore_index = ignore_index\n)\nLRSCHEDULER = dict(\n    step_size = [epochs]\n)\nDATASET = dict(\n    temporal_clip_batch_size = batch_size,",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "documentation": {}
    },
    {
        "label": "LRSCHEDULER",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "peekOfCode": "LRSCHEDULER = dict(\n    step_size = [epochs]\n)\nDATASET = dict(\n    temporal_clip_batch_size = batch_size,\n    video_batch_size = batch_size,\n    num_workers = 2,\n    train = dict(\n        file_path = \"./data/breakfast/splits/train.split\" + str(split) + \".bundle\"\n    ),",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = batch_size,\n    video_batch_size = batch_size,\n    num_workers = 2,\n    train = dict(\n        file_path = \"./data/breakfast/splits/train.split\" + str(split) + \".bundle\"\n    ),\n    test = dict(\n        file_path = \"./data/breakfast/splits/test.split\" + str(split) + \".bundle\"\n    )",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "documentation": {}
    },
    {
        "label": "PIPELINE",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "peekOfCode": "PIPELINE = dict(\n    train = dict(\n        name = \"BasePipline\",\n        decode = dict(\n            name = \"VideoDecoder\",\n            backend = \"opencv\"\n        ),\n        sample = dict(\n            name = \"VideoSampler\",\n            is_train = True,",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_breakfast",
        "documentation": {}
    },
    {
        "label": "_base_",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "peekOfCode": "_base_ = [\n    '../../_base_/schedules/optimizer/adamw.py', '../../_base_/schedules/lr/liner_step_50e.py',\n    '../../_base_/default_runtime.py', '../../_base_/collater/batch_compose.py',\n    '../../_base_/dataset/gtea/gtea_video.py'\n]\nsplit = 1\nnum_classes = 11\nsample_rate = 1\nclip_seg_num = 128\nignore_index = -100",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "documentation": {}
    },
    {
        "label": "split",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "peekOfCode": "split = 1\nnum_classes = 11\nsample_rate = 1\nclip_seg_num = 128\nignore_index = -100\nbatch_size = 1\nepochs = 50\nmodel_name = \"MobileV2_TSM_3DTCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "documentation": {}
    },
    {
        "label": "num_classes",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "peekOfCode": "num_classes = 11\nsample_rate = 1\nclip_seg_num = 128\nignore_index = -100\nbatch_size = 1\nepochs = 50\nmodel_name = \"MobileV2_TSM_3DTCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "documentation": {}
    },
    {
        "label": "sample_rate",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "peekOfCode": "sample_rate = 1\nclip_seg_num = 128\nignore_index = -100\nbatch_size = 1\nepochs = 50\nmodel_name = \"MobileV2_TSM_3DTCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"MobileNetV2TSM\",",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "documentation": {}
    },
    {
        "label": "clip_seg_num",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "peekOfCode": "clip_seg_num = 128\nignore_index = -100\nbatch_size = 1\nepochs = 50\nmodel_name = \"MobileV2_TSM_3DTCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"MobileNetV2TSM\",\n        pretrained = \"./data/checkpoint/tsm_mobilenetv2_dense_320p_1x1x8_100e_kinetics400_rgb_20210202-61135809.pth\",",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "documentation": {}
    },
    {
        "label": "ignore_index",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "peekOfCode": "ignore_index = -100\nbatch_size = 1\nepochs = 50\nmodel_name = \"MobileV2_TSM_3DTCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"MobileNetV2TSM\",\n        pretrained = \"./data/checkpoint/tsm_mobilenetv2_dense_320p_1x1x8_100e_kinetics400_rgb_20210202-61135809.pth\",\n        clip_seg_num = clip_seg_num,",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "peekOfCode": "batch_size = 1\nepochs = 50\nmodel_name = \"MobileV2_TSM_3DTCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"MobileNetV2TSM\",\n        pretrained = \"./data/checkpoint/tsm_mobilenetv2_dense_320p_1x1x8_100e_kinetics400_rgb_20210202-61135809.pth\",\n        clip_seg_num = clip_seg_num,\n        shift_div = 8,",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "documentation": {}
    },
    {
        "label": "epochs",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "peekOfCode": "epochs = 50\nmodel_name = \"MobileV2_TSM_3DTCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"MobileNetV2TSM\",\n        pretrained = \"./data/checkpoint/tsm_mobilenetv2_dense_320p_1x1x8_100e_kinetics400_rgb_20210202-61135809.pth\",\n        clip_seg_num = clip_seg_num,\n        shift_div = 8,\n        out_indices = (7, )",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "peekOfCode": "model_name = \"MobileV2_TSM_3DTCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"MobileNetV2TSM\",\n        pretrained = \"./data/checkpoint/tsm_mobilenetv2_dense_320p_1x1x8_100e_kinetics400_rgb_20210202-61135809.pth\",\n        clip_seg_num = clip_seg_num,\n        shift_div = 8,\n        out_indices = (7, )\n    ),",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "peekOfCode": "MODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"MobileNetV2TSM\",\n        pretrained = \"./data/checkpoint/tsm_mobilenetv2_dense_320p_1x1x8_100e_kinetics400_rgb_20210202-61135809.pth\",\n        clip_seg_num = clip_seg_num,\n        shift_div = 8,\n        out_indices = (7, )\n    ),\n    neck = dict(",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "documentation": {}
    },
    {
        "label": "POSTPRECESSING",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "peekOfCode": "POSTPRECESSING = dict(\n    name = \"ScorePostProcessing\",\n    num_classes = num_classes,\n    ignore_index = ignore_index\n)\nLRSCHEDULER = dict(\n    step_size = [epochs]\n)\nDATASET = dict(\n    temporal_clip_batch_size = batch_size,",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "documentation": {}
    },
    {
        "label": "LRSCHEDULER",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "peekOfCode": "LRSCHEDULER = dict(\n    step_size = [epochs]\n)\nDATASET = dict(\n    temporal_clip_batch_size = batch_size,\n    video_batch_size = batch_size,\n    num_workers = 2,\n    train = dict(\n        file_path = \"./data/gtea/splits/train.split\" + str(split) + \".bundle\"\n    ),",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = batch_size,\n    video_batch_size = batch_size,\n    num_workers = 2,\n    train = dict(\n        file_path = \"./data/gtea/splits/train.split\" + str(split) + \".bundle\"\n    ),\n    test = dict(\n        file_path = \"./data/gtea/splits/test.split\" + str(split) + \".bundle\"\n    )",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "documentation": {}
    },
    {
        "label": "PIPELINE",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "description": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "peekOfCode": "PIPELINE = dict(\n    train = dict(\n        name = \"BasePipline\",\n        decode = dict(\n            name = \"VideoDecoder\",\n            backend = \"decord\"\n        ),\n        sample = dict(\n            name = \"VideoSampler\",\n            is_train = True,",
        "detail": "SVTAS.config.tas.rgb.mobilev2_tsm_3dtcn_gtea",
        "documentation": {}
    },
    {
        "label": "_base_",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "description": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "peekOfCode": "_base_ = [\n    '../../_base_/schedules/adamw_50e.py',\n    '../../_base_/default_runtime.py', '../../_base_/collater/batch_compose.py',\n    '../../_base_/dataset/gtea/gtea_video.py'\n]\nsplit = 1\nnum_classes = 11\nsample_rate = 1\nclip_seg_num = 256\nignore_index = -100",
        "detail": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "documentation": {}
    },
    {
        "label": "split",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "description": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "peekOfCode": "split = 1\nnum_classes = 11\nsample_rate = 1\nclip_seg_num = 256\nignore_index = -100\nbatch_size = 2\nmodel_name = \"SLViT_3DTCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(",
        "detail": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "documentation": {}
    },
    {
        "label": "num_classes",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "description": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "peekOfCode": "num_classes = 11\nsample_rate = 1\nclip_seg_num = 256\nignore_index = -100\nbatch_size = 2\nmodel_name = \"SLViT_3DTCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"SLViT\",",
        "detail": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "documentation": {}
    },
    {
        "label": "sample_rate",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "description": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "peekOfCode": "sample_rate = 1\nclip_seg_num = 256\nignore_index = -100\nbatch_size = 2\nmodel_name = \"SLViT_3DTCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"SLViT\",\n        image_size = 224,",
        "detail": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "documentation": {}
    },
    {
        "label": "clip_seg_num",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "description": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "peekOfCode": "clip_seg_num = 256\nignore_index = -100\nbatch_size = 2\nmodel_name = \"SLViT_3DTCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"SLViT\",\n        image_size = 224,\n        patch_size = 32,",
        "detail": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "documentation": {}
    },
    {
        "label": "ignore_index",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "description": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "peekOfCode": "ignore_index = -100\nbatch_size = 2\nmodel_name = \"SLViT_3DTCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"SLViT\",\n        image_size = 224,\n        patch_size = 32,\n        depth = 4,",
        "detail": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "description": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "peekOfCode": "batch_size = 2\nmodel_name = \"SLViT_3DTCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"SLViT\",\n        image_size = 224,\n        patch_size = 32,\n        depth = 4,\n        heads = 12,",
        "detail": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "description": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "peekOfCode": "model_name = \"SLViT_3DTCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"SLViT\",\n        image_size = 224,\n        patch_size = 32,\n        depth = 4,\n        heads = 12,\n        mlp_dim = 1024,",
        "detail": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "description": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "peekOfCode": "MODEL = dict(\n    architecture = \"Segmentation2D\",\n    backbone = dict(\n        name = \"SLViT\",\n        image_size = 224,\n        patch_size = 32,\n        depth = 4,\n        heads = 12,\n        mlp_dim = 1024,\n        dropout = 0.5,",
        "detail": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "documentation": {}
    },
    {
        "label": "POSTPRECESSING",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "description": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "peekOfCode": "POSTPRECESSING = dict(\n    name = \"ScorePostProcessing\",\n    num_classes = num_classes,\n    ignore_index = ignore_index\n)\nDATASET = dict(\n    temporal_clip_batch_size = batch_size,\n    video_batch_size = batch_size,\n    num_workers = 2,\n    train = dict(",
        "detail": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "description": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = batch_size,\n    video_batch_size = batch_size,\n    num_workers = 2,\n    train = dict(\n        file_path = \"./data/gtea/splits/train.split\" + str(split) + \".bundle\"\n    ),\n    test = dict(\n        file_path = \"./data/gtea/splits/test.split\" + str(split) + \".bundle\"\n    )",
        "detail": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "documentation": {}
    },
    {
        "label": "PIPELINE",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "description": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "peekOfCode": "PIPELINE = dict(\n    train = dict(\n        name = \"BasePipline\",\n        decode = dict(\n            name = \"VideoDecoder\",\n            backend = \"decord\"\n        ),\n        sample = dict(\n            name = \"VideoSampler\",\n            is_train = True,",
        "detail": "SVTAS.config.tas.rgb.slvit_3dtcn_gtea",
        "documentation": {}
    },
    {
        "label": "_base_",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "peekOfCode": "_base_ = [\n    '../../_base_/schedules/optimizer/adamw.py', '../../_base_/schedules/lr/liner_step_50e.py',\n    '../../_base_/default_runtime.py', '../../_base_/collater/batch_compose.py',\n    '../../_base_/dataset/50salads/50salads_video.py'\n]\nsplit = 1\nnum_classes = 19\nsample_rate = 1\nclip_seg_num = 64\nignore_index = -100",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "documentation": {}
    },
    {
        "label": "split",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "peekOfCode": "split = 1\nnum_classes = 19\nsample_rate = 1\nclip_seg_num = 64\nignore_index = -100\nbatch_size = 1\nepochs = 50\nmodel_name = \"TimeSformer_MS_TCN_50salads_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation3D\",",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "documentation": {}
    },
    {
        "label": "num_classes",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "peekOfCode": "num_classes = 19\nsample_rate = 1\nclip_seg_num = 64\nignore_index = -100\nbatch_size = 1\nepochs = 50\nmodel_name = \"TimeSformer_MS_TCN_50salads_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation3D\",\n    backbone = dict(",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "documentation": {}
    },
    {
        "label": "sample_rate",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "peekOfCode": "sample_rate = 1\nclip_seg_num = 64\nignore_index = -100\nbatch_size = 1\nepochs = 50\nmodel_name = \"TimeSformer_MS_TCN_50salads_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation3D\",\n    backbone = dict(\n        name = \"TimeSformer\",",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "documentation": {}
    },
    {
        "label": "clip_seg_num",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "peekOfCode": "clip_seg_num = 64\nignore_index = -100\nbatch_size = 1\nepochs = 50\nmodel_name = \"TimeSformer_MS_TCN_50salads_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation3D\",\n    backbone = dict(\n        name = \"TimeSformer\",\n        pretrained = \"./data/checkpoint/timesformer_divST_8x32x1_15e_kinetics400_rgb-3f8e5d03.pth\",",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "documentation": {}
    },
    {
        "label": "ignore_index",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "peekOfCode": "ignore_index = -100\nbatch_size = 1\nepochs = 50\nmodel_name = \"TimeSformer_MS_TCN_50salads_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation3D\",\n    backbone = dict(\n        name = \"TimeSformer\",\n        pretrained = \"./data/checkpoint/timesformer_divST_8x32x1_15e_kinetics400_rgb-3f8e5d03.pth\",\n        num_frames = clip_seg_num,",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "peekOfCode": "batch_size = 1\nepochs = 50\nmodel_name = \"TimeSformer_MS_TCN_50salads_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation3D\",\n    backbone = dict(\n        name = \"TimeSformer\",\n        pretrained = \"./data/checkpoint/timesformer_divST_8x32x1_15e_kinetics400_rgb-3f8e5d03.pth\",\n        num_frames = clip_seg_num,\n        img_size = 224,",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "documentation": {}
    },
    {
        "label": "epochs",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "peekOfCode": "epochs = 50\nmodel_name = \"TimeSformer_MS_TCN_50salads_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation3D\",\n    backbone = dict(\n        name = \"TimeSformer\",\n        pretrained = \"./data/checkpoint/timesformer_divST_8x32x1_15e_kinetics400_rgb-3f8e5d03.pth\",\n        num_frames = clip_seg_num,\n        img_size = 224,\n        patch_size = 16,",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "peekOfCode": "model_name = \"TimeSformer_MS_TCN_50salads_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation3D\",\n    backbone = dict(\n        name = \"TimeSformer\",\n        pretrained = \"./data/checkpoint/timesformer_divST_8x32x1_15e_kinetics400_rgb-3f8e5d03.pth\",\n        num_frames = clip_seg_num,\n        img_size = 224,\n        patch_size = 16,\n        embed_dims = 768",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "peekOfCode": "MODEL = dict(\n    architecture = \"Segmentation3D\",\n    backbone = dict(\n        name = \"TimeSformer\",\n        pretrained = \"./data/checkpoint/timesformer_divST_8x32x1_15e_kinetics400_rgb-3f8e5d03.pth\",\n        num_frames = clip_seg_num,\n        img_size = 224,\n        patch_size = 16,\n        embed_dims = 768\n    ),",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "documentation": {}
    },
    {
        "label": "POSTPRECESSING",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "peekOfCode": "POSTPRECESSING = dict(\n    name = \"ScorePostProcessing\",\n    num_classes = num_classes,\n    ignore_index = ignore_index\n)\nDATASET = dict(\n    temporal_clip_batch_size = batch_size,\n    video_batch_size = batch_size,\n    num_workers = 2,\n    train = dict(",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = batch_size,\n    video_batch_size = batch_size,\n    num_workers = 2,\n    train = dict(\n        file_path = \"./data/50salads/splits/train.split\" + str(split) + \".bundle\"\n    ),\n    test = dict(\n        file_path = \"./data/50salads/splits/test.split\" + str(split) + \".bundle\"\n    )",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "documentation": {}
    },
    {
        "label": "LRSCHEDULER",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "peekOfCode": "LRSCHEDULER = dict(\n    step_size = [epochs]\n)\nPIPELINE = dict(\n    train = dict(\n        name = \"BasePipline\",\n        decode = dict(\n            name = \"VideoDecoder\",\n            backend = \"decord\"\n        ),",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "documentation": {}
    },
    {
        "label": "PIPELINE",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "peekOfCode": "PIPELINE = dict(\n    train = dict(\n        name = \"BasePipline\",\n        decode = dict(\n            name = \"VideoDecoder\",\n            backend = \"decord\"\n        ),\n        sample = dict(\n            name = \"VideoSampler\",\n            is_train = True,",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_50salads",
        "documentation": {}
    },
    {
        "label": "_base_",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "peekOfCode": "_base_ = [\n    '../../_base_/schedules/optimizer/adamw.py', '../../_base_/schedules/lr/liner_step_50e.py',\n    '../../_base_/default_runtime.py', '../../_base_/collater/batch_compose.py',\n    '../../_base_/dataset/breakfast/breakfast_video.py'\n]\nsplit = 1\nnum_classes = 48\nsample_rate = 1\nclip_seg_num = 64\nignore_index = -100",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "documentation": {}
    },
    {
        "label": "split",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "peekOfCode": "split = 1\nnum_classes = 48\nsample_rate = 1\nclip_seg_num = 64\nignore_index = -100\nbatch_size = 1\nepochs = 50\nlog_interval = 100\nmodel_name = \"TimeSformer_MS_TCN_breakfast_split\" + str(split)\nMODEL = dict(",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "documentation": {}
    },
    {
        "label": "num_classes",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "peekOfCode": "num_classes = 48\nsample_rate = 1\nclip_seg_num = 64\nignore_index = -100\nbatch_size = 1\nepochs = 50\nlog_interval = 100\nmodel_name = \"TimeSformer_MS_TCN_breakfast_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation3D\",",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "documentation": {}
    },
    {
        "label": "sample_rate",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "peekOfCode": "sample_rate = 1\nclip_seg_num = 64\nignore_index = -100\nbatch_size = 1\nepochs = 50\nlog_interval = 100\nmodel_name = \"TimeSformer_MS_TCN_breakfast_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation3D\",\n    backbone = dict(",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "documentation": {}
    },
    {
        "label": "clip_seg_num",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "peekOfCode": "clip_seg_num = 64\nignore_index = -100\nbatch_size = 1\nepochs = 50\nlog_interval = 100\nmodel_name = \"TimeSformer_MS_TCN_breakfast_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation3D\",\n    backbone = dict(\n        name = \"TimeSformer\",",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "documentation": {}
    },
    {
        "label": "ignore_index",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "peekOfCode": "ignore_index = -100\nbatch_size = 1\nepochs = 50\nlog_interval = 100\nmodel_name = \"TimeSformer_MS_TCN_breakfast_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation3D\",\n    backbone = dict(\n        name = \"TimeSformer\",\n        pretrained = \"./data/checkpoint/timesformer_divST_8x32x1_15e_kinetics400_rgb-3f8e5d03.pth\",",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "peekOfCode": "batch_size = 1\nepochs = 50\nlog_interval = 100\nmodel_name = \"TimeSformer_MS_TCN_breakfast_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation3D\",\n    backbone = dict(\n        name = \"TimeSformer\",\n        pretrained = \"./data/checkpoint/timesformer_divST_8x32x1_15e_kinetics400_rgb-3f8e5d03.pth\",\n        num_frames = clip_seg_num,",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "documentation": {}
    },
    {
        "label": "epochs",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "peekOfCode": "epochs = 50\nlog_interval = 100\nmodel_name = \"TimeSformer_MS_TCN_breakfast_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation3D\",\n    backbone = dict(\n        name = \"TimeSformer\",\n        pretrained = \"./data/checkpoint/timesformer_divST_8x32x1_15e_kinetics400_rgb-3f8e5d03.pth\",\n        num_frames = clip_seg_num,\n        img_size = 224,",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "documentation": {}
    },
    {
        "label": "log_interval",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "peekOfCode": "log_interval = 100\nmodel_name = \"TimeSformer_MS_TCN_breakfast_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation3D\",\n    backbone = dict(\n        name = \"TimeSformer\",\n        pretrained = \"./data/checkpoint/timesformer_divST_8x32x1_15e_kinetics400_rgb-3f8e5d03.pth\",\n        num_frames = clip_seg_num,\n        img_size = 224,\n        patch_size = 16,",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "peekOfCode": "model_name = \"TimeSformer_MS_TCN_breakfast_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation3D\",\n    backbone = dict(\n        name = \"TimeSformer\",\n        pretrained = \"./data/checkpoint/timesformer_divST_8x32x1_15e_kinetics400_rgb-3f8e5d03.pth\",\n        num_frames = clip_seg_num,\n        img_size = 224,\n        patch_size = 16,\n        embed_dims = 768",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "peekOfCode": "MODEL = dict(\n    architecture = \"Segmentation3D\",\n    backbone = dict(\n        name = \"TimeSformer\",\n        pretrained = \"./data/checkpoint/timesformer_divST_8x32x1_15e_kinetics400_rgb-3f8e5d03.pth\",\n        num_frames = clip_seg_num,\n        img_size = 224,\n        patch_size = 16,\n        embed_dims = 768\n    ),",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "documentation": {}
    },
    {
        "label": "POSTPRECESSING",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "peekOfCode": "POSTPRECESSING = dict(\n    name = \"ScorePostProcessing\",\n    num_classes = num_classes,\n    ignore_index = ignore_index\n)\nLRSCHEDULER = dict(\n    step_size = [epochs]\n)\nDATASET = dict(\n    temporal_clip_batch_size = batch_size,",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "documentation": {}
    },
    {
        "label": "LRSCHEDULER",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "peekOfCode": "LRSCHEDULER = dict(\n    step_size = [epochs]\n)\nDATASET = dict(\n    temporal_clip_batch_size = batch_size,\n    video_batch_size = batch_size,\n    num_workers = 2,\n    train = dict(\n        file_path = \"./data/breakfast/splits/train.split\" + str(split) + \".bundle\"\n    ),",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = batch_size,\n    video_batch_size = batch_size,\n    num_workers = 2,\n    train = dict(\n        file_path = \"./data/breakfast/splits/train.split\" + str(split) + \".bundle\"\n    ),\n    test = dict(\n        file_path = \"./data/breakfast/splits/test.split\" + str(split) + \".bundle\"\n    )",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "documentation": {}
    },
    {
        "label": "PIPELINE",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "peekOfCode": "PIPELINE = dict(\n    train = dict(\n        name = \"BasePipline\",\n        decode = dict(\n            name = \"VideoDecoder\",\n            backend = \"opencv\"\n        ),\n        sample = dict(\n            name = \"VideoSampler\",\n            is_train = True,",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_breakfast",
        "documentation": {}
    },
    {
        "label": "_base_",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "peekOfCode": "_base_ = [\n    '../../_base_/schedules/optimizer/adamw.py', '../../_base_/schedules/lr/liner_step_50e.py',\n    '../../_base_/default_runtime.py', '../../_base_/collater/batch_compose.py',\n    '../../_base_/dataset/gtea/gtea_video.py'\n]\nsplit = 1\nnum_classes = 11\nsample_rate = 1\nclip_seg_num = 64\nignore_index = -100",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "documentation": {}
    },
    {
        "label": "split",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "peekOfCode": "split = 1\nnum_classes = 11\nsample_rate = 1\nclip_seg_num = 64\nignore_index = -100\nbatch_size = 1\nepochs = 50\nmodel_name = \"TimeSformer_MS_TCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation3D\",",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "documentation": {}
    },
    {
        "label": "num_classes",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "peekOfCode": "num_classes = 11\nsample_rate = 1\nclip_seg_num = 64\nignore_index = -100\nbatch_size = 1\nepochs = 50\nmodel_name = \"TimeSformer_MS_TCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation3D\",\n    backbone = dict(",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "documentation": {}
    },
    {
        "label": "sample_rate",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "peekOfCode": "sample_rate = 1\nclip_seg_num = 64\nignore_index = -100\nbatch_size = 1\nepochs = 50\nmodel_name = \"TimeSformer_MS_TCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation3D\",\n    backbone = dict(\n        name = \"TimeSformer\",",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "documentation": {}
    },
    {
        "label": "clip_seg_num",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "peekOfCode": "clip_seg_num = 64\nignore_index = -100\nbatch_size = 1\nepochs = 50\nmodel_name = \"TimeSformer_MS_TCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation3D\",\n    backbone = dict(\n        name = \"TimeSformer\",\n        pretrained = \"./data/checkpoint/timesformer_divST_8x32x1_15e_kinetics400_rgb-3f8e5d03.pth\",",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "documentation": {}
    },
    {
        "label": "ignore_index",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "peekOfCode": "ignore_index = -100\nbatch_size = 1\nepochs = 50\nmodel_name = \"TimeSformer_MS_TCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation3D\",\n    backbone = dict(\n        name = \"TimeSformer\",\n        pretrained = \"./data/checkpoint/timesformer_divST_8x32x1_15e_kinetics400_rgb-3f8e5d03.pth\",\n        num_frames = clip_seg_num,",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "documentation": {}
    },
    {
        "label": "batch_size",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "peekOfCode": "batch_size = 1\nepochs = 50\nmodel_name = \"TimeSformer_MS_TCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation3D\",\n    backbone = dict(\n        name = \"TimeSformer\",\n        pretrained = \"./data/checkpoint/timesformer_divST_8x32x1_15e_kinetics400_rgb-3f8e5d03.pth\",\n        num_frames = clip_seg_num,\n        img_size = 224,",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "documentation": {}
    },
    {
        "label": "epochs",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "peekOfCode": "epochs = 50\nmodel_name = \"TimeSformer_MS_TCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation3D\",\n    backbone = dict(\n        name = \"TimeSformer\",\n        pretrained = \"./data/checkpoint/timesformer_divST_8x32x1_15e_kinetics400_rgb-3f8e5d03.pth\",\n        num_frames = clip_seg_num,\n        img_size = 224,\n        patch_size = 16,",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "peekOfCode": "model_name = \"TimeSformer_MS_TCN_gtea_split\" + str(split)\nMODEL = dict(\n    architecture = \"Segmentation3D\",\n    backbone = dict(\n        name = \"TimeSformer\",\n        pretrained = \"./data/checkpoint/timesformer_divST_8x32x1_15e_kinetics400_rgb-3f8e5d03.pth\",\n        num_frames = clip_seg_num,\n        img_size = 224,\n        patch_size = 16,\n        embed_dims = 768,",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "peekOfCode": "MODEL = dict(\n    architecture = \"Segmentation3D\",\n    backbone = dict(\n        name = \"TimeSformer\",\n        pretrained = \"./data/checkpoint/timesformer_divST_8x32x1_15e_kinetics400_rgb-3f8e5d03.pth\",\n        num_frames = clip_seg_num,\n        img_size = 224,\n        patch_size = 16,\n        embed_dims = 768,\n        dropout_ratio = 0.5,",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "documentation": {}
    },
    {
        "label": "POSTPRECESSING",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "peekOfCode": "POSTPRECESSING = dict(\n    name = \"ScorePostProcessing\",\n    num_classes = num_classes,\n    ignore_index = ignore_index\n)\nLRSCHEDULER = dict(\n    step_size = [epochs]\n)\nDATASET = dict(\n    temporal_clip_batch_size = batch_size,",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "documentation": {}
    },
    {
        "label": "LRSCHEDULER",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "peekOfCode": "LRSCHEDULER = dict(\n    step_size = [epochs]\n)\nDATASET = dict(\n    temporal_clip_batch_size = batch_size,\n    video_batch_size = batch_size,\n    num_workers = 2,\n    train = dict(\n        file_path = \"./data/gtea/splits/train.split\" + str(split) + \".bundle\"\n    ),",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = batch_size,\n    video_batch_size = batch_size,\n    num_workers = 2,\n    train = dict(\n        file_path = \"./data/gtea/splits/train.split\" + str(split) + \".bundle\"\n    ),\n    test = dict(\n        file_path = \"./data/gtea/splits/test.split\" + str(split) + \".bundle\"\n    )",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "documentation": {}
    },
    {
        "label": "PIPELINE",
        "kind": 5,
        "importPath": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "description": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "peekOfCode": "PIPELINE = dict(\n    train = dict(\n        name = \"BasePipline\",\n        decode = dict(\n            name = \"VideoDecoder\",\n            backend = \"decord\"\n        ),\n        sample = dict(\n            name = \"VideoSampler\",\n            is_train = True,",
        "detail": "SVTAS.config.tas.rgb.timesformer_mstcn_gtea",
        "documentation": {}
    },
    {
        "label": "COLLATE",
        "kind": 5,
        "importPath": "SVTAS.config._base_.collater.batch_compose",
        "description": "SVTAS.config._base_.collater.batch_compose",
        "peekOfCode": "COLLATE = dict(\n    name = \"BatchCompose\",\n    to_tensor_keys = [\"imgs\", \"feature\", \"labels\", \"masks\", \"precise_sliding_num\"],\n    compress_keys = [\"sliding_num\", \"current_sliding_cnt\"],\n    ignore_index = -100\n)",
        "detail": "SVTAS.config._base_.collater.batch_compose",
        "documentation": {}
    },
    {
        "label": "COLLATE",
        "kind": 5,
        "importPath": "SVTAS.config._base_.collater.stream_compose",
        "description": "SVTAS.config._base_.collater.stream_compose",
        "peekOfCode": "COLLATE = dict(\n    name = \"StreamBatchCompose\",\n    to_tensor_keys = [\"imgs\", \"labels\", \"masks\", \"precise_sliding_num\"]\n)",
        "detail": "SVTAS.config._base_.collater.stream_compose",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.50salads.50salads_feaeture",
        "description": "SVTAS.config._base_.dataset.50salads.50salads_feaeture",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = 1,\n    video_batch_size = 1,\n    num_workers = 2,\n    train = dict(\n        name = \"FeatureSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/50salads/splits/train.split1.bundle\",\n        feature_path = \"./data/50salads/features\",\n        gt_path = \"./data/50salads/groundTruth\",",
        "detail": "SVTAS.config._base_.dataset.50salads.50salads_feaeture",
        "documentation": {}
    },
    {
        "label": "METRIC",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.50salads.50salads_feaeture",
        "description": "SVTAS.config._base_.dataset.50salads.50salads_feaeture",
        "peekOfCode": "METRIC = dict(\n    name = \"TASegmentationMetric\",\n    overlap = [.1, .25, .5],\n    actions_map_file_path = \"./data/50salads/mapping.txt\",\n    file_output = False,\n    score_output = False\n)",
        "detail": "SVTAS.config._base_.dataset.50salads.50salads_feaeture",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.50salads.50salads_stream_video",
        "description": "SVTAS.config._base_.dataset.50salads.50salads_stream_video",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = 1,\n    num_workers = 2,\n    train = dict(\n        name = \"RawFrameStreamSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/50salads/splits/train.split1.bundle\",\n        videos_path = \"./data/50salads/Videos\",\n        gt_path = \"./data/50salads/groundTruth\",",
        "detail": "SVTAS.config._base_.dataset.50salads.50salads_stream_video",
        "documentation": {}
    },
    {
        "label": "METRIC",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.50salads.50salads_stream_video",
        "description": "SVTAS.config._base_.dataset.50salads.50salads_stream_video",
        "peekOfCode": "METRIC = dict(\n    name = \"TASegmentationMetric\",\n    overlap = [.1, .25, .5],\n    actions_map_file_path = \"./data/50salads/mapping.txt\",\n    file_output = False,\n    score_output = False\n)",
        "detail": "SVTAS.config._base_.dataset.50salads.50salads_stream_video",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.50salads.50salads_video",
        "description": "SVTAS.config._base_.dataset.50salads.50salads_video",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = 1,\n    num_workers = 2,\n    train = dict(\n        name = \"RawFrameSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/50salads/splits/train.split1.bundle\",\n        videos_path = \"./data/50salads/Videos\",\n        gt_path = \"./data/50salads/groundTruth\",",
        "detail": "SVTAS.config._base_.dataset.50salads.50salads_video",
        "documentation": {}
    },
    {
        "label": "METRIC",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.50salads.50salads_video",
        "description": "SVTAS.config._base_.dataset.50salads.50salads_video",
        "peekOfCode": "METRIC = dict(\n    name = \"TASegmentationMetric\",\n    overlap = [.1, .25, .5],\n    actions_map_file_path = \"./data/50salads/mapping.txt\",\n    file_output = False,\n    score_output = False\n)",
        "detail": "SVTAS.config._base_.dataset.50salads.50salads_video",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.breakfast.breakfast_feature",
        "description": "SVTAS.config._base_.dataset.breakfast.breakfast_feature",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = 1,\n    video_batch_size = 1,\n    num_workers = 2,\n    train = dict(\n        name = \"FeatureSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/breakfast/splits/train.split1.bundle\",\n        feature_path = \"./data/breakfast/features\",\n        gt_path = \"./data/breakfast/groundTruth\",",
        "detail": "SVTAS.config._base_.dataset.breakfast.breakfast_feature",
        "documentation": {}
    },
    {
        "label": "METRIC",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.breakfast.breakfast_feature",
        "description": "SVTAS.config._base_.dataset.breakfast.breakfast_feature",
        "peekOfCode": "METRIC = dict(\n    name = \"TASegmentationMetric\",\n    overlap = [.1, .25, .5],\n    actions_map_file_path = \"./data/breakfast/mapping.txt\",\n    file_output = False,\n    score_output = False\n)",
        "detail": "SVTAS.config._base_.dataset.breakfast.breakfast_feature",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.breakfast.breakfast_stream_video",
        "description": "SVTAS.config._base_.dataset.breakfast.breakfast_stream_video",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = 1,\n    num_workers = 2,\n    train = dict(\n        name = \"RawFrameStreamSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/breakfast/splits/train.split1.bundle\",\n        videos_path = \"./data/breakfast/Videos\",\n        gt_path = \"./data/breakfast/groundTruth\",",
        "detail": "SVTAS.config._base_.dataset.breakfast.breakfast_stream_video",
        "documentation": {}
    },
    {
        "label": "METRIC",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.breakfast.breakfast_stream_video",
        "description": "SVTAS.config._base_.dataset.breakfast.breakfast_stream_video",
        "peekOfCode": "METRIC = dict(\n    name = \"TASegmentationMetric\",\n    overlap = [.1, .25, .5],\n    actions_map_file_path = \"./data/breakfast/mapping.txt\",\n    file_output = False,\n    score_output = False\n)",
        "detail": "SVTAS.config._base_.dataset.breakfast.breakfast_stream_video",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.breakfast.breakfast_video",
        "description": "SVTAS.config._base_.dataset.breakfast.breakfast_video",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = 1,\n    num_workers = 2,\n    train = dict(\n        name = \"RawFrameSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/breakfast/splits/train.split1.bundle\",\n        videos_path = \"./data/breakfast/Videos\",\n        gt_path = \"./data/breakfast/groundTruth\",",
        "detail": "SVTAS.config._base_.dataset.breakfast.breakfast_video",
        "documentation": {}
    },
    {
        "label": "METRIC",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.breakfast.breakfast_video",
        "description": "SVTAS.config._base_.dataset.breakfast.breakfast_video",
        "peekOfCode": "METRIC = dict(\n    name = \"TASegmentationMetric\",\n    overlap = [.1, .25, .5],\n    actions_map_file_path = \"./data/breakfast/mapping.txt\",\n    file_output = False,\n    score_output = False\n)",
        "detail": "SVTAS.config._base_.dataset.breakfast.breakfast_video",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.egtea.egtea_feature",
        "description": "SVTAS.config._base_.dataset.egtea.egtea_feature",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = 1,\n    video_batch_size = 1,\n    num_workers = 2,\n    train = dict(\n        name = \"FeatureSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/egtea/splits/train.split1.bundle\",\n        feature_path = \"./data/egtea/features\",\n        gt_path = \"./data/egtea/groundTruth\",",
        "detail": "SVTAS.config._base_.dataset.egtea.egtea_feature",
        "documentation": {}
    },
    {
        "label": "METRIC",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.egtea.egtea_feature",
        "description": "SVTAS.config._base_.dataset.egtea.egtea_feature",
        "peekOfCode": "METRIC = dict(\n    name = \"TASegmentationMetric\",\n    overlap = [.1, .25, .5],\n    actions_map_file_path = \"./data/egtea/mapping.txt\",\n    file_output = False,\n    score_output = False\n)",
        "detail": "SVTAS.config._base_.dataset.egtea.egtea_feature",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.egtea.egtea_stream_video",
        "description": "SVTAS.config._base_.dataset.egtea.egtea_stream_video",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = 1,\n    num_workers = 2,\n    train = dict(\n        name = \"RawFrameStreamSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/egtea/splits/train_split1.txt\",\n        videos_path = \"./data/egtea/Videos\",\n        gt_path = \"./data/egtea/groundTruth\",",
        "detail": "SVTAS.config._base_.dataset.egtea.egtea_stream_video",
        "documentation": {}
    },
    {
        "label": "METRIC",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.egtea.egtea_stream_video",
        "description": "SVTAS.config._base_.dataset.egtea.egtea_stream_video",
        "peekOfCode": "METRIC = dict(\n    name = \"TASegmentationMetric\",\n    overlap = [.1, .25, .5],\n    actions_map_file_path = \"./data/egtea/mapping.txt\",\n    file_output = False,\n    score_output = False\n)",
        "detail": "SVTAS.config._base_.dataset.egtea.egtea_stream_video",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.egtea.egtea_video",
        "description": "SVTAS.config._base_.dataset.egtea.egtea_video",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = 1,\n    num_workers = 2,\n    train = dict(\n        name = \"RawFrameSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/egtea/splits/train_split1.txt\",\n        videos_path = \"./data/egtea/Videos\",\n        gt_path = \"./data/egtea/groundTruth\",",
        "detail": "SVTAS.config._base_.dataset.egtea.egtea_video",
        "documentation": {}
    },
    {
        "label": "METRIC",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.egtea.egtea_video",
        "description": "SVTAS.config._base_.dataset.egtea.egtea_video",
        "peekOfCode": "METRIC = dict(\n    name = \"TASegmentationMetric\",\n    overlap = [.1, .25, .5],\n    actions_map_file_path = \"./data/egtea/mapping.txt\",\n    file_output = False,\n    score_output = False\n)",
        "detail": "SVTAS.config._base_.dataset.egtea.egtea_video",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.gtea.gtea_feature",
        "description": "SVTAS.config._base_.dataset.gtea.gtea_feature",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = 1,\n    video_batch_size = 1,\n    num_workers = 2,\n    train = dict(\n        name = \"FeatureSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/gtea/splits/train.split1.bundle\",\n        feature_path = \"./data/gtea/features\",\n        gt_path = \"./data/gtea/groundTruth\",",
        "detail": "SVTAS.config._base_.dataset.gtea.gtea_feature",
        "documentation": {}
    },
    {
        "label": "METRIC",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.gtea.gtea_feature",
        "description": "SVTAS.config._base_.dataset.gtea.gtea_feature",
        "peekOfCode": "METRIC = dict(\n    name = \"TASegmentationMetric\",\n    overlap = [.1, .25, .5],\n    actions_map_file_path = \"./data/gtea/mapping.txt\",\n    file_output = False,\n    score_output = False\n)",
        "detail": "SVTAS.config._base_.dataset.gtea.gtea_feature",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.gtea.gtea_stream_video",
        "description": "SVTAS.config._base_.dataset.gtea.gtea_stream_video",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = 1,\n    num_workers = 2,\n    train = dict(\n        name = \"RawFrameStreamSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/gtea/splits/train.split1.bundle\",\n        videos_path = \"./data/gtea/Videos\",\n        gt_path = \"./data/gtea/groundTruth\",",
        "detail": "SVTAS.config._base_.dataset.gtea.gtea_stream_video",
        "documentation": {}
    },
    {
        "label": "METRIC",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.gtea.gtea_stream_video",
        "description": "SVTAS.config._base_.dataset.gtea.gtea_stream_video",
        "peekOfCode": "METRIC = dict(\n    name = \"TASegmentationMetric\",\n    overlap = [.1, .25, .5],\n    actions_map_file_path = \"./data/gtea/mapping.txt\",\n    file_output = False,\n    score_output = False\n)",
        "detail": "SVTAS.config._base_.dataset.gtea.gtea_stream_video",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.gtea.gtea_video",
        "description": "SVTAS.config._base_.dataset.gtea.gtea_video",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = 1,\n    num_workers = 2,\n    train = dict(\n        name = \"RawFrameSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/gtea/splits/train.split1.bundle\",\n        videos_path = \"./data/gtea/Videos\",\n        gt_path = \"./data/gtea/groundTruth\",",
        "detail": "SVTAS.config._base_.dataset.gtea.gtea_video",
        "documentation": {}
    },
    {
        "label": "METRIC",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.gtea.gtea_video",
        "description": "SVTAS.config._base_.dataset.gtea.gtea_video",
        "peekOfCode": "METRIC = dict(\n    name = \"TASegmentationMetric\",\n    overlap = [.1, .25, .5],\n    actions_map_file_path = \"./data/gtea/mapping.txt\",\n    file_output = False,\n    score_output = False\n)",
        "detail": "SVTAS.config._base_.dataset.gtea.gtea_video",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.thumos14.thumos14_feature",
        "description": "SVTAS.config._base_.dataset.thumos14.thumos14_feature",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = 1,\n    video_batch_size = 1,\n    num_workers = 2,\n    train = dict(\n        name = \"FeatureSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/thumos14/val_list.txt\",\n        feature_path = \"./data/thumos14/features\",\n        gt_path = \"./data/thumos14/groundTruth\",",
        "detail": "SVTAS.config._base_.dataset.thumos14.thumos14_feature",
        "documentation": {}
    },
    {
        "label": "METRIC",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.thumos14.thumos14_feature",
        "description": "SVTAS.config._base_.dataset.thumos14.thumos14_feature",
        "peekOfCode": "METRIC = dict(\n    name = \"TASegmentationMetric\",\n    overlap = [.1, .25, .5],\n    actions_map_file_path = \"./data/thumos14/mapping.txt\",\n    file_output = False,\n    score_output = False\n)",
        "detail": "SVTAS.config._base_.dataset.thumos14.thumos14_feature",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.thumos14.thumos14_stream_video",
        "description": "SVTAS.config._base_.dataset.thumos14.thumos14_stream_video",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = 1,\n    num_workers = 2,\n    train = dict(\n        name = \"RawFrameStreamSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/thumos14/val_list.txt\",\n        videos_path = \"./data/thumos14/Videos\",\n        gt_path = \"./data/thumos14/groundTruth\",",
        "detail": "SVTAS.config._base_.dataset.thumos14.thumos14_stream_video",
        "documentation": {}
    },
    {
        "label": "METRIC",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.thumos14.thumos14_stream_video",
        "description": "SVTAS.config._base_.dataset.thumos14.thumos14_stream_video",
        "peekOfCode": "METRIC = dict(\n    name = \"TASegmentationMetric\",\n    overlap = [.1, .25, .5],\n    actions_map_file_path = \"./data/thumos14/mapping.txt\",\n    file_output = False,\n    score_output = False\n)",
        "detail": "SVTAS.config._base_.dataset.thumos14.thumos14_stream_video",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.thumos14.thumos14_video",
        "description": "SVTAS.config._base_.dataset.thumos14.thumos14_video",
        "peekOfCode": "DATASET = dict(\n    temporal_clip_batch_size = 3,\n    video_batch_size = 1,\n    num_workers = 2,\n    train = dict(\n        name = \"RawFrameSegmentationDataset\",\n        data_prefix = \"./\",\n        file_path = \"./data/thumos14/val_list.txt\",\n        videos_path = \"./data/thumos14/Videos\",\n        gt_path = \"./data/thumos14/groundTruth\",",
        "detail": "SVTAS.config._base_.dataset.thumos14.thumos14_video",
        "documentation": {}
    },
    {
        "label": "METRIC",
        "kind": 5,
        "importPath": "SVTAS.config._base_.dataset.thumos14.thumos14_video",
        "description": "SVTAS.config._base_.dataset.thumos14.thumos14_video",
        "peekOfCode": "METRIC = dict(\n    name = \"TASegmentationMetric\",\n    overlap = [.1, .25, .5],\n    actions_map_file_path = \"./data/thumos14/mapping.txt\",\n    file_output = False,\n    score_output = False\n)",
        "detail": "SVTAS.config._base_.dataset.thumos14.thumos14_video",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config._base_.models.action_recognition.action_clip",
        "description": "SVTAS.config._base_.models.action_recognition.action_clip",
        "peekOfCode": "MODEL = dict(\n    architecture = \"ActionCLIP\",\n    pretrained = \"./data/vit-16-32f.pt\",\n    is_feature_extract = True,\n    image_prompt = dict(\n        name = \"CLIP\",\n        # pretrained = \"./data/ViT-B-16.pt\",\n        embed_dim = 512,\n        image_resolution = 224,\n        vision_layers = 12,",
        "detail": "SVTAS.config._base_.models.action_recognition.action_clip",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config._base_.models.action_recognition.i3d",
        "description": "SVTAS.config._base_.models.action_recognition.i3d",
        "peekOfCode": "MODEL = dict(\n    architecture = \"Recognition3D\",\n    backbone = dict(\n        name = \"I3D\",\n        pretrained = \"./data/i3d_rgb.pt\",\n        in_channels = 3\n    ),\n    neck = None,\n    head = dict(\n        name = \"FeatureExtractHead\",",
        "detail": "SVTAS.config._base_.models.action_recognition.i3d",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config._base_.models.action_recognition.mobilev2_tsm",
        "description": "SVTAS.config._base_.models.action_recognition.mobilev2_tsm",
        "peekOfCode": "MODEL = dict(\n    architecture = \"Recognition2D\",\n    backbone = dict(\n        name = \"MobileNetV2TSM\",\n        pretrained = \"./data/tsm_mobilenetv2_dense_320p_1x1x8_100e_kinetics400_rgb_20210202-61135809.pth\",\n        clip_seg_num = 8,\n        shift_div = 8,\n        out_indices = (7, )\n    ),\n    neck = None,",
        "detail": "SVTAS.config._base_.models.action_recognition.mobilev2_tsm",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config._base_.models.action_recognition.mvitv2_b",
        "description": "SVTAS.config._base_.models.action_recognition.mvitv2_b",
        "peekOfCode": "MODEL = dict(\n    architecture = \"Recognition3D\",\n    backbone = dict(\n        name = \"MViT\"\n    ),\n    neck = None,\n    head = dict(\n        name = \"FeatureExtractHead\",\n        in_channels = 1024,\n        input_seg_num = 8,",
        "detail": "SVTAS.config._base_.models.action_recognition.mvitv2_b",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config._base_.models.action_recognition.swin_transformer",
        "description": "SVTAS.config._base_.models.action_recognition.swin_transformer",
        "peekOfCode": "MODEL = dict(\n    architecture = \"Recognition3D\",\n    backbone = dict(\n        name = \"SwinTransformer3D\",\n        pretrained = \"./data/swin_tiny_patch244_window877_kinetics400_1k.pth\",\n        pretrained2d = False,\n        patch_size = [2, 4, 4],\n        embed_dim = 96,\n        depths = [2, 2, 6, 2],\n        num_heads = [3, 6, 12, 24],",
        "detail": "SVTAS.config._base_.models.action_recognition.swin_transformer",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config._base_.models.action_recognition.timesformer",
        "description": "SVTAS.config._base_.models.action_recognition.timesformer",
        "peekOfCode": "MODEL = dict(\n    architecture = \"Recognition3D\",\n    backbone = dict(\n        name = \"TimeSformer\",\n        pretrained = \"./data/timesformer_divST_8x32x1_15e_kinetics400_rgb-3f8e5d03.pth\",\n        num_frames = 8,\n        img_size = 224,\n        patch_size = 16,\n        embed_dims = 768\n    ),",
        "detail": "SVTAS.config._base_.models.action_recognition.timesformer",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config._base_.models.image_classification.slvit",
        "description": "SVTAS.config._base_.models.image_classification.slvit",
        "peekOfCode": "MODEL = dict(\n    architecture = \"Recognition2D\",\n    backbone = dict(\n        name = \"SLViT\",\n        image_size = 224,\n        patch_size = 32,\n        depth = 4,\n        heads = 12,\n        mlp_dim = 1024,\n        dropout = 0.3,",
        "detail": "SVTAS.config._base_.models.image_classification.slvit",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config._base_.models.image_classification.vit",
        "description": "SVTAS.config._base_.models.image_classification.vit",
        "peekOfCode": "MODEL = dict(\n    architecture = \"Recognition2D\",\n    backbone = dict(\n        name = \"ViT\",\n        image_size = 224,\n        patch_size = 32,\n        depth = 4,\n        heads = 12,\n        mlp_dim = 1024,\n        dropout = 0.3,",
        "detail": "SVTAS.config._base_.models.image_classification.vit",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config._base_.models.optical_flow_estimate.raft",
        "description": "SVTAS.config._base_.models.optical_flow_estimate.raft",
        "peekOfCode": "MODEL = dict(\n    architecture = \"OpticalFlowEstimation\",\n    model = dict(\n        name = \"RAFT\",\n        pretrained = \"./data/raft-sintel.pth\",\n        extract_mode = True,\n        freeze = True,\n        mode = \"sintel\"\n    )\n)",
        "detail": "SVTAS.config._base_.models.optical_flow_estimate.raft",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config._base_.models.temporal_action_segmentation.asformer",
        "description": "SVTAS.config._base_.models.temporal_action_segmentation.asformer",
        "peekOfCode": "MODEL = dict(\n    architecture = \"FeatureSegmentation\",\n    backbone = None,\n    neck = None,\n    head = dict(\n        name = \"ASFormer\",\n        num_decoders = 2,\n        num_layers = 4,\n        r1 = 2,\n        r2 = 2,",
        "detail": "SVTAS.config._base_.models.temporal_action_segmentation.asformer",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config._base_.models.temporal_action_segmentation.linformer",
        "description": "SVTAS.config._base_.models.temporal_action_segmentation.linformer",
        "peekOfCode": "MODEL = dict(\n    architecture = \"FeatureSegmentation\",\n    backbone = None,\n    neck = None,\n    head = dict(\n        name =\"LinformerHead\",\n        num_decoders = 3,\n        num_layers = 10,\n        num_f_maps = 64,\n        input_dim = 2048,",
        "detail": "SVTAS.config._base_.models.temporal_action_segmentation.linformer",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config._base_.models.temporal_action_segmentation.ms_tcn",
        "description": "SVTAS.config._base_.models.temporal_action_segmentation.ms_tcn",
        "peekOfCode": "MODEL = dict(\n    architecture = \"FeatureSegmentation\",\n    backbone = None,\n    neck = None,\n    head = dict(\n        name = \"MultiStageModel\",\n        num_stages = 4,\n        num_layers = 10,\n        num_f_maps = 64,\n        dim = 512,",
        "detail": "SVTAS.config._base_.models.temporal_action_segmentation.ms_tcn",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config._base_.models.temporal_action_segmentation.transeger",
        "description": "SVTAS.config._base_.models.temporal_action_segmentation.transeger",
        "peekOfCode": "MODEL = dict(\n    architecture = \"Transeger\",\n    image_backbone = dict(\n        architecture = \"Recognition2D\",\n        backbone = dict(\n            name = \"MobileNetV2TSM\",\n            pretrained = \"./data/tsm_mobilenetv2_dense_320p_1x1x8_100e_kinetics400_rgb_20210202-61135809.pth\",\n            clip_seg_num = 32,\n            shift_div = 8,\n            out_indices = (7, )",
        "detail": "SVTAS.config._base_.models.temporal_action_segmentation.transeger",
        "documentation": {}
    },
    {
        "label": "MODEL",
        "kind": 5,
        "importPath": "SVTAS.config._base_.models.temporal_action_segmentation.vit_3d_3dtcn",
        "description": "SVTAS.config._base_.models.temporal_action_segmentation.vit_3d_3dtcn",
        "peekOfCode": "MODEL = dict(\n    architecture = \"Recognition2D\",\n    backbone = dict(\n        name = \"ViT\",\n        image_size = 224,\n        patch_size = 16,\n        depth = 12,\n        heads = 12,\n        mlp_dim = 3072,\n        dropout = 0.1,",
        "detail": "SVTAS.config._base_.models.temporal_action_segmentation.vit_3d_3dtcn",
        "documentation": {}
    },
    {
        "label": "PIPELINE",
        "kind": 5,
        "importPath": "SVTAS.config._base_.pipline.feature_pipline",
        "description": "SVTAS.config._base_.pipline.feature_pipline",
        "peekOfCode": "PIPELINE = dict(\n    train = dict(\n        name = \"BasePipline\",\n        decode = dict(\n            name = \"FeatureDecoder\",\n            backend = \"numpy\"\n        ),\n        sample = dict(\n            name = \"FeatureSampler\",\n            is_train = True,",
        "detail": "SVTAS.config._base_.pipline.feature_pipline",
        "documentation": {}
    },
    {
        "label": "PIPELINE",
        "kind": 5,
        "importPath": "SVTAS.config._base_.pipline.video_pipline",
        "description": "SVTAS.config._base_.pipline.video_pipline",
        "peekOfCode": "PIPELINE = dict(\n    train = dict(\n        name = \"BasePipline\",\n        decode = dict(\n            name = \"VideoDecoder\",\n            backend = \"decord\"\n        ),\n        sample = dict(\n            name = \"VideoStreamSampler\",\n            is_train = False,",
        "detail": "SVTAS.config._base_.pipline.video_pipline",
        "documentation": {}
    },
    {
        "label": "LRSCHEDULER",
        "kind": 5,
        "importPath": "SVTAS.config._base_.schedules.lr.cosine_50e",
        "description": "SVTAS.config._base_.schedules.lr.cosine_50e",
        "peekOfCode": "LRSCHEDULER = dict(\n    name = \"CosineAnnealingLR\",\n    T_max = [50]\n)",
        "detail": "SVTAS.config._base_.schedules.lr.cosine_50e",
        "documentation": {}
    },
    {
        "label": "LRSCHEDULER",
        "kind": 5,
        "importPath": "SVTAS.config._base_.schedules.lr.cosine_warmup_50e",
        "description": "SVTAS.config._base_.schedules.lr.cosine_warmup_50e",
        "peekOfCode": "LRSCHEDULER = dict(\n    name = \"CosineAnnealingWarmupRestarts\",\n    first_cycle_steps=50,\n    cycle_mult=1.0,\n    max_lr=0.1,\n    min_lr=0.001,\n    warmup_steps=10,\n    gamma=1.0\n)",
        "detail": "SVTAS.config._base_.schedules.lr.cosine_warmup_50e",
        "documentation": {}
    },
    {
        "label": "LRSCHEDULER",
        "kind": 5,
        "importPath": "SVTAS.config._base_.schedules.lr.liner_step_50e",
        "description": "SVTAS.config._base_.schedules.lr.liner_step_50e",
        "peekOfCode": "LRSCHEDULER = dict(\n    name = \"MultiStepLR\",\n    step_size = [50],\n    gamma = 0.1\n)",
        "detail": "SVTAS.config._base_.schedules.lr.liner_step_50e",
        "documentation": {}
    },
    {
        "label": "OPTIMIZER",
        "kind": 5,
        "importPath": "SVTAS.config._base_.schedules.optimizer.adam",
        "description": "SVTAS.config._base_.schedules.optimizer.adam",
        "peekOfCode": "OPTIMIZER = dict(\n    name = \"AdamOptimizer\",\n    learning_rate = 0.0005,\n    weight_decay = 1e-4,\n    betas = (0.9, 0.999)\n)",
        "detail": "SVTAS.config._base_.schedules.optimizer.adam",
        "documentation": {}
    },
    {
        "label": "OPTIMIZER",
        "kind": 5,
        "importPath": "SVTAS.config._base_.schedules.optimizer.adamw",
        "description": "SVTAS.config._base_.schedules.optimizer.adamw",
        "peekOfCode": "OPTIMIZER = dict(\n    name = \"AdamWOptimizer\",\n    learning_rate = 0.0005,\n    weight_decay = 1e-4,\n    betas = (0.9, 0.999)\n)",
        "detail": "SVTAS.config._base_.schedules.optimizer.adamw",
        "documentation": {}
    },
    {
        "label": "OPTIMIZER",
        "kind": 5,
        "importPath": "SVTAS.config._base_.schedules.optimizer.adan",
        "description": "SVTAS.config._base_.schedules.optimizer.adan",
        "peekOfCode": "OPTIMIZER = dict(\n    name = \"AdanOptimizer\",\n    learning_rate = 1e-3,\n    weight_decay = 0.02,\n    betas = (0.98, 0.92, 0.99)\n)",
        "detail": "SVTAS.config._base_.schedules.optimizer.adan",
        "documentation": {}
    },
    {
        "label": "OPTIMIZER",
        "kind": 5,
        "importPath": "SVTAS.config._base_.schedules.optimizer.sgd",
        "description": "SVTAS.config._base_.schedules.optimizer.sgd",
        "peekOfCode": "OPTIMIZER = dict(\n    name = \"SGDOptimizer\",\n    learning_rate = 1e-3,\n    weight_decay = 0.02\n)",
        "detail": "SVTAS.config._base_.schedules.optimizer.sgd",
        "documentation": {}
    },
    {
        "label": "log_interval",
        "kind": 5,
        "importPath": "SVTAS.config._base_.default_runtime",
        "description": "SVTAS.config._base_.default_runtime",
        "peekOfCode": "log_interval = 4 #Optional, the interal of logger, default:10\nsave_interval = 50",
        "detail": "SVTAS.config._base_.default_runtime",
        "documentation": {}
    },
    {
        "label": "save_interval",
        "kind": 5,
        "importPath": "SVTAS.config._base_.default_runtime",
        "description": "SVTAS.config._base_.default_runtime",
        "peekOfCode": "save_interval = 50",
        "detail": "SVTAS.config._base_.default_runtime",
        "documentation": {}
    },
    {
        "label": "load_capture",
        "kind": 2,
        "importPath": "SVTAS.demo.infer",
        "description": "SVTAS.demo.infer",
        "peekOfCode": "def load_capture(args):\n    capture = cv2.VideoCapture(args.input)\n    return capture\ndef make_palette(num_classes):\n    \"\"\"\n    Maps classes to colors in the style of PASCAL VOC.\n    Close values are mapped to far colors for segmentation visualization.\n    See http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#devkit\n    Takes:\n        num_classes: the number of classes",
        "detail": "SVTAS.demo.infer",
        "documentation": {}
    },
    {
        "label": "make_palette",
        "kind": 2,
        "importPath": "SVTAS.demo.infer",
        "description": "SVTAS.demo.infer",
        "peekOfCode": "def make_palette(num_classes):\n    \"\"\"\n    Maps classes to colors in the style of PASCAL VOC.\n    Close values are mapped to far colors for segmentation visualization.\n    See http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#devkit\n    Takes:\n        num_classes: the number of classes\n    Gives:\n        palette: the colormap as a k x 3 array of RGB colors\n    \"\"\"",
        "detail": "SVTAS.demo.infer",
        "documentation": {}
    },
    {
        "label": "draw_action_label",
        "kind": 2,
        "importPath": "SVTAS.demo.infer",
        "description": "SVTAS.demo.infer",
        "peekOfCode": "def draw_action_label(img, palette, action_dict, label):\n    fix_buffer = 12\n    for i in range(len(label)):\n        k = label[i]\n        img = cv2.rectangle(img, (5, 15 + fix_buffer * i), (25, 5 + fix_buffer * i), (int(palette[k][2]), int(palette[k][1]), int(palette[k][0])), thickness=-1)\n        cv2.putText(img, action_dict[k], (30, 12 + fix_buffer * i), cv2.FONT_HERSHEY_COMPLEX, 0.25, (int(palette[k][2]), int(palette[k][1]), int(palette[k][0])), 1)\n    return img\ndef label_arr2img(label_queue, palette):\n    data = list(copy.deepcopy(label_queue.queue))\n    array = np.array(data).transpose()",
        "detail": "SVTAS.demo.infer",
        "documentation": {}
    },
    {
        "label": "label_arr2img",
        "kind": 2,
        "importPath": "SVTAS.demo.infer",
        "description": "SVTAS.demo.infer",
        "peekOfCode": "def label_arr2img(label_queue, palette):\n    data = list(copy.deepcopy(label_queue.queue))\n    array = np.array(data).transpose()\n    arr = array.astype(np.uint8)\n    arr = np.tile(arr, (20, 1))\n    img = Image.fromarray(arr)\n    img = img.convert(\"P\")\n    img.putpalette(palette)\n    return img\ndef parse_args():",
        "detail": "SVTAS.demo.infer",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "SVTAS.demo.infer",
        "description": "SVTAS.demo.infer",
        "peekOfCode": "def parse_args():\n    parser = argparse.ArgumentParser(\"SVTAS infer script\")\n    parser.add_argument('-m',\n                        '--model',\n                        type=str,\n                        default='model.onnx',\n                        help='onnx model path')\n    parser.add_argument('-i',\n                        '--input',\n                        type=str,",
        "detail": "SVTAS.demo.infer",
        "documentation": {}
    },
    {
        "label": "infer",
        "kind": 2,
        "importPath": "SVTAS.demo.infer",
        "description": "SVTAS.demo.infer",
        "peekOfCode": "def infer():\n    args = parse_args()\n    #! set mean and std\n    memory_factor = 3\n    mean = [[[0.551, 0.424, 0.179]]]\n    std = [[[0.133, 0.141, 0.124]]]\n    mean = np.array(mean)[:,:,::-1].transpose((2,0,1))\n    std = np.array(std)[:,:,::-1].transpose((2,0,1))\n    # load model\n    ort_session = onnxruntime.InferenceSession(args.model)",
        "detail": "SVTAS.demo.infer",
        "documentation": {}
    },
    {
        "label": "FeatureSegmentationDataset",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.dataset.item_base_dataset.feature_segmentation_dataset",
        "description": "SVTAS.svtas.loader.dataset.item_base_dataset.feature_segmentation_dataset",
        "peekOfCode": "class FeatureSegmentationDataset(ItemDataset):\n    def __init__(self,\n                 feature_path,\n                 flow_feature_path=None,\n                 **kwargs,\n                 ) -> None:\n        self.flow_feature_path = flow_feature_path\n        self.feature_path = feature_path\n        super().__init__(**kwargs)\n    def load_file(self):",
        "detail": "SVTAS.svtas.loader.dataset.item_base_dataset.feature_segmentation_dataset",
        "documentation": {}
    },
    {
        "label": "ItemDataset",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.dataset.item_base_dataset.item_base_dataset",
        "description": "SVTAS.svtas.loader.dataset.item_base_dataset.item_base_dataset",
        "peekOfCode": "class ItemDataset(data.Dataset):\n    \"\"\"\n    ItemDataset For Temporal Video Segmentation\n    Other TVS ItemDataset should inherite it.\n    \"\"\"\n    def __init__(self,\n                 file_path,\n                 gt_path,\n                 pipeline,\n                 actions_map_file_path,",
        "detail": "SVTAS.svtas.loader.dataset.item_base_dataset.item_base_dataset",
        "documentation": {}
    },
    {
        "label": "RawFrameSegmentationDataset",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.dataset.item_base_dataset.raw_frame_segmentation_dataset",
        "description": "SVTAS.svtas.loader.dataset.item_base_dataset.raw_frame_segmentation_dataset",
        "peekOfCode": "class RawFrameSegmentationDataset(ItemDataset):\n    \"\"\"Video dataset for action recognition\n       The dataset loads raw videos and apply specified transforms on them.\n       The index file is a file with multiple lines, and each line indicates\n       a sample video with the filepath and label, which are split with a whitesapce.\n       Example of a inde file:\n        file tree:\n        ─── gtea\n            ├── Videos\n            │   ├── S1_Cheese_C1.mp4",
        "detail": "SVTAS.svtas.loader.dataset.item_base_dataset.raw_frame_segmentation_dataset",
        "documentation": {}
    },
    {
        "label": "FeatureStreamSegmentationDataset",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.dataset.stream_base_dataset.feature_stream_segmentation_dataset",
        "description": "SVTAS.svtas.loader.dataset.stream_base_dataset.feature_stream_segmentation_dataset",
        "peekOfCode": "class FeatureStreamSegmentationDataset(StreamDataset):\n    def __init__(self,\n                 feature_path,\n                 sliding_window=60,\n                 flow_feature_path=None,\n                 **kwargs):\n        self.flow_feature_path = flow_feature_path\n        self.feature_path = feature_path\n        self.sliding_window = sliding_window\n        super().__init__(**kwargs)",
        "detail": "SVTAS.svtas.loader.dataset.stream_base_dataset.feature_stream_segmentation_dataset",
        "documentation": {}
    },
    {
        "label": "FeatureVideoPredictionDataset",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.dataset.stream_base_dataset.feature_video_prediction_dataset",
        "description": "SVTAS.svtas.loader.dataset.stream_base_dataset.feature_video_prediction_dataset",
        "peekOfCode": "class FeatureVideoPredictionDataset(FeatureStreamSegmentationDataset):\n    def __init__(self,\n                 **kwargs):\n        super().__init__(**kwargs)\n    def _get_one_videos_clip(self, idx, info):\n        feature_list = []\n        labels_list = []\n        pred_labels_list = []\n        masks_list = []\n        vid_list = []",
        "detail": "SVTAS.svtas.loader.dataset.stream_base_dataset.feature_video_prediction_dataset",
        "documentation": {}
    },
    {
        "label": "RawFrameStreamSegmentationDataset",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.dataset.stream_base_dataset.raw_frame_stream_segmentation_dataset",
        "description": "SVTAS.svtas.loader.dataset.stream_base_dataset.raw_frame_stream_segmentation_dataset",
        "peekOfCode": "class RawFrameStreamSegmentationDataset(StreamDataset):\n    \"\"\"Video dataset for action recognition\n       The dataset loads raw videos and apply specified transforms on them.\n       The index file is a file with multiple lines, and each line indicates\n       a sample video with the filepath and label, which are split with a whitesapce.\n       Example of a inde file:\n        file tree:\n        ─── gtea\n            ├── Videos\n            │   ├── S1_Cheese_C1.mp4",
        "detail": "SVTAS.svtas.loader.dataset.stream_base_dataset.raw_frame_stream_segmentation_dataset",
        "documentation": {}
    },
    {
        "label": "RGBFlowFrameStreamSegmentationDataset",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.dataset.stream_base_dataset.rgb_flow_frame_stream_segmentation_dataset",
        "description": "SVTAS.svtas.loader.dataset.stream_base_dataset.rgb_flow_frame_stream_segmentation_dataset",
        "peekOfCode": "class RGBFlowFrameStreamSegmentationDataset(RawFrameStreamSegmentationDataset):\n    def __init__(self,\n                 flows_path,\n                 **kwargs):\n        self.flows_path = flows_path\n        super().__init__(**kwargs)\n    def parse_file_paths(self, input_path):\n        if self.dataset_type in ['gtea', '50salads', 'thumos14', 'egtea']:\n            file_ptr = open(input_path, 'r')\n            info = file_ptr.read().split('\\n')[:-1]",
        "detail": "SVTAS.svtas.loader.dataset.stream_base_dataset.rgb_flow_frame_stream_segmentation_dataset",
        "documentation": {}
    },
    {
        "label": "StreamDataset",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.dataset.stream_base_dataset.stream_base_dataset",
        "description": "SVTAS.svtas.loader.dataset.stream_base_dataset.stream_base_dataset",
        "peekOfCode": "class StreamDataset(data.IterableDataset):\n    def __init__(self,\n                 file_path,\n                 gt_path,\n                 pipeline,\n                 actions_map_file_path,\n                 temporal_clip_batch_size,\n                 video_batch_size,\n                 suffix='',\n                 dataset_type='gtea',",
        "detail": "SVTAS.svtas.loader.dataset.stream_base_dataset.stream_base_dataset",
        "documentation": {}
    },
    {
        "label": "VideoSamplerDataset",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.dataset.stream_base_dataset.stream_base_dataset",
        "description": "SVTAS.svtas.loader.dataset.stream_base_dataset.stream_base_dataset",
        "peekOfCode": "class VideoSamplerDataset(data.Dataset):\n    def __init__(self,\n                 file_path):\n        super().__init__()\n        self.file_path = file_path\n        self.info = self.load_file()\n    def parse_file_paths(self, input_path):\n        file_ptr = open(input_path, 'r')\n        info = file_ptr.read().split('\\n')[:-1]\n        file_ptr.close()",
        "detail": "SVTAS.svtas.loader.dataset.stream_base_dataset.stream_base_dataset",
        "documentation": {}
    },
    {
        "label": "RawFrameStreamCAMDataset",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.dataset.stream_base_dataset.video_cam_raw_frame_stream_dataset",
        "description": "SVTAS.svtas.loader.dataset.stream_base_dataset.video_cam_raw_frame_stream_dataset",
        "peekOfCode": "class RawFrameStreamCAMDataset(RawFrameStreamSegmentationDataset):\n    def __init__(self,\n                 **kwargs):\n        super().__init__(**kwargs)\n    def _get_one_videos_clip(self, idx, info):\n        imgs_list = []\n        labels_list = []\n        masks_list = []\n        vid_list = []\n        raw_imgs_list = []",
        "detail": "SVTAS.svtas.loader.dataset.stream_base_dataset.video_cam_raw_frame_stream_dataset",
        "documentation": {}
    },
    {
        "label": "FlowNPYContainer",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.decode.container",
        "description": "SVTAS.svtas.loader.decode.container",
        "peekOfCode": "class FlowNPYContainer(object):\n    def __init__(self, npy_file):\n        npy_file = re.sub(\"(mp4|avi)\", \"npy\", npy_file)\n        self.data = np.load(npy_file)\n    def get_batch(self, frames_idx):\n        return self.data[frames_idx, :]\n    def __len__(self):\n        return self.data.shape[0]\nclass DecordContainer(object):\n    def __init__(self, video_file):",
        "detail": "SVTAS.svtas.loader.decode.container",
        "documentation": {}
    },
    {
        "label": "DecordContainer",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.decode.container",
        "description": "SVTAS.svtas.loader.decode.container",
        "peekOfCode": "class DecordContainer(object):\n    def __init__(self, video_file):\n        self.data = de.VideoReader(video_file)\n    def get_batch(self, frames_idx):\n        return self.data.get_batch(frames_idx)\n    def __len__(self):\n        return len(self.data)\nclass PyAVContainer(object):\n    \"\"\"\n    ref:https://github.com/facebookresearch/SlowFast/blob/main/slowfast/datasets/decoder.py",
        "detail": "SVTAS.svtas.loader.decode.container",
        "documentation": {}
    },
    {
        "label": "PyAVContainer",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.decode.container",
        "description": "SVTAS.svtas.loader.decode.container",
        "peekOfCode": "class PyAVContainer(object):\n    \"\"\"\n    ref:https://github.com/facebookresearch/SlowFast/blob/main/slowfast/datasets/decoder.py\n    \"\"\"\n    def __init__(self, video_file, multi_thread_decode=False):\n        container = av.open(video_file)\n        if multi_thread_decode:\n            # Enable multiple threads for decoding.\n            container.streams.video[0].thread_type = \"AUTO\"\n        self.data = container",
        "detail": "SVTAS.svtas.loader.decode.container",
        "documentation": {}
    },
    {
        "label": "OpenCVContainer",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.decode.container",
        "description": "SVTAS.svtas.loader.decode.container",
        "peekOfCode": "class OpenCVContainer(object):\n    def __init__(self, video_file):\n        self.data = cv2.VideoCapture(video_file)\n    def get_batch(self, frames_idx):\n        frames = []\n        margin = 128\n        current_frame_idx = max(0, min(frames_idx) - margin)\n        start_frame_idx = current_frame_idx\n        self.data.set(cv2.CAP_PROP_POS_FRAMES, start_frame_idx)\n        for i in range(start_frame_idx, len(self)):",
        "detail": "SVTAS.svtas.loader.decode.container",
        "documentation": {}
    },
    {
        "label": "get_container",
        "kind": 2,
        "importPath": "SVTAS.svtas.loader.decode.container",
        "description": "SVTAS.svtas.loader.decode.container",
        "peekOfCode": "def get_container(backend):\n    if backend == \"numpy\":\n        return FlowNPYContainer\n    elif backend == \"decord\":\n        return DecordContainer\n    elif backend == \"pyav\":\n        return PyAVContainer\n    elif backend == \"opencv\":\n        return OpenCVContainer\n    else:",
        "detail": "SVTAS.svtas.loader.decode.container",
        "documentation": {}
    },
    {
        "label": "FeatureDecoder",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.decode.decode",
        "description": "SVTAS.svtas.loader.decode.decode",
        "peekOfCode": "class FeatureDecoder():\n    \"\"\"\n    Decode mp4 file to frames.\n    Args:\n        filepath: the file path of mp4 file\n    \"\"\"\n    def __init__(self,\n                 backend='numpy',\n                 is_transpose=False):\n        self.backend = backend",
        "detail": "SVTAS.svtas.loader.decode.decode",
        "documentation": {}
    },
    {
        "label": "VideoDecoder",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.decode.decode",
        "description": "SVTAS.svtas.loader.decode.decode",
        "peekOfCode": "class VideoDecoder():\n    \"\"\"\n    Decode mp4 file to frames.\n    Args:\n        filepath: the file path of mp4 file\n    \"\"\"\n    def __init__(self,\n                 backend='decord'):\n        self.backend = backend\n    def __call__(self, results):",
        "detail": "SVTAS.svtas.loader.decode.decode",
        "documentation": {}
    },
    {
        "label": "FlowVideoDecoder",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.decode.decode",
        "description": "SVTAS.svtas.loader.decode.decode",
        "peekOfCode": "class FlowVideoDecoder(object):\n    \"\"\"\n    get flow from file\n    \"\"\"\n    def __init__(self,\n                 backend='numpy'):\n        self.backend = backend\n    def __call__(self, results):\n        file_path = results['filename']\n        results['format'] = 'video'",
        "detail": "SVTAS.svtas.loader.decode.decode",
        "documentation": {}
    },
    {
        "label": "RGBFlowVideoDecoder",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.decode.decode",
        "description": "SVTAS.svtas.loader.decode.decode",
        "peekOfCode": "class RGBFlowVideoDecoder():\n    \"\"\"\n    Decode mp4 file to frames.\n    Args:\n        filepath: the file path of mp4 file\n    \"\"\"\n    def __init__(self,\n                 backend='decord'):\n        self.backend = backend\n    def __call__(self, results):",
        "detail": "SVTAS.svtas.loader.decode.decode",
        "documentation": {}
    },
    {
        "label": "BasePipline",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.pipline.base_pipline",
        "description": "SVTAS.svtas.loader.pipline.base_pipline",
        "peekOfCode": "class BasePipline():\n    def __init__(self,\n                 decode=None,\n                 sample=None,\n                 transform=None):\n        self.decode = build_decode(decode)\n        self.sample = build_sampler(sample)\n        self.transform = build_transform(transform)\n    def __call__(self, results):\n        # decode",
        "detail": "SVTAS.svtas.loader.pipline.base_pipline",
        "documentation": {}
    },
    {
        "label": "StreamBatchCompose",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.pipline.collect_fn",
        "description": "SVTAS.svtas.loader.pipline.collect_fn",
        "peekOfCode": "class StreamBatchCompose():\n    def __init__(self, to_tensor_keys=[\"imgs\", \"masks\", \"labels\"]):\n        self.to_tensor_keys = to_tensor_keys\n    def __call__(self, batch):\n        result_batch = []\n        for index in range(len(batch)):\n            data = {}\n            for key, value in batch[index].items():\n                if key in self.to_tensor_keys:\n                    if not torch.is_tensor(value):",
        "detail": "SVTAS.svtas.loader.pipline.collect_fn",
        "documentation": {}
    },
    {
        "label": "BatchCompose",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.pipline.collect_fn",
        "description": "SVTAS.svtas.loader.pipline.collect_fn",
        "peekOfCode": "class BatchCompose():\n    def __init__(self,\n                 ignore_index=-100,\n                 max_keys=[\"\"],\n                 compress_keys=[\"\"],\n                 dropout_keys=[\"\"],\n                 to_tensor_keys=[\"imgs\", \"masks\", \"labels\"]):\n        self.to_tensor_keys = to_tensor_keys\n        self.max_keys = max_keys\n        self.compress_keys = compress_keys",
        "detail": "SVTAS.svtas.loader.pipline.collect_fn",
        "documentation": {}
    },
    {
        "label": "FeatureFrameSample",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.sampler.feature_sampler",
        "description": "SVTAS.svtas.loader.sampler.feature_sampler",
        "peekOfCode": "class FeatureFrameSample():\n    def __init__(self, mode='random'):\n        assert mode in ['random', 'uniform'], 'not support mode'\n        self.mode = mode\n    def random_sample(self, start_idx, end_idx, sample_rate):\n        sample_idx = list(\n                random.sample(list(range(start_idx, end_idx)),\n                    len(list(range(start_idx, end_idx, sample_rate)))))\n        sample_idx.sort()\n        return sample_idx",
        "detail": "SVTAS.svtas.loader.sampler.feature_sampler",
        "documentation": {}
    },
    {
        "label": "FeatureStreamSampler",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.sampler.feature_sampler",
        "description": "SVTAS.svtas.loader.sampler.feature_sampler",
        "peekOfCode": "class FeatureStreamSampler():\n    \"\"\"\n    Sample frames id.\n    Returns:\n        frames_idx: the index of sampled #frames.\n    \"\"\"\n    def __init__(self,\n                 feature_dim=2048,\n                 is_train=False,\n                 sample_rate=1,",
        "detail": "SVTAS.svtas.loader.sampler.feature_sampler",
        "documentation": {}
    },
    {
        "label": "FeatureSampler",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.sampler.feature_sampler",
        "description": "SVTAS.svtas.loader.sampler.feature_sampler",
        "peekOfCode": "class FeatureSampler():\n    \"\"\"\n    Sample frames id.\n    Returns:\n        frames_idx: the index of sampled #frames.\n    \"\"\"\n    def __init__(self,\n                 is_train=False,\n                 sample_rate=1,\n                 ignore_index=-100,",
        "detail": "SVTAS.svtas.loader.sampler.feature_sampler",
        "documentation": {}
    },
    {
        "label": "VideoFrameSample",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.sampler.frame_sampler",
        "description": "SVTAS.svtas.loader.sampler.frame_sampler",
        "peekOfCode": "class VideoFrameSample():\n    def __init__(self, mode='random'):\n        assert mode in ['random', 'uniform', 'linspace', 'random_choice'], 'not support mode'\n        self.mode = mode\n    def random_sample(self, start_idx, end_idx, sample_rate):\n        sample_idx = list(\n                random.sample(list(range(start_idx, end_idx)),\n                    len(list(range(start_idx, end_idx, sample_rate)))))\n        sample_idx.sort()\n        return sample_idx",
        "detail": "SVTAS.svtas.loader.sampler.frame_sampler",
        "documentation": {}
    },
    {
        "label": "VideoStreamSampler",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.sampler.frame_sampler",
        "description": "SVTAS.svtas.loader.sampler.frame_sampler",
        "peekOfCode": "class VideoStreamSampler():\n    \"\"\"\n    Sample frames id.\n    Returns:\n        frames_idx: the index of sampled #frames.\n    \"\"\"\n    def __init__(self,\n                 is_train=False,\n                 sample_rate=4,\n                 clip_seg_num=15,",
        "detail": "SVTAS.svtas.loader.sampler.frame_sampler",
        "documentation": {}
    },
    {
        "label": "RGBFlowVideoStreamSampler",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.sampler.frame_sampler",
        "description": "SVTAS.svtas.loader.sampler.frame_sampler",
        "peekOfCode": "class RGBFlowVideoStreamSampler():\n    \"\"\"\n    Sample frames id.\n    Returns:\n        frames_idx: the index of sampled #frames.\n    \"\"\"\n    def __init__(self,\n                 is_train=False,\n                 sample_rate=4,\n                 clip_seg_num=15,",
        "detail": "SVTAS.svtas.loader.sampler.frame_sampler",
        "documentation": {}
    },
    {
        "label": "VideoSampler",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.sampler.frame_sampler",
        "description": "SVTAS.svtas.loader.sampler.frame_sampler",
        "peekOfCode": "class VideoSampler():\n    \"\"\"\n    Sample frames id.\n    Returns:\n        frames_idx: the index of sampled #frames.\n    \"\"\"\n    def __init__(self,\n                 is_train=False,\n                 clip_seg_num=15,\n                 ignore_index=-100,",
        "detail": "SVTAS.svtas.loader.sampler.frame_sampler",
        "documentation": {}
    },
    {
        "label": "VideoPredictionFeatureStreamSampler",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.sampler.video_prediction_sampler",
        "description": "SVTAS.svtas.loader.sampler.video_prediction_sampler",
        "peekOfCode": "class VideoPredictionFeatureStreamSampler(FeatureStreamSampler):\n    def __init__(self,\n                 pred_clip_seg_num=8,\n                 **kwargs\n                 ) -> None:\n        super().__init__(**kwargs)\n        self.pred_clip_seg_num = pred_clip_seg_num\n    def _pred_label_sample(self, labels, start_pred_idx, frames_len):\n        pred_end_idx = start_pred_idx + self.pred_clip_seg_num * self.sample_rate\n        if start_pred_idx < frames_len and pred_end_idx < frames_len:",
        "detail": "SVTAS.svtas.loader.sampler.video_prediction_sampler",
        "documentation": {}
    },
    {
        "label": "VideoPredictionVideoStreamSampler",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.sampler.video_prediction_sampler",
        "description": "SVTAS.svtas.loader.sampler.video_prediction_sampler",
        "peekOfCode": "class VideoPredictionVideoStreamSampler(VideoFrameSample):\n    def __init__(self,\n                 pred_clip_seg_num=8,\n                 **kwargs\n                 ) -> None:\n        super().__init__(**kwargs)\n        self.pred_clip_seg_num = pred_clip_seg_num\n    def _pred_label_sample(self, labels, start_pred_idx, frames_len):\n        pred_end_idx = start_pred_idx + self.pred_clip_seg_num * self.sample_rate\n        if start_pred_idx < frames_len and pred_end_idx < frames_len:",
        "detail": "SVTAS.svtas.loader.sampler.video_prediction_sampler",
        "documentation": {}
    },
    {
        "label": "MixUp",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.transform.transform_fn.mixup",
        "description": "SVTAS.svtas.loader.transform.transform_fn.mixup",
        "peekOfCode": "class MixUp:\n    \"\"\"\n    Apply mixup and/or cutmix for videos at batch level.\n    mixup: Beyond Empirical Risk Minimization (https://arxiv.org/abs/1710.09412)\n    CutMix: Regularization Strategy to Train Strong Classifiers with Localizable\n        Features (https://arxiv.org/abs/1905.04899)\n    \"\"\"\n    def __init__(\n        self,\n        mixup_alpha=1.0,",
        "detail": "SVTAS.svtas.loader.transform.transform_fn.mixup",
        "documentation": {}
    },
    {
        "label": "convert_to_one_hot",
        "kind": 2,
        "importPath": "SVTAS.svtas.loader.transform.transform_fn.mixup",
        "description": "SVTAS.svtas.loader.transform.transform_fn.mixup",
        "peekOfCode": "def convert_to_one_hot(targets, num_classes, on_value=1.0, off_value=0.0):\n    \"\"\"\n    This function converts target class indices to one-hot vectors, given the\n    number of classes.\n    Args:\n        targets (loader): Class labels.\n        num_classes (int): Total number of classes.\n        on_value (float): Target Value for ground truth class.\n        off_value (float): Target Value for other classes.This value is used for\n            label smoothing.",
        "detail": "SVTAS.svtas.loader.transform.transform_fn.mixup",
        "documentation": {}
    },
    {
        "label": "mixup_target",
        "kind": 2,
        "importPath": "SVTAS.svtas.loader.transform.transform_fn.mixup",
        "description": "SVTAS.svtas.loader.transform.transform_fn.mixup",
        "peekOfCode": "def mixup_target(target, num_classes, lam=1.0, smoothing=0.0):\n    \"\"\"\n    This function converts target class indices to one-hot vectors, given the\n    number of classes.\n    Args:\n        targets (loader): Class labels.\n        num_classes (int): Total number of classes.\n        lam (float): lamba value for mixup/cutmix.\n        smoothing (float): Label smoothing value.\n    \"\"\"",
        "detail": "SVTAS.svtas.loader.transform.transform_fn.mixup",
        "documentation": {}
    },
    {
        "label": "rand_bbox",
        "kind": 2,
        "importPath": "SVTAS.svtas.loader.transform.transform_fn.mixup",
        "description": "SVTAS.svtas.loader.transform.transform_fn.mixup",
        "peekOfCode": "def rand_bbox(img_shape, lam, margin=0.0, count=None):\n    \"\"\"\n    Generates a random square bbox based on lambda value.\n    Args:\n        img_shape (tuple): Image shape as tuple\n        lam (float): Cutmix lambda value\n        margin (float): Percentage of bbox dimension to enforce as margin (reduce amount of box outside image)\n        count (int): Number of bbox to generate\n    \"\"\"\n    ratio = np.sqrt(1 - lam)",
        "detail": "SVTAS.svtas.loader.transform.transform_fn.mixup",
        "documentation": {}
    },
    {
        "label": "get_cutmix_bbox",
        "kind": 2,
        "importPath": "SVTAS.svtas.loader.transform.transform_fn.mixup",
        "description": "SVTAS.svtas.loader.transform.transform_fn.mixup",
        "peekOfCode": "def get_cutmix_bbox(img_shape, lam, correct_lam=True, count=None):\n    \"\"\"\n    Generates the box coordinates for cutmix.\n    Args:\n        img_shape (tuple): Image shape as tuple\n        lam (float): Cutmix lambda value\n        correct_lam (bool): Apply lambda correction when cutmix bbox clipped by\n            image borders.\n        count (int): Number of bbox to generate\n    \"\"\"",
        "detail": "SVTAS.svtas.loader.transform.transform_fn.mixup",
        "documentation": {}
    },
    {
        "label": "ToUInt8",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "description": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "peekOfCode": "class ToUInt8(object):\n    def __init__(self, bound=20):\n        self.bound = bound\n    def __call__(self, flow_tensor: torch.FloatTensor) -> torch.FloatTensor:\n        # preprocessing as in\n        # https://github.com/deepmind/kinetics-i3d/issues/61#issuecomment-506727158\n        # but for pytorch\n        # [-bound, bound] -> [0, 255]\n        flow_tensor = (flow_tensor + self.bound) * (255.0 / (2 * self.bound))\n        return flow_tensor.round().to(torch.uint8)",
        "detail": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "documentation": {}
    },
    {
        "label": "XToTensor",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "description": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "peekOfCode": "class XToTensor(object):\n    def __call__(self, feature):\n        return torch.tensor(feature)\nclass ToFloat(object):\n    def __call__(self, byte_img):\n        return byte_img.float()\nclass NormalizeColorTo1:\n    def __call__(self, img):\n        return img / 255.0\nclass ScaleTo1_1(object):",
        "detail": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "documentation": {}
    },
    {
        "label": "ToFloat",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "description": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "peekOfCode": "class ToFloat(object):\n    def __call__(self, byte_img):\n        return byte_img.float()\nclass NormalizeColorTo1:\n    def __call__(self, img):\n        return img / 255.0\nclass ScaleTo1_1(object):\n    def __call__(self, tensor: torch.FloatTensor) -> torch.FloatTensor:\n        return (2 * tensor / 255) - 1\nclass Clamp(object):",
        "detail": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "documentation": {}
    },
    {
        "label": "NormalizeColorTo1",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "description": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "peekOfCode": "class NormalizeColorTo1:\n    def __call__(self, img):\n        return img / 255.0\nclass ScaleTo1_1(object):\n    def __call__(self, tensor: torch.FloatTensor) -> torch.FloatTensor:\n        return (2 * tensor / 255) - 1\nclass Clamp(object):\n    def __init__(self, min_val, max_val) -> None:\n        self.min_val = min_val\n        self.max_val = max_val",
        "detail": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "documentation": {}
    },
    {
        "label": "ScaleTo1_1",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "description": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "peekOfCode": "class ScaleTo1_1(object):\n    def __call__(self, tensor: torch.FloatTensor) -> torch.FloatTensor:\n        return (2 * tensor / 255) - 1\nclass Clamp(object):\n    def __init__(self, min_val, max_val) -> None:\n        self.min_val = min_val\n        self.max_val = max_val\n    def __call__(self, tensor):\n        return torch.clamp(tensor, min=self.min_val, max=self.max_val)\nclass PermuteAndUnsqueeze(object):",
        "detail": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "documentation": {}
    },
    {
        "label": "Clamp",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "description": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "peekOfCode": "class Clamp(object):\n    def __init__(self, min_val, max_val) -> None:\n        self.min_val = min_val\n        self.max_val = max_val\n    def __call__(self, tensor):\n        return torch.clamp(tensor, min=self.min_val, max=self.max_val)\nclass PermuteAndUnsqueeze(object):\n    def __call__(self, tensor: torch.FloatTensor) -> torch.FloatTensor:\n        return tensor.permute(1, 0, 2, 3).unsqueeze(0)\nclass TensorCenterCrop(object):",
        "detail": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "documentation": {}
    },
    {
        "label": "PermuteAndUnsqueeze",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "description": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "peekOfCode": "class PermuteAndUnsqueeze(object):\n    def __call__(self, tensor: torch.FloatTensor) -> torch.FloatTensor:\n        return tensor.permute(1, 0, 2, 3).unsqueeze(0)\nclass TensorCenterCrop(object):\n    def __init__(self, crop_size: int) -> None:\n        self.crop_size = crop_size\n    def __call__(self, tensor: torch.FloatTensor) -> torch.FloatTensor:\n        H, W = tensor.size(-2), tensor.size(-1)\n        from_H = ((H - self.crop_size) // 2)\n        from_W = ((W - self.crop_size) // 2)",
        "detail": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "documentation": {}
    },
    {
        "label": "TensorCenterCrop",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "description": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "peekOfCode": "class TensorCenterCrop(object):\n    def __init__(self, crop_size: int) -> None:\n        self.crop_size = crop_size\n    def __call__(self, tensor: torch.FloatTensor) -> torch.FloatTensor:\n        H, W = tensor.size(-2), tensor.size(-1)\n        from_H = ((H - self.crop_size) // 2)\n        from_W = ((W - self.crop_size) // 2)\n        to_H = from_H + self.crop_size\n        to_W = from_W + self.crop_size\n        return tensor[..., from_H:to_H, from_W:to_W]",
        "detail": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "documentation": {}
    },
    {
        "label": "ResizeImproved",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "description": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "peekOfCode": "class ResizeImproved(object):\n    def __init__(self, size: int, resize_to_smaller_edge: bool = True, interpolation=Image.BILINEAR):\n        self.size = size\n        self.resize_to_smaller_edge = resize_to_smaller_edge\n        self.interpolation = interpolation\n    def __call__(self, img):\n        return resize(img, self.size, self.resize_to_smaller_edge, self.interpolation)\nclass TensorImageResize(object):\n    def __init__(self, size):\n        self.size = size",
        "detail": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "documentation": {}
    },
    {
        "label": "TensorImageResize",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "description": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "peekOfCode": "class TensorImageResize(object):\n    def __init__(self, size):\n        self.size = size\n    def __call__(self, vid):\n        # NOTE: for those functions, which generally expect mini-batches, we keep them\n        # as non-minibatch so that they are applied as if they were 4d (thus image).\n        # this way, we only apply the transformation in the spatial domain\n        interpolation = 'bilinear'\n        # NOTE: using bilinear interpolation because we don't work on minibatches\n        # at this level",
        "detail": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "documentation": {}
    },
    {
        "label": "TensorPermute",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "description": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "peekOfCode": "class TensorPermute(object):\n    def __init__(self, permute_list=None):\n        self.permute_list = permute_list\n    def __call__(self, tensor: torch.FloatTensor) -> torch.FloatTensor:\n        return tensor.permute(self.permute_list)\nclass OpencvToPIL(object):\n    def __init__(self, in_channel_model=\"BGR\"):\n        self.in_channel_model = in_channel_model\n    def __call__(self, img: numpy.ndarray) -> Image:\n        if self.in_channel_model == \"RGB\":",
        "detail": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "documentation": {}
    },
    {
        "label": "OpencvToPIL",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "description": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "peekOfCode": "class OpencvToPIL(object):\n    def __init__(self, in_channel_model=\"BGR\"):\n        self.in_channel_model = in_channel_model\n    def __call__(self, img: numpy.ndarray) -> Image:\n        if self.in_channel_model == \"RGB\":\n            img = Image.fromarray(img)\n        else:\n            img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n        return img",
        "detail": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "documentation": {}
    },
    {
        "label": "resize",
        "kind": 2,
        "importPath": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "description": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "peekOfCode": "def resize(img, size, resize_to_smaller_edge=True, interpolation=Image.BILINEAR):\n    r\"\"\"\n    (v-iashin): this is almost the same implementation as in PyTorch except it has no _is_pil_image() check\n    and has an extra argument governing what happens if `size` is `int`.\n    Reference: https://pytorch.org/docs/1.6.0/_modules/torchvision/transforms/functional.html#resize\n    Resize the input PIL Image to the given size.\n    Args:\n        img (PIL Image): Image to be resized.\n        size (sequence or int): Desired output size. If size is a sequence like\n            (h, w), the output size will be matched to this. If size is an int,",
        "detail": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "description": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "peekOfCode": "__all__ = [\n    \"XToTensor\",\n    \"ToFloat\",\n    \"ScaleTo1_1\",\n    \"NormalizeColorTo1\",\n    \"Clamp\",\n    \"PermuteAndUnsqueeze\",\n    \"TensorCenterCrop\",\n    \"ResizeImproved\",\n    \"ToUInt8\",",
        "detail": "SVTAS.svtas.loader.transform.transform_fn.transform_fn",
        "documentation": {}
    },
    {
        "label": "FeatureStreamTransform",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.transform.transform",
        "description": "SVTAS.svtas.loader.transform.transform",
        "peekOfCode": "class FeatureStreamTransform():\n    def __init__(self, transform_list):\n        transform_op_list = []\n        for transforms_op in transform_list:\n            name = list(transforms_op.keys())[0]\n            if list(transforms_op.values())[0] is None:\n                op = getattr(transforms, name, False)\n                if op is False:\n                    op = getattr(custom_transforms, name)()\n                else:",
        "detail": "SVTAS.svtas.loader.transform.transform",
        "documentation": {}
    },
    {
        "label": "VideoStreamTransform",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.transform.transform",
        "description": "SVTAS.svtas.loader.transform.transform",
        "peekOfCode": "class VideoStreamTransform():\n    def __init__(self, transform_list):\n        transform_op_list = []\n        for transforms_op in transform_list:\n            name = list(transforms_op.keys())[0]\n            if list(transforms_op.values())[0] is None:\n                op = getattr(transforms, name, False)\n                if op is False:\n                    op = getattr(custom_transforms, name)()\n                else:",
        "detail": "SVTAS.svtas.loader.transform.transform",
        "documentation": {}
    },
    {
        "label": "RGBFlowVideoStreamTransform",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.transform.transform",
        "description": "SVTAS.svtas.loader.transform.transform",
        "peekOfCode": "class RGBFlowVideoStreamTransform():\n    def __init__(self, rgb, flow):\n        self.imgs_transforms_pipeline_dict = {}\n        # rgb\n        transform_op_list = []\n        for transforms_op in rgb:\n            name = list(transforms_op.keys())[0]\n            if list(transforms_op.values())[0] is None:\n                op = getattr(transforms, name, False)\n                if op is False:",
        "detail": "SVTAS.svtas.loader.transform.transform",
        "documentation": {}
    },
    {
        "label": "VideoStreamRawFrameStoreTransform",
        "kind": 6,
        "importPath": "SVTAS.svtas.loader.transform.transform",
        "description": "SVTAS.svtas.loader.transform.transform",
        "peekOfCode": "class VideoStreamRawFrameStoreTransform(VideoStreamTransform):\n    def __init__(self, transform_list):\n        super().__init__(transform_list)\n    def __call__(self, results):\n        imgs = []\n        results[\"raw_imgs\"] = copy.deepcopy(results[\"imgs\"])\n        for img in results['imgs']:\n            img = self.imgs_transforms_pipeline(img)\n            imgs.append(img.unsqueeze(0))\n        imgs = torch.cat(imgs, dim=0)",
        "detail": "SVTAS.svtas.loader.transform.transform",
        "documentation": {}
    },
    {
        "label": "build_dataset",
        "kind": 2,
        "importPath": "SVTAS.svtas.loader.builder",
        "description": "SVTAS.svtas.loader.builder",
        "peekOfCode": "def build_dataset(cfg):\n    \"\"\"Build dataset.\"\"\"\n    args = cfg.copy()\n    obj_type = args.get('name')\n    if obj_type in DATASET:\n        return build(cfg, DATASET)\n    raise ValueError(f'{obj_type} is not registered in '\n                     'DATASET')\ndef build_pipline(cfg):\n    \"\"\"Build dataset.\"\"\"",
        "detail": "SVTAS.svtas.loader.builder",
        "documentation": {}
    },
    {
        "label": "build_pipline",
        "kind": 2,
        "importPath": "SVTAS.svtas.loader.builder",
        "description": "SVTAS.svtas.loader.builder",
        "peekOfCode": "def build_pipline(cfg):\n    \"\"\"Build dataset.\"\"\"\n    args = cfg.copy()\n    obj_type = args.get('name')\n    if obj_type in PIPLINE:\n        return build(cfg, PIPLINE)\n    raise ValueError(f'{obj_type} is not registered in '\n                     'PIPLINE')\ndef build_decode(cfg):\n    \"\"\"Build decode.\"\"\"",
        "detail": "SVTAS.svtas.loader.builder",
        "documentation": {}
    },
    {
        "label": "build_decode",
        "kind": 2,
        "importPath": "SVTAS.svtas.loader.builder",
        "description": "SVTAS.svtas.loader.builder",
        "peekOfCode": "def build_decode(cfg):\n    \"\"\"Build decode.\"\"\"\n    args = cfg.copy()\n    obj_type = args.get('name')\n    if obj_type in DECODE:\n        return build(cfg, DECODE)\n    raise ValueError(f'{obj_type} is not registered in '\n                     'DECODE')\ndef build_sampler(cfg):\n    \"\"\"Build sampler.\"\"\"",
        "detail": "SVTAS.svtas.loader.builder",
        "documentation": {}
    },
    {
        "label": "build_sampler",
        "kind": 2,
        "importPath": "SVTAS.svtas.loader.builder",
        "description": "SVTAS.svtas.loader.builder",
        "peekOfCode": "def build_sampler(cfg):\n    \"\"\"Build sampler.\"\"\"\n    args = cfg.copy()\n    obj_type = args.get('name')\n    if obj_type in SAMPLER:\n        return build(cfg, SAMPLER)\n    raise ValueError(f'{obj_type} is not registered in '\n                     'SAMPLER')\ndef build_transform(cfg):\n    \"\"\"Build transform.\"\"\"",
        "detail": "SVTAS.svtas.loader.builder",
        "documentation": {}
    },
    {
        "label": "build_transform",
        "kind": 2,
        "importPath": "SVTAS.svtas.loader.builder",
        "description": "SVTAS.svtas.loader.builder",
        "peekOfCode": "def build_transform(cfg):\n    \"\"\"Build transform.\"\"\"\n    args = cfg.copy()\n    obj_type = args.get('name')\n    if obj_type in TRANSFORM:\n        return build(cfg, TRANSFORM)\n    raise ValueError(f'{obj_type} is not registered in '\n                     'TRANSFORM')",
        "detail": "SVTAS.svtas.loader.builder",
        "documentation": {}
    },
    {
        "label": "DATASET",
        "kind": 5,
        "importPath": "SVTAS.svtas.loader.builder",
        "description": "SVTAS.svtas.loader.builder",
        "peekOfCode": "DATASET = Registry('dataset')\nPIPLINE = Registry('pipline')\nDECODE = Registry('decode')\nSAMPLER = Registry('sampler')\nTRANSFORM = Registry('transform')\ndef build_dataset(cfg):\n    \"\"\"Build dataset.\"\"\"\n    args = cfg.copy()\n    obj_type = args.get('name')\n    if obj_type in DATASET:",
        "detail": "SVTAS.svtas.loader.builder",
        "documentation": {}
    },
    {
        "label": "PIPLINE",
        "kind": 5,
        "importPath": "SVTAS.svtas.loader.builder",
        "description": "SVTAS.svtas.loader.builder",
        "peekOfCode": "PIPLINE = Registry('pipline')\nDECODE = Registry('decode')\nSAMPLER = Registry('sampler')\nTRANSFORM = Registry('transform')\ndef build_dataset(cfg):\n    \"\"\"Build dataset.\"\"\"\n    args = cfg.copy()\n    obj_type = args.get('name')\n    if obj_type in DATASET:\n        return build(cfg, DATASET)",
        "detail": "SVTAS.svtas.loader.builder",
        "documentation": {}
    },
    {
        "label": "DECODE",
        "kind": 5,
        "importPath": "SVTAS.svtas.loader.builder",
        "description": "SVTAS.svtas.loader.builder",
        "peekOfCode": "DECODE = Registry('decode')\nSAMPLER = Registry('sampler')\nTRANSFORM = Registry('transform')\ndef build_dataset(cfg):\n    \"\"\"Build dataset.\"\"\"\n    args = cfg.copy()\n    obj_type = args.get('name')\n    if obj_type in DATASET:\n        return build(cfg, DATASET)\n    raise ValueError(f'{obj_type} is not registered in '",
        "detail": "SVTAS.svtas.loader.builder",
        "documentation": {}
    },
    {
        "label": "SAMPLER",
        "kind": 5,
        "importPath": "SVTAS.svtas.loader.builder",
        "description": "SVTAS.svtas.loader.builder",
        "peekOfCode": "SAMPLER = Registry('sampler')\nTRANSFORM = Registry('transform')\ndef build_dataset(cfg):\n    \"\"\"Build dataset.\"\"\"\n    args = cfg.copy()\n    obj_type = args.get('name')\n    if obj_type in DATASET:\n        return build(cfg, DATASET)\n    raise ValueError(f'{obj_type} is not registered in '\n                     'DATASET')",
        "detail": "SVTAS.svtas.loader.builder",
        "documentation": {}
    },
    {
        "label": "TRANSFORM",
        "kind": 5,
        "importPath": "SVTAS.svtas.loader.builder",
        "description": "SVTAS.svtas.loader.builder",
        "peekOfCode": "TRANSFORM = Registry('transform')\ndef build_dataset(cfg):\n    \"\"\"Build dataset.\"\"\"\n    args = cfg.copy()\n    obj_type = args.get('name')\n    if obj_type in DATASET:\n        return build(cfg, DATASET)\n    raise ValueError(f'{obj_type} is not registered in '\n                     'DATASET')\ndef build_pipline(cfg):",
        "detail": "SVTAS.svtas.loader.builder",
        "documentation": {}
    },
    {
        "label": "BaseTASegmentationMetric",
        "kind": 6,
        "importPath": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_base_class",
        "description": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_base_class",
        "peekOfCode": "class BaseTASegmentationMetric(BaseMetric):\n    \"\"\"\n    Test for Video Segmentation based model.\n    \"\"\"\n    def __init__(self,\n                 overlap,\n                 actions_map_file_path,\n                 train_mode=False,\n                 max_proposal=100,\n                 tiou_thresholds=np.linspace(0.5, 0.95, 10),",
        "detail": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_base_class",
        "documentation": {}
    },
    {
        "label": "TASegmentationMetric",
        "kind": 6,
        "importPath": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric",
        "description": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric",
        "peekOfCode": "class TASegmentationMetric(BaseTASegmentationMetric):\n    \"\"\"\n    Test for Video Segmentation based model.\n    \"\"\"\n    def __init__(self,\n                 overlap,\n                 actions_map_file_path,\n                 train_mode=False,\n                 max_proposal=100,\n                 tiou_thresholds=np.linspace(0.5, 0.95, 10),",
        "detail": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric",
        "documentation": {}
    },
    {
        "label": "get_labels_scores_start_end_time",
        "kind": 2,
        "importPath": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "description": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "peekOfCode": "def get_labels_scores_start_end_time(input_np,\n                                     frame_wise_labels,\n                                     actions_dict,\n                                     bg_class=ignore_bg_class):\n    labels = []\n    starts = []\n    ends = []\n    scores = []\n    boundary_score_ptr = 0\n    last_label = frame_wise_labels[0]",
        "detail": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "documentation": {}
    },
    {
        "label": "get_labels_start_end_time",
        "kind": 2,
        "importPath": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "description": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "peekOfCode": "def get_labels_start_end_time(frame_wise_labels, bg_class=ignore_bg_class):\n    labels = []\n    starts = []\n    ends = []\n    last_label = frame_wise_labels[0]\n    if frame_wise_labels[0] not in bg_class:\n        labels.append(frame_wise_labels[0])\n        starts.append(0)\n    for i in range(len(frame_wise_labels)):\n        if frame_wise_labels[i] != last_label:",
        "detail": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "documentation": {}
    },
    {
        "label": "levenstein",
        "kind": 2,
        "importPath": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "description": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "peekOfCode": "def levenstein(p, y, norm=False):\n    m_row = len(p)\n    n_col = len(y)\n    D = np.zeros([m_row + 1, n_col + 1])\n    for i in range(m_row + 1):\n        D[i, 0] = i\n    for i in range(n_col + 1):\n        D[0, i] = i\n    for j in range(1, n_col + 1):\n        for i in range(1, m_row + 1):",
        "detail": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "documentation": {}
    },
    {
        "label": "edit_score",
        "kind": 2,
        "importPath": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "description": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "peekOfCode": "def edit_score(recognized, ground_truth, norm=True, bg_class=ignore_bg_class):\n    P, _, _ = get_labels_start_end_time(recognized, bg_class)\n    Y, _, _ = get_labels_start_end_time(ground_truth, bg_class)\n    return levenstein(P, Y, norm)\ndef f_score(recognized, ground_truth, overlap, bg_class=ignore_bg_class):\n    p_label, p_start, p_end = get_labels_start_end_time(recognized, bg_class)\n    y_label, y_start, y_end = get_labels_start_end_time(ground_truth, bg_class)\n    tp = 0\n    fp = 0\n    if len(y_label) > 0:",
        "detail": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "documentation": {}
    },
    {
        "label": "f_score",
        "kind": 2,
        "importPath": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "description": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "peekOfCode": "def f_score(recognized, ground_truth, overlap, bg_class=ignore_bg_class):\n    p_label, p_start, p_end = get_labels_start_end_time(recognized, bg_class)\n    y_label, y_start, y_end = get_labels_start_end_time(ground_truth, bg_class)\n    tp = 0\n    fp = 0\n    if len(y_label) > 0:\n        hits = np.zeros(len(y_label))\n        for j in range(len(p_label)):\n            intersection = np.minimum(p_end[j], y_end) - np.maximum(\n                p_start[j], y_start)",
        "detail": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "documentation": {}
    },
    {
        "label": "boundary_AR",
        "kind": 2,
        "importPath": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "description": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "peekOfCode": "def boundary_AR(pred_boundary, gt_boundary, overlap_list, max_proposal):\n    p_label, p_start, p_end, p_scores = pred_boundary\n    y_label, y_start, y_end, _ = gt_boundary\n    # sort proposal\n    pred_dict = {\n        \"label\": p_label,\n        \"start\": p_start,\n        \"end\": p_end,\n        \"scores\": p_scores\n    }",
        "detail": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "documentation": {}
    },
    {
        "label": "interpolated_prec_rec",
        "kind": 2,
        "importPath": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "description": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "peekOfCode": "def interpolated_prec_rec(prec, rec):\n    \"\"\"Interpolated AP - VOCdevkit from VOC 2011.\n    \"\"\"\n    mprec = np.hstack([[0], prec, [0]])\n    mrec = np.hstack([[0], rec, [1]])\n    for i in range(len(mprec) - 1)[::-1]:\n        mprec[i] = max(mprec[i], mprec[i + 1])\n    idx = np.where(mrec[1::] != mrec[0:-1])[0] + 1\n    ap = np.sum((mrec[idx] - mrec[idx - 1]) * mprec[idx])\n    return ap",
        "detail": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "documentation": {}
    },
    {
        "label": "segment_iou",
        "kind": 2,
        "importPath": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "description": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "peekOfCode": "def segment_iou(target_segment, candidate_segments):\n    \"\"\"Compute the temporal intersection over union between a\n    target segment and all the test segments.\n    Parameters\n    ----------\n    target_segment : 1d array\n        Temporal target segment containing [starting, ending] times.\n    candidate_segments : 2d array\n        Temporal candidate segments containing N x [starting, ending] times.\n    Outputs",
        "detail": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "documentation": {}
    },
    {
        "label": "compute_average_precision_detection",
        "kind": 2,
        "importPath": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "description": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "peekOfCode": "def compute_average_precision_detection(ground_truth,\n                                        prediction,\n                                        tiou_thresholds=np.linspace(\n                                            0.5, 0.95, 10)):\n    \"\"\"Compute average precision (detection task) between ground truth and\n    predictions data frames. If multiple predictions occurs for the same\n    predicted segment, only the one with highest score is matches as\n    true positive. This code is greatly inspired by Pascal VOC devkit.\n    Parameters\n    ----------",
        "detail": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "documentation": {}
    },
    {
        "label": "get_predictions_with_label",
        "kind": 2,
        "importPath": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "description": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "peekOfCode": "def get_predictions_with_label(prediction_by_label, label_name, cidx):\n    \"\"\"Get all predicitons of the given label. Return empty DataFrame if there\n        is no predcitions with the given label.\n        \"\"\"\n    try:\n        return prediction_by_label.get_group(label_name).reset_index(drop=True)\n    except:\n        return pd.DataFrame()\ndef wrapper_compute_average_precision(prediction, ground_truth, tiou_thresholds,\n                                      activity_index):",
        "detail": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "documentation": {}
    },
    {
        "label": "wrapper_compute_average_precision",
        "kind": 2,
        "importPath": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "description": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "peekOfCode": "def wrapper_compute_average_precision(prediction, ground_truth, tiou_thresholds,\n                                      activity_index):\n    \"\"\"Computes average precision for each class in the subset.\n        \"\"\"\n    activity_dict = activity_index.copy()\n    # del background class\n    for label_name in list(activity_dict.keys()):\n        if label_name in ignore_bg_class:\n            del activity_dict[label_name]\n    ap = np.zeros((len(tiou_thresholds), len(activity_dict)))",
        "detail": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "documentation": {}
    },
    {
        "label": "ignore_bg_class",
        "kind": 5,
        "importPath": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "description": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "peekOfCode": "ignore_bg_class = [\"background\", \"None\"]\ndef get_labels_scores_start_end_time(input_np,\n                                     frame_wise_labels,\n                                     actions_dict,\n                                     bg_class=ignore_bg_class):\n    labels = []\n    starts = []\n    ends = []\n    scores = []\n    boundary_score_ptr = 0",
        "detail": "SVTAS.svtas.metric.temporal_action_segmentation.temporal_action_segmentation_metric_utils",
        "documentation": {}
    },
    {
        "label": "BaseMetric",
        "kind": 6,
        "importPath": "SVTAS.svtas.metric.base_metric",
        "description": "SVTAS.svtas.metric.base_metric",
        "peekOfCode": "class BaseMetric(metaclass=abc.ABCMeta):\n    def __init__(self):\n        pass\n    @abc.abstractmethod\n    def update(self, outputs):\n        \"\"\"update metrics during each iter\n        \"\"\"\n        raise NotImplementedError\n    @abc.abstractmethod\n    def accumulate(self):",
        "detail": "SVTAS.svtas.metric.base_metric",
        "documentation": {}
    },
    {
        "label": "build_metric",
        "kind": 2,
        "importPath": "SVTAS.svtas.metric.builder",
        "description": "SVTAS.svtas.metric.builder",
        "peekOfCode": "def build_metric(cfg):\n    \"\"\"Build metric.\"\"\"\n    args = cfg.copy()\n    obj_type = args.get('name')\n    if obj_type in METRIC:\n        return build(cfg, METRIC)\n    raise ValueError(f'{obj_type} is not registered in '\n                     'METRIC')",
        "detail": "SVTAS.svtas.metric.builder",
        "documentation": {}
    },
    {
        "label": "METRIC",
        "kind": 5,
        "importPath": "SVTAS.svtas.metric.builder",
        "description": "SVTAS.svtas.metric.builder",
        "peekOfCode": "METRIC = Registry('metric')\ndef build_metric(cfg):\n    \"\"\"Build metric.\"\"\"\n    args = cfg.copy()\n    obj_type = args.get('name')\n    if obj_type in METRIC:\n        return build(cfg, METRIC)\n    raise ValueError(f'{obj_type} is not registered in '\n                     'METRIC')",
        "detail": "SVTAS.svtas.metric.builder",
        "documentation": {}
    },
    {
        "label": "Encoder2Decoder",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.architectures.general.encoder_2_decoder",
        "description": "SVTAS.svtas.model.architectures.general.encoder_2_decoder",
        "peekOfCode": "class Encoder2Decoder(nn.Module):\n    def __init__(self,\n                 encoder,\n                 decoder,\n                 head):\n        super().__init__()\n        if encoder is not None:\n            self.encoder = build_backbone(encoder)\n        else:\n            self.encoder = None",
        "detail": "SVTAS.svtas.model.architectures.general.encoder_2_decoder",
        "documentation": {}
    },
    {
        "label": "OpticalFlowEstimation",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.architectures.optical_flow.optical_flow_estimator",
        "description": "SVTAS.svtas.model.architectures.optical_flow.optical_flow_estimator",
        "peekOfCode": "class OpticalFlowEstimation(nn.Module):\n    def __init__(self,\n                 model=None,\n                 loss=None):\n        super().__init__()\n        if model is not None:\n            self.model = build_backbone(model)\n        else:\n            self.model = None\n        self.init_weights()",
        "detail": "SVTAS.svtas.model.architectures.optical_flow.optical_flow_estimator",
        "documentation": {}
    },
    {
        "label": "ActionCLIP",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.architectures.recognition.action_clip",
        "description": "SVTAS.svtas.model.architectures.recognition.action_clip",
        "peekOfCode": "class ActionCLIP(nn.Module):\n    def __init__(self,\n                 pretrained=None,\n                 image_prompt=None,\n                 text_prompt=None,\n                 fusion_neck=None,\n                 head=None,\n                 loss=None,\n                 is_feature_extract=False):\n        super().__init__()",
        "detail": "SVTAS.svtas.model.architectures.recognition.action_clip",
        "documentation": {}
    },
    {
        "label": "Recognition2D",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.architectures.recognition.recognition2d",
        "description": "SVTAS.svtas.model.architectures.recognition.recognition2d",
        "peekOfCode": "class Recognition2D(nn.Module):\n    def __init__(self,\n                 backbone=None,\n                 neck=None,\n                 head=None,\n                 loss=None):\n        super().__init__()\n        if backbone is not None:\n            self.backbone = build_backbone(backbone)\n        else:",
        "detail": "SVTAS.svtas.model.architectures.recognition.recognition2d",
        "documentation": {}
    },
    {
        "label": "Recognition3D",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.architectures.recognition.recognition3d",
        "description": "SVTAS.svtas.model.architectures.recognition.recognition3d",
        "peekOfCode": "class Recognition3D(nn.Module):\n    def __init__(self,\n                 backbone=None,\n                 neck=None,\n                 head=None,\n                 loss=None):\n        super().__init__()\n        if backbone is not None:\n            self.backbone = build_backbone(backbone)\n        else:",
        "detail": "SVTAS.svtas.model.architectures.recognition.recognition3d",
        "documentation": {}
    },
    {
        "label": "FeatureSegmentation",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.architectures.segmentation.feature.feature_segmentation",
        "description": "SVTAS.svtas.model.architectures.segmentation.feature.feature_segmentation",
        "peekOfCode": "class FeatureSegmentation(nn.Module):\n    def __init__(self,\n                 backbone=None,\n                 neck=None,\n                 head=None,\n                 loss=None):\n        super().__init__()\n        if backbone is not None:\n            self.backbone = build_backbone(backbone)\n        else:",
        "detail": "SVTAS.svtas.model.architectures.segmentation.feature.feature_segmentation",
        "documentation": {}
    },
    {
        "label": "MulModStreamSegmentation",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.architectures.segmentation.stream_video.multi_modality_stream_segmentation",
        "description": "SVTAS.svtas.model.architectures.segmentation.stream_video.multi_modality_stream_segmentation",
        "peekOfCode": "class MulModStreamSegmentation(nn.Module):\n    def __init__(self,\n                 rgb_backbone=None,\n                 flow_backbone=None,\n                 audio_backbone=None,\n                 neck=None,\n                 head=None,\n                 loss=None):\n        super().__init__()\n        self.rgb_backbone = build_backbone(rgb_backbone)",
        "detail": "SVTAS.svtas.model.architectures.segmentation.stream_video.multi_modality_stream_segmentation",
        "documentation": {}
    },
    {
        "label": "StreamSegmentation2D",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.architectures.segmentation.stream_video.stream_segmentation2d",
        "description": "SVTAS.svtas.model.architectures.segmentation.stream_video.stream_segmentation2d",
        "peekOfCode": "class StreamSegmentation2D(nn.Module):\n    def __init__(self,\n                 backbone=None,\n                 neck=None,\n                 head=None,\n                 loss=None):\n        super().__init__()\n        self.backbone = build_backbone(backbone)\n        self.neck = build_neck(neck)\n        self.head = build_head(head)",
        "detail": "SVTAS.svtas.model.architectures.segmentation.stream_video.stream_segmentation2d",
        "documentation": {}
    },
    {
        "label": "StreamSegmentation2DWithBackbone",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.architectures.segmentation.stream_video.stream_segmentation2d_with_backboneloss",
        "description": "SVTAS.svtas.model.architectures.segmentation.stream_video.stream_segmentation2d_with_backboneloss",
        "peekOfCode": "class StreamSegmentation2DWithBackbone(nn.Module):\n    def __init__(self,\n                 backbone=None,\n                 neck=None,\n                 head=None,\n                 loss=None):\n        super().__init__()\n        self.backbone = build_backbone(backbone)\n        self.neck = build_neck(neck)\n        self.head = build_head(head)",
        "detail": "SVTAS.svtas.model.architectures.segmentation.stream_video.stream_segmentation2d_with_backboneloss",
        "documentation": {}
    },
    {
        "label": "StreamSegmentation2DWithNeck",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.architectures.segmentation.stream_video.stream_segmentation2d_with_neck",
        "description": "SVTAS.svtas.model.architectures.segmentation.stream_video.stream_segmentation2d_with_neck",
        "peekOfCode": "class StreamSegmentation2DWithNeck(nn.Module):\n    def __init__(self,\n                 backbone=None,\n                 neck=None,\n                 head=None,\n                 loss=None):\n        super().__init__()\n        self.backbone = build_backbone(backbone)\n        self.neck = build_neck(neck)\n        self.head = build_head(head)",
        "detail": "SVTAS.svtas.model.architectures.segmentation.stream_video.stream_segmentation2d_with_neck",
        "documentation": {}
    },
    {
        "label": "StreamSegmentation3D",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.architectures.segmentation.stream_video.stream_segmentation3d",
        "description": "SVTAS.svtas.model.architectures.segmentation.stream_video.stream_segmentation3d",
        "peekOfCode": "class StreamSegmentation3D(nn.Module):\n    def __init__(self,\n                 backbone=None,\n                 neck=None,\n                 head=None,\n                 loss=None):\n        super().__init__()\n        self.backbone = build_backbone(backbone)\n        self.neck = build_neck(neck)\n        self.head = build_head(head)",
        "detail": "SVTAS.svtas.model.architectures.segmentation.stream_video.stream_segmentation3d",
        "documentation": {}
    },
    {
        "label": "StreamSegmentation3DWithBackbone",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.architectures.segmentation.stream_video.stream_segmentation3d_with_backboneloss",
        "description": "SVTAS.svtas.model.architectures.segmentation.stream_video.stream_segmentation3d_with_backboneloss",
        "peekOfCode": "class StreamSegmentation3DWithBackbone(nn.Module):\n    def __init__(self,\n                 backbone=None,\n                 neck=None,\n                 head=None,\n                 loss=None):\n        super().__init__()\n        self.backbone = build_backbone(backbone)\n        self.neck = build_neck(neck)\n        self.head = build_head(head)",
        "detail": "SVTAS.svtas.model.architectures.segmentation.stream_video.stream_segmentation3d_with_backboneloss",
        "documentation": {}
    },
    {
        "label": "Transeger",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.architectures.segmentation.stream_video.transeger",
        "description": "SVTAS.svtas.model.architectures.segmentation.stream_video.transeger",
        "peekOfCode": "class Transeger(nn.Module):\n    def __init__(self,\n                 image_backbone=None,\n                 text_backbone=None,\n                 joint=None,\n                 loss=None):\n        super().__init__()\n        self.image_backbone = build_architecture(image_backbone)\n        self.text_backbone = build_architecture(text_backbone)\n        self.joint = build_neck(joint)",
        "detail": "SVTAS.svtas.model.architectures.segmentation.stream_video.transeger",
        "documentation": {}
    },
    {
        "label": "ActionCLIPSegmentation",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.architectures.segmentation.video.action_clip_segmentation",
        "description": "SVTAS.svtas.model.architectures.segmentation.video.action_clip_segmentation",
        "peekOfCode": "class ActionCLIPSegmentation(nn.Module):\n    def __init__(self,\n                 pretrained=None,\n                 image_prompt=None,\n                 text_prompt=None,\n                 fusion_neck=None,\n                 head=None,\n                 loss=None,\n                 aligin_head=None,\n                 is_feature_extract=False):",
        "detail": "SVTAS.svtas.model.architectures.segmentation.video.action_clip_segmentation",
        "documentation": {}
    },
    {
        "label": "Segmentation2D",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.architectures.segmentation.video.segmentation2d",
        "description": "SVTAS.svtas.model.architectures.segmentation.video.segmentation2d",
        "peekOfCode": "class Segmentation2D(nn.Module):\n    def __init__(self,\n                 backbone=None,\n                 neck=None,\n                 head=None,\n                 aligin_head=None,\n                 loss=None):\n        super().__init__()\n        if backbone is not None:\n            self.backbone = build_backbone(backbone)",
        "detail": "SVTAS.svtas.model.architectures.segmentation.video.segmentation2d",
        "documentation": {}
    },
    {
        "label": "Segmentation3D",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.architectures.segmentation.video.segmentation3d",
        "description": "SVTAS.svtas.model.architectures.segmentation.video.segmentation3d",
        "peekOfCode": "class Segmentation3D(nn.Module):\n    def __init__(self,\n                 backbone=None,\n                 neck=None,\n                 head=None,\n                 aligin_head=None,\n                 loss=None):\n        super().__init__()\n        if backbone is not None:\n            self.backbone = build_backbone(backbone)",
        "detail": "SVTAS.svtas.model.architectures.segmentation.video.segmentation3d",
        "documentation": {}
    },
    {
        "label": "TransducerAudioEncoder",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.audio.transudcer_audio_encoder",
        "description": "SVTAS.svtas.model.backbones.audio.transudcer_audio_encoder",
        "peekOfCode": "class TransducerAudioEncoder(nn.Module):\n    \"\"\"\n    Converts the audio signal to higher feature values\n    Args:\n        device (torch.device): flag indication whether cpu or cuda\n        input_size (int): dimension of input vector (default : 80)\n        model_dim (int): the number of features in the audio encoder (default : 512)\n        ff_dim (int): the number of features in the feed forward layers (default : 2048)\n        num_layers (int): the number of audio encoder layers (default: 18)\n        num_heads (int): the number of heads in the multi-head attention (default: 8)",
        "detail": "SVTAS.svtas.model.backbones.audio.transudcer_audio_encoder",
        "documentation": {}
    },
    {
        "label": "Correlation",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.flow.fastflownet",
        "description": "SVTAS.svtas.model.backbones.flow.fastflownet",
        "peekOfCode": "class Correlation(nn.Module):\n    def __init__(self, max_displacement):\n        super(Correlation, self).__init__()\n        self.max_displacement = max_displacement\n        self.kernel_size = 2*max_displacement+1\n        self.corr = SpatialCorrelationSampler(1, self.kernel_size, 1, 0, 1)\n    def forward(self, x, y):\n        b, c, h, w = x.shape\n        return self.corr(x, y).view(b, -1, h, w) / c\ndef convrelu(in_channels, out_channels, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True):",
        "detail": "SVTAS.svtas.model.backbones.flow.fastflownet",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.flow.fastflownet",
        "description": "SVTAS.svtas.model.backbones.flow.fastflownet",
        "peekOfCode": "class Decoder(nn.Module):\n    def __init__(self, in_channels, groups):\n        super(Decoder, self).__init__()\n        self.in_channels = in_channels\n        self.groups = groups\n        self.conv1 = convrelu(in_channels, 96, 3, 1)\n        self.conv2 = convrelu(96, 96, 3, 1, groups=groups)\n        self.conv3 = convrelu(96, 96, 3, 1, groups=groups)\n        self.conv4 = convrelu(96, 96, 3, 1, groups=groups)\n        self.conv5 = convrelu(96, 64, 3, 1)",
        "detail": "SVTAS.svtas.model.backbones.flow.fastflownet",
        "documentation": {}
    },
    {
        "label": "FastFlowNet",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.flow.fastflownet",
        "description": "SVTAS.svtas.model.backbones.flow.fastflownet",
        "peekOfCode": "class FastFlowNet(nn.Module):\n    def __init__(self,\n                 pretrained=None,\n                 extract_mode=True,\n                 freeze=True,\n                 div_size=64,\n                 div_flow=20,\n                 groups=3):\n        super(FastFlowNet, self).__init__()\n        self.groups = groups",
        "detail": "SVTAS.svtas.model.backbones.flow.fastflownet",
        "documentation": {}
    },
    {
        "label": "convrelu",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.flow.fastflownet",
        "description": "SVTAS.svtas.model.backbones.flow.fastflownet",
        "peekOfCode": "def convrelu(in_channels, out_channels, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias=bias), \n        nn.LeakyReLU(0.1, inplace=True)\n    )\ndef deconv(in_planes, out_planes, kernel_size=4, stride=2, padding=1):\n    return nn.ConvTranspose2d(in_planes, out_planes, kernel_size, stride, padding, bias=True)\nclass Decoder(nn.Module):\n    def __init__(self, in_channels, groups):\n        super(Decoder, self).__init__()",
        "detail": "SVTAS.svtas.model.backbones.flow.fastflownet",
        "documentation": {}
    },
    {
        "label": "deconv",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.flow.fastflownet",
        "description": "SVTAS.svtas.model.backbones.flow.fastflownet",
        "peekOfCode": "def deconv(in_planes, out_planes, kernel_size=4, stride=2, padding=1):\n    return nn.ConvTranspose2d(in_planes, out_planes, kernel_size, stride, padding, bias=True)\nclass Decoder(nn.Module):\n    def __init__(self, in_channels, groups):\n        super(Decoder, self).__init__()\n        self.in_channels = in_channels\n        self.groups = groups\n        self.conv1 = convrelu(in_channels, 96, 3, 1)\n        self.conv2 = convrelu(96, 96, 3, 1, groups=groups)\n        self.conv3 = convrelu(96, 96, 3, 1, groups=groups)",
        "detail": "SVTAS.svtas.model.backbones.flow.fastflownet",
        "documentation": {}
    },
    {
        "label": "LiteFlowNetV3",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.flow.liteflownet_v3",
        "description": "SVTAS.svtas.model.backbones.flow.liteflownet_v3",
        "peekOfCode": "class LiteFlowNetV3(nn.Module):\n    def __init__(self,\n                 pretrained=None,\n                 freeze=True,\n                 extract_mode=True,\n                 scale_factor_flow=20.0):\n        super(LiteFlowNetV3, self).__init__()\n        self.pretrained = pretrained\n        self.scale_factor_flow = scale_factor_flow\n        self.freeze = freeze",
        "detail": "SVTAS.svtas.model.backbones.flow.liteflownet_v3",
        "documentation": {}
    },
    {
        "label": "InputPadder",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.flow.raft",
        "description": "SVTAS.svtas.model.backbones.flow.raft",
        "peekOfCode": "class InputPadder:\n    \"\"\" Pads images such that dimensions are divisible by 8\"\"\"\n    def __init__(self, dims, mode='sintel'):\n        self.ht, self.wd = dims[-2:]\n        pad_ht = (((self.ht // 8) + 1) * 8 - self.ht) % 8\n        pad_wd = (((self.wd // 8) + 1) * 8 - self.wd) % 8\n        if mode == 'sintel':\n            self._pad = [pad_wd//2, pad_wd - pad_wd//2, pad_ht//2, pad_ht - pad_ht//2]\n        else:\n            self._pad = [pad_wd//2, pad_wd - pad_wd//2, 0, pad_ht]",
        "detail": "SVTAS.svtas.model.backbones.flow.raft",
        "documentation": {}
    },
    {
        "label": "RAFT",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.flow.raft",
        "description": "SVTAS.svtas.model.backbones.flow.raft",
        "peekOfCode": "class RAFT(nn.Module):\n    # (v-iashin) def __init__(self, model_is_small):\n    def __init__(self,\n                 pretrained=None,\n                 extract_mode=True,\n                 freeze=True,\n                 model_is_small=False,\n                 alternate_corr=False,\n                 mixed_precision=False,\n                 dropout=0,",
        "detail": "SVTAS.svtas.model.backbones.flow.raft",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.clip",
        "description": "SVTAS.svtas.model.backbones.image.clip",
        "peekOfCode": "class DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\nclass Bottleneck(nn.Module):\n    expansion = 4",
        "detail": "SVTAS.svtas.model.backbones.image.clip",
        "documentation": {}
    },
    {
        "label": "Bottleneck",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.clip",
        "description": "SVTAS.svtas.model.backbones.image.clip",
        "peekOfCode": "class Bottleneck(nn.Module):\n    expansion = 4\n    def __init__(self, inplanes, planes, stride=1):\n        super().__init__()\n        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)",
        "detail": "SVTAS.svtas.model.backbones.image.clip",
        "documentation": {}
    },
    {
        "label": "AttentionPool2d",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.clip",
        "description": "SVTAS.svtas.model.backbones.image.clip",
        "peekOfCode": "class AttentionPool2d(nn.Module):\n    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n        super().__init__()\n        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n        self.num_heads = num_heads\n    def forward(self, x):",
        "detail": "SVTAS.svtas.model.backbones.image.clip",
        "documentation": {}
    },
    {
        "label": "ModifiedResNet",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.clip",
        "description": "SVTAS.svtas.model.backbones.image.clip",
        "peekOfCode": "class ModifiedResNet(nn.Module):\n    \"\"\"\n    A ResNet class that is similar to torchvision's but contains the following changes:\n    - There are now 3 \"stem\" convolutions as opposed to 1, with an average pool instead of a max pool.\n    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1\n    - The final pooling layer is a QKV attention instead of an average pool\n    \"\"\"\n    def __init__(self, layers, output_dim, heads, input_resolution=224, width=64):\n        super().__init__()\n        self.output_dim = output_dim",
        "detail": "SVTAS.svtas.model.backbones.image.clip",
        "documentation": {}
    },
    {
        "label": "LayerNorm",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.clip",
        "description": "SVTAS.svtas.model.backbones.image.clip",
        "peekOfCode": "class LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        ret = super().forward(x.type(torch.float32))\n        return ret.type(orig_type)\nclass QuickGELU(nn.Module):\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\nclass ResidualAttentionBlock(nn.Module):",
        "detail": "SVTAS.svtas.model.backbones.image.clip",
        "documentation": {}
    },
    {
        "label": "QuickGELU",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.clip",
        "description": "SVTAS.svtas.model.backbones.image.clip",
        "peekOfCode": "class QuickGELU(nn.Module):\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\nclass ResidualAttentionBlock(nn.Module):\n    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None, dropout = 0.):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(d_model, n_head, dropout=dropout)\n        self.ln_1 = LayerNorm(d_model)\n        self.drop_path = DropPath(dropout) if dropout > 0. else nn.Identity()\n        self.mlp = nn.Sequential(OrderedDict([",
        "detail": "SVTAS.svtas.model.backbones.image.clip",
        "documentation": {}
    },
    {
        "label": "ResidualAttentionBlock",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.clip",
        "description": "SVTAS.svtas.model.backbones.image.clip",
        "peekOfCode": "class ResidualAttentionBlock(nn.Module):\n    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None, dropout = 0.):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(d_model, n_head, dropout=dropout)\n        self.ln_1 = LayerNorm(d_model)\n        self.drop_path = DropPath(dropout) if dropout > 0. else nn.Identity()\n        self.mlp = nn.Sequential(OrderedDict([\n            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n            (\"gelu\", QuickGELU()),\n            (\"c_proj\", nn.Linear(d_model * 4, d_model))",
        "detail": "SVTAS.svtas.model.backbones.image.clip",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.clip",
        "description": "SVTAS.svtas.model.backbones.image.clip",
        "peekOfCode": "class Transformer(nn.Module):\n    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None, dropout=None):\n        super().__init__()\n        if dropout is None:\n            dropout = [0.0 for i in range(layers)] \n        self.width = width\n        self.layers = layers\n        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask, dropout=dropout[i]) for i in range(layers)])\n    def forward(self, x: torch.Tensor):\n        return self.resblocks(x)",
        "detail": "SVTAS.svtas.model.backbones.image.clip",
        "documentation": {}
    },
    {
        "label": "VisionTransformer",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.clip",
        "description": "SVTAS.svtas.model.backbones.image.clip",
        "peekOfCode": "class VisionTransformer(nn.Module):\n    def __init__(self,\n                 input_resolution: int,\n                 patch_size: int,\n                 width: int,\n                 layers: int,\n                 heads: int,\n                 output_dim: int,\n                 T: int,\n                 dropout = None,",
        "detail": "SVTAS.svtas.model.backbones.image.clip",
        "documentation": {}
    },
    {
        "label": "CLIP",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.clip",
        "description": "SVTAS.svtas.model.backbones.image.clip",
        "peekOfCode": "class CLIP(nn.Module):\n    def __init__(self,\n                 embed_dim: int,\n                 # vision\n                 image_resolution: int,\n                 vision_layers: Union[Tuple[int, int, int, int], int],\n                 vision_width: int,\n                 vision_patch_size: int,\n                 # text\n                 context_length: int,",
        "detail": "SVTAS.svtas.model.backbones.image.clip",
        "documentation": {}
    },
    {
        "label": "drop_path",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.image.clip",
        "description": "SVTAS.svtas.model.backbones.image.clip",
        "peekOfCode": "def drop_path(x, drop_prob: float = 0., training: bool = False):\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n    'survival rate' as the argument.\n    \"\"\"\n    if drop_prob == 0. or not training:\n        return x",
        "detail": "SVTAS.svtas.model.backbones.image.clip",
        "documentation": {}
    },
    {
        "label": "InvertedResidual",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.mobilenet_v2",
        "description": "SVTAS.svtas.model.backbones.image.mobilenet_v2",
        "peekOfCode": "class InvertedResidual(nn.Module):\n    \"\"\"InvertedResidual block for MobileNetV2.\n    Args:\n        in_channels (int): The input channels of the InvertedResidual block.\n        out_channels (int): The output channels of the InvertedResidual block.\n        stride (int): Stride of the middle (first) 3x3 convolution.\n        expand_ratio (int): adjusts number of channels of the hidden layer\n            in InvertedResidual by this amount.\n        conv_cfg (dict): Config dict for convolution layer.\n            Default: None, which means using conv2d.",
        "detail": "SVTAS.svtas.model.backbones.image.mobilenet_v2",
        "documentation": {}
    },
    {
        "label": "MobileNetV2",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.mobilenet_v2",
        "description": "SVTAS.svtas.model.backbones.image.mobilenet_v2",
        "peekOfCode": "class MobileNetV2(nn.Module):\n    \"\"\"MobileNetV2 backbone.\n    Args:\n        pretrained (str | None): Name of pretrained model. Default: None.\n        widen_factor (float): Width multiplier, multiply number of\n            channels in each layer by this amount. Default: 1.0.\n        out_indices (None or Sequence[int]): Output from which stages.\n            Default: (7, ).\n        frozen_stages (int): Stages to be frozen (all param fixed). Note that\n            the last stage in ``MobileNetV2`` is ``conv2``. Default: -1,",
        "detail": "SVTAS.svtas.model.backbones.image.mobilenet_v2",
        "documentation": {}
    },
    {
        "label": "make_divisible",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.image.mobilenet_v2",
        "description": "SVTAS.svtas.model.backbones.image.mobilenet_v2",
        "peekOfCode": "def make_divisible(value, divisor, min_value=None, min_ratio=0.9):\n    \"\"\"Make divisible function.\n    This function rounds the channel number down to the nearest value that can\n    be divisible by the divisor.\n    Args:\n        value (int): The original channel number.\n        divisor (int): The divisor to fully divide the channel number.\n        min_value (int, optional): The minimum value of the output channel.\n            Default: None, means that the minimum value equal to the divisor.\n        min_ratio (float, optional): The minimum ratio of the rounded channel",
        "detail": "SVTAS.svtas.model.backbones.image.mobilenet_v2",
        "documentation": {}
    },
    {
        "label": "PreNorm",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.mobilevit",
        "description": "SVTAS.svtas.model.backbones.image.mobilevit",
        "peekOfCode": "class PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout=0.):\n        super().__init__()",
        "detail": "SVTAS.svtas.model.backbones.image.mobilevit",
        "documentation": {}
    },
    {
        "label": "FeedForward",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.mobilevit",
        "description": "SVTAS.svtas.model.backbones.image.mobilevit",
        "peekOfCode": "class FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout=0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.SiLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )",
        "detail": "SVTAS.svtas.model.backbones.image.mobilevit",
        "documentation": {}
    },
    {
        "label": "Attention",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.mobilevit",
        "description": "SVTAS.svtas.model.backbones.image.mobilevit",
        "peekOfCode": "class Attention(nn.Module):\n    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n        super().__init__()\n        inner_dim = dim_head * heads\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n        self.attend = nn.Softmax(dim=-1)\n        self.dropout = nn.Dropout(dropout)\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n        self.to_out = nn.Sequential(",
        "detail": "SVTAS.svtas.model.backbones.image.mobilevit",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.mobilevit",
        "description": "SVTAS.svtas.model.backbones.image.mobilevit",
        "peekOfCode": "class Transformer(nn.Module):\n    \"\"\"Transformer block described in ViT.\n    Paper: https://arxiv.org/abs/2010.11929\n    Based on: https://github.com/lucidrains/vit-pytorch\n    \"\"\"\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([",
        "detail": "SVTAS.svtas.model.backbones.image.mobilevit",
        "documentation": {}
    },
    {
        "label": "MV2Block",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.mobilevit",
        "description": "SVTAS.svtas.model.backbones.image.mobilevit",
        "peekOfCode": "class MV2Block(nn.Module):\n    \"\"\"MV2 block described in MobileNetV2.\n    Paper: https://arxiv.org/pdf/1801.04381\n    Based on: https://github.com/tonylins/pytorch-mobilenet-v2\n    \"\"\"\n    def __init__(self, inp, oup, stride=1, expansion=4):\n        super().__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n        hidden_dim = int(inp * expansion)",
        "detail": "SVTAS.svtas.model.backbones.image.mobilevit",
        "documentation": {}
    },
    {
        "label": "MobileViTBlock",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.mobilevit",
        "description": "SVTAS.svtas.model.backbones.image.mobilevit",
        "peekOfCode": "class MobileViTBlock(nn.Module):\n    def __init__(self, dim, depth, channel, kernel_size, patch_size, mlp_dim, dropout=0.):\n        super().__init__()\n        self.ph, self.pw = patch_size\n        self.conv1 = conv_nxn_bn(channel, channel, kernel_size)\n        self.conv2 = conv_1x1_bn(channel, dim)\n        self.transformer = Transformer(dim, depth, 4, 8, mlp_dim, dropout)\n        self.conv3 = conv_1x1_bn(dim, channel)\n        self.conv4 = conv_nxn_bn(2 * channel, channel, kernel_size, padding=1)\n    def forward(self, x):",
        "detail": "SVTAS.svtas.model.backbones.image.mobilevit",
        "documentation": {}
    },
    {
        "label": "MobileViT",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.mobilevit",
        "description": "SVTAS.svtas.model.backbones.image.mobilevit",
        "peekOfCode": "class MobileViT(nn.Module):\n    \"\"\"MobileViT.\n    Paper: https://arxiv.org/abs/2110.02178\n    Based on: https://github.com/chinhsuanwu/mobilevit-pytorch\n    \"\"\"\n    def __init__(\n        self,\n        image_size=(256, 256),\n        dims=[96, 120, 144],\n        channels=[16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 384],",
        "detail": "SVTAS.svtas.model.backbones.image.mobilevit",
        "documentation": {}
    },
    {
        "label": "conv_1x1_bn",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.image.mobilevit",
        "description": "SVTAS.svtas.model.backbones.image.mobilevit",
        "peekOfCode": "def conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.SiLU()\n    )\ndef conv_nxn_bn(inp, oup, kernel_size=3, stride=1, padding=None, dilation=1):\n    if padding is None:\n        padding = (kernel_size - 1) // 2 * dilation\n    return nn.Sequential(",
        "detail": "SVTAS.svtas.model.backbones.image.mobilevit",
        "documentation": {}
    },
    {
        "label": "conv_nxn_bn",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.image.mobilevit",
        "description": "SVTAS.svtas.model.backbones.image.mobilevit",
        "peekOfCode": "def conv_nxn_bn(inp, oup, kernel_size=3, stride=1, padding=None, dilation=1):\n    if padding is None:\n        padding = (kernel_size - 1) // 2 * dilation\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, kernel_size, stride, padding=padding, dilation=dilation, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.SiLU()\n    )\n# classes\nclass PreNorm(nn.Module):",
        "detail": "SVTAS.svtas.model.backbones.image.mobilevit",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.resnet",
        "description": "SVTAS.svtas.model.backbones.image.resnet",
        "peekOfCode": "class BasicBlock(nn.Module):\n    \"\"\"Basic block for ResNet.\n    Args:\n        inplanes (int): Number of channels for the input in first conv2d layer.\n        planes (int): Number of channels produced by some norm/conv2d layers.\n        stride (int): Stride in the conv layer. Default: 1.\n        dilation (int): Spacing between kernel elements. Default: 1.\n        downsample (nn.Module | None): Downsample layer. Default: None.\n        style (str): `pytorch` or `caffe`. If set to \"pytorch\", the stride-two\n            layer is the 3x3 conv layer, otherwise the stride-two layer is",
        "detail": "SVTAS.svtas.model.backbones.image.resnet",
        "documentation": {}
    },
    {
        "label": "Bottleneck",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.resnet",
        "description": "SVTAS.svtas.model.backbones.image.resnet",
        "peekOfCode": "class Bottleneck(nn.Module):\n    \"\"\"Bottleneck block for ResNet.\n    Args:\n        inplanes (int):\n            Number of channels for the input feature in first conv layer.\n        planes (int):\n            Number of channels produced by some norm layes and conv layers\n        stride (int): Spatial stride in the conv layer. Default: 1.\n        dilation (int): Spacing between kernel elements. Default: 1.\n        downsample (nn.Module | None): Downsample layer. Default: None.",
        "detail": "SVTAS.svtas.model.backbones.image.resnet",
        "documentation": {}
    },
    {
        "label": "ResNet",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.resnet",
        "description": "SVTAS.svtas.model.backbones.image.resnet",
        "peekOfCode": "class ResNet(nn.Module):\n    \"\"\"ResNet backbone.\n    Args:\n        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.\n        pretrained (str | None): Name of pretrained model. Default: None.\n        in_channels (int): Channel num of input features. Default: 3.\n        num_stages (int): Resnet stages. Default: 4.\n        strides (Sequence[int]): Strides of the first block of each stage.\n        out_indices (Sequence[int]): Indices of output feature. Default: (3, ).\n        dilations (Sequence[int]): Dilation of each stage.",
        "detail": "SVTAS.svtas.model.backbones.image.resnet",
        "documentation": {}
    },
    {
        "label": "make_res_layer",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.image.resnet",
        "description": "SVTAS.svtas.model.backbones.image.resnet",
        "peekOfCode": "def make_res_layer(block,\n                   inplanes,\n                   planes,\n                   blocks,\n                   stride=1,\n                   dilation=1,\n                   style='pytorch',\n                   conv_cfg=None,\n                   norm_cfg=None,\n                   act_cfg=None,",
        "detail": "SVTAS.svtas.model.backbones.image.resnet",
        "documentation": {}
    },
    {
        "label": "FeedForward",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.sample_vit",
        "description": "SVTAS.svtas.model.backbones.image.sample_vit",
        "peekOfCode": "class FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Linear(hidden_dim, dim),\n        )\n    def forward(self, x):",
        "detail": "SVTAS.svtas.model.backbones.image.sample_vit",
        "documentation": {}
    },
    {
        "label": "Attention",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.sample_vit",
        "description": "SVTAS.svtas.model.backbones.image.sample_vit",
        "peekOfCode": "class Attention(nn.Module):\n    def __init__(self, dim, heads = 8, dim_head = 64):\n        super().__init__()\n        inner_dim = dim_head *  heads\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n        self.norm = nn.LayerNorm(dim)\n        self.attend = nn.Softmax(dim = -1)\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n        self.to_out = nn.Linear(inner_dim, dim, bias = False)",
        "detail": "SVTAS.svtas.model.backbones.image.sample_vit",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.sample_vit",
        "description": "SVTAS.svtas.model.backbones.image.sample_vit",
        "peekOfCode": "class Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Attention(dim, heads = heads, dim_head = dim_head),\n                FeedForward(dim, mlp_dim)\n            ]))\n    def forward(self, x):",
        "detail": "SVTAS.svtas.model.backbones.image.sample_vit",
        "documentation": {}
    },
    {
        "label": "SimpleViT",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.sample_vit",
        "description": "SVTAS.svtas.model.backbones.image.sample_vit",
        "peekOfCode": "class SimpleViT(nn.Module):\n    def __init__(self, \n                 image_size,\n                 patch_size,\n                 dim,\n                 depth,\n                 heads,\n                 mlp_dim,\n                 channels = 3,\n                 dim_head = 64,",
        "detail": "SVTAS.svtas.model.backbones.image.sample_vit",
        "documentation": {}
    },
    {
        "label": "pair",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.image.sample_vit",
        "description": "SVTAS.svtas.model.backbones.image.sample_vit",
        "peekOfCode": "def pair(t):\n    return t if isinstance(t, tuple) else (t, t)\ndef posemb_sincos_2d(patches, temperature = 10000, dtype = torch.float32):\n    _, h, w, dim, device, dtype = *patches.shape, patches.device, patches.dtype\n    y, x = torch.meshgrid(torch.arange(h, device = device), torch.arange(w, device = device), indexing = 'ij')\n    assert (dim % 4) == 0, 'feature dimension must be multiple of 4 for sincos emb'\n    omega = torch.arange(dim // 4, device = device) / (dim // 4 - 1)\n    omega = 1. / (temperature ** omega)\n    y = y.flatten()[:, None] * omega[None, :]\n    x = x.flatten()[:, None] * omega[None, :] ",
        "detail": "SVTAS.svtas.model.backbones.image.sample_vit",
        "documentation": {}
    },
    {
        "label": "posemb_sincos_2d",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.image.sample_vit",
        "description": "SVTAS.svtas.model.backbones.image.sample_vit",
        "peekOfCode": "def posemb_sincos_2d(patches, temperature = 10000, dtype = torch.float32):\n    _, h, w, dim, device, dtype = *patches.shape, patches.device, patches.dtype\n    y, x = torch.meshgrid(torch.arange(h, device = device), torch.arange(w, device = device), indexing = 'ij')\n    assert (dim % 4) == 0, 'feature dimension must be multiple of 4 for sincos emb'\n    omega = torch.arange(dim // 4, device = device) / (dim // 4 - 1)\n    omega = 1. / (temperature ** omega)\n    y = y.flatten()[:, None] * omega[None, :]\n    x = x.flatten()[:, None] * omega[None, :] \n    pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim = 1)\n    return pe.type(dtype)",
        "detail": "SVTAS.svtas.model.backbones.image.sample_vit",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.swin_v2_transformer",
        "description": "SVTAS.svtas.model.backbones.image.swin_v2_transformer",
        "peekOfCode": "class Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n    def forward(self, x):",
        "detail": "SVTAS.svtas.model.backbones.image.swin_v2_transformer",
        "documentation": {}
    },
    {
        "label": "WindowAttention",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.swin_v2_transformer",
        "description": "SVTAS.svtas.model.backbones.image.swin_v2_transformer",
        "peekOfCode": "class WindowAttention(nn.Module):\n    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0",
        "detail": "SVTAS.svtas.model.backbones.image.swin_v2_transformer",
        "documentation": {}
    },
    {
        "label": "SwinTransformerBlock",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.swin_v2_transformer",
        "description": "SVTAS.svtas.model.backbones.image.swin_v2_transformer",
        "peekOfCode": "class SwinTransformerBlock(nn.Module):\n    r\"\"\" Swin Transformer Block.\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resulotion.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True",
        "detail": "SVTAS.svtas.model.backbones.image.swin_v2_transformer",
        "documentation": {}
    },
    {
        "label": "PatchMerging",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.swin_v2_transformer",
        "description": "SVTAS.svtas.model.backbones.image.swin_v2_transformer",
        "peekOfCode": "class PatchMerging(nn.Module):\n    r\"\"\" Patch Merging Layer.\n    Args:\n        input_resolution (tuple[int]): Resolution of input feature.\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.input_resolution = input_resolution",
        "detail": "SVTAS.svtas.model.backbones.image.swin_v2_transformer",
        "documentation": {}
    },
    {
        "label": "BasicLayer",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.swin_v2_transformer",
        "description": "SVTAS.svtas.model.backbones.image.swin_v2_transformer",
        "peekOfCode": "class BasicLayer(nn.Module):\n    \"\"\" A basic Swin Transformer layer for one stage.\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True",
        "detail": "SVTAS.svtas.model.backbones.image.swin_v2_transformer",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.swin_v2_transformer",
        "description": "SVTAS.svtas.model.backbones.image.swin_v2_transformer",
        "peekOfCode": "class PatchEmbed(nn.Module):\n    r\"\"\" Image to Patch Embedding\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):",
        "detail": "SVTAS.svtas.model.backbones.image.swin_v2_transformer",
        "documentation": {}
    },
    {
        "label": "SwinTransformerV2",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.swin_v2_transformer",
        "description": "SVTAS.svtas.model.backbones.image.swin_v2_transformer",
        "peekOfCode": "class SwinTransformerV2(nn.Module):\n    r\"\"\" Swin Transformer\n        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n          https://arxiv.org/pdf/2103.14030\n    Args:\n        img_size (int | tuple(int)): Input image size. Default 224\n        patch_size (int | tuple(int)): Patch size. Default: 4\n        in_chans (int): Number of input image channels. Default: 3\n        num_classes (int): Number of classes for classification head. Default: 1000\n        embed_dim (int): Patch embedding dimension. Default: 96",
        "detail": "SVTAS.svtas.model.backbones.image.swin_v2_transformer",
        "documentation": {}
    },
    {
        "label": "window_partition",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.image.swin_v2_transformer",
        "description": "SVTAS.svtas.model.backbones.image.swin_v2_transformer",
        "peekOfCode": "def window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)",
        "detail": "SVTAS.svtas.model.backbones.image.swin_v2_transformer",
        "documentation": {}
    },
    {
        "label": "window_reverse",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.image.swin_v2_transformer",
        "description": "SVTAS.svtas.model.backbones.image.swin_v2_transformer",
        "peekOfCode": "def window_reverse(windows, window_size, H, W):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"",
        "detail": "SVTAS.svtas.model.backbones.image.swin_v2_transformer",
        "documentation": {}
    },
    {
        "label": "PreNorm",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.vit",
        "description": "SVTAS.svtas.model.backbones.image.vit",
        "peekOfCode": "class PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__()",
        "detail": "SVTAS.svtas.model.backbones.image.vit",
        "documentation": {}
    },
    {
        "label": "FeedForward",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.vit",
        "description": "SVTAS.svtas.model.backbones.image.vit",
        "peekOfCode": "class FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )",
        "detail": "SVTAS.svtas.model.backbones.image.vit",
        "documentation": {}
    },
    {
        "label": "Attention",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.vit",
        "description": "SVTAS.svtas.model.backbones.image.vit",
        "peekOfCode": "class Attention(nn.Module):\n    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n        super().__init__()\n        inner_dim = dim_head *  heads\n        project_out = not (heads == 1 and dim_head == dim)\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n        self.attend = nn.Softmax(dim = -1)\n        self.dropout = nn.Dropout(dropout)\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)",
        "detail": "SVTAS.svtas.model.backbones.image.vit",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.vit",
        "description": "SVTAS.svtas.model.backbones.image.vit",
        "peekOfCode": "class Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n            ]))\n    def forward(self, x):",
        "detail": "SVTAS.svtas.model.backbones.image.vit",
        "documentation": {}
    },
    {
        "label": "ViT",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.vit",
        "description": "SVTAS.svtas.model.backbones.image.vit",
        "peekOfCode": "class ViT(nn.Module):\n    def __init__(self,\n                 image_size=224,\n                 patch_size=32,\n                 dim=1024,\n                 depth=6,\n                 heads=16,\n                 mlp_dim=2048,\n                 pool = 'cls',\n                 channels = 3,",
        "detail": "SVTAS.svtas.model.backbones.image.vit",
        "documentation": {}
    },
    {
        "label": "pair",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.image.vit",
        "description": "SVTAS.svtas.model.backbones.image.vit",
        "peekOfCode": "def pair(t):\n    return t if isinstance(t, tuple) else (t, t)\n# classes\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)",
        "detail": "SVTAS.svtas.model.backbones.image.vit",
        "documentation": {}
    },
    {
        "label": "PreNorm",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.vit_for_small_dataset",
        "description": "SVTAS.svtas.model.backbones.image.vit_for_small_dataset",
        "peekOfCode": "class PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__()",
        "detail": "SVTAS.svtas.model.backbones.image.vit_for_small_dataset",
        "documentation": {}
    },
    {
        "label": "FeedForward",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.vit_for_small_dataset",
        "description": "SVTAS.svtas.model.backbones.image.vit_for_small_dataset",
        "peekOfCode": "class FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )",
        "detail": "SVTAS.svtas.model.backbones.image.vit_for_small_dataset",
        "documentation": {}
    },
    {
        "label": "LSA",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.vit_for_small_dataset",
        "description": "SVTAS.svtas.model.backbones.image.vit_for_small_dataset",
        "peekOfCode": "class LSA(nn.Module):\n    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n        super().__init__()\n        inner_dim = dim_head *  heads\n        self.heads = heads\n        self.temperature = nn.Parameter(torch.log(torch.tensor(dim_head ** -0.5)))\n        self.attend = nn.Softmax(dim = -1)\n        self.dropout = nn.Dropout(dropout)\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n        self.to_out = nn.Sequential(",
        "detail": "SVTAS.svtas.model.backbones.image.vit_for_small_dataset",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.vit_for_small_dataset",
        "description": "SVTAS.svtas.model.backbones.image.vit_for_small_dataset",
        "peekOfCode": "class Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PreNorm(dim, LSA(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n            ]))\n    def forward(self, x):",
        "detail": "SVTAS.svtas.model.backbones.image.vit_for_small_dataset",
        "documentation": {}
    },
    {
        "label": "SPT",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.vit_for_small_dataset",
        "description": "SVTAS.svtas.model.backbones.image.vit_for_small_dataset",
        "peekOfCode": "class SPT(nn.Module):\n    def __init__(self, *, dim, patch_size, channels = 3):\n        super().__init__()\n        patch_dim = patch_size * patch_size * 5 * channels\n        self.to_patch_tokens = nn.Sequential(\n            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n            nn.LayerNorm(patch_dim),\n            nn.Linear(patch_dim, dim)\n        )\n    def forward(self, x):",
        "detail": "SVTAS.svtas.model.backbones.image.vit_for_small_dataset",
        "documentation": {}
    },
    {
        "label": "SLViT",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.image.vit_for_small_dataset",
        "description": "SVTAS.svtas.model.backbones.image.vit_for_small_dataset",
        "peekOfCode": "class SLViT(nn.Module):\n    def __init__(self,\n                 image_size=224,\n                 patch_size=32, \n                 dim=1024,\n                 depth=6,\n                 heads=16,\n                 mlp_dim=2048,\n                 pool = 'cls',\n                 channels = 3,",
        "detail": "SVTAS.svtas.model.backbones.image.vit_for_small_dataset",
        "documentation": {}
    },
    {
        "label": "pair",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.image.vit_for_small_dataset",
        "description": "SVTAS.svtas.model.backbones.image.vit_for_small_dataset",
        "peekOfCode": "def pair(t):\n    return t if isinstance(t, tuple) else (t, t)\n# classes\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)",
        "detail": "SVTAS.svtas.model.backbones.image.vit_for_small_dataset",
        "documentation": {}
    },
    {
        "label": "BridgePromptTextEncoder",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.language.bridge_prompt",
        "description": "SVTAS.svtas.model.backbones.language.bridge_prompt",
        "peekOfCode": "class BridgePromptTextEncoder(nn.Module):\n    def __init__(self,\n                 clip_model,\n                 actions_map_file_path,\n                 cnt_max=7,\n                 sample_rate=4,\n                 max_len=77,\n                 dataset_type=\"gtea\",\n                 ignore_index=-100):\n        super().__init__()",
        "detail": "SVTAS.svtas.model.backbones.language.bridge_prompt",
        "documentation": {}
    },
    {
        "label": "BridgePrompt",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.language.bridge_prompt",
        "description": "SVTAS.svtas.model.backbones.language.bridge_prompt",
        "peekOfCode": "class BridgePrompt(nn.Module):\n    def __init__(self,\n                 dataset_type=\"gtea\",\n                 cnt_max=7,\n                 sample_rate=4,\n                 max_len=77,\n                 labels_id2classesname=None,\n                 ignore_index=-100):\n        super().__init__()\n        self._tokenizer = _Tokenizer(max_len)",
        "detail": "SVTAS.svtas.model.backbones.language.bridge_prompt",
        "documentation": {}
    },
    {
        "label": "FixPromptTextEncoder",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.language.fix_prompt",
        "description": "SVTAS.svtas.model.backbones.language.fix_prompt",
        "peekOfCode": "class FixPromptTextEncoder(nn.Module):\n    def __init__(self,\n                 actions_map_file_path,\n                 embedding_dim,\n                 encoder_layers_num,\n                 encoder_heads_num,\n                 text_embed_dim,\n                 vocab_size=49408,\n                 sample_rate=4,\n                 clip_seg_num=32,",
        "detail": "SVTAS.svtas.model.backbones.language.fix_prompt",
        "documentation": {}
    },
    {
        "label": "Prompt",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.language.fix_prompt",
        "description": "SVTAS.svtas.model.backbones.language.fix_prompt",
        "peekOfCode": "class Prompt(nn.Module):\n    def __init__(self,\n                 classnames,\n                 embedding_dim,\n                 vocab_size=49408,\n                 max_len=40,\n                 sample_rate=4,\n                 n_ctx=8,\n                 class_token_position=\"end\",\n                 labels_id2classesname=None,",
        "detail": "SVTAS.svtas.model.backbones.language.fix_prompt",
        "documentation": {}
    },
    {
        "label": "LearnerPromptTextEncoder",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.language.learner_prompt",
        "description": "SVTAS.svtas.model.backbones.language.learner_prompt",
        "peekOfCode": "class LearnerPromptTextEncoder(nn.Module):\n    def __init__(self,\n                 actions_map_file_path,\n                 embedding_dim,\n                 encoder_layers_num,\n                 encoder_heads_num,\n                 text_embed_dim,\n                 vocab_size=49408,\n                 sample_rate=4,\n                 clip_seg_num=32,",
        "detail": "SVTAS.svtas.model.backbones.language.learner_prompt",
        "documentation": {}
    },
    {
        "label": "PromptLearner",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.language.learner_prompt",
        "description": "SVTAS.svtas.model.backbones.language.learner_prompt",
        "peekOfCode": "class PromptLearner(nn.Module):\n    def __init__(self,\n                 classnames,\n                 embedding_dim,\n                 vocab_size=49408,\n                 max_len=40,\n                 sample_rate=4,\n                 n_ctx=8,\n                 ctx_init=\"\",\n                 class_token_position=\"end\",",
        "detail": "SVTAS.svtas.model.backbones.language.learner_prompt",
        "documentation": {}
    },
    {
        "label": "TextCLIP",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.language.text_clip",
        "description": "SVTAS.svtas.model.backbones.language.text_clip",
        "peekOfCode": "class TextCLIP(nn.Module):\n    def __init__(self,\n                 clip_model,\n                 actions_map_file_path,\n                 max_len=77) -> None:\n        super().__init__()\n        self.clip_model = clip_model\n        self._tokenizer = _Tokenizer(max_len)\n        self.text_aug = [f\"a photo of action {{}}\", f\"a picture of action {{}}\", f\"Human action of {{}}\", f\"{{}}, an action\",\n                f\"{{}} this is an action\", f\"{{}}, a video of action\", f\"Playing action of {{}}\", f\"{{}}\",",
        "detail": "SVTAS.svtas.model.backbones.language.text_clip",
        "documentation": {}
    },
    {
        "label": "TransducerTextEncoder",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.language.transducer_text_encoder",
        "description": "SVTAS.svtas.model.backbones.language.transducer_text_encoder",
        "peekOfCode": "class TransducerTextEncoder(nn.Module):\n    \"\"\"\n    Converts the label to higher feature values\n    Args:\n        device (torch.device): flag indication whether cpu or cuda\n        num_vocabs (int): the number of vocabulary\n        model_dim (int): the number of features in the label encoder (default : 512)\n        ff_dim (int): the number of features in the feed forward layers (default : 2048)\n        num_layers (int): the number of label encoder layers (default: 2)\n        num_heads (int): the number of heads in the multi-head attention (default: 8)",
        "detail": "SVTAS.svtas.model.backbones.language.transducer_text_encoder",
        "documentation": {}
    },
    {
        "label": "LayerNorm",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.clip.module",
        "description": "SVTAS.svtas.model.backbones.utils.clip.module",
        "peekOfCode": "class LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        ret = super().forward(x.type(torch.float32))\n        return ret.type(orig_type)\nclass QuickGELU(nn.Module):\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\nclass ResidualAttentionBlock(nn.Module):",
        "detail": "SVTAS.svtas.model.backbones.utils.clip.module",
        "documentation": {}
    },
    {
        "label": "QuickGELU",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.clip.module",
        "description": "SVTAS.svtas.model.backbones.utils.clip.module",
        "peekOfCode": "class QuickGELU(nn.Module):\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\nclass ResidualAttentionBlock(nn.Module):\n    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(d_model, n_head)\n        self.ln_1 = LayerNorm(d_model)\n        self.mlp = nn.Sequential(OrderedDict([\n            (\"c_fc\", nn.Linear(d_model, d_model * 4)),",
        "detail": "SVTAS.svtas.model.backbones.utils.clip.module",
        "documentation": {}
    },
    {
        "label": "ResidualAttentionBlock",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.clip.module",
        "description": "SVTAS.svtas.model.backbones.utils.clip.module",
        "peekOfCode": "class ResidualAttentionBlock(nn.Module):\n    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(d_model, n_head)\n        self.ln_1 = LayerNorm(d_model)\n        self.mlp = nn.Sequential(OrderedDict([\n            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n            (\"gelu\", QuickGELU()),\n            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n        ]))",
        "detail": "SVTAS.svtas.model.backbones.utils.clip.module",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.clip.module",
        "description": "SVTAS.svtas.model.backbones.utils.clip.module",
        "peekOfCode": "class Transformer(nn.Module):\n    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n        self.width = width\n        self.layers = layers\n        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n    def forward(self, x: torch.Tensor):\n        return self.resblocks(x)",
        "detail": "SVTAS.svtas.model.backbones.utils.clip.module",
        "documentation": {}
    },
    {
        "label": "SimpleTokenizer",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.clip.simple_tokenizer",
        "description": "SVTAS.svtas.model.backbones.utils.clip.simple_tokenizer",
        "peekOfCode": "class SimpleTokenizer(object):\n    def __init__(self, context_length: int = 77, bpe_path: str = default_bpe()):\n        self.context_length = context_length\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n        merges = merges[1:49152-256-2+1]\n        merges = [tuple(merge.split()) for merge in merges]\n        vocab = list(bytes_to_unicode().values())\n        vocab = vocab + [v+'</w>' for v in vocab]",
        "detail": "SVTAS.svtas.model.backbones.utils.clip.simple_tokenizer",
        "documentation": {}
    },
    {
        "label": "default_bpe",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.utils.clip.simple_tokenizer",
        "description": "SVTAS.svtas.model.backbones.utils.clip.simple_tokenizer",
        "peekOfCode": "def default_bpe():\n    return os.path.join(os.getcwd(), \"svtas\", \"model\", \"backbones\", \"utils\", \"clip\", \"bpe_simple_vocab_16e6.txt.gz\")\n@lru_cache()\ndef bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.",
        "detail": "SVTAS.svtas.model.backbones.utils.clip.simple_tokenizer",
        "documentation": {}
    },
    {
        "label": "bytes_to_unicode",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.utils.clip.simple_tokenizer",
        "description": "SVTAS.svtas.model.backbones.utils.clip.simple_tokenizer",
        "peekOfCode": "def bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"",
        "detail": "SVTAS.svtas.model.backbones.utils.clip.simple_tokenizer",
        "documentation": {}
    },
    {
        "label": "get_pairs",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.utils.clip.simple_tokenizer",
        "description": "SVTAS.svtas.model.backbones.utils.clip.simple_tokenizer",
        "peekOfCode": "def get_pairs(word):\n    \"\"\"Return set of symbol pairs in a word.\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    \"\"\"\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs",
        "detail": "SVTAS.svtas.model.backbones.utils.clip.simple_tokenizer",
        "documentation": {}
    },
    {
        "label": "basic_clean",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.utils.clip.simple_tokenizer",
        "description": "SVTAS.svtas.model.backbones.utils.clip.simple_tokenizer",
        "peekOfCode": "def basic_clean(text):\n    text = ftfy.fix_text(text)\n    text = html.unescape(html.unescape(text))\n    return text.strip()\ndef whitespace_clean(text):\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.strip()\n    return text\nclass SimpleTokenizer(object):\n    def __init__(self, context_length: int = 77, bpe_path: str = default_bpe()):",
        "detail": "SVTAS.svtas.model.backbones.utils.clip.simple_tokenizer",
        "documentation": {}
    },
    {
        "label": "whitespace_clean",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.utils.clip.simple_tokenizer",
        "description": "SVTAS.svtas.model.backbones.utils.clip.simple_tokenizer",
        "peekOfCode": "def whitespace_clean(text):\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.strip()\n    return text\nclass SimpleTokenizer(object):\n    def __init__(self, context_length: int = 77, bpe_path: str = default_bpe()):\n        self.context_length = context_length\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')",
        "detail": "SVTAS.svtas.model.backbones.utils.clip.simple_tokenizer",
        "documentation": {}
    },
    {
        "label": "Conv2plus1d",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.conv2plus1d.conv2plus1d",
        "description": "SVTAS.svtas.model.backbones.utils.conv2plus1d.conv2plus1d",
        "peekOfCode": "class Conv2plus1d(nn.Module):\n    \"\"\"(2+1)d Conv module for R(2+1)d backbone.\n    https://arxiv.org/pdf/1711.11248.pdf.\n    Args:\n        in_channels (int): Same as nn.Conv3d.\n        out_channels (int): Same as nn.Conv3d.\n        kernel_size (int | tuple[int]): Same as nn.Conv3d.\n        stride (int | tuple[int]): Same as nn.Conv3d.\n        padding (int | tuple[int]): Same as nn.Conv3d.\n        dilation (int | tuple[int]): Same as nn.Conv3d.",
        "detail": "SVTAS.svtas.model.backbones.utils.conv2plus1d.conv2plus1d",
        "documentation": {}
    },
    {
        "label": "CorrelationFunction",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.liteflownet_v3.correlation_package.correlation",
        "description": "SVTAS.svtas.model.backbones.utils.liteflownet_v3.correlation_package.correlation",
        "peekOfCode": "class CorrelationFunction(Function):\n    @staticmethod\n    def forward(ctx, input1, input2, pad_size=3, kernel_size=3, max_displacement=20, stride1=1, stride2=2, corr_multiply=1):\n        ctx.save_for_backward(input1, input2)\n        ctx.pad_size = pad_size\n        ctx.kernel_size = kernel_size\n        ctx.max_displacement = max_displacement\n        ctx.stride1 = stride1\n        ctx.stride2 = stride2\n        ctx.corr_multiply = corr_multiply",
        "detail": "SVTAS.svtas.model.backbones.utils.liteflownet_v3.correlation_package.correlation",
        "documentation": {}
    },
    {
        "label": "Correlation",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.liteflownet_v3.correlation_package.correlation",
        "description": "SVTAS.svtas.model.backbones.utils.liteflownet_v3.correlation_package.correlation",
        "peekOfCode": "class Correlation(Module):\n    def __init__(self, pad_size=0, kernel_size=0, max_displacement=0, stride1=1, stride2=2, corr_multiply=1):\n        super(Correlation, self).__init__()\n        self.pad_size = pad_size\n        self.kernel_size = kernel_size\n        self.max_displacement = max_displacement\n        self.stride1 = stride1\n        self.stride2 = stride2\n        self.corr_multiply = corr_multiply\n    def forward(self, input1, input2):",
        "detail": "SVTAS.svtas.model.backbones.utils.liteflownet_v3.correlation_package.correlation",
        "documentation": {}
    },
    {
        "label": "cxx_args",
        "kind": 5,
        "importPath": "SVTAS.svtas.model.backbones.utils.liteflownet_v3.correlation_package.setup",
        "description": "SVTAS.svtas.model.backbones.utils.liteflownet_v3.correlation_package.setup",
        "peekOfCode": "cxx_args = ['-std=c++14']\nnvcc_args = [\n    '-gencode', 'arch=compute_52,code=sm_52',\n    '-gencode', 'arch=compute_60,code=sm_60',\n    '-gencode', 'arch=compute_61,code=sm_61',\n    '-gencode', 'arch=compute_70,code=sm_70',\n    '-gencode', 'arch=compute_70,code=sm_70',\n    '-gencode', 'arch=compute_75,code=sm_75',\n    '-gencode', 'arch=compute_80,code=sm_80',\n    '-gencode', 'arch=compute_86,code=sm_86',",
        "detail": "SVTAS.svtas.model.backbones.utils.liteflownet_v3.correlation_package.setup",
        "documentation": {}
    },
    {
        "label": "nvcc_args",
        "kind": 5,
        "importPath": "SVTAS.svtas.model.backbones.utils.liteflownet_v3.correlation_package.setup",
        "description": "SVTAS.svtas.model.backbones.utils.liteflownet_v3.correlation_package.setup",
        "peekOfCode": "nvcc_args = [\n    '-gencode', 'arch=compute_52,code=sm_52',\n    '-gencode', 'arch=compute_60,code=sm_60',\n    '-gencode', 'arch=compute_61,code=sm_61',\n    '-gencode', 'arch=compute_70,code=sm_70',\n    '-gencode', 'arch=compute_70,code=sm_70',\n    '-gencode', 'arch=compute_75,code=sm_75',\n    '-gencode', 'arch=compute_80,code=sm_80',\n    '-gencode', 'arch=compute_86,code=sm_86',\n    '-gencode', 'arch=compute_86,code=compute_86',",
        "detail": "SVTAS.svtas.model.backbones.utils.liteflownet_v3.correlation_package.setup",
        "documentation": {}
    },
    {
        "label": "BackWarp",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.liteflownet_v3.layers",
        "description": "SVTAS.svtas.model.backbones.utils.liteflownet_v3.layers",
        "peekOfCode": "class BackWarp(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backwarp_tenGrid = {}\n    def forward(self, tenInput, tenFlow):\n        if str(tenFlow.shape) not in self.backwarp_tenGrid:\n            tenHor = torch.linspace(-1.0 + (1.0 / tenFlow.shape[3]), 1.0 - (1.0 / tenFlow.shape[3]), tenFlow.shape[3]).view(1, 1, 1, -1).expand(-1, -1, tenFlow.shape[2], -1)\n            tenVer = torch.linspace(-1.0 + (1.0 / tenFlow.shape[2]), 1.0 - (1.0 / tenFlow.shape[2]), tenFlow.shape[2]).view(1, 1, -1, 1).expand(-1, -1, -1, tenFlow.shape[3])\n            self.backwarp_tenGrid[str(tenFlow.shape)] = torch.cat([ tenHor, tenVer ], 1).cuda()\n        tenFlow = torch.cat([ tenFlow[:, 0:1, :, :] / ((tenInput.shape[3] - 1.0) / 2.0), tenFlow[:, 1:2, :, :] / ((tenInput.shape[2] - 1.0) / 2.0) ], 1)",
        "detail": "SVTAS.svtas.model.backbones.utils.liteflownet_v3.layers",
        "documentation": {}
    },
    {
        "label": "Features",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.liteflownet_v3.layers",
        "description": "SVTAS.svtas.model.backbones.utils.liteflownet_v3.layers",
        "peekOfCode": "class Features(nn.Module):\n    def __init__(self):\n        super(Features, self).__init__()\n        self.netOne = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=7, stride=1, padding=3),\n            nn.LeakyReLU(inplace=False, negative_slope=0.1)\n        )\n        self.netTwo = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1),\n            nn.LeakyReLU(inplace=False, negative_slope=0.1),",
        "detail": "SVTAS.svtas.model.backbones.utils.liteflownet_v3.layers",
        "documentation": {}
    },
    {
        "label": "Matching",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.liteflownet_v3.layers",
        "description": "SVTAS.svtas.model.backbones.utils.liteflownet_v3.layers",
        "peekOfCode": "class Matching(nn.Module):\n    def __init__(self, intLevel):\n        super(Matching, self).__init__()\n        self.fltBackwarp = [ 0.0, 0.0, 0.0, 5.0, 2.5, 1.25, 0.625 ][intLevel]\n        self.crossCorr = Correlation(pad_size=4, kernel_size=1, max_displacement=4, stride1=1, stride2=1)\n        if intLevel == 4:\n            self.autoCorr = Correlation(pad_size=6, kernel_size=1, max_displacement=6, stride1=1, stride2=2)\n        elif intLevel == 3:\n            self.autoCorr = Correlation(pad_size=8, kernel_size=1, max_displacement=8, stride1=1, stride2=2)\n        if intLevel > 4:",
        "detail": "SVTAS.svtas.model.backbones.utils.liteflownet_v3.layers",
        "documentation": {}
    },
    {
        "label": "Subpixel",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.liteflownet_v3.layers",
        "description": "SVTAS.svtas.model.backbones.utils.liteflownet_v3.layers",
        "peekOfCode": "class Subpixel(nn.Module):\n    def __init__(self, intLevel):\n        super(Subpixel, self).__init__()\n        self.fltBackward = [ 0.0, 0.0, 0.0, 5.0, 2.5, 1.25, 0.625 ][intLevel]\n        self.netMain = nn.Sequential(\n            nn.Conv2d(in_channels=[ 0, 0, 0, 130, 194, 258, 386 ][intLevel], out_channels=128, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(inplace=False, negative_slope=0.1),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(inplace=False, negative_slope=0.1),\n            nn.Conv2d(in_channels=128, out_channels=96, kernel_size=3, stride=1, padding=1),",
        "detail": "SVTAS.svtas.model.backbones.utils.liteflownet_v3.layers",
        "documentation": {}
    },
    {
        "label": "Regularization",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.liteflownet_v3.layers",
        "description": "SVTAS.svtas.model.backbones.utils.liteflownet_v3.layers",
        "peekOfCode": "class Regularization(nn.Module):\n    def __init__(self, intLevel):\n        super(Regularization, self).__init__()\n        self.fltBackward = [ 0.0, 0.0, 0.0, 5.0, 2.5, 1.25, 0.625 ][intLevel]\n        self.intUnfold = [ 0, 0, 7, 5, 5, 3, 3 ][intLevel]\n        if intLevel > 4:\n            self.netFeat = nn.Sequential()\n        elif intLevel <= 4:\n            self.netFeat = nn.Sequential(\n                nn.Conv2d(in_channels=[ 0, 0, 32, 64, 96, 128, 192 ][intLevel], out_channels=128, kernel_size=1, stride=1, padding=0),",
        "detail": "SVTAS.svtas.model.backbones.utils.liteflownet_v3.layers",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.mvit.attention",
        "description": "SVTAS.svtas.model.backbones.utils.mvit.attention",
        "peekOfCode": "class PatchEmbed(nn.Module):\n    \"\"\"\n    PatchEmbed.\n    \"\"\"\n    def __init__(\n        self,\n        dim_in=3,\n        dim_out=768,\n        kernel=(1, 16, 16),\n        stride=(1, 4, 4),",
        "detail": "SVTAS.svtas.model.backbones.utils.mvit.attention",
        "documentation": {}
    },
    {
        "label": "MultiScaleAttention",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.mvit.attention",
        "description": "SVTAS.svtas.model.backbones.utils.mvit.attention",
        "peekOfCode": "class MultiScaleAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        input_size,\n        num_heads=8,\n        qkv_bias=False,\n        drop_rate=0.0,\n        kernel_q=(1, 1, 1),",
        "detail": "SVTAS.svtas.model.backbones.utils.mvit.attention",
        "documentation": {}
    },
    {
        "label": "MultiScaleBlock",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.mvit.attention",
        "description": "SVTAS.svtas.model.backbones.utils.mvit.attention",
        "peekOfCode": "class MultiScaleBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out,\n        num_heads,\n        input_size,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,",
        "detail": "SVTAS.svtas.model.backbones.utils.mvit.attention",
        "documentation": {}
    },
    {
        "label": "attention_pool",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.utils.mvit.attention",
        "description": "SVTAS.svtas.model.backbones.utils.mvit.attention",
        "peekOfCode": "def attention_pool(tensor, pool, thw_shape, has_cls_embed=True, norm=None):\n    if pool is None:\n        return tensor, thw_shape\n    tensor_dim = tensor.ndim\n    if tensor_dim == 4:\n        pass\n    elif tensor_dim == 3:\n        tensor = tensor.unsqueeze(1)\n    else:\n        raise NotImplementedError(f\"Unsupported input dimension {tensor.shape}\")",
        "detail": "SVTAS.svtas.model.backbones.utils.mvit.attention",
        "documentation": {}
    },
    {
        "label": "get_rel_pos",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.utils.mvit.attention",
        "description": "SVTAS.svtas.model.backbones.utils.mvit.attention",
        "peekOfCode": "def get_rel_pos(rel_pos, d):\n    if isinstance(d, int):\n        ori_d = rel_pos.shape[0]\n        if ori_d == d:\n            return rel_pos\n        else:\n            # Interpolate rel pos.\n            new_pos_embed = F.interpolate(\n                rel_pos.reshape(1, ori_d, -1).permute(0, 2, 1),\n                size=d,",
        "detail": "SVTAS.svtas.model.backbones.utils.mvit.attention",
        "documentation": {}
    },
    {
        "label": "cal_rel_pos_spatial",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.utils.mvit.attention",
        "description": "SVTAS.svtas.model.backbones.utils.mvit.attention",
        "peekOfCode": "def cal_rel_pos_spatial(\n    attn, q, k, has_cls_embed, q_shape, k_shape, rel_pos_h, rel_pos_w\n):\n    \"\"\"\n    Decomposed Spatial Relative Positional Embeddings.\n    \"\"\"\n    sp_idx = 1 if has_cls_embed else 0\n    q_t, q_h, q_w = q_shape\n    k_t, k_h, k_w = k_shape\n    dh = int(2 * max(q_h, k_h) - 1)",
        "detail": "SVTAS.svtas.model.backbones.utils.mvit.attention",
        "documentation": {}
    },
    {
        "label": "cal_rel_pos_temporal",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.utils.mvit.attention",
        "description": "SVTAS.svtas.model.backbones.utils.mvit.attention",
        "peekOfCode": "def cal_rel_pos_temporal(attn, q, has_cls_embed, q_shape, k_shape, rel_pos_t):\n    \"\"\"\n    Temporal Relative Positional Embeddings.\n    \"\"\"\n    sp_idx = 1 if has_cls_embed else 0\n    q_t, q_h, q_w = q_shape\n    k_t, k_h, k_w = k_shape\n    dt = int(2 * max(q_t, k_t) - 1)\n    # Intepolate rel pos if needed.\n    rel_pos_t = get_rel_pos(rel_pos_t, dt)",
        "detail": "SVTAS.svtas.model.backbones.utils.mvit.attention",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.mvit.common",
        "description": "SVTAS.svtas.model.backbones.utils.mvit.common",
        "peekOfCode": "class Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop_rate=0.0,\n    ):\n        super().__init__()",
        "detail": "SVTAS.svtas.model.backbones.utils.mvit.common",
        "documentation": {}
    },
    {
        "label": "Permute",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.mvit.common",
        "description": "SVTAS.svtas.model.backbones.utils.mvit.common",
        "peekOfCode": "class Permute(nn.Module):\n    def __init__(self, dims):\n        super().__init__()\n        self.dims = dims\n    def forward(self, x):\n        return x.permute(*self.dims)\ndef drop_path(x, drop_prob: float = 0.0, training: bool = False):\n    \"\"\"\n    Stochastic Depth per sample.\n    \"\"\"",
        "detail": "SVTAS.svtas.model.backbones.utils.mvit.common",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.mvit.common",
        "description": "SVTAS.svtas.model.backbones.utils.mvit.common",
        "peekOfCode": "class DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\nclass TwoStreamFusion(nn.Module):\n    def __init__(self, mode, dim=None, kernel=3, padding=1):\n        \"\"\"",
        "detail": "SVTAS.svtas.model.backbones.utils.mvit.common",
        "documentation": {}
    },
    {
        "label": "TwoStreamFusion",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.mvit.common",
        "description": "SVTAS.svtas.model.backbones.utils.mvit.common",
        "peekOfCode": "class TwoStreamFusion(nn.Module):\n    def __init__(self, mode, dim=None, kernel=3, padding=1):\n        \"\"\"\n        A general constructor for neural modules fusing two equal sized tensors\n        in forward. Following options are supported:\n        \"add\" / \"max\" / \"min\" / \"avg\"             : respective operations on the two halves.\n        \"concat\"                                  : NOOP.\n        \"concat_linear_{dim_mult}_{drop_rate}\"    : MLP to fuse with hidden dim \"dim_mult\"\n                                                    (optional, def 1.) higher than input dim\n                                                    with optional dropout \"drop_rate\" (def: 0.)",
        "detail": "SVTAS.svtas.model.backbones.utils.mvit.common",
        "documentation": {}
    },
    {
        "label": "drop_path",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.utils.mvit.common",
        "description": "SVTAS.svtas.model.backbones.utils.mvit.common",
        "peekOfCode": "def drop_path(x, drop_prob: float = 0.0, training: bool = False):\n    \"\"\"\n    Stochastic Depth per sample.\n    \"\"\"\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (\n        x.ndim - 1\n    )  # work with diff dim tensors, not just 2D ConvNets",
        "detail": "SVTAS.svtas.model.backbones.utils.mvit.common",
        "documentation": {}
    },
    {
        "label": "get_3d_sincos_pos_embed",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.utils.mvit.utils",
        "description": "SVTAS.svtas.model.backbones.utils.mvit.utils",
        "peekOfCode": "def get_3d_sincos_pos_embed(embed_dim, grid_size, t_size, cls_token=False):\n    \"\"\"\n    grid_size: int of the grid height and width\n    t_size: int of the temporal size\n    return:\n    pos_embed: [t_size*grid_size*grid_size, embed_dim] or [1+t_size*grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n    \"\"\"\n    assert embed_dim % 4 == 0\n    embed_dim_spatial = embed_dim // 4 * 3\n    embed_dim_temporal = embed_dim // 4",
        "detail": "SVTAS.svtas.model.backbones.utils.mvit.utils",
        "documentation": {}
    },
    {
        "label": "get_2d_sincos_pos_embed",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.utils.mvit.utils",
        "description": "SVTAS.svtas.model.backbones.utils.mvit.utils",
        "peekOfCode": "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n    \"\"\"\n    grid_size: int of the grid height and width\n    return:\n    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n    \"\"\"\n    grid_h = np.arange(grid_size, dtype=np.float32)\n    grid_w = np.arange(grid_size, dtype=np.float32)\n    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n    grid = np.stack(grid, axis=0)",
        "detail": "SVTAS.svtas.model.backbones.utils.mvit.utils",
        "documentation": {}
    },
    {
        "label": "get_2d_sincos_pos_embed_from_grid",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.utils.mvit.utils",
        "description": "SVTAS.svtas.model.backbones.utils.mvit.utils",
        "peekOfCode": "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n    assert embed_dim % 2 == 0\n    # use half of dimensions to encode grid_h\n    emb_h = get_1d_sincos_pos_embed_from_grid(\n        embed_dim // 2, grid[0]\n    )  # (H*W, D/2)\n    emb_w = get_1d_sincos_pos_embed_from_grid(\n        embed_dim // 2, grid[1]\n    )  # (H*W, D/2)\n    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)",
        "detail": "SVTAS.svtas.model.backbones.utils.mvit.utils",
        "documentation": {}
    },
    {
        "label": "get_1d_sincos_pos_embed_from_grid",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.utils.mvit.utils",
        "description": "SVTAS.svtas.model.backbones.utils.mvit.utils",
        "peekOfCode": "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n    \"\"\"\n    embed_dim: output dimension for each position\n    pos: a list of positions to be encoded: size (M,)\n    out: (M, D)\n    \"\"\"\n    assert embed_dim % 2 == 0\n    omega = np.arange(embed_dim // 2, dtype=np.float)\n    omega /= embed_dim / 2.0\n    omega = 1.0 / 10000**omega  # (D/2,)",
        "detail": "SVTAS.svtas.model.backbones.utils.mvit.utils",
        "documentation": {}
    },
    {
        "label": "calc_mvit_feature_geometry",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.utils.mvit.utils",
        "description": "SVTAS.svtas.model.backbones.utils.mvit.utils",
        "peekOfCode": "def calc_mvit_feature_geometry(num_frames,\n                               patch_stride,\n                               crop_size,\n                               depth,\n                               pool_q_stride):\n    feat_size = [\n        [\n            num_frames // patch_stride[0]\n            if len(patch_stride) > 2\n            else 1,",
        "detail": "SVTAS.svtas.model.backbones.utils.mvit.utils",
        "documentation": {}
    },
    {
        "label": "round_width",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.utils.mvit.utils",
        "description": "SVTAS.svtas.model.backbones.utils.mvit.utils",
        "peekOfCode": "def round_width(width, multiplier, min_width=1, divisor=1, verbose=False):\n    if not multiplier:\n        return width\n    width *= multiplier\n    min_width = min_width or divisor\n    if verbose:\n        logger = get_logger(\"SVTAS\")\n        logger.info(f\"min width {min_width}\")\n        logger.info(f\"width {width} divisor {divisor}\")\n        logger.info(f\"other {int(width + divisor / 2) // divisor * divisor}\")",
        "detail": "SVTAS.svtas.model.backbones.utils.mvit.utils",
        "documentation": {}
    },
    {
        "label": "CorrBlock",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.raft.corr",
        "description": "SVTAS.svtas.model.backbones.utils.raft.corr",
        "peekOfCode": "class CorrBlock:\n    def __init__(self, fmap1, fmap2, num_levels=4, radius=4):\n        self.num_levels = num_levels\n        self.radius = radius\n        self.corr_pyramid = []\n        # all pairs correlation\n        corr = CorrBlock.corr(fmap1, fmap2)\n        batch, h1, w1, dim, h2, w2 = corr.shape\n        corr = corr.reshape(batch*h1*w1, dim, h2, w2)\n        self.corr_pyramid.append(corr)",
        "detail": "SVTAS.svtas.model.backbones.utils.raft.corr",
        "documentation": {}
    },
    {
        "label": "AlternateCorrBlock",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.raft.corr",
        "description": "SVTAS.svtas.model.backbones.utils.raft.corr",
        "peekOfCode": "class AlternateCorrBlock:\n    def __init__(self, fmap1, fmap2, num_levels=4, radius=4):\n        self.num_levels = num_levels\n        self.radius = radius\n        self.pyramid = [(fmap1, fmap2)]\n        for i in range(self.num_levels):\n            fmap1 = F.avg_pool2d(fmap1, 2, stride=2)\n            fmap2 = F.avg_pool2d(fmap2, 2, stride=2)\n            self.pyramid.append((fmap1, fmap2))\n    def __call__(self, coords):",
        "detail": "SVTAS.svtas.model.backbones.utils.raft.corr",
        "documentation": {}
    },
    {
        "label": "ResidualBlock",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.raft.extractor",
        "description": "SVTAS.svtas.model.backbones.utils.raft.extractor",
        "peekOfCode": "class ResidualBlock(nn.Module):\n    def __init__(self, in_planes, planes, norm_fn='group', stride=1):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, stride=stride)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        num_groups = planes // 8\n        if norm_fn == 'group':\n            self.norm1 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)\n            self.norm2 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)",
        "detail": "SVTAS.svtas.model.backbones.utils.raft.extractor",
        "documentation": {}
    },
    {
        "label": "BottleneckBlock",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.raft.extractor",
        "description": "SVTAS.svtas.model.backbones.utils.raft.extractor",
        "peekOfCode": "class BottleneckBlock(nn.Module):\n    def __init__(self, in_planes, planes, norm_fn='group', stride=1):\n        super(BottleneckBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes//4, kernel_size=1, padding=0)\n        self.conv2 = nn.Conv2d(planes//4, planes//4, kernel_size=3, padding=1, stride=stride)\n        self.conv3 = nn.Conv2d(planes//4, planes, kernel_size=1, padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        num_groups = planes // 8\n        if norm_fn == 'group':\n            self.norm1 = nn.GroupNorm(num_groups=num_groups, num_channels=planes//4)",
        "detail": "SVTAS.svtas.model.backbones.utils.raft.extractor",
        "documentation": {}
    },
    {
        "label": "BasicEncoder",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.raft.extractor",
        "description": "SVTAS.svtas.model.backbones.utils.raft.extractor",
        "peekOfCode": "class BasicEncoder(nn.Module):\n    def __init__(self, output_dim=128, norm_fn='batch', dropout=0.0):\n        super(BasicEncoder, self).__init__()\n        self.norm_fn = norm_fn\n        if self.norm_fn == 'group':\n            self.norm1 = nn.GroupNorm(num_groups=8, num_channels=64)\n        elif self.norm_fn == 'batch':\n            self.norm1 = nn.BatchNorm2d(64)\n        elif self.norm_fn == 'instance':\n            self.norm1 = nn.InstanceNorm2d(64)",
        "detail": "SVTAS.svtas.model.backbones.utils.raft.extractor",
        "documentation": {}
    },
    {
        "label": "SmallEncoder",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.raft.extractor",
        "description": "SVTAS.svtas.model.backbones.utils.raft.extractor",
        "peekOfCode": "class SmallEncoder(nn.Module):\n    def __init__(self, output_dim=128, norm_fn='batch', dropout=0.0):\n        super(SmallEncoder, self).__init__()\n        self.norm_fn = norm_fn\n        if self.norm_fn == 'group':\n            self.norm1 = nn.GroupNorm(num_groups=8, num_channels=32)\n        elif self.norm_fn == 'batch':\n            self.norm1 = nn.BatchNorm2d(32)\n        elif self.norm_fn == 'instance':\n            self.norm1 = nn.InstanceNorm2d(32)",
        "detail": "SVTAS.svtas.model.backbones.utils.raft.extractor",
        "documentation": {}
    },
    {
        "label": "FlowHead",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.raft.update",
        "description": "SVTAS.svtas.model.backbones.utils.raft.update",
        "peekOfCode": "class FlowHead(nn.Module):\n    def __init__(self, input_dim=128, hidden_dim=256):\n        super(FlowHead, self).__init__()\n        self.conv1 = nn.Conv2d(input_dim, hidden_dim, 3, padding=1)\n        self.conv2 = nn.Conv2d(hidden_dim, 2, 3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n    def forward(self, x):\n        return self.conv2(self.relu(self.conv1(x)))\nclass ConvGRU(nn.Module):\n    def __init__(self, hidden_dim=128, input_dim=192+128):",
        "detail": "SVTAS.svtas.model.backbones.utils.raft.update",
        "documentation": {}
    },
    {
        "label": "ConvGRU",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.raft.update",
        "description": "SVTAS.svtas.model.backbones.utils.raft.update",
        "peekOfCode": "class ConvGRU(nn.Module):\n    def __init__(self, hidden_dim=128, input_dim=192+128):\n        super(ConvGRU, self).__init__()\n        self.convz = nn.Conv2d(hidden_dim+input_dim, hidden_dim, 3, padding=1)\n        self.convr = nn.Conv2d(hidden_dim+input_dim, hidden_dim, 3, padding=1)\n        self.convq = nn.Conv2d(hidden_dim+input_dim, hidden_dim, 3, padding=1)\n    def forward(self, h, x):\n        hx = torch.cat([h, x], dim=1)\n        z = torch.sigmoid(self.convz(hx))\n        r = torch.sigmoid(self.convr(hx))",
        "detail": "SVTAS.svtas.model.backbones.utils.raft.update",
        "documentation": {}
    },
    {
        "label": "SepConvGRU",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.raft.update",
        "description": "SVTAS.svtas.model.backbones.utils.raft.update",
        "peekOfCode": "class SepConvGRU(nn.Module):\n    def __init__(self, hidden_dim=128, input_dim=192+128):\n        super(SepConvGRU, self).__init__()\n        self.convz1 = nn.Conv2d(hidden_dim+input_dim, hidden_dim, (1,5), padding=(0,2))\n        self.convr1 = nn.Conv2d(hidden_dim+input_dim, hidden_dim, (1,5), padding=(0,2))\n        self.convq1 = nn.Conv2d(hidden_dim+input_dim, hidden_dim, (1,5), padding=(0,2))\n        self.convz2 = nn.Conv2d(hidden_dim+input_dim, hidden_dim, (5,1), padding=(2,0))\n        self.convr2 = nn.Conv2d(hidden_dim+input_dim, hidden_dim, (5,1), padding=(2,0))\n        self.convq2 = nn.Conv2d(hidden_dim+input_dim, hidden_dim, (5,1), padding=(2,0))\n    def forward(self, h, x):",
        "detail": "SVTAS.svtas.model.backbones.utils.raft.update",
        "documentation": {}
    },
    {
        "label": "SmallMotionEncoder",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.raft.update",
        "description": "SVTAS.svtas.model.backbones.utils.raft.update",
        "peekOfCode": "class SmallMotionEncoder(nn.Module):\n    def __init__(self, corr_levels, corr_radius):\n        super(SmallMotionEncoder, self).__init__()\n        cor_planes = corr_levels * (2*corr_radius + 1)**2\n        self.convc1 = nn.Conv2d(cor_planes, 96, 1, padding=0)\n        self.convf1 = nn.Conv2d(2, 64, 7, padding=3)\n        self.convf2 = nn.Conv2d(64, 32, 3, padding=1)\n        self.conv = nn.Conv2d(128, 80, 3, padding=1)\n    def forward(self, flow, corr):\n        cor = F.relu(self.convc1(corr))",
        "detail": "SVTAS.svtas.model.backbones.utils.raft.update",
        "documentation": {}
    },
    {
        "label": "BasicMotionEncoder",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.raft.update",
        "description": "SVTAS.svtas.model.backbones.utils.raft.update",
        "peekOfCode": "class BasicMotionEncoder(nn.Module):\n    def __init__(self, corr_levels, corr_radius):\n        super(BasicMotionEncoder, self).__init__()\n        cor_planes = corr_levels * (2*corr_radius + 1)**2\n        self.convc1 = nn.Conv2d(cor_planes, 256, 1, padding=0)\n        self.convc2 = nn.Conv2d(256, 192, 3, padding=1)\n        self.convf1 = nn.Conv2d(2, 128, 7, padding=3)\n        self.convf2 = nn.Conv2d(128, 64, 3, padding=1)\n        self.conv = nn.Conv2d(64+192, 128-2, 3, padding=1)\n    def forward(self, flow, corr):",
        "detail": "SVTAS.svtas.model.backbones.utils.raft.update",
        "documentation": {}
    },
    {
        "label": "SmallUpdateBlock",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.raft.update",
        "description": "SVTAS.svtas.model.backbones.utils.raft.update",
        "peekOfCode": "class SmallUpdateBlock(nn.Module):\n    def __init__(self, corr_levels, corr_radius, hidden_dim=96):\n        super(SmallUpdateBlock, self).__init__()\n        self.encoder = SmallMotionEncoder(corr_levels, corr_radius)\n        self.gru = ConvGRU(hidden_dim=hidden_dim, input_dim=82+64)\n        self.flow_head = FlowHead(hidden_dim, hidden_dim=128)\n    def forward(self, net, inp, corr, flow):\n        motion_features = self.encoder(flow, corr)\n        inp = torch.cat([inp, motion_features], dim=1)\n        net = self.gru(net, inp)",
        "detail": "SVTAS.svtas.model.backbones.utils.raft.update",
        "documentation": {}
    },
    {
        "label": "BasicUpdateBlock",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.raft.update",
        "description": "SVTAS.svtas.model.backbones.utils.raft.update",
        "peekOfCode": "class BasicUpdateBlock(nn.Module):\n    def __init__(self, corr_levels, corr_radius, hidden_dim=128, input_dim=128):\n        super(BasicUpdateBlock, self).__init__()\n        self.encoder = BasicMotionEncoder(corr_levels, corr_radius)\n        self.gru = SepConvGRU(hidden_dim=hidden_dim, input_dim=128+hidden_dim)\n        self.flow_head = FlowHead(hidden_dim, hidden_dim=256)\n        self.mask = nn.Sequential(\n            nn.Conv2d(128, 256, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 64*9, 1, padding=0))",
        "detail": "SVTAS.svtas.model.backbones.utils.raft.update",
        "documentation": {}
    },
    {
        "label": "InputPadder",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.raft.utils",
        "description": "SVTAS.svtas.model.backbones.utils.raft.utils",
        "peekOfCode": "class InputPadder:\n    \"\"\" Pads images such that dimensions are divisible by 8 \"\"\"\n    def __init__(self, dims, mode='sintel'):\n        self.ht, self.wd = dims[-2:]\n        pad_ht = (((self.ht // 8) + 1) * 8 - self.ht) % 8\n        pad_wd = (((self.wd // 8) + 1) * 8 - self.wd) % 8\n        if mode == 'sintel':\n            self._pad = [pad_wd//2, pad_wd - pad_wd//2, pad_ht//2, pad_ht - pad_ht//2]\n        else:\n            self._pad = [pad_wd//2, pad_wd - pad_wd//2, 0, pad_ht]",
        "detail": "SVTAS.svtas.model.backbones.utils.raft.utils",
        "documentation": {}
    },
    {
        "label": "forward_interpolate",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.utils.raft.utils",
        "description": "SVTAS.svtas.model.backbones.utils.raft.utils",
        "peekOfCode": "def forward_interpolate(flow):\n    flow = flow.detach().cpu().numpy()\n    dx, dy = flow[0], flow[1]\n    ht, wd = dx.shape\n    x0, y0 = np.meshgrid(np.arange(wd), np.arange(ht))\n    x1 = x0 + dx\n    y1 = y0 + dy\n    x1 = x1.reshape(-1)\n    y1 = y1.reshape(-1)\n    dx = dx.reshape(-1)",
        "detail": "SVTAS.svtas.model.backbones.utils.raft.utils",
        "documentation": {}
    },
    {
        "label": "bilinear_sampler",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.utils.raft.utils",
        "description": "SVTAS.svtas.model.backbones.utils.raft.utils",
        "peekOfCode": "def bilinear_sampler(img, coords, mode='bilinear', mask=False):\n    \"\"\" Wrapper for grid_sample, uses pixel coordinates \"\"\"\n    H, W = img.shape[-2:]\n    xgrid, ygrid = coords.split([1, 1], dim=-1)\n    xgrid = 2*xgrid/(W-1) - 1\n    ygrid = 2*ygrid/(H-1) - 1\n    grid = torch.cat([xgrid, ygrid], dim=-1)\n    img = F.grid_sample(img, grid, align_corners=True)\n    if mask:\n        mask = (xgrid > -1) & (ygrid > -1) & (xgrid < 1) & (ygrid < 1)",
        "detail": "SVTAS.svtas.model.backbones.utils.raft.utils",
        "documentation": {}
    },
    {
        "label": "coords_grid",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.utils.raft.utils",
        "description": "SVTAS.svtas.model.backbones.utils.raft.utils",
        "peekOfCode": "def coords_grid(batch, ht, wd):\n    coords = torch.meshgrid(torch.arange(ht), torch.arange(wd))\n    coords = torch.stack(coords[::-1], dim=0).float()\n    return coords[None].repeat(batch, 1, 1, 1)\ndef upflow8(flow, mode='bilinear'):\n    new_size = (8 * flow.shape[2], 8 * flow.shape[3])\n    return 8 * F.interpolate(flow, size=new_size, mode=mode, align_corners=True)",
        "detail": "SVTAS.svtas.model.backbones.utils.raft.utils",
        "documentation": {}
    },
    {
        "label": "upflow8",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.utils.raft.utils",
        "description": "SVTAS.svtas.model.backbones.utils.raft.utils",
        "peekOfCode": "def upflow8(flow, mode='bilinear'):\n    new_size = (8 * flow.shape[2], 8 * flow.shape[3])\n    return 8 * F.interpolate(flow, size=new_size, mode=mode, align_corners=True)",
        "detail": "SVTAS.svtas.model.backbones.utils.raft.utils",
        "documentation": {}
    },
    {
        "label": "SpatioTemporalLSTMCell",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.stlstm.stlstm",
        "description": "SVTAS.svtas.model.backbones.utils.stlstm.stlstm",
        "peekOfCode": "class SpatioTemporalLSTMCell(nn.Module):\n    def __init__(self, in_channel, num_hidden, width, filter_size, stride, layer_norm, is_deep_wise_conv=True):\n        super(SpatioTemporalLSTMCell, self).__init__()\n        self.num_hidden = num_hidden\n        self.padding = filter_size // 2\n        self._forget_bias = 1.0\n        if is_deep_wise_conv is False:\n            if layer_norm:\n                self.conv_x = nn.Sequential(\n                    nn.Conv2d(in_channel, num_hidden * 7, kernel_size=filter_size, stride=stride, padding=self.padding, bias=False),",
        "detail": "SVTAS.svtas.model.backbones.utils.stlstm.stlstm",
        "documentation": {}
    },
    {
        "label": "DividedTemporalAttentionWithNorm",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.timesformer.transformer",
        "description": "SVTAS.svtas.model.backbones.utils.timesformer.transformer",
        "peekOfCode": "class DividedTemporalAttentionWithNorm(BaseModule):\n    \"\"\"Temporal Attention in Divided Space Time Attention.\n    Args:\n        embed_dims (int): Dimensions of embedding.\n        num_heads (int): Number of parallel attention heads in\n            TransformerCoder.\n        num_frames (int): Number of frames in the video.\n        attn_drop (float): A Dropout layer on attn_output_weights. Defaults to\n            0..\n        proj_drop (float): A Dropout layer after `nn.MultiheadAttention`.",
        "detail": "SVTAS.svtas.model.backbones.utils.timesformer.transformer",
        "documentation": {}
    },
    {
        "label": "DividedSpatialAttentionWithNorm",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.timesformer.transformer",
        "description": "SVTAS.svtas.model.backbones.utils.timesformer.transformer",
        "peekOfCode": "class DividedSpatialAttentionWithNorm(BaseModule):\n    \"\"\"Spatial Attention in Divided Space Time Attention.\n    Args:\n        embed_dims (int): Dimensions of embedding.\n        num_heads (int): Number of parallel attention heads in\n            TransformerCoder.\n        num_frames (int): Number of frames in the video.\n        attn_drop (float): A Dropout layer on attn_output_weights. Defaults to\n            0..\n        proj_drop (float): A Dropout layer after `nn.MultiheadAttention`.",
        "detail": "SVTAS.svtas.model.backbones.utils.timesformer.transformer",
        "documentation": {}
    },
    {
        "label": "FFNWithNorm",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.timesformer.transformer",
        "description": "SVTAS.svtas.model.backbones.utils.timesformer.transformer",
        "peekOfCode": "class FFNWithNorm(FFN):\n    \"\"\"FFN with pre normalization layer.\n    FFNWithNorm is implemented to be compatible with `BaseTransformerLayer`\n    when using `DividedTemporalAttentionWithNorm` and\n    `DividedSpatialAttentionWithNorm`.\n    FFNWithNorm has one main difference with FFN:\n    - It apply one normalization layer before forwarding the input data to\n        feed-forward networks.\n    Args:\n        embed_dims (int): Dimensions of embedding. Defaults to 256.",
        "detail": "SVTAS.svtas.model.backbones.utils.timesformer.transformer",
        "documentation": {}
    },
    {
        "label": "ScaledDotProductAttention",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.transducer.attention",
        "description": "SVTAS.svtas.model.backbones.utils.transducer.attention",
        "peekOfCode": "class ScaledDotProductAttention(nn.Module):\n    \"\"\"\n    Compute the dot products of the query with all keys, divide each by sqrt(key_dim),\n    and apply a softmax function to obtain the weights on the values\n    Args: key_dim\n        key_dim (int): dimension of key\n    Inputs: query, key, value\n        - **query** (batch, q_len, hidden_dim): tensor containing projection vector for decoder\n        - **key** (batch, k_len, hidden_dim): tensor containing projection vector for encoder\n        - **value** (batch, v_len, hidden_dim): value and key are the same",
        "detail": "SVTAS.svtas.model.backbones.utils.transducer.attention",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttention",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.transducer.attention",
        "description": "SVTAS.svtas.model.backbones.utils.transducer.attention",
        "peekOfCode": "class MultiHeadAttention(nn.Module):\n    \"\"\"\n    This technique is proposed in this paper. https://arxiv.org/abs/1706.03762\n    Perform the scaled dot-product attention in parallel.\n    Args:\n        model_dim (int): the number of features in the multi-head attention (default : 512)\n        num_heads (int): the number of heads in the multi-head attention (default: 8)\n    Inputs: query, key, value, mask\n        - **query** (batch, q_len, hidden_dim): tensor containing projection vector for encoder\n        - **key** (batch, k_len, hidden_dim): tensor containing projection vector for encoder",
        "detail": "SVTAS.svtas.model.backbones.utils.transducer.attention",
        "documentation": {}
    },
    {
        "label": "EncoderLayer",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.transducer.layer",
        "description": "SVTAS.svtas.model.backbones.utils.transducer.layer",
        "peekOfCode": "class EncoderLayer(nn.Module):\n    \"\"\"\n    Repeated layers common to audio encoders and label encoders\n    Args:\n        model_dim (int): the number of features in the encoder (default : 512)\n        ff_dim (int): the number of features in the feed forward layers (default : 2048)\n        num_heads (int): the number of heads in the multi-head attention (default: 8)\n        dropout (float): dropout probability of encoder layer (default: 0.1)\n    Inputs: inputs, self_attn_mask\n        - **inputs**: Audio feature or label feature",
        "detail": "SVTAS.svtas.model.backbones.utils.transducer.layer",
        "documentation": {}
    },
    {
        "label": "get_attn_pad_mask",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.utils.transducer.mask",
        "description": "SVTAS.svtas.model.backbones.utils.transducer.mask",
        "peekOfCode": "def get_attn_pad_mask(inputs: Tensor, inputs_lens: Tensor, expand_lens):\n    pad_attn_mask = _get_pad_mask(inputs, inputs_lens)\n    pad_attn_mask = pad_attn_mask.unsqueeze(1).repeat(1, expand_lens, 1)  # (batch, dec_T, enc_T)\n    return pad_attn_mask",
        "detail": "SVTAS.svtas.model.backbones.utils.transducer.mask",
        "documentation": {}
    },
    {
        "label": "PositionalEncoding",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.transducer.position_encoding",
        "description": "SVTAS.svtas.model.backbones.utils.transducer.position_encoding",
        "peekOfCode": "class PositionalEncoding(nn.Module):\n    \"\"\"\n    Implement the positional encoding (PE) function.\n    PE_(pos, 2i)    =  sin(pos / 10000 ** (2i / d_model))\n    PE_(pos, 2i+1)  =  cos(pos / 10000 ** (2i / d_model))\n    \"\"\"\n    def __init__(\n            self,\n            d_model: int = 512,\n            max_len: int = 5000",
        "detail": "SVTAS.svtas.model.backbones.utils.transducer.position_encoding",
        "documentation": {}
    },
    {
        "label": "PositionWiseFeedForward",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.transducer.position_encoding",
        "description": "SVTAS.svtas.model.backbones.utils.transducer.position_encoding",
        "peekOfCode": "class PositionWiseFeedForward(nn.Module):\n    \"\"\"\n    Implement position-wise feed forward layer.\n    FFN(x) = max(0, xW1 + b1)W2 + b2\n    \"\"\"\n    def __init__(\n            self,\n            model_dim: int = 512,\n            ff_dim: int = 2048,\n            dropout: float = 0.1,",
        "detail": "SVTAS.svtas.model.backbones.utils.transducer.position_encoding",
        "documentation": {}
    },
    {
        "label": "TemporalShift",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.vit_tsm.temporal_shift_vit",
        "description": "SVTAS.svtas.model.backbones.utils.vit_tsm.temporal_shift_vit",
        "peekOfCode": "class TemporalShift(nn.Module):\n    def __init__(self, net, n_segment=3, n_div=8, inplace=False):\n        super(TemporalShift, self).__init__()\n        self.net = net\n        self.n_segment = n_segment\n        self.fold_div = n_div\n        self.inplace = inplace\n        if inplace:\n            print('=> Using in-place shift...')\n        print('=> Using fold div: {}'.format(self.fold_div))",
        "detail": "SVTAS.svtas.model.backbones.utils.vit_tsm.temporal_shift_vit",
        "documentation": {}
    },
    {
        "label": "TemporalShift_VIT",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.vit_tsm.temporal_shift_vit",
        "description": "SVTAS.svtas.model.backbones.utils.vit_tsm.temporal_shift_vit",
        "peekOfCode": "class TemporalShift_VIT(nn.Module):\n    def __init__(self, net, n_segment=3, n_div=8, inplace=False):\n        super(TemporalShift_VIT, self).__init__()\n        self.net = net\n        self.n_segment = n_segment\n        self.fold_div = n_div\n        self.inplace = inplace\n        if inplace:\n            print('=> Using in-place shift...')\n        print('=> Using fold div: {}'.format(self.fold_div))",
        "detail": "SVTAS.svtas.model.backbones.utils.vit_tsm.temporal_shift_vit",
        "documentation": {}
    },
    {
        "label": "InplaceShift",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.vit_tsm.temporal_shift_vit",
        "description": "SVTAS.svtas.model.backbones.utils.vit_tsm.temporal_shift_vit",
        "peekOfCode": "class InplaceShift(torch.autograd.Function):\n    # Special thanks to @raoyongming for the help to this function\n    @staticmethod\n    def forward(ctx, input, fold):\n        # not support higher order gradient\n        # input = input.detach_()\n        ctx.fold_ = fold\n        n, t, c, h, w = input.size()\n        buffer = input.data.new(n, t, fold, h, w).zero_()\n        buffer[:, :-1] = input.data[:, 1:, :fold]",
        "detail": "SVTAS.svtas.model.backbones.utils.vit_tsm.temporal_shift_vit",
        "documentation": {}
    },
    {
        "label": "TemporalPool",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.utils.vit_tsm.temporal_shift_vit",
        "description": "SVTAS.svtas.model.backbones.utils.vit_tsm.temporal_shift_vit",
        "peekOfCode": "class TemporalPool(nn.Module):\n    def __init__(self, net, n_segment):\n        super(TemporalPool, self).__init__()\n        self.net = net\n        self.n_segment = n_segment\n    def forward(self, x):\n        x = self.temporal_pool(x, n_segment=self.n_segment)\n        return self.net(x)\n    @staticmethod\n    def temporal_pool(x, n_segment):",
        "detail": "SVTAS.svtas.model.backbones.utils.vit_tsm.temporal_shift_vit",
        "documentation": {}
    },
    {
        "label": "make_temporal_pool",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.utils.vit_tsm.temporal_shift_vit",
        "description": "SVTAS.svtas.model.backbones.utils.vit_tsm.temporal_shift_vit",
        "peekOfCode": "def make_temporal_pool(net, n_segment):\n    import torchvision\n    if isinstance(net, torchvision.models.ResNet):\n        print('=> Injecting nonlocal pooling')\n        net.layer2 = TemporalPool(net.layer2, n_segment)\n    else:\n        raise NotImplementedError\nif __name__ == '__main__':\n    # test inplace shift v.s. vanilla shift\n    tsm1 = TemporalShift(nn.Sequential(), n_segment=8, n_div=8, inplace=False)",
        "detail": "SVTAS.svtas.model.backbones.utils.vit_tsm.temporal_shift_vit",
        "documentation": {}
    },
    {
        "label": "Unit3Dpy",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.i3d",
        "description": "SVTAS.svtas.model.backbones.video.i3d",
        "peekOfCode": "class Unit3Dpy(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size=(1, 1, 1),\n                 stride=(1, 1, 1),\n                 activation='relu',\n                 padding='SAME',\n                 use_bias=False,\n                 use_bn=True):",
        "detail": "SVTAS.svtas.model.backbones.video.i3d",
        "documentation": {}
    },
    {
        "label": "MaxPool3dTFPadding",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.i3d",
        "description": "SVTAS.svtas.model.backbones.video.i3d",
        "peekOfCode": "class MaxPool3dTFPadding(nn.Module):\n    def __init__(self, kernel_size, stride=None, padding='SAME'):\n        super(MaxPool3dTFPadding, self).__init__()\n        if padding == 'SAME':\n            padding_shape = get_padding_shape(kernel_size, stride)\n            self.padding_shape = padding_shape\n            self.pad = nn.ConstantPad3d(padding_shape, 0)\n        self.pool = nn.MaxPool3d(kernel_size, stride, ceil_mode=True)\n    def forward(self, inp):\n        inp = self.pad(inp)",
        "detail": "SVTAS.svtas.model.backbones.video.i3d",
        "documentation": {}
    },
    {
        "label": "Mixed",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.i3d",
        "description": "SVTAS.svtas.model.backbones.video.i3d",
        "peekOfCode": "class Mixed(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(Mixed, self).__init__()\n        # Branch 0\n        self.branch_0 = Unit3Dpy(\n            in_channels, out_channels[0], kernel_size=(1, 1, 1))\n        # Branch 1\n        branch_1_conv1 = Unit3Dpy(\n            in_channels, out_channels[1], kernel_size=(1, 1, 1))\n        branch_1_conv2 = Unit3Dpy(",
        "detail": "SVTAS.svtas.model.backbones.video.i3d",
        "documentation": {}
    },
    {
        "label": "I3D",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.i3d",
        "description": "SVTAS.svtas.model.backbones.video.i3d",
        "peekOfCode": "class I3D(nn.Module):\n    def __init__(self,\n                 in_channels=3,\n                 pretrained=None,\n                 modality='rgb'):\n        super(I3D, self).__init__()\n        self.modality = modality\n        self.pretrained = pretrained\n        conv3d_1a_7x7 = Unit3Dpy(\n            out_channels=64,",
        "detail": "SVTAS.svtas.model.backbones.video.i3d",
        "documentation": {}
    },
    {
        "label": "get_padding_shape",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.video.i3d",
        "description": "SVTAS.svtas.model.backbones.video.i3d",
        "peekOfCode": "def get_padding_shape(filter_shape, stride):\n    def _pad_top_bottom(filter_dim, stride_val):\n        pad_along = max(filter_dim - stride_val, 0)\n        pad_top = pad_along // 2\n        pad_bottom = pad_along - pad_top\n        return pad_top, pad_bottom\n    padding_shape = []\n    for filter_dim, stride_val in zip(filter_shape, stride):\n        pad_top, pad_bottom = _pad_top_bottom(filter_dim, stride_val)\n        padding_shape.append(pad_top)",
        "detail": "SVTAS.svtas.model.backbones.video.i3d",
        "documentation": {}
    },
    {
        "label": "simplify_padding",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.video.i3d",
        "description": "SVTAS.svtas.model.backbones.video.i3d",
        "peekOfCode": "def simplify_padding(padding_shapes):\n    all_same = True\n    padding_init = padding_shapes[0]\n    for pad in padding_shapes[1:]:\n        if pad != padding_init:\n            all_same = False\n    return all_same, padding_init\nclass Unit3Dpy(nn.Module):\n    def __init__(self,\n                 in_channels,",
        "detail": "SVTAS.svtas.model.backbones.video.i3d",
        "documentation": {}
    },
    {
        "label": "Conv3DLSTMCell",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.mobilenet_v2_tmm",
        "description": "SVTAS.svtas.model.backbones.video.mobilenet_v2_tmm",
        "peekOfCode": "class Conv3DLSTMCell(nn.Module):\n    def __init__(self, in_channels, hidden_channels, kernel_size, bias):\n        super(Conv3DLSTMCell, self).__init__()\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.kernel_size = kernel_size\n        self.bias = bias\n        self.padding = kernel_size[0] // 2, kernel_size[1] // 2, kernel_size[2] // 2\n        # deepwise conv\n        conv_cfg=dict(type='Conv3d')",
        "detail": "SVTAS.svtas.model.backbones.video.mobilenet_v2_tmm",
        "documentation": {}
    },
    {
        "label": "Conv3DLSTM",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.mobilenet_v2_tmm",
        "description": "SVTAS.svtas.model.backbones.video.mobilenet_v2_tmm",
        "peekOfCode": "class Conv3DLSTM(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 hidden_channels,\n                 kernel_size,\n                 num_layers,\n                 batch_first=True,\n                 bias=True):\n        super(Conv3DLSTM, self).__init__()\n        self._check_kernel_size_consistency(kernel_size)",
        "detail": "SVTAS.svtas.model.backbones.video.mobilenet_v2_tmm",
        "documentation": {}
    },
    {
        "label": "TemporalMemoryBlock",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.mobilenet_v2_tmm",
        "description": "SVTAS.svtas.model.backbones.video.mobilenet_v2_tmm",
        "peekOfCode": "class TemporalMemoryBlock(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 hidden_channels,\n                 kernel_size,\n                 num_layers,\n                 num_segments,\n                 batch_first=True,\n                 bias=True):\n        super().__init__()",
        "detail": "SVTAS.svtas.model.backbones.video.mobilenet_v2_tmm",
        "documentation": {}
    },
    {
        "label": "MobileNetV2TMM",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.mobilenet_v2_tmm",
        "description": "SVTAS.svtas.model.backbones.video.mobilenet_v2_tmm",
        "peekOfCode": "class MobileNetV2TMM(MobileNetV2TSM):\n    def __init__(self,\n                 is_memory=True,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.is_memory = is_memory\n    def make_temporal_memory(self):\n        def make_layer(stage):\n            blocks = list(stage.children())\n            conv3dlstm = TemporalMemoryBlock(",
        "detail": "SVTAS.svtas.model.backbones.video.mobilenet_v2_tmm",
        "documentation": {}
    },
    {
        "label": "MobileNetV2TSM",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.mobilenet_v2_tsm",
        "description": "SVTAS.svtas.model.backbones.video.mobilenet_v2_tsm",
        "peekOfCode": "class MobileNetV2TSM(MobileNetV2):\n    \"\"\"MobileNetV2 backbone for TSM.\n    Args:\n        num_segments (int): Number of frame segments. Default: 8.\n        is_shift (bool): Whether to make temporal shift in reset layers.\n            Default: True.\n        shift_div (int): Number of div for shift. Default: 8.\n        **kwargs (keyword arguments, optional): Arguments for MobilNetV2.\n    \"\"\"\n    def __init__(self, clip_seg_num=8, is_shift=True, shift_div=8, modality=\"RGB\", **kwargs):",
        "detail": "SVTAS.svtas.model.backbones.video.mobilenet_v2_tsm",
        "documentation": {}
    },
    {
        "label": "ObjectDict",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.movinet",
        "description": "SVTAS.svtas.model.backbones.video.movinet",
        "peekOfCode": "class ObjectDict(dict):\n    def __getattr__(self, item):\n        val = self.__getitem__(item)\n        if isinstance(val, dict):\n            return self.__class__(**val)\n        elif isinstance(val, list):\n            return [self.__class__(**d) for d in val]\n        else:\n            return val\ndef get_movinet_config(name):",
        "detail": "SVTAS.svtas.model.backbones.video.movinet",
        "documentation": {}
    },
    {
        "label": "Hardsigmoid",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.movinet",
        "description": "SVTAS.svtas.model.backbones.video.movinet",
        "peekOfCode": "class Hardsigmoid(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n    def forward(self, x: Tensor) -> Tensor:\n        x = (0.2 * x + 0.5).clamp(min=0.0, max=1.0)\n        return x\nclass Swish(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n    def forward(self, x: Tensor) -> Tensor:",
        "detail": "SVTAS.svtas.model.backbones.video.movinet",
        "documentation": {}
    },
    {
        "label": "Swish",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.movinet",
        "description": "SVTAS.svtas.model.backbones.video.movinet",
        "peekOfCode": "class Swish(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n    def forward(self, x: Tensor) -> Tensor:\n        return x * torch.sigmoid(x)\nclass CausalModule(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.activation = None\n    def reset_activation(self) -> None:",
        "detail": "SVTAS.svtas.model.backbones.video.movinet",
        "documentation": {}
    },
    {
        "label": "CausalModule",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.movinet",
        "description": "SVTAS.svtas.model.backbones.video.movinet",
        "peekOfCode": "class CausalModule(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.activation = None\n    def reset_activation(self) -> None:\n        self.activation = None\nclass TemporalCGAvgPool3D(CausalModule):\n    def __init__(self,) -> None:\n        super().__init__()\n        self.n_cumulated_values = 0",
        "detail": "SVTAS.svtas.model.backbones.video.movinet",
        "documentation": {}
    },
    {
        "label": "TemporalCGAvgPool3D",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.movinet",
        "description": "SVTAS.svtas.model.backbones.video.movinet",
        "peekOfCode": "class TemporalCGAvgPool3D(CausalModule):\n    def __init__(self,) -> None:\n        super().__init__()\n        self.n_cumulated_values = 0\n        self.register_forward_hook(self._detach_activation)\n    def forward(self, x: Tensor) -> Tensor:\n        input_shape = x.shape\n        device = x.device\n        cumulative_sum = torch.cumsum(x, dim=2)\n        if self.activation is None:",
        "detail": "SVTAS.svtas.model.backbones.video.movinet",
        "documentation": {}
    },
    {
        "label": "Conv2dBNActivation",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.movinet",
        "description": "SVTAS.svtas.model.backbones.video.movinet",
        "peekOfCode": "class Conv2dBNActivation(nn.Sequential):\n    def __init__(\n                 self,\n                 in_planes: int,\n                 out_planes: int,\n                 *,\n                 kernel_size: Union[int, Tuple[int, int]],\n                 padding: Union[int, Tuple[int, int]],\n                 stride: Union[int, Tuple[int, int]] = 1,\n                 groups: int = 1,",
        "detail": "SVTAS.svtas.model.backbones.video.movinet",
        "documentation": {}
    },
    {
        "label": "Conv3DBNActivation",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.movinet",
        "description": "SVTAS.svtas.model.backbones.video.movinet",
        "peekOfCode": "class Conv3DBNActivation(nn.Sequential):\n    def __init__(\n                 self,\n                 in_planes: int,\n                 out_planes: int,\n                 *,\n                 kernel_size: Union[int, Tuple[int, int, int]],\n                 padding: Union[int, Tuple[int, int, int]],\n                 stride: Union[int, Tuple[int, int, int]] = 1,\n                 groups: int = 1,",
        "detail": "SVTAS.svtas.model.backbones.video.movinet",
        "documentation": {}
    },
    {
        "label": "ConvBlock3D",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.movinet",
        "description": "SVTAS.svtas.model.backbones.video.movinet",
        "peekOfCode": "class ConvBlock3D(CausalModule):\n    def __init__(\n            self,\n            in_planes: int,\n            out_planes: int,\n            *,\n            kernel_size: Union[int, Tuple[int, int, int]],\n            tf_like: bool,\n            causal: bool,\n            conv_type: str,",
        "detail": "SVTAS.svtas.model.backbones.video.movinet",
        "documentation": {}
    },
    {
        "label": "SqueezeExcitation",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.movinet",
        "description": "SVTAS.svtas.model.backbones.video.movinet",
        "peekOfCode": "class SqueezeExcitation(nn.Module):\n    def __init__(self, input_channels: int,  # TODO rename activations\n                 activation_2: nn.Module,\n                 activation_1: nn.Module,\n                 conv_type: str,\n                 causal: bool,\n                 squeeze_factor: int = 4,\n                 bias: bool = True) -> None:\n        super().__init__()\n        self.causal = causal",
        "detail": "SVTAS.svtas.model.backbones.video.movinet",
        "documentation": {}
    },
    {
        "label": "tfAvgPool3D",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.movinet",
        "description": "SVTAS.svtas.model.backbones.video.movinet",
        "peekOfCode": "class tfAvgPool3D(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.avgf = nn.AvgPool3d((1, 3, 3), stride=(1, 2, 2))\n    def forward(self, x: Tensor) -> Tensor:\n        if x.shape[-1] != x.shape[-2]:\n            raise RuntimeError('only same shape for h and w ' +\n                               'are supported by avg with tf_like')\n        if x.shape[-1] != x.shape[-2]:\n            raise RuntimeError('only same shape for h and w ' +",
        "detail": "SVTAS.svtas.model.backbones.video.movinet",
        "documentation": {}
    },
    {
        "label": "BasicBneck",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.movinet",
        "description": "SVTAS.svtas.model.backbones.video.movinet",
        "peekOfCode": "class BasicBneck(nn.Module):\n    def __init__(self,\n                 cfg,\n                 causal: bool,\n                 tf_like: bool,\n                 conv_type: str,\n                 norm_layer: Optional[Callable[..., nn.Module]] = None,\n                 activation_layer: Optional[Callable[..., nn.Module]] = None,\n                 ) -> None:\n        super().__init__()",
        "detail": "SVTAS.svtas.model.backbones.video.movinet",
        "documentation": {}
    },
    {
        "label": "MoViNet",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.movinet",
        "description": "SVTAS.svtas.model.backbones.video.movinet",
        "peekOfCode": "class MoViNet(nn.Module):\n    def __init__(self,\n                 cfg_name=\"A0\",\n                 causal=True,\n                 pretrained=None,\n                 conv_type=\"2plus1d\",\n                 tf_like=False\n                 ) -> None:\n        super().__init__()\n        \"\"\"",
        "detail": "SVTAS.svtas.model.backbones.video.movinet",
        "documentation": {}
    },
    {
        "label": "get_movinet_config",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.video.movinet",
        "description": "SVTAS.svtas.model.backbones.video.movinet",
        "peekOfCode": "def get_movinet_config(name):\n    if name not in [\"A0\", \"A1\", \"A2\"]:\n            raise ValueError(\"Only A0,A1,A2 networks are available reproducted\")\n    if name in [\"A0\"]:\n        conv1 = {\"input_channels\": 3, \"out_channels\": 8, \"kernel_size\": (1, 3, 3), \"stride\": (1, 2, 2), \"padding\": (0, 1, 1)}\n        blocks = [\n            [{\"input_channels\": 8, \"out_channels\": 8, \"expanded_channels\": 24, \"kernel_size\": (1, 5, 5), \"stride\": (1, 2, 2), \"padding\": (0, 2, 2), \"padding_avg\": (0, 1, 1)}],\n            [{\"input_channels\": 8, \"out_channels\": 32, \"expanded_channels\": 80, \"kernel_size\": (3, 3, 3), \"stride\": (1, 2, 2), \"padding\": (1, 0, 0), \"padding_avg\": (0, 1, 1)},\n             {\"input_channels\": 32, \"out_channels\": 32, \"expanded_channels\": 80, \"kernel_size\": (3, 3, 3), \"stride\": (1, 1, 1), \"padding\": (1, 1, 1), \"padding_avg\": (0, 1, 1)},\n             {\"input_channels\": 32, \"out_channels\": 32, \"expanded_channels\": 80, \"kernel_size\": (3, 3, 3), \"stride\": (1, 1, 1), \"padding\": (1, 1, 1), \"padding_avg\": (0, 1, 1)}],",
        "detail": "SVTAS.svtas.model.backbones.video.movinet",
        "documentation": {}
    },
    {
        "label": "same_padding",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.video.movinet",
        "description": "SVTAS.svtas.model.backbones.video.movinet",
        "peekOfCode": "def same_padding(x: Tensor,\n                 in_height: int, in_width: int,\n                 stride_h: int, stride_w: int,\n                 filter_height: int, filter_width: int) -> Tensor:\n    if (in_height % stride_h == 0):\n        pad_along_height = max(filter_height - stride_h, 0)\n    else:\n        pad_along_height = max(filter_height - (in_height % stride_h), 0)\n    if (in_width % stride_w == 0):\n        pad_along_width = max(filter_width - stride_w, 0)",
        "detail": "SVTAS.svtas.model.backbones.video.movinet",
        "documentation": {}
    },
    {
        "label": "MViT",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.mvit",
        "description": "SVTAS.svtas.model.backbones.video.mvit",
        "peekOfCode": "class MViT(nn.Module):\n    \"\"\"\n    Model builder for MViTv1 and MViTv2.\n    \"MViTv2: Improved Multiscale Vision Transformers for Classification and Detection\"\n    Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, Christoph Feichtenhofer\n    https://arxiv.org/abs/2112.01526\n    \"Multiscale Vision Transformers\"\n    Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, Christoph Feichtenhofer\n    https://arxiv.org/abs/2104.11227\n    \"\"\"",
        "detail": "SVTAS.svtas.model.backbones.video.mvit",
        "documentation": {}
    },
    {
        "label": "PredRNNV2",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.predrnn_v2",
        "description": "SVTAS.svtas.model.backbones.video.predrnn_v2",
        "peekOfCode": "class PredRNNV2(nn.Module):\n    def __init__(self,\n                 num_layers=4,\n                 num_hidden=[128, 128, 128, 128],\n                 img_width=224,\n                 img_channel=3,\n                 filter_size=5,\n                 stride=1,\n                 layer_norm=False,\n                 pretrained=None):",
        "detail": "SVTAS.svtas.model.backbones.video.predrnn_v2",
        "documentation": {}
    },
    {
        "label": "ResNet2Plus1d",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.resnet2plus1d",
        "description": "SVTAS.svtas.model.backbones.video.resnet2plus1d",
        "peekOfCode": "class ResNet2Plus1d(ResNet3d):\n    \"\"\"ResNet (2+1)d backbone.\n    This model is proposed in `A Closer Look at Spatiotemporal Convolutions for\n    Action Recognition <https://arxiv.org/abs/1711.11248>`_\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        assert self.pretrained2d is False\n        assert self.conv_cfg['type'] == 'Conv2plus1d'\n    def _freeze_stages(self):",
        "detail": "SVTAS.svtas.model.backbones.video.resnet2plus1d",
        "documentation": {}
    },
    {
        "label": "BasicBlock3d",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.resnet_3d",
        "description": "SVTAS.svtas.model.backbones.video.resnet_3d",
        "peekOfCode": "class BasicBlock3d(nn.Module):\n    \"\"\"BasicBlock 3d block for ResNet3D.\n    Args:\n        inplanes (int): Number of channels for the input in first conv3d layer.\n        planes (int): Number of channels produced by some norm/conv3d layers.\n        spatial_stride (int): Spatial stride in the conv3d layer. Default: 1.\n        temporal_stride (int): Temporal stride in the conv3d layer. Default: 1.\n        dilation (int): Spacing between kernel elements. Default: 1.\n        downsample (nn.Module | None): Downsample layer. Default: None.\n        style (str): ``pytorch`` or ``caffe``. If set to \"pytorch\", the",
        "detail": "SVTAS.svtas.model.backbones.video.resnet_3d",
        "documentation": {}
    },
    {
        "label": "Bottleneck3d",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.resnet_3d",
        "description": "SVTAS.svtas.model.backbones.video.resnet_3d",
        "peekOfCode": "class Bottleneck3d(nn.Module):\n    \"\"\"Bottleneck 3d block for ResNet3D.\n    Args:\n        inplanes (int): Number of channels for the input in first conv3d layer.\n        planes (int): Number of channels produced by some norm/conv3d layers.\n        spatial_stride (int): Spatial stride in the conv3d layer. Default: 1.\n        temporal_stride (int): Temporal stride in the conv3d layer. Default: 1.\n        dilation (int): Spacing between kernel elements. Default: 1.\n        downsample (nn.Module | None): Downsample layer. Default: None.\n        style (str): ``pytorch`` or ``caffe``. If set to \"pytorch\", the",
        "detail": "SVTAS.svtas.model.backbones.video.resnet_3d",
        "documentation": {}
    },
    {
        "label": "ResNet3d",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.resnet_3d",
        "description": "SVTAS.svtas.model.backbones.video.resnet_3d",
        "peekOfCode": "class ResNet3d(nn.Module):\n    \"\"\"ResNet 3d backbone.\n    Args:\n        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.\n        pretrained (str | None): Name of pretrained model.\n        stage_blocks (tuple | None): Set number of stages for each res layer.\n            Default: None.\n        pretrained2d (bool): Whether to load pretrained 2D model.\n            Default: True.\n        in_channels (int): Channel num of input features. Default: 3.",
        "detail": "SVTAS.svtas.model.backbones.video.resnet_3d",
        "documentation": {}
    },
    {
        "label": "ResNet3dLayer",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.resnet_3d",
        "description": "SVTAS.svtas.model.backbones.video.resnet_3d",
        "peekOfCode": "class ResNet3dLayer(nn.Module):\n    \"\"\"ResNet 3d Layer.\n    Args:\n        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.\n        pretrained (str | None): Name of pretrained model.\n        pretrained2d (bool): Whether to load pretrained 2D model.\n            Default: True.\n        stage (int): The index of Resnet stage. Default: 3.\n        base_channels (int): Channel num of stem output features. Default: 64.\n        spatial_stride (int): The 1st res block's spatial stride. Default 2.",
        "detail": "SVTAS.svtas.model.backbones.video.resnet_3d",
        "documentation": {}
    },
    {
        "label": "NL3DWrapper",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.resnet_tsm",
        "description": "SVTAS.svtas.model.backbones.video.resnet_tsm",
        "peekOfCode": "class NL3DWrapper(nn.Module):\n    \"\"\"3D Non-local wrapper for ResNet50.\n    Wrap ResNet layers with 3D NonLocal modules.\n    Args:\n        block (nn.Module): Residual blocks to be built.\n        num_segments (int): Number of frame segments.\n        non_local_cfg (dict): Config for non-local layers. Default: ``dict()``.\n    \"\"\"\n    def __init__(self, block, num_segments, non_local_cfg=dict()):\n        super(NL3DWrapper, self).__init__()",
        "detail": "SVTAS.svtas.model.backbones.video.resnet_tsm",
        "documentation": {}
    },
    {
        "label": "TemporalShift",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.resnet_tsm",
        "description": "SVTAS.svtas.model.backbones.video.resnet_tsm",
        "peekOfCode": "class TemporalShift(nn.Module):\n    \"\"\"Temporal shift module.\n    This module is proposed in\n    `TSM: Temporal Shift Module for Efficient Video Understanding\n    <https://arxiv.org/abs/1811.08383>`_\n    Args:\n        net (nn.module): Module to make temporal shift.\n        num_segments (int): Number of frame segments. Default: 3.\n        shift_div (int): Number of divisions for shift. Default: 8.\n    \"\"\"",
        "detail": "SVTAS.svtas.model.backbones.video.resnet_tsm",
        "documentation": {}
    },
    {
        "label": "ResNetTSM",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.resnet_tsm",
        "description": "SVTAS.svtas.model.backbones.video.resnet_tsm",
        "peekOfCode": "class ResNetTSM(ResNet):\n    \"\"\"ResNet backbone for TSM.\n    Args:\n        num_segments (int): Number of frame segments. Default: 8.\n        is_shift (bool): Whether to make temporal shift in reset layers.\n            Default: True.\n        non_local (Sequence[int]): Determine whether to apply non-local module\n            in the corresponding block of each stages. Default: (0, 0, 0, 0).\n        non_local_cfg (dict): Config for non-local module. Default: ``dict()``.\n        shift_div (int): Number of div for shift. Default: 8.",
        "detail": "SVTAS.svtas.model.backbones.video.resnet_tsm",
        "documentation": {}
    },
    {
        "label": "ReversibleMViT",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.reversible_mvit",
        "description": "SVTAS.svtas.model.backbones.video.reversible_mvit",
        "peekOfCode": "class ReversibleMViT(nn.Module):\n    \"\"\"\n    Reversible model builder. This builds the reversible transformer encoder\n    and allows reversible training.\n    Karttikeya Mangalam, Haoqi Fan, Yanghao Li, Chao-Yuan Wu, Bo Xiong,\n    Christoph Feichtenhofer, Jitendra Malik\n    \"Reversible Vision Transformers\"\n    https://openaccess.thecvf.com/content/CVPR2022/papers/Mangalam_Reversible_Vision_Transformers_CVPR_2022_paper.pdf\n    \"\"\"\n    def __init__(self,",
        "detail": "SVTAS.svtas.model.backbones.video.reversible_mvit",
        "documentation": {}
    },
    {
        "label": "RevBackProp",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.reversible_mvit",
        "description": "SVTAS.svtas.model.backbones.video.reversible_mvit",
        "peekOfCode": "class RevBackProp(Function):\n    \"\"\"\n    Custom Backpropagation function to allow (A) flusing memory in foward\n    and (B) activation recomputation reversibly in backward for gradient calculation.\n    Inspired by https://github.com/RobinBruegger/RevTorch/blob/master/revtorch/revtorch.py\n    \"\"\"\n    @staticmethod\n    def forward(\n        ctx,\n        x,",
        "detail": "SVTAS.svtas.model.backbones.video.reversible_mvit",
        "documentation": {}
    },
    {
        "label": "StageTransitionBlock",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.reversible_mvit",
        "description": "SVTAS.svtas.model.backbones.video.reversible_mvit",
        "peekOfCode": "class StageTransitionBlock(nn.Module):\n    \"\"\"\n    Blocks for changing the feature dimensions in MViT (using Q-pooling).\n    See Section 3.3.1 in paper for details.\n    \"\"\"\n    def __init__(\n        self,\n        dim,\n        input_size,\n        dim_out,",
        "detail": "SVTAS.svtas.model.backbones.video.reversible_mvit",
        "documentation": {}
    },
    {
        "label": "ReversibleBlock",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.reversible_mvit",
        "description": "SVTAS.svtas.model.backbones.video.reversible_mvit",
        "peekOfCode": "class ReversibleBlock(nn.Module):\n    \"\"\"\n    Reversible Blocks for Reversible Vision Transformer and also\n    for state-preserving blocks in Reversible MViT. See Section\n    3.3.2 in paper for details.\n    \"\"\"\n    def __init__(\n        self,\n        dim,\n        input_size,",
        "detail": "SVTAS.svtas.model.backbones.video.reversible_mvit",
        "documentation": {}
    },
    {
        "label": "MLPSubblock",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.reversible_mvit",
        "description": "SVTAS.svtas.model.backbones.video.reversible_mvit",
        "peekOfCode": "class MLPSubblock(nn.Module):\n    \"\"\"\n    This creates the function G such that the entire block can be\n    expressed as F(G(X)). Includes pre-LayerNorm.\n    \"\"\"\n    def __init__(\n        self,\n        dim,\n        mlp_ratio,\n        norm_layer=nn.LayerNorm,",
        "detail": "SVTAS.svtas.model.backbones.video.reversible_mvit",
        "documentation": {}
    },
    {
        "label": "AttentionSubBlock",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.reversible_mvit",
        "description": "SVTAS.svtas.model.backbones.video.reversible_mvit",
        "peekOfCode": "class AttentionSubBlock(nn.Module):\n    \"\"\"\n    This creates the function F such that the entire block can be\n    expressed as F(G(X)). Includes pre-LayerNorm.\n    \"\"\"\n    def __init__(\n        self,\n        dim,\n        input_size,\n        num_heads,",
        "detail": "SVTAS.svtas.model.backbones.video.reversible_mvit",
        "documentation": {}
    },
    {
        "label": "FeedForward",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.sample_vit_3d",
        "description": "SVTAS.svtas.model.backbones.video.sample_vit_3d",
        "peekOfCode": "class FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Linear(hidden_dim, dim),\n        )\n    def forward(self, x):",
        "detail": "SVTAS.svtas.model.backbones.video.sample_vit_3d",
        "documentation": {}
    },
    {
        "label": "Attention",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.sample_vit_3d",
        "description": "SVTAS.svtas.model.backbones.video.sample_vit_3d",
        "peekOfCode": "class Attention(nn.Module):\n    def __init__(self, dim, heads = 8, dim_head = 64):\n        super().__init__()\n        inner_dim = dim_head *  heads\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n        self.norm = nn.LayerNorm(dim)\n        self.attend = nn.Softmax(dim = -1)\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n        self.to_out = nn.Linear(inner_dim, dim, bias = False)",
        "detail": "SVTAS.svtas.model.backbones.video.sample_vit_3d",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.sample_vit_3d",
        "description": "SVTAS.svtas.model.backbones.video.sample_vit_3d",
        "peekOfCode": "class Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Attention(dim, heads = heads, dim_head = dim_head),\n                FeedForward(dim, mlp_dim)\n            ]))\n    def forward(self, x):",
        "detail": "SVTAS.svtas.model.backbones.video.sample_vit_3d",
        "documentation": {}
    },
    {
        "label": "SampleViT3D",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.sample_vit_3d",
        "description": "SVTAS.svtas.model.backbones.video.sample_vit_3d",
        "peekOfCode": "class SampleViT3D(nn.Module):\n    def __init__(self,\n                 image_size,\n                 image_patch_size,\n                 frames,\n                 frame_patch_size,\n                 num_classes,\n                 dim,\n                 depth,\n                 heads,",
        "detail": "SVTAS.svtas.model.backbones.video.sample_vit_3d",
        "documentation": {}
    },
    {
        "label": "pair",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.video.sample_vit_3d",
        "description": "SVTAS.svtas.model.backbones.video.sample_vit_3d",
        "peekOfCode": "def pair(t):\n    return t if isinstance(t, tuple) else (t, t)\ndef posemb_sincos_3d(patches, temperature = 10000, dtype = torch.float32):\n    _, f, h, w, dim, device, dtype = *patches.shape, patches.device, patches.dtype\n    z, y, x = torch.meshgrid(\n        torch.arange(f, device = device),\n        torch.arange(h, device = device),\n        torch.arange(w, device = device),\n    indexing = 'ij')\n    fourier_dim = dim // 6",
        "detail": "SVTAS.svtas.model.backbones.video.sample_vit_3d",
        "documentation": {}
    },
    {
        "label": "posemb_sincos_3d",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.video.sample_vit_3d",
        "description": "SVTAS.svtas.model.backbones.video.sample_vit_3d",
        "peekOfCode": "def posemb_sincos_3d(patches, temperature = 10000, dtype = torch.float32):\n    _, f, h, w, dim, device, dtype = *patches.shape, patches.device, patches.dtype\n    z, y, x = torch.meshgrid(\n        torch.arange(f, device = device),\n        torch.arange(h, device = device),\n        torch.arange(w, device = device),\n    indexing = 'ij')\n    fourier_dim = dim // 6\n    omega = torch.arange(fourier_dim, device = device) / (fourier_dim - 1)\n    omega = 1. / (temperature ** omega)",
        "detail": "SVTAS.svtas.model.backbones.video.sample_vit_3d",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "description": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "peekOfCode": "class Mlp(nn.Module):\n    \"\"\" Multilayer perceptron.\"\"\"\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)",
        "detail": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "documentation": {}
    },
    {
        "label": "WindowAttention3D",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "description": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "peekOfCode": "class WindowAttention3D(nn.Module):\n    \"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The temporal length, height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0",
        "detail": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "documentation": {}
    },
    {
        "label": "SwinTransformerBlock3D",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "description": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "peekOfCode": "class SwinTransformerBlock3D(nn.Module):\n    \"\"\" Swin Transformer Block.\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        window_size (tuple[int]): Window size.\n        shift_size (tuple[int]): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.",
        "detail": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "documentation": {}
    },
    {
        "label": "PatchMerging",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "description": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "peekOfCode": "class PatchMerging(nn.Module):\n    \"\"\" Patch Merging Layer\n    Args:\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n    def __init__(self, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)",
        "detail": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "documentation": {}
    },
    {
        "label": "BasicLayer",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "description": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "peekOfCode": "class BasicLayer(nn.Module):\n    \"\"\" A basic Swin Transformer layer for one stage.\n    Args:\n        dim (int): Number of feature channels\n        depth (int): Depths of this stage.\n        num_heads (int): Number of attention head.\n        window_size (tuple[int]): Local window size. Default: (1,7,7).\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.",
        "detail": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "documentation": {}
    },
    {
        "label": "PatchEmbed3D",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "description": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "peekOfCode": "class PatchEmbed3D(nn.Module):\n    \"\"\" Video to Patch Embedding.\n    Args:\n        patch_size (int): Patch token size. Default: (2,4,4).\n        in_chans (int): Number of input video channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n    def __init__(self, patch_size=(2,4,4), in_chans=3, embed_dim=96, norm_layer=None):\n        super().__init__()",
        "detail": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "documentation": {}
    },
    {
        "label": "SwinTransformer3D",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "description": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "peekOfCode": "class SwinTransformer3D(nn.Module):\n    \"\"\" Swin Transformer backbone.\n        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n          https://arxiv.org/pdf/2103.14030\n    Args:\n        patch_size (int | tuple(int)): Patch size. Default: (4,4,4).\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        depths (tuple[int]): Depths of each Swin Transformer stage.\n        num_heads (tuple[int]): Number of attention head of each stage.",
        "detail": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "documentation": {}
    },
    {
        "label": "window_partition",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "description": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "peekOfCode": "def window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, D, H, W, C)\n        window_size (tuple[int]): window size\n    Returns:\n        windows: (B*num_windows, window_size*window_size, C)\n    \"\"\"\n    B, D, H, W, C = x.shape\n    x = x.view(B, D // window_size[0], window_size[0], H // window_size[1], window_size[1], W // window_size[2], window_size[2], C)",
        "detail": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "documentation": {}
    },
    {
        "label": "window_reverse",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "description": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "peekOfCode": "def window_reverse(windows, window_size, B, D, H, W):\n    \"\"\"\n    Args:\n        windows: (B*num_windows, window_size, window_size, C)\n        window_size (tuple[int]): Window size\n        H (int): Height of image\n        W (int): Width of image\n    Returns:\n        x: (B, D, H, W, C)\n    \"\"\"",
        "detail": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "documentation": {}
    },
    {
        "label": "get_window_size",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "description": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "peekOfCode": "def get_window_size(x_size, window_size, shift_size=None):\n    use_window_size = list(window_size)\n    if shift_size is not None:\n        use_shift_size = list(shift_size)\n    for i in range(len(x_size)):\n        if x_size[i] <= window_size[i]:\n            use_window_size[i] = x_size[i]\n            if shift_size is not None:\n                use_shift_size[i] = 0\n    if shift_size is None:",
        "detail": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "documentation": {}
    },
    {
        "label": "compute_mask",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "description": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "peekOfCode": "def compute_mask(D, H, W, window_size, shift_size, device):\n    img_mask = torch.zeros((1, D, H, W, 1), device=device)  # 1 Dp Hp Wp 1\n    cnt = 0\n    for d in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0],None):\n        for h in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1],None):\n            for w in slice(-window_size[2]), slice(-window_size[2], -shift_size[2]), slice(-shift_size[2],None):\n                img_mask[:, d, h, w, :] = cnt\n                cnt += 1\n    mask_windows = window_partition(img_mask, window_size)  # nW, ws[0]*ws[1]*ws[2], 1\n    mask_windows = mask_windows.squeeze(-1)  # nW, ws[0]*ws[1]*ws[2]",
        "detail": "SVTAS.svtas.model.backbones.video.swin_transformer",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.timesfromer",
        "description": "SVTAS.svtas.model.backbones.video.timesfromer",
        "peekOfCode": "class PatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding.\n    Args:\n        img_size (int | tuple): Size of input image.\n        patch_size (int): Size of one patch.\n        in_channels (int): Channel num of input features. Defaults to 3.\n        embed_dims (int): Dimensions of embedding. Defaults to 768.\n        conv_cfg (dict | None): Config dict for convolution layer. Defaults to\n            `dict(type='Conv2d')`.\n    \"\"\"",
        "detail": "SVTAS.svtas.model.backbones.video.timesfromer",
        "documentation": {}
    },
    {
        "label": "TimeSformer",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.timesfromer",
        "description": "SVTAS.svtas.model.backbones.video.timesfromer",
        "peekOfCode": "class TimeSformer(nn.Module):\n    \"\"\"TimeSformer. A PyTorch impl of `Is Space-Time Attention All You Need for\n    Video Understanding? <https://arxiv.org/abs/2102.05095>`_\n    Args:\n        num_frames (int): Number of frames in the video.\n        img_size (int | tuple): Size of input image.\n        patch_size (int): Size of one patch.\n        pretrained (str | None): Name of pretrained model. Default: None.\n        embed_dims (int): Dimensions of embedding. Defaults to 768.\n        num_heads (int): Number of parallel attention heads in",
        "detail": "SVTAS.svtas.model.backbones.video.timesfromer",
        "documentation": {}
    },
    {
        "label": "PreNorm",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.vit_3d",
        "description": "SVTAS.svtas.model.backbones.video.vit_3d",
        "peekOfCode": "class PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__()",
        "detail": "SVTAS.svtas.model.backbones.video.vit_3d",
        "documentation": {}
    },
    {
        "label": "FeedForward",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.vit_3d",
        "description": "SVTAS.svtas.model.backbones.video.vit_3d",
        "peekOfCode": "class FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )",
        "detail": "SVTAS.svtas.model.backbones.video.vit_3d",
        "documentation": {}
    },
    {
        "label": "Attention",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.vit_3d",
        "description": "SVTAS.svtas.model.backbones.video.vit_3d",
        "peekOfCode": "class Attention(nn.Module):\n    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n        super().__init__()\n        inner_dim = dim_head *  heads\n        project_out = not (heads == 1 and dim_head == dim)\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n        self.attend = nn.Softmax(dim = -1)\n        self.dropout = nn.Dropout(dropout)\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)",
        "detail": "SVTAS.svtas.model.backbones.video.vit_3d",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.vit_3d",
        "description": "SVTAS.svtas.model.backbones.video.vit_3d",
        "peekOfCode": "class Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n            ]))\n    def forward(self, x):",
        "detail": "SVTAS.svtas.model.backbones.video.vit_3d",
        "documentation": {}
    },
    {
        "label": "ViT3D",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.vit_3d",
        "description": "SVTAS.svtas.model.backbones.video.vit_3d",
        "peekOfCode": "class ViT3D(nn.Module):\n    def __init__(self,\n                 image_size,\n                 image_patch_size,\n                 frames,\n                 frame_patch_size,\n                 dim,\n                 depth,\n                 heads,\n                 mlp_dim,",
        "detail": "SVTAS.svtas.model.backbones.video.vit_3d",
        "documentation": {}
    },
    {
        "label": "pair",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.backbones.video.vit_3d",
        "description": "SVTAS.svtas.model.backbones.video.vit_3d",
        "peekOfCode": "def pair(t):\n    return t if isinstance(t, tuple) else (t, t)\n# classes\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)",
        "detail": "SVTAS.svtas.model.backbones.video.vit_3d",
        "documentation": {}
    },
    {
        "label": "X3D",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.backbones.video.x3d",
        "description": "SVTAS.svtas.model.backbones.video.x3d",
        "peekOfCode": "class X3D(nn.Module):\n    \"\"\"\n    X3D model builder. It builds a X3D network backbone, which is a ResNet.\n    Christoph Feichtenhofer.\n    \"X3D: Expanding Architectures for Efficient Video Recognition.\"\n    https://arxiv.org/abs/2004.04730\n    \"\"\"\n    def __init__(self, cfg):\n        \"\"\"\n        The `__init__` method of any subclass should also contain these",
        "detail": "SVTAS.svtas.model.backbones.video.x3d",
        "documentation": {}
    },
    {
        "label": "InterploteAlignHead",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.align_heads.interplote_align",
        "description": "SVTAS.svtas.model.heads.align_heads.interplote_align",
        "peekOfCode": "class InterploteAlignHead(nn.Module):\n    def __init__(self):\n        super(InterploteAlignHead, self).__init__()\n    def init_weights(self):\n        pass\n    def _clear_memory_buffer(self):\n        pass\n    def forward(self, seg_score, labels, mask):\n        # seg_score [num_stages, N, C, T]\n        seg_score = F.interpolate(seg_score, size=[seg_score.shape[-2], labels.shape[-1]], mode=\"bilinear\")",
        "detail": "SVTAS.svtas.model.heads.align_heads.interplote_align",
        "documentation": {}
    },
    {
        "label": "Conformer",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.automatic_speech_recognition.conformer",
        "description": "SVTAS.svtas.model.heads.automatic_speech_recognition.conformer",
        "peekOfCode": "class Conformer(nn.Module):\n    \"\"\"\n    Conformer: Convolution-augmented Transformer for Speech Recognition\n    The paper used a one-lstm Transducer decoder, currently still only implemented\n    the conformer encoder shown in the paper.\n    Args:\n        num_classes (int): Number of classification classes\n        input_dim (int, optional): Dimension of input vector\n        encoder_dim (int, optional): Dimension of conformer encoder\n        num_encoder_layers (int, optional): Number of conformer blocks",
        "detail": "SVTAS.svtas.model.heads.automatic_speech_recognition.conformer",
        "documentation": {}
    },
    {
        "label": "FeatureExtractHead",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.feature_extractor.feature_extract_head",
        "description": "SVTAS.svtas.model.heads.feature_extractor.feature_extract_head",
        "peekOfCode": "class FeatureExtractHead(nn.Module):\n    def __init__(self,\n                 in_channels=2048,\n                 input_seg_num=8,\n                 output_seg_num=32,\n                 sample_rate=1,\n                 pool_space=True,\n                 in_format=\"N,C,T,H,W\",\n                 out_format=\"NCT\"):\n        super().__init__()",
        "detail": "SVTAS.svtas.model.heads.feature_extractor.feature_extract_head",
        "documentation": {}
    },
    {
        "label": "IdentityEmbeddingHead",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.feature_extractor.identity_embedding_head",
        "description": "SVTAS.svtas.model.heads.feature_extractor.identity_embedding_head",
        "peekOfCode": "class IdentityEmbeddingHead(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 sample_rate=4):\n        super().__init__()\n        self.sample_rate = sample_rate\n        if in_channels != out_channels:\n            self.project = nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=False)\n        else:",
        "detail": "SVTAS.svtas.model.heads.feature_extractor.identity_embedding_head",
        "documentation": {}
    },
    {
        "label": "TransudcerJointNet",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.joint.transducer_joint_head",
        "description": "SVTAS.svtas.model.heads.joint.transducer_joint_head",
        "peekOfCode": "class TransudcerJointNet(nn.Module):\n    \"\"\"\n    Combine the audio encoder and label encoders.\n    Convert them into log probability values for each word.\n    Args:\n        num_vocabs (int): the number of vocabulary\n        output_size (int): the number of features combined output of audio and label encoders (default : 1024)\n        inner_size (int): the number of inner features (default : 512)\n    Inputs: audio_encoder, label_encoder\n        - **audio_encoder**: Audio encoder output",
        "detail": "SVTAS.svtas.model.heads.joint.transducer_joint_head",
        "documentation": {}
    },
    {
        "label": "TransegerFCJointNet",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.joint.transeger_fc_joint_head",
        "description": "SVTAS.svtas.model.heads.joint.transeger_fc_joint_head",
        "peekOfCode": "class TransegerFCJointNet(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 in_channels,\n                 hidden_channels=128,\n                 sample_rate=4):\n        super().__init__()\n        self.sample_rate = sample_rate\n        self.fc1 = nn.Linear(in_channels * 2, hidden_channels)\n        self.tanh = nn.Tanh()",
        "detail": "SVTAS.svtas.model.heads.joint.transeger_fc_joint_head",
        "documentation": {}
    },
    {
        "label": "TransegerMemoryTCNJointNet",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.joint.transeger_memory_tcn_joint_head",
        "description": "SVTAS.svtas.model.heads.joint.transeger_memory_tcn_joint_head",
        "peekOfCode": "class TransegerMemoryTCNJointNet(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 in_channels,\n                 num_layers=4,\n                 hidden_channels=128,\n                 sample_rate=4):\n        super().__init__()\n        self.sample_rate = sample_rate\n        self.layers = nn.ModuleList([copy.deepcopy(MemoryDilationResidualLyaer(2 ** (num_layers-1-i), hidden_channels, hidden_channels)) for i in range(num_layers)])",
        "detail": "SVTAS.svtas.model.heads.joint.transeger_memory_tcn_joint_head",
        "documentation": {}
    },
    {
        "label": "TransegerTransformerJointNet",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.joint.transeger_transformer_joint_head",
        "description": "SVTAS.svtas.model.heads.joint.transeger_transformer_joint_head",
        "peekOfCode": "class TransegerTransformerJointNet(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 in_channels,\n                 clip_seg_num=32,\n                 num_layers=3,\n                 dropout_rate=0.1,\n                 encoder_heads_num=8,\n                 hidden_channels=128,\n                 sample_rate=4,",
        "detail": "SVTAS.svtas.model.heads.joint.transeger_transformer_joint_head",
        "documentation": {}
    },
    {
        "label": "OadTRHead",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.online_action_detection.oadtr",
        "description": "SVTAS.svtas.model.heads.online_action_detection.oadtr",
        "peekOfCode": "class OadTRHead(nn.Module):\n    def __init__(self,\n                 clip_seg_num,\n                 num_classes,\n                 pred_clip_seg_num=8,\n                 sample_rate=1,\n                 patch_dim=1,\n                 in_channels=2048,\n                 embedding_dim=1024,\n                 num_heads=8,",
        "detail": "SVTAS.svtas.model.heads.online_action_detection.oadtr",
        "documentation": {}
    },
    {
        "label": "FCHead",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.recognition.fc_head",
        "description": "SVTAS.svtas.model.heads.recognition.fc_head",
        "peekOfCode": "class FCHead(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 clip_seg_num=15,\n                 sample_rate=4,\n                 drop_ratio=0.5,\n                 in_channels=2048):\n        super().__init__()\n        self.in_channels = in_channels\n        self.num_classes = num_classes",
        "detail": "SVTAS.svtas.model.heads.recognition.fc_head",
        "documentation": {}
    },
    {
        "label": "I3DHead",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.recognition.i3d_head",
        "description": "SVTAS.svtas.model.heads.recognition.i3d_head",
        "peekOfCode": "class I3DHead(nn.Module):\n    \"\"\"Classification head for I3D.\n    Args:\n        num_classes (int): Number of classes to be classified.\n        in_channels (int): Number of channels in input feature.\n        loss_cls (dict): Config for building loss.\n            Default: dict(type='CrossEntropyLoss')\n        spatial_type (str): Pooling type in spatial dimension. Default: 'avg'.\n        dropout_ratio (float): Probability of dropout layer. Default: 0.5.\n        init_std (float): Std value for Initiation. Default: 0.01.",
        "detail": "SVTAS.svtas.model.heads.recognition.i3d_head",
        "documentation": {}
    },
    {
        "label": "MoViNetHead",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.recognition.movinet_head",
        "description": "SVTAS.svtas.model.heads.recognition.movinet_head",
        "peekOfCode": "class MoViNetHead(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 causal=True,\n                 tf_like=True,\n                 conv_type=\"3d\",\n                 clip_seg_num=15,\n                 sample_rate=4,\n                 drop_ratio=0.5,\n                 in_channels=2048,",
        "detail": "SVTAS.svtas.model.heads.recognition.movinet_head",
        "documentation": {}
    },
    {
        "label": "TimeSformerHead",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.recognition.timesformer_head",
        "description": "SVTAS.svtas.model.heads.recognition.timesformer_head",
        "peekOfCode": "class TimeSformerHead(nn.Module):\n    \"\"\"Classification head for TimeSformer.\n    Args:\n        num_classes (int): Number of classes to be classified.\n        in_channels (int): Number of channels in input feature.\n        loss_cls (dict): Config for building loss.\n            Defaults to `dict(type='CrossEntropyLoss')`.\n        init_std (float): Std value for Initiation. Defaults to 0.02.\n        kwargs (dict, optional): Any keyword argument to be used to initialize\n            the head.",
        "detail": "SVTAS.svtas.model.heads.recognition.timesformer_head",
        "documentation": {}
    },
    {
        "label": "TSMHead",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.recognition.tsm_head",
        "description": "SVTAS.svtas.model.heads.recognition.tsm_head",
        "peekOfCode": "class TSMHead(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 clip_seg_num=15,\n                 sample_rate=4,\n                 drop_ratio=0.5,\n                 in_channels=2048):\n        super().__init__()\n        self.in_channels = in_channels\n        self.num_classes = num_classes",
        "detail": "SVTAS.svtas.model.heads.recognition.tsm_head",
        "documentation": {}
    },
    {
        "label": "AttentionHelper",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.segmentation.asformer",
        "description": "SVTAS.svtas.model.heads.segmentation.asformer",
        "peekOfCode": "class AttentionHelper(nn.Module):\n    def __init__(self):\n        super(AttentionHelper, self).__init__()\n        self.softmax = nn.Softmax(dim=-1)\n    def scalar_dot_att(self, proj_query, proj_key, proj_val, padding_mask):\n        '''\n        scalar dot attention.\n        :param proj_query: shape of (B, C, L) => (Batch_Size, Feature_Dimension, Length)\n        :param proj_key: shape of (B, C, L)\n        :param proj_val: shape of (B, C, L)",
        "detail": "SVTAS.svtas.model.heads.segmentation.asformer",
        "documentation": {}
    },
    {
        "label": "AttLayer",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.segmentation.asformer",
        "description": "SVTAS.svtas.model.heads.segmentation.asformer",
        "peekOfCode": "class AttLayer(nn.Module):\n    def __init__(self, q_dim, k_dim, v_dim, r1, r2, r3, bl, stage, att_type): # r1 = r2\n        super(AttLayer, self).__init__()\n        self.query_conv = nn.Conv1d(in_channels=q_dim, out_channels=q_dim // r1, kernel_size=1)\n        self.key_conv = nn.Conv1d(in_channels=k_dim, out_channels=k_dim // r2, kernel_size=1)\n        self.value_conv = nn.Conv1d(in_channels=v_dim, out_channels=v_dim // r3, kernel_size=1)\n        self.conv_out = nn.Conv1d(in_channels=v_dim // r3, out_channels=v_dim, kernel_size=1)\n        self.bl = bl\n        self.stage = stage\n        self.att_type = att_type",
        "detail": "SVTAS.svtas.model.heads.segmentation.asformer",
        "documentation": {}
    },
    {
        "label": "ConvFeedForward",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.segmentation.asformer",
        "description": "SVTAS.svtas.model.heads.segmentation.asformer",
        "peekOfCode": "class ConvFeedForward(nn.Module):\n    def __init__(self, dilation, in_channels, out_channels):\n        super(ConvFeedForward, self).__init__()\n        self.layer = nn.Sequential(\n            nn.Conv1d(in_channels, out_channels, 3, padding=dilation, dilation=dilation),\n            nn.ReLU()\n        )\n    def forward(self, x):\n        return self.layer(x)\nclass FCFeedForward(nn.Module):",
        "detail": "SVTAS.svtas.model.heads.segmentation.asformer",
        "documentation": {}
    },
    {
        "label": "FCFeedForward",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.segmentation.asformer",
        "description": "SVTAS.svtas.model.heads.segmentation.asformer",
        "peekOfCode": "class FCFeedForward(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(FCFeedForward, self).__init__()\n        self.layer = nn.Sequential(\n            nn.Conv1d(in_channels, out_channels, 1),  # conv1d equals fc\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.Conv1d(out_channels, out_channels, 1)\n        )\n    def forward(self, x):",
        "detail": "SVTAS.svtas.model.heads.segmentation.asformer",
        "documentation": {}
    },
    {
        "label": "AttModule",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.segmentation.asformer",
        "description": "SVTAS.svtas.model.heads.segmentation.asformer",
        "peekOfCode": "class AttModule(nn.Module):\n    def __init__(self, dilation, in_channels, out_channels, r1, r2, att_type, stage, alpha):\n        super(AttModule, self).__init__()\n        self.feed_forward = ConvFeedForward(dilation, in_channels, out_channels)\n        self.instance_norm = nn.InstanceNorm1d(in_channels, track_running_stats=False)\n        self.att_layer = AttLayer(in_channels, in_channels, out_channels, r1, r1, r2, dilation, att_type=att_type, stage=stage) # dilation\n        self.conv_1x1 = nn.Conv1d(out_channels, out_channels, 1)\n        self.dropout = nn.Dropout()\n        self.alpha = alpha\n    def forward(self, x, f, mask):",
        "detail": "SVTAS.svtas.model.heads.segmentation.asformer",
        "documentation": {}
    },
    {
        "label": "PositionalEncoding",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.segmentation.asformer",
        "description": "SVTAS.svtas.model.heads.segmentation.asformer",
        "peekOfCode": "class PositionalEncoding(nn.Module):\n    \"Implement the PE function.\"\n    def __init__(self, d_model, max_len=10000):\n        super(PositionalEncoding, self).__init__()\n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) *\n                             -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)",
        "detail": "SVTAS.svtas.model.heads.segmentation.asformer",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.segmentation.asformer",
        "description": "SVTAS.svtas.model.heads.segmentation.asformer",
        "peekOfCode": "class Encoder(nn.Module):\n    def __init__(self, num_layers, r1, r2, num_f_maps, input_dim, num_classes, channel_masking_rate, att_type, alpha):\n        super(Encoder, self).__init__()\n        self.conv_1x1 = nn.Conv1d(input_dim, num_f_maps, 1) # fc layer\n        self.layers = nn.ModuleList(\n            [AttModule(2 ** i, num_f_maps, num_f_maps, r1, r2, att_type, 'encoder', alpha) for i in # 2**i\n             range(num_layers)])\n        self.conv_out = nn.Conv1d(num_f_maps, num_classes, 1)\n        self.dropout = nn.Dropout2d(p=channel_masking_rate)\n        self.channel_masking_rate = channel_masking_rate",
        "detail": "SVTAS.svtas.model.heads.segmentation.asformer",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.segmentation.asformer",
        "description": "SVTAS.svtas.model.heads.segmentation.asformer",
        "peekOfCode": "class Decoder(nn.Module):\n    def __init__(self, num_layers, r1, r2, num_f_maps, input_dim, num_classes, att_type, alpha):\n        super(Decoder, self).__init__()\n        self.conv_1x1 = nn.Conv1d(input_dim, num_f_maps, 1)\n        # self.position_en = PositionalEncoding(d_model=num_f_maps)\n        self.layers = nn.ModuleList(\n            [AttModule(2 ** i, num_f_maps, num_f_maps, r1, r2, att_type, 'decoder', alpha) for i in # 2 ** i\n             range(num_layers)])\n        self.conv_out = nn.Conv1d(num_f_maps, num_classes, 1)\n    def forward(self, x, fencoder, mask):",
        "detail": "SVTAS.svtas.model.heads.segmentation.asformer",
        "documentation": {}
    },
    {
        "label": "ASFormer",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.segmentation.asformer",
        "description": "SVTAS.svtas.model.heads.segmentation.asformer",
        "peekOfCode": "class ASFormer(nn.Module):\n    def __init__(self,\n                 num_decoders=3,\n                 num_layers=10,\n                 r1=2,\n                 r2=2,\n                 num_f_maps=64,\n                 input_dim=2048,\n                 num_classes=11,\n                 channel_masking_rate=0.5,",
        "detail": "SVTAS.svtas.model.heads.segmentation.asformer",
        "documentation": {}
    },
    {
        "label": "exponential_descrease",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.heads.segmentation.asformer",
        "description": "SVTAS.svtas.model.heads.segmentation.asformer",
        "peekOfCode": "def exponential_descrease(idx_decoder, p=3):\n    return math.exp(-p*idx_decoder)\nclass AttentionHelper(nn.Module):\n    def __init__(self):\n        super(AttentionHelper, self).__init__()\n        self.softmax = nn.Softmax(dim=-1)\n    def scalar_dot_att(self, proj_query, proj_key, proj_val, padding_mask):\n        '''\n        scalar dot attention.\n        :param proj_query: shape of (B, C, L) => (Batch_Size, Feature_Dimension, Length)",
        "detail": "SVTAS.svtas.model.heads.segmentation.asformer",
        "documentation": {}
    },
    {
        "label": "ETESVSHead",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.segmentation.etesvs_head",
        "description": "SVTAS.svtas.model.heads.segmentation.etesvs_head",
        "peekOfCode": "class ETESVSHead(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 num_layers=4,\n                 sample_rate=4,\n                 sliding_window=60,\n                 seg_in_channels=2048,\n                 num_f_maps=64):\n        super().__init__()\n        self.seg_in_channels = seg_in_channels",
        "detail": "SVTAS.svtas.model.heads.segmentation.etesvs_head",
        "documentation": {}
    },
    {
        "label": "LinAttLayer",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.segmentation.linear_transformer",
        "description": "SVTAS.svtas.model.heads.segmentation.linear_transformer",
        "peekOfCode": "class LinAttLayer(nn.Module):\n    def __init__(self, q_dim, k_dim, v_dim, bl, stage, att_type, r1, r2, r3, method=\"learnable\"): # r1 = r2\n        super(LinAttLayer, self).__init__()\n        self.query_conv = nn.Conv1d(in_channels=q_dim, out_channels=q_dim // r1, kernel_size=1)\n        self.key_conv = nn.Conv1d(in_channels=k_dim, out_channels=k_dim // r2, kernel_size=1)\n        self.value_conv = nn.Conv1d(in_channels=v_dim, out_channels=v_dim // r3, kernel_size=1)\n        self.conv_out = nn.Conv1d(in_channels=v_dim // r3, out_channels=v_dim, kernel_size=1)\n        self.dropout = nn.Dropout()\n        self.P_bar = None\n        self.E = get_EF(k_dim // r1, k_dim // r1, method, q_dim // r1)",
        "detail": "SVTAS.svtas.model.heads.segmentation.linear_transformer",
        "documentation": {}
    },
    {
        "label": "AttModule",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.segmentation.linear_transformer",
        "description": "SVTAS.svtas.model.heads.segmentation.linear_transformer",
        "peekOfCode": "class AttModule(nn.Module):\n    def __init__(self, dilation, in_channels, out_channels, r1, r2, att_type, stage, alpha):\n        super(AttModule, self).__init__()\n        self.feed_forward = ConvFeedForward(dilation, in_channels, out_channels)\n        self.instance_norm = nn.InstanceNorm1d(in_channels, track_running_stats=False)\n        self.att_layer = LinAttLayer(q_dim=in_channels, k_dim=in_channels, v_dim=out_channels, bl=dilation, r1=r1, r2=r1, r3=r2, att_type=att_type, stage=stage) # dilation\n        self.conv_1x1 = nn.Conv1d(out_channels, out_channels, 1)\n        self.dropout = nn.Dropout()\n        self.alpha = alpha\n    def forward(self, x, f, mask):",
        "detail": "SVTAS.svtas.model.heads.segmentation.linear_transformer",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.segmentation.linear_transformer",
        "description": "SVTAS.svtas.model.heads.segmentation.linear_transformer",
        "peekOfCode": "class Encoder(nn.Module):\n    def __init__(self, num_layers, r1, r2, num_f_maps, input_dim, num_classes, channel_masking_rate, att_type, alpha):\n        super(Encoder, self).__init__()\n        self.conv_1x1 = nn.Conv1d(input_dim, num_f_maps, 1) # fc layer\n        self.layers = nn.ModuleList(\n            [AttModule(2 ** i, num_f_maps, num_f_maps, r1, r2, att_type, 'encoder', alpha) for i in # 2**i\n             range(num_layers)])\n        self.conv_out = nn.Conv1d(num_f_maps, num_classes, 1)\n        self.dropout = nn.Dropout2d(p=channel_masking_rate)\n        self.channel_masking_rate = channel_masking_rate",
        "detail": "SVTAS.svtas.model.heads.segmentation.linear_transformer",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.segmentation.linear_transformer",
        "description": "SVTAS.svtas.model.heads.segmentation.linear_transformer",
        "peekOfCode": "class Decoder(nn.Module):\n    def __init__(self, num_layers, r1, r2, num_f_maps, input_dim, num_classes, att_type, alpha):\n        super(Decoder, self).__init__()\n        self.conv_1x1 = nn.Conv1d(input_dim, num_f_maps, 1)\n        # self.position_en = PositionalEncoding(d_model=num_f_maps)\n        self.layers = nn.ModuleList(\n            [AttModule(2 ** i, num_f_maps, num_f_maps, r1, r2, att_type, 'decoder', alpha) for i in # 2 ** i\n             range(num_layers)])\n        self.conv_out = nn.Conv1d(num_f_maps, num_classes, 1)\n    def forward(self, x, fencoder, mask):",
        "detail": "SVTAS.svtas.model.heads.segmentation.linear_transformer",
        "documentation": {}
    },
    {
        "label": "LinformerHead",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.segmentation.linear_transformer",
        "description": "SVTAS.svtas.model.heads.segmentation.linear_transformer",
        "peekOfCode": "class LinformerHead(nn.Module):\n    \"\"\"\n    LinFormer Head for action segmentation\n    \"\"\"\n    def __init__(self,\n                 num_decoders=3,\n                 num_layers=10,\n                 num_f_maps=64,\n                 input_dim=2048,\n                 num_classes=11,",
        "detail": "SVTAS.svtas.model.heads.segmentation.linear_transformer",
        "documentation": {}
    },
    {
        "label": "get_EF",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.heads.segmentation.linear_transformer",
        "description": "SVTAS.svtas.model.heads.segmentation.linear_transformer",
        "peekOfCode": "def get_EF(input_size, dim, method=\"learnable\", head_dim=None, bias=True):\n    \"\"\"\n    Retuns the E or F matrix, initialized via xavier initialization.\n    This is the recommended way to do it according to the authors of the paper.\n    Includes a method for convolution, as well as a method for no additional params.\n    \"\"\"\n    assert method == \"learnable\" or method == \"convolution\" or method == \"no_params\", \"The method flag needs to be either 'learnable', 'convolution', or 'no_params'!\"\n    if method == \"convolution\":\n        conv = nn.Conv1d(head_dim, head_dim, kernel_size=int(input_size/dim), stride=int(input_size/dim))\n        return conv",
        "detail": "SVTAS.svtas.model.heads.segmentation.linear_transformer",
        "documentation": {}
    },
    {
        "label": "LSTMSegmentationHead",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.segmentation.lstm_head",
        "description": "SVTAS.svtas.model.heads.segmentation.lstm_head",
        "peekOfCode": "class LSTMSegmentationHead(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 num_classes,\n                 sample_rate=1,\n                 hidden_channels=1024,\n                 num_layers=3,\n                 batch_first=True,\n                 dropout=0.5,\n                 bidirectional=True,",
        "detail": "SVTAS.svtas.model.heads.segmentation.lstm_head",
        "documentation": {}
    },
    {
        "label": "MemoryTCNHead",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.segmentation.memory_tcn",
        "description": "SVTAS.svtas.model.heads.segmentation.memory_tcn",
        "peekOfCode": "class MemoryTCNHead(nn.Module):\n    def __init__(self,\n                 num_stages,\n                 num_layers,\n                 num_f_maps,\n                 dim,\n                 num_classes,\n                 sample_rate=1,\n                 out_feature=False):\n        super(MemoryTCNHead, self).__init__()",
        "detail": "SVTAS.svtas.model.heads.segmentation.memory_tcn",
        "documentation": {}
    },
    {
        "label": "MemoryDilationResidualLyaer",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.segmentation.memory_tcn",
        "description": "SVTAS.svtas.model.heads.segmentation.memory_tcn",
        "peekOfCode": "class MemoryDilationResidualLyaer(nn.Module):\n    def __init__(self, dilation, in_channels, out_channels):\n        super(MemoryDilationResidualLyaer, self).__init__()\n        self.conv_dilated = nn.Conv1d(in_channels, out_channels, 3, padding=0, dilation=dilation)\n        self.conv_1x1 = nn.Conv1d(out_channels, out_channels, 1)\n        # self.norm = nn.Dropout()\n        self.norm = nn.BatchNorm1d(out_channels)\n        self.dilation = dilation\n        self.memory = None\n    def _resert_memory(self):",
        "detail": "SVTAS.svtas.model.heads.segmentation.memory_tcn",
        "documentation": {}
    },
    {
        "label": "MemoryTemporalConvolutionBlock",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.segmentation.memory_tcn",
        "description": "SVTAS.svtas.model.heads.segmentation.memory_tcn",
        "peekOfCode": "class MemoryTemporalConvolutionBlock(nn.Module):\n    def __init__(self,\n                 num_layers,\n                 num_f_maps,\n                 dim,\n                 num_classes,\n                 out_feature=False):\n        super(MemoryTemporalConvolutionBlock, self).__init__()\n        self.out_feature = out_feature\n        self.conv_1x1 = nn.Conv1d(dim, num_f_maps, 1)",
        "detail": "SVTAS.svtas.model.heads.segmentation.memory_tcn",
        "documentation": {}
    },
    {
        "label": "MultiStageModel",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.segmentation.mstcn",
        "description": "SVTAS.svtas.model.heads.segmentation.mstcn",
        "peekOfCode": "class MultiStageModel(nn.Module):\n    def __init__(self,\n                 num_stages,\n                 num_layers,\n                 num_f_maps,\n                 dim,\n                 num_classes,\n                 sample_rate=1,\n                 out_feature=False):\n        super(MultiStageModel, self).__init__()",
        "detail": "SVTAS.svtas.model.heads.segmentation.mstcn",
        "documentation": {}
    },
    {
        "label": "SingleStageModel",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.segmentation.mstcn",
        "description": "SVTAS.svtas.model.heads.segmentation.mstcn",
        "peekOfCode": "class SingleStageModel(nn.Module):\n    def __init__(self, num_layers, num_f_maps, dim, num_classes, out_feature=False):\n        super(SingleStageModel, self).__init__()\n        self.out_feature = out_feature\n        self.conv_1x1 = nn.Conv1d(dim, num_f_maps, 1)\n        self.layers = nn.ModuleList([copy.deepcopy(DilatedResidualLayer(2 ** i, num_f_maps, num_f_maps)) for i in range(num_layers)])\n        self.conv_out = nn.Conv1d(num_f_maps, num_classes, 1)\n    def forward(self, x, mask):\n        feature_embedding = self.conv_1x1(x)\n        feature = feature_embedding",
        "detail": "SVTAS.svtas.model.heads.segmentation.mstcn",
        "documentation": {}
    },
    {
        "label": "DilatedResidualLayer",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.segmentation.mstcn",
        "description": "SVTAS.svtas.model.heads.segmentation.mstcn",
        "peekOfCode": "class DilatedResidualLayer(nn.Module):\n    def __init__(self, dilation, in_channels, out_channels):\n        super(DilatedResidualLayer, self).__init__()\n        self.conv_dilated = nn.Conv1d(in_channels, out_channels, 3, padding=dilation, dilation=dilation)\n        self.conv_1x1 = nn.Conv1d(out_channels, out_channels, 1)\n        # self.norm = nn.BatchNorm1d(out_channels)\n        self.norm = nn.Dropout()\n    def forward(self, x, mask):\n        out = F.relu(self.conv_dilated(x))\n        out = self.conv_1x1(out)",
        "detail": "SVTAS.svtas.model.heads.segmentation.mstcn",
        "documentation": {}
    },
    {
        "label": "TCN3DHead",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.segmentation.tcn_3d_head",
        "description": "SVTAS.svtas.model.heads.segmentation.tcn_3d_head",
        "peekOfCode": "class TCN3DHead(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 num_stages=1,\n                 num_layers=4,\n                 sample_rate=4,\n                 seg_in_channels=2048,\n                 hidden_channels=128,\n                 num_f_maps=64,\n                 out_feature=False):",
        "detail": "SVTAS.svtas.model.heads.segmentation.tcn_3d_head",
        "documentation": {}
    },
    {
        "label": "SingleStage3DModel",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.segmentation.tcn_3d_head",
        "description": "SVTAS.svtas.model.heads.segmentation.tcn_3d_head",
        "peekOfCode": "class SingleStage3DModel(nn.Module):\n    def __init__(self, num_layers, num_f_maps, dim, hidden_channels, out_feature=False):\n        super(SingleStage3DModel, self).__init__()\n        self.out_feature = out_feature\n        self.conv_1x1 = nn.Conv3d(dim, num_f_maps, (1, 1, 1))\n        self.layers = nn.ModuleList([copy.deepcopy(DilatedResidual3DLayer(2 ** i, num_f_maps, num_f_maps)) for i in range(num_layers)])\n        self.conv_out = nn.Conv3d(num_f_maps, hidden_channels, (1, 1, 1))\n    def forward(self, x, mask):\n        feature_embedding = self.conv_1x1(x)\n        feature = feature_embedding",
        "detail": "SVTAS.svtas.model.heads.segmentation.tcn_3d_head",
        "documentation": {}
    },
    {
        "label": "DilatedResidual3DLayer",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.segmentation.tcn_3d_head",
        "description": "SVTAS.svtas.model.heads.segmentation.tcn_3d_head",
        "peekOfCode": "class DilatedResidual3DLayer(nn.Module):\n    def __init__(self, dilation, in_channels, out_channels):\n        super(DilatedResidual3DLayer, self).__init__()\n        self.conv_dilated = nn.Conv3d(in_channels, out_channels, (3,3,3), stride=(1, 1, 1), padding=(dilation, dilation, dilation), dilation=(dilation, dilation, dilation))\n        self.conv_1x1 = nn.Conv3d(out_channels, out_channels, (1, 1, 1))\n        # self.norm = nn.LazyBatchNorm3d()\n        self.norm = nn.Dropout()\n    def forward(self, x, mask):\n        # !\n        # out = F.leaky_relu(self.conv_dilated(x))",
        "detail": "SVTAS.svtas.model.heads.segmentation.tcn_3d_head",
        "documentation": {}
    },
    {
        "label": "TextPredFCHead",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.text_pred.text_pred_fc_head",
        "description": "SVTAS.svtas.model.heads.text_pred.text_pred_fc_head",
        "peekOfCode": "class TextPredFCHead(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 clip_seg_num=15,\n                 sample_rate=4,\n                 drop_ratio=0.5,\n                 in_channels=2048,\n                 out_feature=False,\n                 init_std=0.01):\n        super().__init__()",
        "detail": "SVTAS.svtas.model.heads.text_pred.text_pred_fc_head",
        "documentation": {}
    },
    {
        "label": "Swish",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.conformer.activation",
        "description": "SVTAS.svtas.model.heads.utils.conformer.activation",
        "peekOfCode": "class Swish(nn.Module):\n    \"\"\"\n    Swish is a smooth, non-monotonic function that consistently matches or outperforms ReLU on deep networks applied\n    to a variety of challenging domains such as Image classification and Machine translation.\n    \"\"\"\n    def __init__(self):\n        super(Swish, self).__init__()\n    def forward(self, inputs: Tensor) -> Tensor:\n        return inputs * inputs.sigmoid()\nclass GLU(nn.Module):",
        "detail": "SVTAS.svtas.model.heads.utils.conformer.activation",
        "documentation": {}
    },
    {
        "label": "GLU",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.conformer.activation",
        "description": "SVTAS.svtas.model.heads.utils.conformer.activation",
        "peekOfCode": "class GLU(nn.Module):\n    \"\"\"\n    The gating mechanism is called Gated Linear Units (GLU), which was first introduced for natural language processing\n    in the paper “Language Modeling with Gated Convolutional Networks”\n    \"\"\"\n    def __init__(self, dim: int) -> None:\n        super(GLU, self).__init__()\n        self.dim = dim\n    def forward(self, inputs: Tensor) -> Tensor:\n        outputs, gate = inputs.chunk(2, dim=self.dim)",
        "detail": "SVTAS.svtas.model.heads.utils.conformer.activation",
        "documentation": {}
    },
    {
        "label": "RelativeMultiHeadAttention",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.conformer.attention",
        "description": "SVTAS.svtas.model.heads.utils.conformer.attention",
        "peekOfCode": "class RelativeMultiHeadAttention(nn.Module):\n    \"\"\"\n    Multi-head attention with relative positional encoding.\n    This concept was proposed in the \"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context\"\n    Args:\n        d_model (int): The dimension of model\n        num_heads (int): The number of attention heads.\n        dropout_p (float): probability of dropout\n    Inputs: query, key, value, pos_embedding, mask\n        - **query** (batch, time, dim): Tensor containing query vector",
        "detail": "SVTAS.svtas.model.heads.utils.conformer.attention",
        "documentation": {}
    },
    {
        "label": "MultiHeadedSelfAttentionModule",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.conformer.attention",
        "description": "SVTAS.svtas.model.heads.utils.conformer.attention",
        "peekOfCode": "class MultiHeadedSelfAttentionModule(nn.Module):\n    \"\"\"\n    Conformer employ multi-headed self-attention (MHSA) while integrating an important technique from Transformer-XL,\n    the relative sinusoidal positional encoding scheme. The relative positional encoding allows the self-attention\n    module to generalize better on different input length and the resulting encoder is more robust to the variance of\n    the utterance length. Conformer use prenorm residual units with dropout which helps training\n    and regularizing deeper models.\n    Args:\n        d_model (int): The dimension of model\n        num_heads (int): The number of attention heads.",
        "detail": "SVTAS.svtas.model.heads.utils.conformer.attention",
        "documentation": {}
    },
    {
        "label": "DepthwiseConv1d",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.conformer.convolution",
        "description": "SVTAS.svtas.model.heads.utils.conformer.convolution",
        "peekOfCode": "class DepthwiseConv1d(nn.Module):\n    \"\"\"\n    When groups == in_channels and out_channels == K * in_channels, where K is a positive integer,\n    this operation is termed in literature as depthwise convolution.\n    Args:\n        in_channels (int): Number of channels in the input\n        out_channels (int): Number of channels produced by the convolution\n        kernel_size (int or tuple): Size of the convolving kernel\n        stride (int, optional): Stride of the convolution. Default: 1\n        padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0",
        "detail": "SVTAS.svtas.model.heads.utils.conformer.convolution",
        "documentation": {}
    },
    {
        "label": "PointwiseConv1d",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.conformer.convolution",
        "description": "SVTAS.svtas.model.heads.utils.conformer.convolution",
        "peekOfCode": "class PointwiseConv1d(nn.Module):\n    \"\"\"\n    When kernel size == 1 conv1d, this operation is termed in literature as pointwise convolution.\n    This operation often used to match dimensions.\n    Args:\n        in_channels (int): Number of channels in the input\n        out_channels (int): Number of channels produced by the convolution\n        stride (int, optional): Stride of the convolution. Default: 1\n        padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0\n        bias (bool, optional): If True, adds a learnable bias to the output. Default: True",
        "detail": "SVTAS.svtas.model.heads.utils.conformer.convolution",
        "documentation": {}
    },
    {
        "label": "ConformerConvModule",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.conformer.convolution",
        "description": "SVTAS.svtas.model.heads.utils.conformer.convolution",
        "peekOfCode": "class ConformerConvModule(nn.Module):\n    \"\"\"\n    Conformer convolution module starts with a pointwise convolution and a gated linear unit (GLU).\n    This is followed by a single 1-D depthwise convolution layer. Batchnorm is  deployed just after the convolution\n    to aid training deep models.\n    Args:\n        in_channels (int): Number of channels in the input\n        kernel_size (int or tuple, optional): Size of the convolving kernel Default: 31\n        dropout_p (float, optional): probability of dropout\n    Inputs: inputs",
        "detail": "SVTAS.svtas.model.heads.utils.conformer.convolution",
        "documentation": {}
    },
    {
        "label": "Conv2dSubampling",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.conformer.convolution",
        "description": "SVTAS.svtas.model.heads.utils.conformer.convolution",
        "peekOfCode": "class Conv2dSubampling(nn.Module):\n    \"\"\"\n    Convolutional 2D subsampling (to 1/4 length)\n    Args:\n        in_channels (int): Number of channels in the input image\n        out_channels (int): Number of channels produced by the convolution\n    Inputs: inputs\n        - **inputs** (batch, time, dim): Tensor containing sequence of inputs\n    Returns: outputs, output_lengths\n        - **outputs** (batch, time, dim): Tensor produced by the convolution",
        "detail": "SVTAS.svtas.model.heads.utils.conformer.convolution",
        "documentation": {}
    },
    {
        "label": "PositionalEncoding",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.conformer.embedding",
        "description": "SVTAS.svtas.model.heads.utils.conformer.embedding",
        "peekOfCode": "class PositionalEncoding(nn.Module):\n    \"\"\"\n    Positional Encoding proposed in \"Attention Is All You Need\".\n    Since transformer contains no recurrence and no convolution, in order for the model to make\n    use of the order of the sequence, we must add some positional information.\n    \"Attention Is All You Need\" use sine and cosine functions of different frequencies:\n        PE_(pos, 2i)    =  sin(pos / power(10000, 2i / d_model))\n        PE_(pos, 2i+1)  =  cos(pos / power(10000, 2i / d_model))\n    \"\"\"\n    def __init__(self, d_model: int = 512, max_len: int = 10000) -> None:",
        "detail": "SVTAS.svtas.model.heads.utils.conformer.embedding",
        "documentation": {}
    },
    {
        "label": "ConformerBlock",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.conformer.encoder",
        "description": "SVTAS.svtas.model.heads.utils.conformer.encoder",
        "peekOfCode": "class ConformerBlock(nn.Module):\n    \"\"\"\n    Conformer block contains two Feed Forward modules sandwiching the Multi-Headed Self-Attention module\n    and the Convolution module. This sandwich structure is inspired by Macaron-Net, which proposes replacing\n    the original feed-forward layer in the Transformer block into two half-step feed-forward layers,\n    one before the attention layer and one after.\n    Args:\n        encoder_dim (int, optional): Dimension of conformer encoder\n        num_attention_heads (int, optional): Number of attention heads\n        feed_forward_expansion_factor (int, optional): Expansion factor of feed forward module",
        "detail": "SVTAS.svtas.model.heads.utils.conformer.encoder",
        "documentation": {}
    },
    {
        "label": "ConformerEncoder",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.conformer.encoder",
        "description": "SVTAS.svtas.model.heads.utils.conformer.encoder",
        "peekOfCode": "class ConformerEncoder(nn.Module):\n    \"\"\"\n    Conformer encoder first processes the input with a convolution subsampling layer and then\n    with a number of conformer blocks.\n    Args:\n        input_dim (int, optional): Dimension of input vector\n        encoder_dim (int, optional): Dimension of conformer encoder\n        num_layers (int, optional): Number of conformer blocks\n        num_attention_heads (int, optional): Number of attention heads\n        feed_forward_expansion_factor (int, optional): Expansion factor of feed forward module",
        "detail": "SVTAS.svtas.model.heads.utils.conformer.encoder",
        "documentation": {}
    },
    {
        "label": "FeedForwardModule",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.conformer.feed_forward",
        "description": "SVTAS.svtas.model.heads.utils.conformer.feed_forward",
        "peekOfCode": "class FeedForwardModule(nn.Module):\n    \"\"\"\n    Conformer Feed Forward Module follow pre-norm residual units and apply layer normalization within the residual unit\n    and on the input before the first ConFormerLinear layer. This module also apply Swish activation and dropout, which helps\n    regularizing the network.\n    Args:\n        encoder_dim (int): Dimension of conformer encoder\n        expansion_factor (int): Expansion factor of feed forward module.\n        dropout_p (float): Ratio of dropout\n    Inputs: inputs",
        "detail": "SVTAS.svtas.model.heads.utils.conformer.feed_forward",
        "documentation": {}
    },
    {
        "label": "ResidualConnectionModule",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.conformer.modules",
        "description": "SVTAS.svtas.model.heads.utils.conformer.modules",
        "peekOfCode": "class ResidualConnectionModule(nn.Module):\n    \"\"\"\n    Residual Connection Module.\n    outputs = (module(inputs) x module_factor + inputs x input_factor)\n    \"\"\"\n    def __init__(self, module: nn.Module, module_factor: float = 1.0, input_factor: float = 1.0):\n        super(ResidualConnectionModule, self).__init__()\n        self.module = module\n        self.module_factor = module_factor\n        self.input_factor = input_factor",
        "detail": "SVTAS.svtas.model.heads.utils.conformer.modules",
        "documentation": {}
    },
    {
        "label": "ConFormerLinear",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.conformer.modules",
        "description": "SVTAS.svtas.model.heads.utils.conformer.modules",
        "peekOfCode": "class ConFormerLinear(nn.Module):\n    \"\"\"\n    Wrapper class of torch.nn.Linear\n    Weight initialize by xavier initialization and bias initialize to zeros.\n    \"\"\"\n    def __init__(self, in_features: int, out_features: int, bias: bool = True) -> None:\n        super(ConFormerLinear, self).__init__()\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n        init.xavier_uniform_(self.linear.weight)\n        if bias:",
        "detail": "SVTAS.svtas.model.heads.utils.conformer.modules",
        "documentation": {}
    },
    {
        "label": "View",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.conformer.modules",
        "description": "SVTAS.svtas.model.heads.utils.conformer.modules",
        "peekOfCode": "class View(nn.Module):\n    \"\"\" Wrapper class of torch.view() for Sequential module. \"\"\"\n    def __init__(self, shape: tuple, contiguous: bool = False):\n        super(View, self).__init__()\n        self.shape = shape\n        self.contiguous = contiguous\n    def forward(self, x: Tensor) -> Tensor:\n        if self.contiguous:\n            x = x.contiguous()\n        return x.view(*self.shape)",
        "detail": "SVTAS.svtas.model.heads.utils.conformer.modules",
        "documentation": {}
    },
    {
        "label": "Transpose",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.conformer.modules",
        "description": "SVTAS.svtas.model.heads.utils.conformer.modules",
        "peekOfCode": "class Transpose(nn.Module):\n    \"\"\" Wrapper class of torch.transpose() for Sequential module. \"\"\"\n    def __init__(self, shape: tuple):\n        super(Transpose, self).__init__()\n        self.shape = shape\n    def forward(self, x: Tensor) -> Tensor:\n        return x.transpose(*self.shape)",
        "detail": "SVTAS.svtas.model.heads.utils.conformer.modules",
        "documentation": {}
    },
    {
        "label": "SelfAttention",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.oadtr.attention",
        "description": "SVTAS.svtas.model.heads.utils.oadtr.attention",
        "peekOfCode": "class SelfAttention(nn.Module):\n    def __init__(\n        self, dim, heads=8, qkv_bias=False, qk_scale=None, dropout_rate=0.0\n    ):\n        super().__init__()\n        self.num_heads = heads\n        head_dim = dim // heads\n        self.scale = qk_scale or head_dim ** -0.5\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(dropout_rate)",
        "detail": "SVTAS.svtas.model.heads.utils.oadtr.attention",
        "documentation": {}
    },
    {
        "label": "TriangularCausalMask",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.oadtr.attn",
        "description": "SVTAS.svtas.model.heads.utils.oadtr.attn",
        "peekOfCode": "class TriangularCausalMask():\n    def __init__(self, B, L, device=\"cpu\"):\n        mask_shape = [B, 1, L, L]\n        with torch.no_grad():\n            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n    @property\n    def mask(self):\n        return self._mask\nclass ProbMask():\n    def __init__(self, B, H, L, index, scores, device=\"cpu\"):",
        "detail": "SVTAS.svtas.model.heads.utils.oadtr.attn",
        "documentation": {}
    },
    {
        "label": "ProbMask",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.oadtr.attn",
        "description": "SVTAS.svtas.model.heads.utils.oadtr.attn",
        "peekOfCode": "class ProbMask():\n    def __init__(self, B, H, L, index, scores, device=\"cpu\"):\n        _mask = torch.ones(L, scores.shape[-1], dytpe=torch.bool).to(device).triu(1)\n        _mask_ex = _mask[None, None, :].expand(B, H, L, scores.shape[-1])\n        indicator = _mask_ex[torch.arange(B)[:, None, None],\n                    torch.arange(H)[None, :, None],\n                    index, :].to(device)\n        self._mask = indicator.view(scores.shape).to(device)\n    @property\n    def mask(self):",
        "detail": "SVTAS.svtas.model.heads.utils.oadtr.attn",
        "documentation": {}
    },
    {
        "label": "FullAttention",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.oadtr.attn",
        "description": "SVTAS.svtas.model.heads.utils.oadtr.attn",
        "peekOfCode": "class FullAttention(nn.Module):\n    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1):\n        super(FullAttention, self).__init__()\n        self.scale = scale\n        self.mask_flag = mask_flag\n        self.dropout = nn.Dropout(attention_dropout)\n    def forward(self, queries, keys, values, attn_mask):\n        B, L, H, E = queries.shape\n        _, S, _, D = values.shape\n        scale = self.scale or 1. / sqrt(E)",
        "detail": "SVTAS.svtas.model.heads.utils.oadtr.attn",
        "documentation": {}
    },
    {
        "label": "ProbAttention",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.oadtr.attn",
        "description": "SVTAS.svtas.model.heads.utils.oadtr.attn",
        "peekOfCode": "class ProbAttention(nn.Module):\n    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1):\n        super(ProbAttention, self).__init__()\n        self.factor = factor\n        self.scale = scale\n        self.mask_flag = mask_flag\n        self.dropout = nn.Dropout(attention_dropout)\n    def _prob_QK(self, Q, K, sample_k, n_top):\n        # Q [B, H, L, D]\n        B, H, L, E = K.shape",
        "detail": "SVTAS.svtas.model.heads.utils.oadtr.attn",
        "documentation": {}
    },
    {
        "label": "AttentionLayer",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.oadtr.attn",
        "description": "SVTAS.svtas.model.heads.utils.oadtr.attn",
        "peekOfCode": "class AttentionLayer(nn.Module):\n    def __init__(self, attention, d_model, n_heads, d_keys=None,\n                 d_values=None):\n        super(AttentionLayer, self).__init__()\n        d_keys = d_keys or (d_model // n_heads)\n        d_values = d_values or (d_model // n_heads)\n        self.inner_attention = attention\n        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n        self.value_projection = nn.Linear(d_model, d_values * n_heads)",
        "detail": "SVTAS.svtas.model.heads.utils.oadtr.attn",
        "documentation": {}
    },
    {
        "label": "DecoderLayer",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.oadtr.decoder",
        "description": "SVTAS.svtas.model.heads.utils.oadtr.decoder",
        "peekOfCode": "class DecoderLayer(nn.Module):\n    def __init__(self, self_attention, cross_attention, d_model, d_ff=None,\n                 dropout=0.1, activation=\"relu\"):\n        super(DecoderLayer, self).__init__()\n        d_ff = d_ff or 4*d_model\n        self.self_attention = self_attention\n        self.cross_attention = cross_attention\n        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n        self.norm1 = nn.LayerNorm(d_model)",
        "detail": "SVTAS.svtas.model.heads.utils.oadtr.decoder",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.oadtr.decoder",
        "description": "SVTAS.svtas.model.heads.utils.oadtr.decoder",
        "peekOfCode": "class Decoder(nn.Module):\n    def __init__(self, layers, norm_layer=None):\n        super(Decoder, self).__init__()\n        self.layers = nn.ModuleList(layers)\n        self.norm = norm_layer\n    def forward(self, x, cross, x_mask=None, cross_mask=None):\n        for layer in self.layers:\n            x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)\n        if self.norm is not None:\n            x = self.norm(x)",
        "detail": "SVTAS.svtas.model.heads.utils.oadtr.decoder",
        "documentation": {}
    },
    {
        "label": "FixedPositionalEncoding",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.oadtr.position_encoding",
        "description": "SVTAS.svtas.model.heads.utils.oadtr.position_encoding",
        "peekOfCode": "class FixedPositionalEncoding(nn.Module):\n    def __init__(self, embedding_dim, max_length=5000):\n        super(FixedPositionalEncoding, self).__init__()\n        pe = torch.zeros(max_length, embedding_dim)\n        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, embedding_dim, 2).float()\n            * (-torch.log(torch.tensor(10000.0)) / embedding_dim)\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)",
        "detail": "SVTAS.svtas.model.heads.utils.oadtr.position_encoding",
        "documentation": {}
    },
    {
        "label": "LearnedPositionalEncoding",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.oadtr.position_encoding",
        "description": "SVTAS.svtas.model.heads.utils.oadtr.position_encoding",
        "peekOfCode": "class LearnedPositionalEncoding(nn.Module):\n    def __init__(self, max_position_embeddings, embedding_dim, seq_length):\n        super(LearnedPositionalEncoding, self).__init__()\n        self.pe = nn.Embedding(max_position_embeddings, embedding_dim)\n        self.seq_length = seq_length\n        self.register_buffer(\n            \"position_ids\",\n            torch.arange(max_position_embeddings).expand((1, -1)),\n        )\n    def forward(self, x, position_ids=None):",
        "detail": "SVTAS.svtas.model.heads.utils.oadtr.position_encoding",
        "documentation": {}
    },
    {
        "label": "Residual",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.oadtr.transformer",
        "description": "SVTAS.svtas.model.heads.utils.oadtr.transformer",
        "peekOfCode": "class Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n    def forward(self, x):\n        return self.fn(x) + x\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)",
        "detail": "SVTAS.svtas.model.heads.utils.oadtr.transformer",
        "documentation": {}
    },
    {
        "label": "PreNorm",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.oadtr.transformer",
        "description": "SVTAS.svtas.model.heads.utils.oadtr.transformer",
        "peekOfCode": "class PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n    def forward(self, x):\n        return self.fn(self.norm(x))\nclass PreNormDrop(nn.Module):\n    def __init__(self, dim, dropout_rate, fn):\n        super().__init__()",
        "detail": "SVTAS.svtas.model.heads.utils.oadtr.transformer",
        "documentation": {}
    },
    {
        "label": "PreNormDrop",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.oadtr.transformer",
        "description": "SVTAS.svtas.model.heads.utils.oadtr.transformer",
        "peekOfCode": "class PreNormDrop(nn.Module):\n    def __init__(self, dim, dropout_rate, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.dropout = nn.Dropout(p=dropout_rate)\n        self.fn = fn\n    def forward(self, x):\n        return self.dropout(self.fn(self.norm(x)))\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout_rate):",
        "detail": "SVTAS.svtas.model.heads.utils.oadtr.transformer",
        "documentation": {}
    },
    {
        "label": "FeedForward",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.oadtr.transformer",
        "description": "SVTAS.svtas.model.heads.utils.oadtr.transformer",
        "peekOfCode": "class FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout_rate):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(p=dropout_rate),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(p=dropout_rate),\n        )",
        "detail": "SVTAS.svtas.model.heads.utils.oadtr.transformer",
        "documentation": {}
    },
    {
        "label": "TransformerModel",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.heads.utils.oadtr.transformer",
        "description": "SVTAS.svtas.model.heads.utils.oadtr.transformer",
        "peekOfCode": "class TransformerModel(nn.Module):\n    def __init__(\n        self,\n        dim,\n        depth,\n        heads,\n        mlp_dim,\n        dropout_rate=0.1,\n        attn_dropout_rate=0.1,\n    ):",
        "detail": "SVTAS.svtas.model.heads.utils.oadtr.transformer",
        "documentation": {}
    },
    {
        "label": "BridgePromptCLIPSegmentationLoss",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.losses.bridge_prompt_clip_loss",
        "description": "SVTAS.svtas.model.losses.bridge_prompt_clip_loss",
        "peekOfCode": "class BridgePromptCLIPSegmentationLoss(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 cnt_max=7,\n                 sample_rate=4,\n                 smooth_weight=0.15,\n                 img_seg_loss_weights=1.0,\n                 clip_loss_weight=1.0,\n                 ignore_index=-100,\n                 is_segmentation=False):",
        "detail": "SVTAS.svtas.model.losses.bridge_prompt_clip_loss",
        "documentation": {}
    },
    {
        "label": "BridgePromptCLIPLoss",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.losses.bridge_prompt_clip_loss",
        "description": "SVTAS.svtas.model.losses.bridge_prompt_clip_loss",
        "peekOfCode": "class BridgePromptCLIPLoss(nn.Module):\n    def __init__(self, loss_weight=1.0, need_logit_scale=True, ignore_index=-100) -> None:\n        super().__init__()\n        self.ignore_index = ignore_index\n        self.loss_weight = loss_weight\n        self.error_metric = nn.KLDivLoss(reduction=\"none\")\n        if need_logit_scale is True:\n            self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n        else:\n            self.logit_scale = None",
        "detail": "SVTAS.svtas.model.losses.bridge_prompt_clip_loss",
        "documentation": {}
    },
    {
        "label": "ETESVSLoss",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.losses.etesvs_loss",
        "description": "SVTAS.svtas.model.losses.etesvs_loss",
        "peekOfCode": "class ETESVSLoss(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 sample_rate=4,\n                 backone_loss_weight=1.0,\n                 neck_loss_weight=1.0,\n                 head_loss_weight=1.0,\n                 smooth_weight=0.15,\n                 ignore_index=-100):\n        super().__init__()",
        "detail": "SVTAS.svtas.model.losses.etesvs_loss",
        "documentation": {}
    },
    {
        "label": "TemporalSplitMeanPoolingLoss",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.losses.etesvs_loss",
        "description": "SVTAS.svtas.model.losses.etesvs_loss",
        "peekOfCode": "class TemporalSplitMeanPoolingLoss(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 ignore_index=-100):\n        super().__init__()\n        self.num_classes = num_classes\n        self.ignore_index = ignore_index\n        self.ce = nn.CrossEntropyLoss(ignore_index=self.ignore_index, reduction='none')\n        self.elps = 1e-10\n    def forward(self, x, gt, mask):",
        "detail": "SVTAS.svtas.model.losses.etesvs_loss",
        "documentation": {}
    },
    {
        "label": "TemporalClassNumMSELoss",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.losses.etesvs_loss",
        "description": "SVTAS.svtas.model.losses.etesvs_loss",
        "peekOfCode": "class TemporalClassNumMSELoss(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 ignore_index=-100):\n        super().__init__()\n        self.num_classes = num_classes\n        self.ignore_index = ignore_index\n        # self.mse = nn.MSELoss(reduction='none')\n        self.mse = nn.L1Loss(reduction='none')\n        self.softmax = nn.Softmax(dim=1)",
        "detail": "SVTAS.svtas.model.losses.etesvs_loss",
        "documentation": {}
    },
    {
        "label": "FocalLoss",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.losses.etesvs_loss",
        "description": "SVTAS.svtas.model.losses.etesvs_loss",
        "peekOfCode": "class FocalLoss(nn.Module):\n    def __init__(self, num_classes, alpha=0.25, gamma=2, size_average=True):\n        super(FocalLoss,self).__init__()\n        self.size_average = size_average\n        if isinstance(alpha, list):\n            assert len(alpha)==num_classes   \n            self.alpha = torch.Tensor(alpha)\n        else:\n            assert alpha < 1  \n            self.alpha = torch.zeros(num_classes)",
        "detail": "SVTAS.svtas.model.losses.etesvs_loss",
        "documentation": {}
    },
    {
        "label": "RecognitionSegmentationLoss",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.losses.recognition_segmentation_loss",
        "description": "SVTAS.svtas.model.losses.recognition_segmentation_loss",
        "peekOfCode": "class RecognitionSegmentationLoss(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 label_mode='soft',\n                 sample_rate=4,\n                 loss_weight=1.0,\n                 smooth_weight=0.15,\n                 ignore_index=-100):\n        super().__init__()\n        self.loss_weight = loss_weight",
        "detail": "SVTAS.svtas.model.losses.recognition_segmentation_loss",
        "documentation": {}
    },
    {
        "label": "SoftLabelRocgnitionLoss",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.losses.recognition_segmentation_loss",
        "description": "SVTAS.svtas.model.losses.recognition_segmentation_loss",
        "peekOfCode": "class SoftLabelRocgnitionLoss(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 loss_weight=1.0,\n                 ignore_index=-100):\n        super().__init__()\n        self.loss_weight = loss_weight\n        self.num_classes = num_classes\n        self.ignore_index = ignore_index\n        self.ce = nn.CrossEntropyLoss(ignore_index=self.ignore_index, reduction='none')",
        "detail": "SVTAS.svtas.model.losses.recognition_segmentation_loss",
        "documentation": {}
    },
    {
        "label": "SgementationCLIPLoss",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.losses.segmentation_clip_loss",
        "description": "SVTAS.svtas.model.losses.segmentation_clip_loss",
        "peekOfCode": "class SgementationCLIPLoss(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 sample_rate=4,\n                 smooth_weight=0.15,\n                 seg_loss_weight=1.0,\n                 clip_loss_weight=1.0,\n                 ignore_index=-100):\n        super().__init__()\n        self.ignore_index = ignore_index",
        "detail": "SVTAS.svtas.model.losses.segmentation_clip_loss",
        "documentation": {}
    },
    {
        "label": "CLIPLoss",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.losses.segmentation_clip_loss",
        "description": "SVTAS.svtas.model.losses.segmentation_clip_loss",
        "peekOfCode": "class CLIPLoss(nn.Module):\n    def __init__(self,\n                 sample_rate=4,\n                 loss_weights=1.0):\n        super().__init__()\n        self.loss_weights = loss_weights\n        self.sample_rate = sample_rate\n        self.ce_i = nn.CrossEntropyLoss(reduction='none')\n        self.ce_t = nn.CrossEntropyLoss(reduction='none')\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))",
        "detail": "SVTAS.svtas.model.losses.segmentation_clip_loss",
        "documentation": {}
    },
    {
        "label": "SegmentationLoss",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.losses.segmentation_loss",
        "description": "SVTAS.svtas.model.losses.segmentation_loss",
        "peekOfCode": "class SegmentationLoss(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 loss_weight=1.0,\n                 sample_rate=1,\n                 smooth_weight=0.5,\n                 ignore_index=-100,\n                 class_weight=None):\n        super().__init__()\n        self.smooth_weight = smooth_weight",
        "detail": "SVTAS.svtas.model.losses.segmentation_loss",
        "documentation": {}
    },
    {
        "label": "ActionCLIPSegmentationLoss",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.losses.segmentation_loss",
        "description": "SVTAS.svtas.model.losses.segmentation_loss",
        "peekOfCode": "class ActionCLIPSegmentationLoss(SegmentationLoss):\n    def __init__(self,\n                 **kwargs):\n        super().__init__(**kwargs)\n    def forward(self, model_output, input_data):\n        # score shape [stage_num N C T]\n        # masks shape [N T]\n        img_feature, text_feature, head_score = model_output[\"image_feature\"], model_output[\"text_feature\"], model_output[\"output\"]\n        masks, labels, precise_sliding_num = input_data[\"masks\"], input_data[\"labels\"], input_data['precise_sliding_num']\n        _, b, _, t = head_score.shape",
        "detail": "SVTAS.svtas.model.losses.segmentation_loss",
        "documentation": {}
    },
    {
        "label": "StreamSegmentationLoss",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.losses.steam_segmentation_loss",
        "description": "SVTAS.svtas.model.losses.steam_segmentation_loss",
        "peekOfCode": "class StreamSegmentationLoss(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 sample_rate=4,\n                 backone_loss_weight=1.0,\n                 head_loss_weight=1.0,\n                 smooth_weight=0.15,\n                 ignore_index=-100):\n        super().__init__()\n        self.backone_loss_weight = backone_loss_weight",
        "detail": "SVTAS.svtas.model.losses.steam_segmentation_loss",
        "documentation": {}
    },
    {
        "label": "VideoPredictionLoss",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.losses.video_prediction_loss",
        "description": "SVTAS.svtas.model.losses.video_prediction_loss",
        "peekOfCode": "class VideoPredictionLoss(nn.Module):\n    def __init__(self,\n                 num_classes,\n                 sample_rate=1,\n                 smooth_weight=0.5,\n                 pred_loss_weight=1.0,\n                 segment_loss_weight=1.0,\n                 ignore_index=-100):\n        super().__init__()\n        self.smooth_weight = smooth_weight",
        "detail": "SVTAS.svtas.model.losses.video_prediction_loss",
        "documentation": {}
    },
    {
        "label": "LayerNorm",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.necks.action_clip_fusion_model",
        "description": "SVTAS.svtas.model.necks.action_clip_fusion_model",
        "peekOfCode": "class LayerNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-12):\n        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n        \"\"\"\n        super(LayerNorm, self).__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.bias = nn.Parameter(torch.zeros(hidden_size))\n        self.variance_epsilon = eps\n    def forward(self, x):\n        u = x.mean(-1, keepdim=True)",
        "detail": "SVTAS.svtas.model.necks.action_clip_fusion_model",
        "documentation": {}
    },
    {
        "label": "QuickGELU",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.necks.action_clip_fusion_model",
        "description": "SVTAS.svtas.model.necks.action_clip_fusion_model",
        "peekOfCode": "class QuickGELU(nn.Module):\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\nclass ResidualAttentionBlock(nn.Module):\n    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(d_model, n_head)\n        self.ln_1 = LayerNorm(d_model)\n        self.mlp = nn.Sequential(OrderedDict([\n            (\"c_fc\", nn.Linear(d_model, d_model * 4)),",
        "detail": "SVTAS.svtas.model.necks.action_clip_fusion_model",
        "documentation": {}
    },
    {
        "label": "ResidualAttentionBlock",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.necks.action_clip_fusion_model",
        "description": "SVTAS.svtas.model.necks.action_clip_fusion_model",
        "peekOfCode": "class ResidualAttentionBlock(nn.Module):\n    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(d_model, n_head)\n        self.ln_1 = LayerNorm(d_model)\n        self.mlp = nn.Sequential(OrderedDict([\n            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n            (\"gelu\", QuickGELU()),\n            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n        ]))",
        "detail": "SVTAS.svtas.model.necks.action_clip_fusion_model",
        "documentation": {}
    },
    {
        "label": "TAggregate",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.necks.action_clip_fusion_model",
        "description": "SVTAS.svtas.model.necks.action_clip_fusion_model",
        "peekOfCode": "class TAggregate(nn.Module):\n    def __init__(self, clip_length=None, embed_dim=2048, n_layers=6):\n        super(TAggregate, self).__init__()\n        self.clip_length = clip_length\n        drop_rate = 0.\n        enc_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=8)\n        self.transformer_enc = nn.TransformerEncoder(enc_layer, num_layers=n_layers, norm=nn.LayerNorm(\n            embed_dim))\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, clip_length + 1, embed_dim))",
        "detail": "SVTAS.svtas.model.necks.action_clip_fusion_model",
        "documentation": {}
    },
    {
        "label": "TemporalTransformer",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.necks.action_clip_fusion_model",
        "description": "SVTAS.svtas.model.necks.action_clip_fusion_model",
        "peekOfCode": "class TemporalTransformer(nn.Module):\n    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n        super().__init__()\n        self.width = width\n        self.layers = layers\n        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n    def forward(self, x: torch.Tensor):\n        return self.resblocks((x))\n@NECKS.register()\nclass ActionCLIPFusionNeck(nn.Module):",
        "detail": "SVTAS.svtas.model.necks.action_clip_fusion_model",
        "documentation": {}
    },
    {
        "label": "ActionCLIPFusionNeck",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.necks.action_clip_fusion_model",
        "description": "SVTAS.svtas.model.necks.action_clip_fusion_model",
        "peekOfCode": "class ActionCLIPFusionNeck(nn.Module):\n    def __init__(self,\n                 sim_head,\n                 embed_dim_cfg,\n                 context_length_cfg,\n                 transformer_width_cfg,\n                 clip_seg_num,\n                 TemporalTransformer_layer_num=6,\n                 pretrained=None):\n        super().__init__()",
        "detail": "SVTAS.svtas.model.necks.action_clip_fusion_model",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.necks.action_clip_fusion_model",
        "description": "SVTAS.svtas.model.necks.action_clip_fusion_model",
        "peekOfCode": "def trunc_normal_(x, mean=0., std=1.):\n    # From https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/12\n    return x.normal_().fmod_(2).mul_(std).add_(mean)\nclass TAggregate(nn.Module):\n    def __init__(self, clip_length=None, embed_dim=2048, n_layers=6):\n        super(TAggregate, self).__init__()\n        self.clip_length = clip_length\n        drop_rate = 0.\n        enc_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=8)\n        self.transformer_enc = nn.TransformerEncoder(enc_layer, num_layers=n_layers, norm=nn.LayerNorm(",
        "detail": "SVTAS.svtas.model.necks.action_clip_fusion_model",
        "documentation": {}
    },
    {
        "label": "AvgPoolNeck",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.necks.avg_pool_neck",
        "description": "SVTAS.svtas.model.necks.avg_pool_neck",
        "peekOfCode": "class AvgPoolNeck(nn.Module):\n    def __init__(self,\n                 num_classes=11,\n                 in_channels=1280,\n                 clip_seg_num=30,\n                 drop_ratio=0.5,\n                 need_pool=True,\n                 need_pre_cls=False):\n        super().__init__()\n        self.clip_seg_num = clip_seg_num",
        "detail": "SVTAS.svtas.model.necks.avg_pool_neck",
        "documentation": {}
    },
    {
        "label": "LayerNorm",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.necks.bridge_fusion_earlyhyp",
        "description": "SVTAS.svtas.model.necks.bridge_fusion_earlyhyp",
        "peekOfCode": "class LayerNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-12):\n        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n        \"\"\"\n        super(LayerNorm, self).__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.bias = nn.Parameter(torch.zeros(hidden_size))\n        self.variance_epsilon = eps\n    def forward(self, x):\n        u = x.mean(-1, keepdim=True)",
        "detail": "SVTAS.svtas.model.necks.bridge_fusion_earlyhyp",
        "documentation": {}
    },
    {
        "label": "BPromptFusing",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.necks.bridge_fusion_earlyhyp",
        "description": "SVTAS.svtas.model.necks.bridge_fusion_earlyhyp",
        "peekOfCode": "class BPromptFusing(nn.Module):\n    def __init__(self, clip_length=None, embed_dim=2048, n_layers=6, heads=8):\n        super(BPromptFusing, self).__init__()\n        self.clip_length = clip_length\n        drop_rate = 0.\n        enc_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=heads)\n        self.transformer_enc = nn.TransformerEncoder(enc_layer, num_layers=n_layers, norm=nn.LayerNorm(\n            embed_dim))\n        self.cnt_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.sep_token = nn.Parameter(torch.zeros(1, 1, embed_dim))",
        "detail": "SVTAS.svtas.model.necks.bridge_fusion_earlyhyp",
        "documentation": {}
    },
    {
        "label": "BridgePromptFusionEarlyhyp",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.necks.bridge_fusion_earlyhyp",
        "description": "SVTAS.svtas.model.necks.bridge_fusion_earlyhyp",
        "peekOfCode": "class BridgePromptFusionEarlyhyp(nn.Module):\n    def __init__(self,\n                 embedding_dim=512,\n                 num_layers=6,\n                 cnt_max=7,\n                 context_length=77,\n                 transformer_width=512,\n                 clip_seg_num=32):\n        super().__init__()\n        self.cnt_max = cnt_max",
        "detail": "SVTAS.svtas.model.necks.bridge_fusion_earlyhyp",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.necks.bridge_fusion_earlyhyp",
        "description": "SVTAS.svtas.model.necks.bridge_fusion_earlyhyp",
        "peekOfCode": "def trunc_normal_(x, mean=0., std=1.):\n    return x.normal_().fmod_(2).mul_(std).add_(mean)\nclass BPromptFusing(nn.Module):\n    def __init__(self, clip_length=None, embed_dim=2048, n_layers=6, heads=8):\n        super(BPromptFusing, self).__init__()\n        self.clip_length = clip_length\n        drop_rate = 0.\n        enc_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=heads)\n        self.transformer_enc = nn.TransformerEncoder(enc_layer, num_layers=n_layers, norm=nn.LayerNorm(\n            embed_dim))",
        "detail": "SVTAS.svtas.model.necks.bridge_fusion_earlyhyp",
        "documentation": {}
    },
    {
        "label": "ConvLSTMCell",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.necks.convlstm",
        "description": "SVTAS.svtas.model.necks.convlstm",
        "peekOfCode": "class ConvLSTMCell(nn.Module):\n    def __init__(self,\n                 input_dim,\n                 hidden_dim,\n                 kernel_size,\n                 bias,\n                 is_deep_wise_conv=True):\n        \"\"\"\n        Initialize ConvLSTM cell.\n        Parameters",
        "detail": "SVTAS.svtas.model.necks.convlstm",
        "documentation": {}
    },
    {
        "label": "ConvLSTM",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.necks.convlstm",
        "description": "SVTAS.svtas.model.necks.convlstm",
        "peekOfCode": "class ConvLSTM(nn.Module):\n    \"\"\"\n    Parameters:\n        input_dim: Number of channels in input\n        hidden_dim: Number of hidden channels\n        kernel_size: Size of kernel in convolutions\n        num_layers: Number of LSTM layers stacked on each other\n        batch_first: Whether or not dimension 0 is the batch or not\n        bias: Bias or no bias in Convolution\n        return_all_layers: Return the list of computations for all layers",
        "detail": "SVTAS.svtas.model.necks.convlstm",
        "documentation": {}
    },
    {
        "label": "ETESVSNeck",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.necks.etesvs_neck",
        "description": "SVTAS.svtas.model.necks.etesvs_neck",
        "peekOfCode": "class ETESVSNeck(nn.Module):\n    def __init__(self,\n                 num_classes=11,\n                 num_layers=1,\n                 cls_in_channel=1280,\n                 cls_hidden_channel=1280,\n                 seg_in_channel=320,\n                 seg_hidden_channel=320,\n                 clip_seg_num=30,\n                 drop_ratio=0.5,",
        "detail": "SVTAS.svtas.model.necks.etesvs_neck",
        "documentation": {}
    },
    {
        "label": "ConvLSTMResidualLayer",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.necks.memory_layer",
        "description": "SVTAS.svtas.model.necks.memory_layer",
        "peekOfCode": "class ConvLSTMResidualLayer(nn.Module):\n    def __init__(self, in_channels, hidden_channels, num_classes, num_layers=1, bidirectional=False):\n        super().__init__()\n        self.hidden_channels = hidden_channels\n        self.in_channels = in_channels\n        self.num_classes = num_classes\n        self.num_layers = num_layers\n        self.sample_rate = 4\n        if bidirectional is False:\n            self.direction = 1",
        "detail": "SVTAS.svtas.model.necks.memory_layer",
        "documentation": {}
    },
    {
        "label": "ST3DNeck",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.necks.st_3d_neck",
        "description": "SVTAS.svtas.model.necks.st_3d_neck",
        "peekOfCode": "class ST3DNeck(nn.Module):\n    def __init__(self,\n                 num_classes=11,\n                 num_layers=1,\n                 cls_in_channel=1280,\n                 cls_hidden_channel=1280,\n                 seg_in_channel=320,\n                 seg_hidden_channel=320,\n                 clip_seg_num=30,\n                 drop_ratio=0.5,",
        "detail": "SVTAS.svtas.model.necks.st_3d_neck",
        "documentation": {}
    },
    {
        "label": "StreamScorePostProcessingWithLBS",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.post_precessings.lbs",
        "description": "SVTAS.svtas.model.post_precessings.lbs",
        "peekOfCode": "class StreamScorePostProcessingWithLBS():\n    def __init__(self,\n                 num_classes,\n                 sliding_window,\n                 actions_map_file_path,\n                 lbs_burr=7,\n                 lbs_window=40,\n                 lbs_Confidence=3.1,\n                 ignore_index=-100,\n                 bg_class_name=[\"background\", \"None\"]):",
        "detail": "SVTAS.svtas.model.post_precessings.lbs",
        "documentation": {}
    },
    {
        "label": "OpticalFlowPostProcessing",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.post_precessings.optical_flow_post_processing",
        "description": "SVTAS.svtas.model.post_precessings.optical_flow_post_processing",
        "peekOfCode": "class OpticalFlowPostProcessing():\n    def __init__(self,\n                 sliding_window,\n                 post_transforms=[dict(Clamp = dict(min_val=-20, max_val=20)),\n                                            dict(ToUInt8 = None)],\n                 fps=15,\n                 need_visualize=False,\n                 ignore_index=-100):\n        self.sliding_window = sliding_window\n        self.fps = fps",
        "detail": "SVTAS.svtas.model.post_precessings.optical_flow_post_processing",
        "documentation": {}
    },
    {
        "label": "ScorePostProcessing",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.post_precessings.score_post_processing",
        "description": "SVTAS.svtas.model.post_precessings.score_post_processing",
        "peekOfCode": "class ScorePostProcessing():\n    def __init__(self,\n                 num_classes,\n                 ignore_index=-100):\n        self.num_classes = num_classes\n        self.ignore_index = ignore_index\n        self.init_flag = False\n        self.epls = 1e-10\n    def init_scores(self, sliding_num, batch_size):\n        self.pred_scores = None",
        "detail": "SVTAS.svtas.model.post_precessings.score_post_processing",
        "documentation": {}
    },
    {
        "label": "StreamFeaturePostProcessing",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.post_precessings.stream_feature_post_processing",
        "description": "SVTAS.svtas.model.post_precessings.stream_feature_post_processing",
        "peekOfCode": "class StreamFeaturePostProcessing():\n    def __init__(self,\n                 sliding_window,\n                 ignore_index=-100):\n        self.sliding_window = sliding_window\n        self.ignore_index = ignore_index\n        self.init_flag = False\n    def init_scores(self, sliding_num, batch_size):\n        self.pred_feature = []\n        self.video_gt = []",
        "detail": "SVTAS.svtas.model.post_precessings.stream_feature_post_processing",
        "documentation": {}
    },
    {
        "label": "StreamScorePostProcessing",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.post_precessings.stream_score_post_processing",
        "description": "SVTAS.svtas.model.post_precessings.stream_score_post_processing",
        "peekOfCode": "class StreamScorePostProcessing():\n    def __init__(self,\n                 sliding_window,\n                 ignore_index=-100):\n        self.sliding_window = sliding_window\n        self.ignore_index = ignore_index\n        self.init_flag = False\n        self.epls = 1e-10\n    def init_scores(self, sliding_num, batch_size):\n        self.pred_scores = []",
        "detail": "SVTAS.svtas.model.post_precessings.stream_score_post_processing",
        "documentation": {}
    },
    {
        "label": "build_backbone",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.builder",
        "description": "SVTAS.svtas.model.builder",
        "peekOfCode": "def build_backbone(cfg):\n    \"\"\"Build backbone.\"\"\"\n    return build(cfg, BACKBONES)\ndef build_head(cfg):\n    \"\"\"Build head.\"\"\"\n    return build(cfg, HEADS)\ndef build_neck(cfg):\n    \"\"\"Build neck.\"\"\"\n    return build(cfg, NECKS)\ndef build_post_precessing(cfg):",
        "detail": "SVTAS.svtas.model.builder",
        "documentation": {}
    },
    {
        "label": "build_head",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.builder",
        "description": "SVTAS.svtas.model.builder",
        "peekOfCode": "def build_head(cfg):\n    \"\"\"Build head.\"\"\"\n    return build(cfg, HEADS)\ndef build_neck(cfg):\n    \"\"\"Build neck.\"\"\"\n    return build(cfg, NECKS)\ndef build_post_precessing(cfg):\n    \"\"\"Build loss.\"\"\"\n    return build(cfg, POSTPRECESSING)\ndef build_loss(cfg):",
        "detail": "SVTAS.svtas.model.builder",
        "documentation": {}
    },
    {
        "label": "build_neck",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.builder",
        "description": "SVTAS.svtas.model.builder",
        "peekOfCode": "def build_neck(cfg):\n    \"\"\"Build neck.\"\"\"\n    return build(cfg, NECKS)\ndef build_post_precessing(cfg):\n    \"\"\"Build loss.\"\"\"\n    return build(cfg, POSTPRECESSING)\ndef build_loss(cfg):\n    \"\"\"Build loss.\"\"\"\n    return build(cfg, LOSSES)\ndef build_architecture(cfg):",
        "detail": "SVTAS.svtas.model.builder",
        "documentation": {}
    },
    {
        "label": "build_post_precessing",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.builder",
        "description": "SVTAS.svtas.model.builder",
        "peekOfCode": "def build_post_precessing(cfg):\n    \"\"\"Build loss.\"\"\"\n    return build(cfg, POSTPRECESSING)\ndef build_loss(cfg):\n    \"\"\"Build loss.\"\"\"\n    return build(cfg, LOSSES)\ndef build_architecture(cfg):\n    \"\"\"Build recognizer.\"\"\"\n    return build(cfg, ARCHITECTURE, key='architecture')\ndef build_model(cfg):",
        "detail": "SVTAS.svtas.model.builder",
        "documentation": {}
    },
    {
        "label": "build_loss",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.builder",
        "description": "SVTAS.svtas.model.builder",
        "peekOfCode": "def build_loss(cfg):\n    \"\"\"Build loss.\"\"\"\n    return build(cfg, LOSSES)\ndef build_architecture(cfg):\n    \"\"\"Build recognizer.\"\"\"\n    return build(cfg, ARCHITECTURE, key='architecture')\ndef build_model(cfg):\n    \"\"\"Build model.\"\"\"\n    args = cfg.copy()\n    obj_type = args.get('architecture')",
        "detail": "SVTAS.svtas.model.builder",
        "documentation": {}
    },
    {
        "label": "build_architecture",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.builder",
        "description": "SVTAS.svtas.model.builder",
        "peekOfCode": "def build_architecture(cfg):\n    \"\"\"Build recognizer.\"\"\"\n    return build(cfg, ARCHITECTURE, key='architecture')\ndef build_model(cfg):\n    \"\"\"Build model.\"\"\"\n    args = cfg.copy()\n    obj_type = args.get('architecture')\n    if obj_type in ARCHITECTURE:\n        return build_architecture(cfg)\n    raise ValueError(f'{obj_type} is not registered in '",
        "detail": "SVTAS.svtas.model.builder",
        "documentation": {}
    },
    {
        "label": "build_model",
        "kind": 2,
        "importPath": "SVTAS.svtas.model.builder",
        "description": "SVTAS.svtas.model.builder",
        "peekOfCode": "def build_model(cfg):\n    \"\"\"Build model.\"\"\"\n    args = cfg.copy()\n    obj_type = args.get('architecture')\n    if obj_type in ARCHITECTURE:\n        return build_architecture(cfg)\n    raise ValueError(f'{obj_type} is not registered in '\n                     'ARCHITECTURE')",
        "detail": "SVTAS.svtas.model.builder",
        "documentation": {}
    },
    {
        "label": "BACKBONES",
        "kind": 5,
        "importPath": "SVTAS.svtas.model.builder",
        "description": "SVTAS.svtas.model.builder",
        "peekOfCode": "BACKBONES = Registry('backbone')\nNECKS = Registry('neck')\nHEADS = Registry('head')\nARCHITECTURE = Registry('architecture')\nLOSSES = Registry('loss')\nPOSTPRECESSING = Registry('post_precessing')\ndef build_backbone(cfg):\n    \"\"\"Build backbone.\"\"\"\n    return build(cfg, BACKBONES)\ndef build_head(cfg):",
        "detail": "SVTAS.svtas.model.builder",
        "documentation": {}
    },
    {
        "label": "NECKS",
        "kind": 5,
        "importPath": "SVTAS.svtas.model.builder",
        "description": "SVTAS.svtas.model.builder",
        "peekOfCode": "NECKS = Registry('neck')\nHEADS = Registry('head')\nARCHITECTURE = Registry('architecture')\nLOSSES = Registry('loss')\nPOSTPRECESSING = Registry('post_precessing')\ndef build_backbone(cfg):\n    \"\"\"Build backbone.\"\"\"\n    return build(cfg, BACKBONES)\ndef build_head(cfg):\n    \"\"\"Build head.\"\"\"",
        "detail": "SVTAS.svtas.model.builder",
        "documentation": {}
    },
    {
        "label": "HEADS",
        "kind": 5,
        "importPath": "SVTAS.svtas.model.builder",
        "description": "SVTAS.svtas.model.builder",
        "peekOfCode": "HEADS = Registry('head')\nARCHITECTURE = Registry('architecture')\nLOSSES = Registry('loss')\nPOSTPRECESSING = Registry('post_precessing')\ndef build_backbone(cfg):\n    \"\"\"Build backbone.\"\"\"\n    return build(cfg, BACKBONES)\ndef build_head(cfg):\n    \"\"\"Build head.\"\"\"\n    return build(cfg, HEADS)",
        "detail": "SVTAS.svtas.model.builder",
        "documentation": {}
    },
    {
        "label": "ARCHITECTURE",
        "kind": 5,
        "importPath": "SVTAS.svtas.model.builder",
        "description": "SVTAS.svtas.model.builder",
        "peekOfCode": "ARCHITECTURE = Registry('architecture')\nLOSSES = Registry('loss')\nPOSTPRECESSING = Registry('post_precessing')\ndef build_backbone(cfg):\n    \"\"\"Build backbone.\"\"\"\n    return build(cfg, BACKBONES)\ndef build_head(cfg):\n    \"\"\"Build head.\"\"\"\n    return build(cfg, HEADS)\ndef build_neck(cfg):",
        "detail": "SVTAS.svtas.model.builder",
        "documentation": {}
    },
    {
        "label": "LOSSES",
        "kind": 5,
        "importPath": "SVTAS.svtas.model.builder",
        "description": "SVTAS.svtas.model.builder",
        "peekOfCode": "LOSSES = Registry('loss')\nPOSTPRECESSING = Registry('post_precessing')\ndef build_backbone(cfg):\n    \"\"\"Build backbone.\"\"\"\n    return build(cfg, BACKBONES)\ndef build_head(cfg):\n    \"\"\"Build head.\"\"\"\n    return build(cfg, HEADS)\ndef build_neck(cfg):\n    \"\"\"Build neck.\"\"\"",
        "detail": "SVTAS.svtas.model.builder",
        "documentation": {}
    },
    {
        "label": "POSTPRECESSING",
        "kind": 5,
        "importPath": "SVTAS.svtas.model.builder",
        "description": "SVTAS.svtas.model.builder",
        "peekOfCode": "POSTPRECESSING = Registry('post_precessing')\ndef build_backbone(cfg):\n    \"\"\"Build backbone.\"\"\"\n    return build(cfg, BACKBONES)\ndef build_head(cfg):\n    \"\"\"Build head.\"\"\"\n    return build(cfg, HEADS)\ndef build_neck(cfg):\n    \"\"\"Build neck.\"\"\"\n    return build(cfg, NECKS)",
        "detail": "SVTAS.svtas.model.builder",
        "documentation": {}
    },
    {
        "label": "DebugOp",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.debugger",
        "description": "SVTAS.svtas.model.debugger",
        "peekOfCode": "class DebugOp(torch.autograd.Function): \n    @staticmethod \n    def forward(ctx, x, name): \n        return x \n    @staticmethod \n    def symbolic(g, x, name): \n        return g.op(\"my::Debug\", x, name_s=name) \ndebug_apply = DebugOp.apply \nclass Debugger(): \n    def __init__(self): ",
        "detail": "SVTAS.svtas.model.debugger",
        "documentation": {}
    },
    {
        "label": "Debugger",
        "kind": 6,
        "importPath": "SVTAS.svtas.model.debugger",
        "description": "SVTAS.svtas.model.debugger",
        "peekOfCode": "class Debugger(): \n    def __init__(self): \n        super().__init__() \n        self.torch_value = dict() \n        self.onnx_value = dict() \n        self.output_debug_name = []\n        self.logger = None\n    def debug(self, x, name): \n        self.torch_value[name] = x.detach().cpu().numpy() \n        return debug_apply(x, name) ",
        "detail": "SVTAS.svtas.model.debugger",
        "documentation": {}
    },
    {
        "label": "debug_apply",
        "kind": 5,
        "importPath": "SVTAS.svtas.model.debugger",
        "description": "SVTAS.svtas.model.debugger",
        "peekOfCode": "debug_apply = DebugOp.apply \nclass Debugger(): \n    def __init__(self): \n        super().__init__() \n        self.torch_value = dict() \n        self.onnx_value = dict() \n        self.output_debug_name = []\n        self.logger = None\n    def debug(self, x, name): \n        self.torch_value[name] = x.detach().cpu().numpy() ",
        "detail": "SVTAS.svtas.model.debugger",
        "documentation": {}
    },
    {
        "label": "CosineAnnealingLR",
        "kind": 6,
        "importPath": "SVTAS.svtas.optimizer.lr_scheduler.cosine_annealing_lr",
        "description": "SVTAS.svtas.optimizer.lr_scheduler.cosine_annealing_lr",
        "peekOfCode": "class CosineAnnealingLR(torch.optim.lr_scheduler.CosineAnnealingLR):\n    def __init__(self,\n                 optimizer,\n                 T_max,\n                 eta_min=0,\n                 last_epoch=- 1,\n                 verbose=False) -> None:\n        super().__init__(optimizer=optimizer, T_max=T_max, eta_min=eta_min, last_epoch=last_epoch, verbose=verbose)",
        "detail": "SVTAS.svtas.optimizer.lr_scheduler.cosine_annealing_lr",
        "documentation": {}
    },
    {
        "label": "CosineAnnealingWarmupRestarts",
        "kind": 6,
        "importPath": "SVTAS.svtas.optimizer.lr_scheduler.cosine_annealing_warmup_lr",
        "description": "SVTAS.svtas.optimizer.lr_scheduler.cosine_annealing_warmup_lr",
        "peekOfCode": "class CosineAnnealingWarmupRestarts(_LRScheduler):\n    \"\"\"\n        optimizer (Optimizer): Wrapped optimizer.\n        first_cycle_steps (int): First cycle step size.\n        cycle_mult(float): Cycle steps magnification. Default: -1.\n        max_lr(float): First cycle's max learning rate. Default: 0.1.\n        min_lr(float): Min learning rate. Default: 0.001.\n        warmup_steps(int): Linear warmup step size. Default: 0.\n        gamma(float): Decrease rate of max learning rate by cycle. Default: 1.\n        last_epoch (int): The index of last epoch. Default: -1.",
        "detail": "SVTAS.svtas.optimizer.lr_scheduler.cosine_annealing_warmup_lr",
        "documentation": {}
    },
    {
        "label": "MultiStepLR",
        "kind": 6,
        "importPath": "SVTAS.svtas.optimizer.lr_scheduler.multistep_lr",
        "description": "SVTAS.svtas.optimizer.lr_scheduler.multistep_lr",
        "peekOfCode": "class MultiStepLR(torch.optim.lr_scheduler.MultiStepLR):\n    def __init__(self,\n                 optimizer,\n                 step_size=[10, 30],\n                 gamma=0.1) -> None:\n        super().__init__(optimizer=optimizer, milestones=step_size, gamma=gamma)",
        "detail": "SVTAS.svtas.optimizer.lr_scheduler.multistep_lr",
        "documentation": {}
    },
    {
        "label": "AdamWOptimizer",
        "kind": 6,
        "importPath": "SVTAS.svtas.optimizer.optim.adamw_optimizer",
        "description": "SVTAS.svtas.optimizer.optim.adamw_optimizer",
        "peekOfCode": "class AdamWOptimizer(torch.optim.AdamW):\n    def __init__(self,\n                 model,\n                 learning_rate=0.001,\n                 betas=(0.9, 0.999),\n                 weight_decay=0.01,\n                 amsgrad=False,\n                 maximize=False,\n                 foreach=None,\n                 capturable=False) -> None:",
        "detail": "SVTAS.svtas.optimizer.optim.adamw_optimizer",
        "documentation": {}
    },
    {
        "label": "AdamOptimizer",
        "kind": 6,
        "importPath": "SVTAS.svtas.optimizer.optim.adam_optimizer",
        "description": "SVTAS.svtas.optimizer.optim.adam_optimizer",
        "peekOfCode": "class AdamOptimizer(torch.optim.Adam):\n    def __init__(self,\n                 model,\n                 learning_rate=0.01,\n                 betas=(0.9, 0.999),\n                 weight_decay=1e-4) -> None:\n        super().__init__(params=filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, betas=betas, weight_decay=weight_decay)",
        "detail": "SVTAS.svtas.optimizer.optim.adam_optimizer",
        "documentation": {}
    },
    {
        "label": "AdanOptimizer",
        "kind": 6,
        "importPath": "SVTAS.svtas.optimizer.optim.adan_optimizer",
        "description": "SVTAS.svtas.optimizer.optim.adan_optimizer",
        "peekOfCode": "class AdanOptimizer(Optimizer):\n    \"\"\"\n    Implements a pytorch variant of Adan\n    Adan was proposed in\n    Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models[J]. arXiv preprint arXiv:2208.06677, 2022.\n    https://arxiv.org/abs/2208.06677\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining parameter groups.\n        lr (float, optional): learning rate. (default: 1e-3)\n        betas (Tuple[float, float, flot], optional): coefficients used for computing",
        "detail": "SVTAS.svtas.optimizer.optim.adan_optimizer",
        "documentation": {}
    },
    {
        "label": "SGDOptimizer",
        "kind": 6,
        "importPath": "SVTAS.svtas.optimizer.optim.sgd_optimizer",
        "description": "SVTAS.svtas.optimizer.optim.sgd_optimizer",
        "peekOfCode": "class SGDOptimizer(torch.optim.SGD):\n    def __init__(self,\n                 model,\n                 learning_rate=0.01,\n                 momentum=0.9,\n                 weight_decay=1e-4) -> None:\n        super().__init__(params=filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)",
        "detail": "SVTAS.svtas.optimizer.optim.sgd_optimizer",
        "documentation": {}
    },
    {
        "label": "TSMAdamOptimizer",
        "kind": 6,
        "importPath": "SVTAS.svtas.optimizer.optim.tsm_adam_optimizer",
        "description": "SVTAS.svtas.optimizer.optim.tsm_adam_optimizer",
        "peekOfCode": "class TSMAdamOptimizer(torch.optim.Adam):\n    def __init__(self,\n                 model,\n                 fc_lr5=True,\n                 learning_rate=0.01,\n                 betas=(0.9, 0.999),\n                 weight_decay=1e-4) -> None:\n        self.paramwise_cfg = dict(fc_lr5=fc_lr5)\n        params = self.get_optim_policies(model)\n        super().__init__(params=params, lr=learning_rate, betas=betas, weight_decay=weight_decay)",
        "detail": "SVTAS.svtas.optimizer.optim.tsm_adam_optimizer",
        "documentation": {}
    },
    {
        "label": "TSMSGDOptimizer",
        "kind": 6,
        "importPath": "SVTAS.svtas.optimizer.optim.tsm_sgd_optimizer",
        "description": "SVTAS.svtas.optimizer.optim.tsm_sgd_optimizer",
        "peekOfCode": "class TSMSGDOptimizer(torch.optim.SGD):\n    def __init__(self,\n                 model,\n                 fc_lr5=True,\n                 learning_rate=0.01,\n                 momentum=0.9,\n                 weight_decay=1e-4) -> None:\n        self.paramwise_cfg = dict(fc_lr5=fc_lr5)\n        params = self.get_optim_policies(model)\n        super().__init__(params=params, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)",
        "detail": "SVTAS.svtas.optimizer.optim.tsm_sgd_optimizer",
        "documentation": {}
    },
    {
        "label": "build_optimizer",
        "kind": 2,
        "importPath": "SVTAS.svtas.optimizer.builder",
        "description": "SVTAS.svtas.optimizer.builder",
        "peekOfCode": "def build_optimizer(cfg):\n    \"\"\"Build optimizer.\"\"\"\n    args = cfg.copy()\n    obj_type = args.get('name')\n    if obj_type in OPTIMIZER:\n        return build(cfg, OPTIMIZER)\n    raise ValueError(f'{obj_type} is not registered in '\n                     'OPTIMIZER')\ndef build_lr_scheduler(cfg):\n    \"\"\"Build lr_scheduler.\"\"\"",
        "detail": "SVTAS.svtas.optimizer.builder",
        "documentation": {}
    },
    {
        "label": "build_lr_scheduler",
        "kind": 2,
        "importPath": "SVTAS.svtas.optimizer.builder",
        "description": "SVTAS.svtas.optimizer.builder",
        "peekOfCode": "def build_lr_scheduler(cfg):\n    \"\"\"Build lr_scheduler.\"\"\"\n    args = cfg.copy()\n    obj_type = args.get('name')\n    if obj_type in LRSCHEDULER:\n        return build(cfg, LRSCHEDULER)\n    raise ValueError(f'{obj_type} is not registered in '\n                     'lr_scheduler')",
        "detail": "SVTAS.svtas.optimizer.builder",
        "documentation": {}
    },
    {
        "label": "OPTIMIZER",
        "kind": 5,
        "importPath": "SVTAS.svtas.optimizer.builder",
        "description": "SVTAS.svtas.optimizer.builder",
        "peekOfCode": "OPTIMIZER = Registry('optimizer')\nLRSCHEDULER = Registry('lr_scheduler')\ndef build_optimizer(cfg):\n    \"\"\"Build optimizer.\"\"\"\n    args = cfg.copy()\n    obj_type = args.get('name')\n    if obj_type in OPTIMIZER:\n        return build(cfg, OPTIMIZER)\n    raise ValueError(f'{obj_type} is not registered in '\n                     'OPTIMIZER')",
        "detail": "SVTAS.svtas.optimizer.builder",
        "documentation": {}
    },
    {
        "label": "LRSCHEDULER",
        "kind": 5,
        "importPath": "SVTAS.svtas.optimizer.builder",
        "description": "SVTAS.svtas.optimizer.builder",
        "peekOfCode": "LRSCHEDULER = Registry('lr_scheduler')\ndef build_optimizer(cfg):\n    \"\"\"Build optimizer.\"\"\"\n    args = cfg.copy()\n    obj_type = args.get('name')\n    if obj_type in OPTIMIZER:\n        return build(cfg, OPTIMIZER)\n    raise ValueError(f'{obj_type} is not registered in '\n                     'OPTIMIZER')\ndef build_lr_scheduler(cfg):",
        "detail": "SVTAS.svtas.optimizer.builder",
        "documentation": {}
    },
    {
        "label": "ExtractRunner",
        "kind": 6,
        "importPath": "SVTAS.svtas.runner.extract_runner",
        "description": "SVTAS.svtas.runner.extract_runner",
        "peekOfCode": "class ExtractRunner():\n    def __init__(self,\n                 logger,\n                 model,\n                 post_processing,\n                 out_path,\n                 logger_interval=10):\n        self.model = model\n        self.logger = logger\n        self.post_processing = post_processing",
        "detail": "SVTAS.svtas.runner.extract_runner",
        "documentation": {}
    },
    {
        "label": "ExtractFeatureRunner",
        "kind": 6,
        "importPath": "SVTAS.svtas.runner.extract_runner",
        "description": "SVTAS.svtas.runner.extract_runner",
        "peekOfCode": "class ExtractFeatureRunner(ExtractRunner):\n    def init_file_dir(self):\n        pass\n    def duil_will_end_extract(self, extract_output, current_vid_list):\n        for extract_feature, vid in zip(extract_output, current_vid_list):\n            feature_save_path = os.path.join(self.out_path, vid + \".npy\")\n            np.save(feature_save_path, extract_feature)\nclass ExtractOpticalFlowRunner(ExtractRunner):\n    def init_file_dir(self):\n        self.flow_out_path = os.path.join(self.out_path, \"flow\")",
        "detail": "SVTAS.svtas.runner.extract_runner",
        "documentation": {}
    },
    {
        "label": "ExtractOpticalFlowRunner",
        "kind": 6,
        "importPath": "SVTAS.svtas.runner.extract_runner",
        "description": "SVTAS.svtas.runner.extract_runner",
        "peekOfCode": "class ExtractOpticalFlowRunner(ExtractRunner):\n    def init_file_dir(self):\n        self.flow_out_path = os.path.join(self.out_path, \"flow\")\n        isExists = os.path.exists(self.flow_out_path)\n        if not isExists:\n            os.makedirs(self.flow_out_path)\n            print(self.flow_out_path + ' created successful')\n        if self.post_processing.need_visualize:\n            self.video_out_path = os.path.join(self.out_path, \"flow_video\")\n            isExists = os.path.exists(self.video_out_path)",
        "detail": "SVTAS.svtas.runner.extract_runner",
        "documentation": {}
    },
    {
        "label": "InferONNXRunner",
        "kind": 6,
        "importPath": "SVTAS.svtas.runner.infer_runner",
        "description": "SVTAS.svtas.runner.infer_runner",
        "peekOfCode": "class InferONNXRunner(Runner):\n    def __init__(self,\n                 logger,\n                 video_batch_size,\n                 Metric,\n                 cfg,\n                 model,\n                 post_processing,\n                 record_dict=None,\n                 nprocs=1,",
        "detail": "SVTAS.svtas.runner.infer_runner",
        "documentation": {}
    },
    {
        "label": "Runner",
        "kind": 6,
        "importPath": "SVTAS.svtas.runner.runner",
        "description": "SVTAS.svtas.runner.runner",
        "peekOfCode": "class Runner():\n    def __init__(self,\n                 logger,\n                 video_batch_size,\n                 Metric,\n                 cfg,\n                 model,\n                 post_processing,\n                 record_dict=None,\n                 criterion=None,",
        "detail": "SVTAS.svtas.runner.runner",
        "documentation": {}
    },
    {
        "label": "reduce_mean",
        "kind": 2,
        "importPath": "SVTAS.svtas.runner.runner",
        "description": "SVTAS.svtas.runner.runner",
        "peekOfCode": "def reduce_mean(tensor, nprocs):\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.ReduceOp.SUM) # sum-up as the all-reduce operation\n    rt /= nprocs # NOTE this is necessary, since all_reduce here do not perform average \n    return rt\nclass Runner():\n    def __init__(self,\n                 logger,\n                 video_batch_size,\n                 Metric,",
        "detail": "SVTAS.svtas.runner.runner",
        "documentation": {}
    },
    {
        "label": "VisualRunner",
        "kind": 6,
        "importPath": "SVTAS.svtas.runner.visual_runner",
        "description": "SVTAS.svtas.runner.visual_runner",
        "peekOfCode": "class VisualRunner():\n    def __init__(self,\n                 cam_method,\n                 use_cuda,\n                 eigen_smooth,\n                 aug_smooth,\n                 logger,\n                 model,\n                 visualize_cfg,\n                 post_processing,",
        "detail": "SVTAS.svtas.runner.visual_runner",
        "documentation": {}
    },
    {
        "label": "reshape_transform",
        "kind": 2,
        "importPath": "SVTAS.svtas.runner.visual_runner",
        "description": "SVTAS.svtas.runner.visual_runner",
        "peekOfCode": "def reshape_transform(transform_form):\n# # class activation transform [N C T]\n    def reshape_transform_NCT(tensor, height=1, width=1):\n        result = torch.reshape(tensor, [tensor.shape[0], tensor.shape[1], height, width])\n        result = torch.permute(result, [0, 2, 3, 1])\n        # Bring the channels to the first dimension,\n        # like in CNNs.\n        result = result.transpose(2, 3).transpose(1, 2)\n        return result\n    # feature activation transform [N P C]",
        "detail": "SVTAS.svtas.runner.visual_runner",
        "documentation": {}
    },
    {
        "label": "infer_forward",
        "kind": 2,
        "importPath": "SVTAS.svtas.tasks.debug_infer_forward_func",
        "description": "SVTAS.svtas.tasks.debug_infer_forward_func",
        "peekOfCode": "def infer_forward(self, input_data):\n    \"\"\"\n        Use Like \n        ```\n        feature = debugger.debug(feature, 'backbone_output') \n        ```\n        for Debug\n    \"\"\"\n    masks = input_data['masks']\n    imgs = input_data['imgs']",
        "detail": "SVTAS.svtas.tasks.debug_infer_forward_func",
        "documentation": {}
    },
    {
        "label": "debugger",
        "kind": 5,
        "importPath": "SVTAS.svtas.tasks.debug_infer_forward_func",
        "description": "SVTAS.svtas.tasks.debug_infer_forward_func",
        "peekOfCode": "debugger = Debugger()\ndef infer_forward(self, input_data):\n    \"\"\"\n        Use Like \n        ```\n        feature = debugger.debug(feature, 'backbone_output') \n        ```\n        for Debug\n    \"\"\"\n    masks = input_data['masks']",
        "detail": "SVTAS.svtas.tasks.debug_infer_forward_func",
        "documentation": {}
    },
    {
        "label": "infer",
        "kind": 2,
        "importPath": "SVTAS.svtas.tasks.infer",
        "description": "SVTAS.svtas.tasks.infer",
        "peekOfCode": "def infer(cfg,\n          args,\n          local_rank,\n          nprocs,\n          weights=None,\n          validate=True,):\n    \"\"\"\n    Infer model entry\n    \"\"\"\n    logger = get_logger(\"SVTAS\")",
        "detail": "SVTAS.svtas.tasks.infer",
        "documentation": {}
    },
    {
        "label": "test",
        "kind": 2,
        "importPath": "SVTAS.svtas.tasks.test",
        "description": "SVTAS.svtas.tasks.test",
        "peekOfCode": "def test(cfg,\n         args,\n         local_rank,\n         nprocs,\n         use_amp=False,\n         weights=None):\n    logger = get_logger(\"SVTAS\")\n    if args.use_tensorboard and local_rank <= 0:\n        tensorboard_writer = get_logger(\"SVTAS\", tensorboard=args.use_tensorboard)\n    # wheather use amp",
        "detail": "SVTAS.svtas.tasks.test",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "SVTAS.svtas.tasks.train",
        "description": "SVTAS.svtas.tasks.train",
        "peekOfCode": "def train(cfg,\n          args,\n          local_rank,\n          nprocs,\n          use_amp=False,\n          weights=None,\n          validate=True,):\n    \"\"\"Train model entry\n    \"\"\"\n    logger = get_logger(\"SVTAS\")",
        "detail": "SVTAS.svtas.tasks.train",
        "documentation": {}
    },
    {
        "label": "Registry",
        "kind": 6,
        "importPath": "SVTAS.svtas.utils.build",
        "description": "SVTAS.svtas.utils.build",
        "peekOfCode": "class Registry(object):\n    \"\"\"\n    The registry that provides name -> object mapping, to support third-party users' custom modules.\n    To register an object:\n    .. code-block:: python\n        BACKBONES = Registry('backbone')\n        @BACKBONES.register()\n        class ResNet:\n            pass\n    Or:",
        "detail": "SVTAS.svtas.utils.build",
        "documentation": {}
    },
    {
        "label": "build",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.build",
        "description": "SVTAS.svtas.utils.build",
        "peekOfCode": "def build(cfg, registry, key='name'):\n    \"\"\"Build a module from config dict.\n    Args:\n        cfg (dict): Config dict. It should at least contain the key.\n        registry (XXX): The registry to search the type from.\n        key (str): the key.\n    Returns:\n        obj: The constructed object.\n    \"\"\"\n    if cfg is None:",
        "detail": "SVTAS.svtas.utils.build",
        "documentation": {}
    },
    {
        "label": "collect_env",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.collect_env",
        "description": "SVTAS.svtas.utils.collect_env",
        "peekOfCode": "def collect_env():\n    env_info = collect_basic_env()\n    env_info['SVTAS'] = (get_git_hash(digits=7))\n    return env_info\nif __name__ == '__main__':\n    for name, val in collect_env().items():\n        print(f'{name}: {val}')",
        "detail": "SVTAS.svtas.utils.collect_env",
        "documentation": {}
    },
    {
        "label": "ConfigDict",
        "kind": 6,
        "importPath": "SVTAS.svtas.utils.config",
        "description": "SVTAS.svtas.utils.config",
        "peekOfCode": "class ConfigDict(Dict):\n    def __missing__(self, name):\n        raise KeyError(name)\n    def __getattr__(self, name):\n        try:\n            value = super().__getattr__(name)\n        except KeyError:\n            ex = AttributeError(f\"'{self.__class__.__name__}' object has no \"\n                                f\"attribute '{name}'\")\n        except Exception as e:",
        "detail": "SVTAS.svtas.utils.config",
        "documentation": {}
    },
    {
        "label": "Config",
        "kind": 6,
        "importPath": "SVTAS.svtas.utils.config",
        "description": "SVTAS.svtas.utils.config",
        "peekOfCode": "class Config:\n    \"\"\"A facility for config and config files.\n    It supports common file formats as configs: python/json/yaml. The interface\n    is the same as a dict object and also allows access config values as\n    attributes.\n    Example:\n        >>> cfg = Config(dict(a=1, b=dict(b1=[0, 1])))\n        >>> cfg.a\n        1\n        >>> cfg.b",
        "detail": "SVTAS.svtas.utils.config",
        "documentation": {}
    },
    {
        "label": "DictAction",
        "kind": 6,
        "importPath": "SVTAS.svtas.utils.config",
        "description": "SVTAS.svtas.utils.config",
        "peekOfCode": "class DictAction(Action):\n    \"\"\"\n    argparse action to split an argument into KEY=VALUE form\n    on the first = and append to a dictionary. List options can\n    be passed as comma separated values, i.e 'KEY=V1,V2,V3', or with explicit\n    brackets, i.e. 'KEY=[V1,V2,V3]'. It also support nested brackets to build\n    list/tuple values. e.g. 'KEY=[(V1,V2),(V3,V4)]'\n    \"\"\"\n    @staticmethod\n    def _parse_int_float_bool(val):",
        "detail": "SVTAS.svtas.utils.config",
        "documentation": {}
    },
    {
        "label": "get_config",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.config",
        "description": "SVTAS.svtas.utils.config",
        "peekOfCode": "def get_config(fname, overrides=None, show=True, tensorboard=False, logger_path=\"output\"):\n    \"\"\"\n    Read config from file\n    \"\"\"\n    assert os.path.exists(fname), ('config file({}) is not exist'.format(fname))\n    config = Config.fromfile(fname)\n    if \"work_dir\" not in config:\n        config.work_dir = \"output\"\n    if os.path.isabs(config.work_dir):\n        os.environ['ROS_LOG_DIR'] = config.work_dir",
        "detail": "SVTAS.svtas.utils.config",
        "documentation": {}
    },
    {
        "label": "override",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.config",
        "description": "SVTAS.svtas.utils.config",
        "peekOfCode": "def override(dl, ks, v):\n    \"\"\"\n    Recursively replace dict of list\n    Args:\n        dl(dict or list): dict or list to be replaced\n        ks(list): list of keys\n        v(str): value to be replaced\n    \"\"\"\n    logger = get_logger(\"SVTAS\")\n    def str2num(v):",
        "detail": "SVTAS.svtas.utils.config",
        "documentation": {}
    },
    {
        "label": "override_config",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.config",
        "description": "SVTAS.svtas.utils.config",
        "peekOfCode": "def override_config(config, options=None):\n    \"\"\"\n    Recursively override the config\n    Args:\n        config(dict): dict to be replaced\n        options(list): list of pairs(key0.key1.idx.key2=value)\n            such as: [\n                epochs=20',\n                'PIPELINE.train.transform.1.ResizeImage.resize_short=300'\n            ]",
        "detail": "SVTAS.svtas.utils.config",
        "documentation": {}
    },
    {
        "label": "print_config",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.config",
        "description": "SVTAS.svtas.utils.config",
        "peekOfCode": "def print_config(config):\n    \"\"\"\n    visualize configs\n    Arguments:\n        config: configs\n    \"\"\"\n    print_dict(config)\ndef print_dict(d, delimiter=0):\n    \"\"\"\n    Recursively visualize a dict and",
        "detail": "SVTAS.svtas.utils.config",
        "documentation": {}
    },
    {
        "label": "print_dict",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.config",
        "description": "SVTAS.svtas.utils.config",
        "peekOfCode": "def print_dict(d, delimiter=0):\n    \"\"\"\n    Recursively visualize a dict and\n    indenting acrrording by the relationship of keys.\n    \"\"\"\n    logger = get_logger(\"SVTAS\")\n    placeholder = \"-\" * 60\n    for k, v in sorted(d.items()):\n        if isinstance(v, dict):\n            logger.info(\"{}{} : \".format(delimiter * \" \", coloring(k,",
        "detail": "SVTAS.svtas.utils.config",
        "documentation": {}
    },
    {
        "label": "add_args",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.config",
        "description": "SVTAS.svtas.utils.config",
        "peekOfCode": "def add_args(parser, cfg, prefix=''):\n    for k, v in cfg.items():\n        if isinstance(v, str):\n            parser.add_argument('--' + prefix + k)\n        elif isinstance(v, int):\n            parser.add_argument('--' + prefix + k, type=int)\n        elif isinstance(v, float):\n            parser.add_argument('--' + prefix + k, type=float)\n        elif isinstance(v, bool):\n            parser.add_argument('--' + prefix + k, action='store_true')",
        "detail": "SVTAS.svtas.utils.config",
        "documentation": {}
    },
    {
        "label": "BASE_KEY",
        "kind": 5,
        "importPath": "SVTAS.svtas.utils.config",
        "description": "SVTAS.svtas.utils.config",
        "peekOfCode": "BASE_KEY = '_base_'\nDELETE_KEY = '_delete_'\nDEPRECATION_KEY = '_deprecation_'\nRESERVED_KEYS = ['filename', 'text', 'pretty_text']\ndef get_config(fname, overrides=None, show=True, tensorboard=False, logger_path=\"output\"):\n    \"\"\"\n    Read config from file\n    \"\"\"\n    assert os.path.exists(fname), ('config file({}) is not exist'.format(fname))\n    config = Config.fromfile(fname)",
        "detail": "SVTAS.svtas.utils.config",
        "documentation": {}
    },
    {
        "label": "DELETE_KEY",
        "kind": 5,
        "importPath": "SVTAS.svtas.utils.config",
        "description": "SVTAS.svtas.utils.config",
        "peekOfCode": "DELETE_KEY = '_delete_'\nDEPRECATION_KEY = '_deprecation_'\nRESERVED_KEYS = ['filename', 'text', 'pretty_text']\ndef get_config(fname, overrides=None, show=True, tensorboard=False, logger_path=\"output\"):\n    \"\"\"\n    Read config from file\n    \"\"\"\n    assert os.path.exists(fname), ('config file({}) is not exist'.format(fname))\n    config = Config.fromfile(fname)\n    if \"work_dir\" not in config:",
        "detail": "SVTAS.svtas.utils.config",
        "documentation": {}
    },
    {
        "label": "DEPRECATION_KEY",
        "kind": 5,
        "importPath": "SVTAS.svtas.utils.config",
        "description": "SVTAS.svtas.utils.config",
        "peekOfCode": "DEPRECATION_KEY = '_deprecation_'\nRESERVED_KEYS = ['filename', 'text', 'pretty_text']\ndef get_config(fname, overrides=None, show=True, tensorboard=False, logger_path=\"output\"):\n    \"\"\"\n    Read config from file\n    \"\"\"\n    assert os.path.exists(fname), ('config file({}) is not exist'.format(fname))\n    config = Config.fromfile(fname)\n    if \"work_dir\" not in config:\n        config.work_dir = \"output\"",
        "detail": "SVTAS.svtas.utils.config",
        "documentation": {}
    },
    {
        "label": "RESERVED_KEYS",
        "kind": 5,
        "importPath": "SVTAS.svtas.utils.config",
        "description": "SVTAS.svtas.utils.config",
        "peekOfCode": "RESERVED_KEYS = ['filename', 'text', 'pretty_text']\ndef get_config(fname, overrides=None, show=True, tensorboard=False, logger_path=\"output\"):\n    \"\"\"\n    Read config from file\n    \"\"\"\n    assert os.path.exists(fname), ('config file({}) is not exist'.format(fname))\n    config = Config.fromfile(fname)\n    if \"work_dir\" not in config:\n        config.work_dir = \"output\"\n    if os.path.isabs(config.work_dir):",
        "detail": "SVTAS.svtas.utils.config",
        "documentation": {}
    },
    {
        "label": "make_colorwheel",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.flow_vis",
        "description": "SVTAS.svtas.utils.flow_vis",
        "peekOfCode": "def make_colorwheel():\n    \"\"\"\n    Generates a color wheel for optical flow visualization as presented in:\n        Baker et al. \"A Database and Evaluation Methodology for Optical Flow\" (ICCV, 2007)\n        URL: http://vision.middlebury.edu/flow/flowEval-iccv07.pdf\n    Code follows the original C++ source code of Daniel Scharstein.\n    Code follows the the Matlab source code of Deqing Sun.\n    Returns:\n        np.ndarray: Color wheel\n    \"\"\"",
        "detail": "SVTAS.svtas.utils.flow_vis",
        "documentation": {}
    },
    {
        "label": "flow_uv_to_colors",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.flow_vis",
        "description": "SVTAS.svtas.utils.flow_vis",
        "peekOfCode": "def flow_uv_to_colors(u, v, convert_to_bgr=False):\n    \"\"\"\n    Applies the flow color wheel to (possibly clipped) flow components u and v.\n    According to the C++ source code of Daniel Scharstein\n    According to the Matlab source code of Deqing Sun\n    Args:\n        u (np.ndarray): Input horizontal flow of shape [H,W]flo\n        v (np.ndarray): Input vertical flow of shape [H,W]\n        convert_to_bgr (bool, optional): Convert output image to BGR. Defaults to False.\n    Returns:",
        "detail": "SVTAS.svtas.utils.flow_vis",
        "documentation": {}
    },
    {
        "label": "flow_to_color",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.flow_vis",
        "description": "SVTAS.svtas.utils.flow_vis",
        "peekOfCode": "def flow_to_color(flow_uv, clip_flow=None, convert_to_bgr=False):\n    \"\"\"\n    Expects a two dimensional flow image of shape.\n    Args:\n        flow_uv (np.ndarray): Flow UV image of shape [H,W,2]\n        clip_flow (float, optional): Clip maximum of flow values. Defaults to None.\n        convert_to_bgr (bool, optional): Convert output image to BGR. Defaults to False.\n    Returns:\n        np.ndarray: Flow visualization image of shape [H,W,3]\n    \"\"\"",
        "detail": "SVTAS.svtas.utils.flow_vis",
        "documentation": {}
    },
    {
        "label": "AverageMeter",
        "kind": 6,
        "importPath": "SVTAS.svtas.utils.logger",
        "description": "SVTAS.svtas.utils.logger",
        "peekOfCode": "class AverageMeter(object):\n    \"\"\"\n    Computes and stores the average and current value\n    \"\"\"\n    def __init__(self, name='', fmt='f', need_avg=True):\n        self.name = name\n        self.fmt = fmt\n        self.need_avg = need_avg\n        self.reset()\n    def reset(self):",
        "detail": "SVTAS.svtas.utils.logger",
        "documentation": {}
    },
    {
        "label": "coloring",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.logger",
        "description": "SVTAS.svtas.utils.logger",
        "peekOfCode": "def coloring(message, color=\"OKGREEN\"):\n    assert color in Color.keys()\n    if os.environ.get('COLORING', True):\n        return Color[color] + str(message) + Color[\"ENDC\"]\n    else:\n        return message\nlogger_initialized = {}\ndef setup_logger(output=None, name=\"SVTAS\", level=\"INFO\", tensorboard=False):\n    \"\"\"\n    Initialize the SVTAS logger and set its verbosity level to \"INFO\".",
        "detail": "SVTAS.svtas.utils.logger",
        "documentation": {}
    },
    {
        "label": "setup_logger",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.logger",
        "description": "SVTAS.svtas.utils.logger",
        "peekOfCode": "def setup_logger(output=None, name=\"SVTAS\", level=\"INFO\", tensorboard=False):\n    \"\"\"\n    Initialize the SVTAS logger and set its verbosity level to \"INFO\".\n    \"\"\"\n    def time_zone(sec, fmt):\n        real_time = datetime.datetime.now()\n        return real_time.timetuple()\n    logging.Formatter.converter = time_zone\n    logger = logging.getLogger(name)\n    if level == \"INFO\":",
        "detail": "SVTAS.svtas.utils.logger",
        "documentation": {}
    },
    {
        "label": "get_logger",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.logger",
        "description": "SVTAS.svtas.utils.logger",
        "peekOfCode": "def get_logger(name, output=None, tensorboard=False):\n    logger = logging.getLogger(name)\n    if name in list(logger_initialized.keys()):\n        if tensorboard is True:\n            return logger_initialized[name]['tensorboard']\n        return logger\n    return setup_logger(name=name, output=name)\nclass AverageMeter(object):\n    \"\"\"\n    Computes and stores the average and current value",
        "detail": "SVTAS.svtas.utils.logger",
        "documentation": {}
    },
    {
        "label": "log_batch",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.logger",
        "description": "SVTAS.svtas.utils.logger",
        "peekOfCode": "def log_batch(metric_list, batch_id, epoch_id, total_epoch, mode, ips, logger):\n    batch_cost = str(metric_list['batch_time'].value) + ' sec,'\n    reader_cost = str(metric_list['reader_time'].value) + ' sec,'\n    metric_values = []\n    for m in metric_list:\n        if not (m == 'batch_time' or m == 'reader_time'):\n            metric_values.append(metric_list[m].value)\n    metric_str = ' '.join([str(v) for v in metric_values])\n    if mode in [\"train\", \"validation\"]:\n        epoch_str = \"epoch:[{:>3d}/{:<3d}]\".format(epoch_id, total_epoch)",
        "detail": "SVTAS.svtas.utils.logger",
        "documentation": {}
    },
    {
        "label": "log_epoch",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.logger",
        "description": "SVTAS.svtas.utils.logger",
        "peekOfCode": "def log_epoch(metric_list, epoch, mode, ips, logger):\n    batch_cost = 'avg_' + str(metric_list['batch_time'].value) + ' sec,'\n    reader_cost = 'avg_' + str(metric_list['reader_time'].value) + ' sec,'\n    batch_sum = str(metric_list['batch_time'].total) + ' sec,'\n    metric_values = []\n    for m in metric_list:\n        if not (m == 'batch_time' or m == 'reader_time'):\n            metric_values.append(metric_list[m].mean)\n    metric_str = ' '.join([str(v) for v in metric_values])\n    end_epoch_str = \"END epoch:{:<3d}\".format(epoch)",
        "detail": "SVTAS.svtas.utils.logger",
        "documentation": {}
    },
    {
        "label": "tenorboard_log_epoch",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.logger",
        "description": "SVTAS.svtas.utils.logger",
        "peekOfCode": "def tenorboard_log_epoch(metric_list, epoch, mode, writer):\n    if isinstance(writer, SummaryWriter):\n        for m in metric_list:\n            if not (m == 'batch_time' or m == 'reader_time'):\n                writer.add_scalar(mode + \"/\" + m, metric_list[m].get_mean, epoch)",
        "detail": "SVTAS.svtas.utils.logger",
        "documentation": {}
    },
    {
        "label": "Color",
        "kind": 5,
        "importPath": "SVTAS.svtas.utils.logger",
        "description": "SVTAS.svtas.utils.logger",
        "peekOfCode": "Color = {\n    'RED': '\\033[31m',\n    'HEADER': '\\033[35m',  # deep purple\n    'PURPLE': '\\033[95m',  # purple\n    'OKBLUE': '\\033[94m',\n    'OKGREEN': '\\033[92m',\n    'WARNING': '\\033[93m',\n    'FAIL': '\\033[91m',\n    'ENDC': '\\033[0m'\n}",
        "detail": "SVTAS.svtas.utils.logger",
        "documentation": {}
    },
    {
        "label": "logger_initialized",
        "kind": 5,
        "importPath": "SVTAS.svtas.utils.logger",
        "description": "SVTAS.svtas.utils.logger",
        "peekOfCode": "logger_initialized = {}\ndef setup_logger(output=None, name=\"SVTAS\", level=\"INFO\", tensorboard=False):\n    \"\"\"\n    Initialize the SVTAS logger and set its verbosity level to \"INFO\".\n    \"\"\"\n    def time_zone(sec, fmt):\n        real_time = datetime.datetime.now()\n        return real_time.timetuple()\n    logging.Formatter.converter = time_zone\n    logger = logging.getLogger(name)",
        "detail": "SVTAS.svtas.utils.logger",
        "documentation": {}
    },
    {
        "label": "is_str",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.misc",
        "description": "SVTAS.svtas.utils.misc",
        "peekOfCode": "def is_str(x):\n    \"\"\"Whether the input is an string instance.\n    Note: This method is deprecated since python 2 is no longer supported.\n    \"\"\"\n    return isinstance(x, str)\ndef import_modules_from_strings(imports, allow_failed_imports=False):\n    \"\"\"Import modules from the given list of strings.\n    Args:\n        imports (list | str | None): The given module names to be imported.\n        allow_failed_imports (bool): If True, the failed imports will return",
        "detail": "SVTAS.svtas.utils.misc",
        "documentation": {}
    },
    {
        "label": "import_modules_from_strings",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.misc",
        "description": "SVTAS.svtas.utils.misc",
        "peekOfCode": "def import_modules_from_strings(imports, allow_failed_imports=False):\n    \"\"\"Import modules from the given list of strings.\n    Args:\n        imports (list | str | None): The given module names to be imported.\n        allow_failed_imports (bool): If True, the failed imports will return\n            None. Otherwise, an ImportError is raise. Default: False.\n    Returns:\n        list[module] | module | None: The imported modules.\n    Examples:\n        >>> osp, sys = import_modules_from_strings(",
        "detail": "SVTAS.svtas.utils.misc",
        "documentation": {}
    },
    {
        "label": "iter_cast",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.misc",
        "description": "SVTAS.svtas.utils.misc",
        "peekOfCode": "def iter_cast(inputs, dst_type, return_type=None):\n    \"\"\"Cast elements of an iterable object into some type.\n    Args:\n        inputs (Iterable): The input object.\n        dst_type (type): Destination type.\n        return_type (type, optional): If specified, the output object will be\n            converted to this type, otherwise an iterator.\n    Returns:\n        iterator or specified type: The converted object.\n    \"\"\"",
        "detail": "SVTAS.svtas.utils.misc",
        "documentation": {}
    },
    {
        "label": "list_cast",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.misc",
        "description": "SVTAS.svtas.utils.misc",
        "peekOfCode": "def list_cast(inputs, dst_type):\n    \"\"\"Cast elements of an iterable object into a list of some type.\n    A partial method of :func:`iter_cast`.\n    \"\"\"\n    return iter_cast(inputs, dst_type, return_type=list)\ndef tuple_cast(inputs, dst_type):\n    \"\"\"Cast elements of an iterable object into a tuple of some type.\n    A partial method of :func:`iter_cast`.\n    \"\"\"\n    return iter_cast(inputs, dst_type, return_type=tuple)",
        "detail": "SVTAS.svtas.utils.misc",
        "documentation": {}
    },
    {
        "label": "tuple_cast",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.misc",
        "description": "SVTAS.svtas.utils.misc",
        "peekOfCode": "def tuple_cast(inputs, dst_type):\n    \"\"\"Cast elements of an iterable object into a tuple of some type.\n    A partial method of :func:`iter_cast`.\n    \"\"\"\n    return iter_cast(inputs, dst_type, return_type=tuple)\ndef is_seq_of(seq, expected_type, seq_type=None):\n    \"\"\"Check whether it is a sequence of some type.\n    Args:\n        seq (Sequence): The sequence to be checked.\n        expected_type (type): Expected type of sequence items.",
        "detail": "SVTAS.svtas.utils.misc",
        "documentation": {}
    },
    {
        "label": "is_seq_of",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.misc",
        "description": "SVTAS.svtas.utils.misc",
        "peekOfCode": "def is_seq_of(seq, expected_type, seq_type=None):\n    \"\"\"Check whether it is a sequence of some type.\n    Args:\n        seq (Sequence): The sequence to be checked.\n        expected_type (type): Expected type of sequence items.\n        seq_type (type, optional): Expected sequence type.\n    Returns:\n        bool: Whether the sequence is valid.\n    \"\"\"\n    if seq_type is None:",
        "detail": "SVTAS.svtas.utils.misc",
        "documentation": {}
    },
    {
        "label": "is_list_of",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.misc",
        "description": "SVTAS.svtas.utils.misc",
        "peekOfCode": "def is_list_of(seq, expected_type):\n    \"\"\"Check whether it is a list of some type.\n    A partial method of :func:`is_seq_of`.\n    \"\"\"\n    return is_seq_of(seq, expected_type, seq_type=list)\ndef is_tuple_of(seq, expected_type):\n    \"\"\"Check whether it is a tuple of some type.\n    A partial method of :func:`is_seq_of`.\n    \"\"\"\n    return is_seq_of(seq, expected_type, seq_type=tuple)",
        "detail": "SVTAS.svtas.utils.misc",
        "documentation": {}
    },
    {
        "label": "is_tuple_of",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.misc",
        "description": "SVTAS.svtas.utils.misc",
        "peekOfCode": "def is_tuple_of(seq, expected_type):\n    \"\"\"Check whether it is a tuple of some type.\n    A partial method of :func:`is_seq_of`.\n    \"\"\"\n    return is_seq_of(seq, expected_type, seq_type=tuple)\ndef slice_list(in_list, lens):\n    \"\"\"Slice a list into several sub lists by a list of given length.\n    Args:\n        in_list (list): The list to be sliced.\n        lens(int or list): The expected length of each out list.",
        "detail": "SVTAS.svtas.utils.misc",
        "documentation": {}
    },
    {
        "label": "slice_list",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.misc",
        "description": "SVTAS.svtas.utils.misc",
        "peekOfCode": "def slice_list(in_list, lens):\n    \"\"\"Slice a list into several sub lists by a list of given length.\n    Args:\n        in_list (list): The list to be sliced.\n        lens(int or list): The expected length of each out list.\n    Returns:\n        list: A list of sliced list.\n    \"\"\"\n    if isinstance(lens, int):\n        assert len(in_list) % lens == 0",
        "detail": "SVTAS.svtas.utils.misc",
        "documentation": {}
    },
    {
        "label": "concat_list",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.misc",
        "description": "SVTAS.svtas.utils.misc",
        "peekOfCode": "def concat_list(in_list):\n    \"\"\"Concatenate a list of list into a single list.\n    Args:\n        in_list (list): The list of list to be merged.\n    Returns:\n        list: The concatenated flat list.\n    \"\"\"\n    return list(itertools.chain(*in_list))\ndef check_prerequisites(\n        prerequisites,",
        "detail": "SVTAS.svtas.utils.misc",
        "documentation": {}
    },
    {
        "label": "check_prerequisites",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.misc",
        "description": "SVTAS.svtas.utils.misc",
        "peekOfCode": "def check_prerequisites(\n        prerequisites,\n        checker,\n        msg_tmpl='Prerequisites \"{}\" are required in method \"{}\" but not '\n        'found, please install them first.'):  # yapf: disable\n    \"\"\"A decorator factory to check if prerequisites are satisfied.\n    Args:\n        prerequisites (str of list[str]): Prerequisites to be checked.\n        checker (callable): The checker method that returns True if a\n            prerequisite is meet, False otherwise.",
        "detail": "SVTAS.svtas.utils.misc",
        "documentation": {}
    },
    {
        "label": "requires_package",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.misc",
        "description": "SVTAS.svtas.utils.misc",
        "peekOfCode": "def requires_package(prerequisites):\n    \"\"\"A decorator to check if some python packages are installed.\n    Example:\n        >>> @requires_package('numpy')\n        >>> func(arg1, args):\n        >>>     return numpy.zeros(1)\n        array([0.])\n        >>> @requires_package(['numpy', 'non_package'])\n        >>> func(arg1, args):\n        >>>     return numpy.zeros(1)",
        "detail": "SVTAS.svtas.utils.misc",
        "documentation": {}
    },
    {
        "label": "requires_executable",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.misc",
        "description": "SVTAS.svtas.utils.misc",
        "peekOfCode": "def requires_executable(prerequisites):\n    \"\"\"A decorator to check if some executable files are installed.\n    Example:\n        >>> @requires_executable('ffmpeg')\n        >>> func(arg1, args):\n        >>>     print(1)\n        1\n    \"\"\"\n    return check_prerequisites(prerequisites, checker=_check_executable)\ndef deprecated_api_warning(name_dict, cls_name=None):",
        "detail": "SVTAS.svtas.utils.misc",
        "documentation": {}
    },
    {
        "label": "deprecated_api_warning",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.misc",
        "description": "SVTAS.svtas.utils.misc",
        "peekOfCode": "def deprecated_api_warning(name_dict, cls_name=None):\n    \"\"\"A decorator to check if some arguments are deprecate and try to replace\n    deprecate src_arg_name to dst_arg_name.\n    Args:\n        name_dict(dict):\n            key (str): Deprecate argument names.\n            val (str): Expected argument names.\n    Returns:\n        func: New function.\n    \"\"\"",
        "detail": "SVTAS.svtas.utils.misc",
        "documentation": {}
    },
    {
        "label": "is_method_overridden",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.misc",
        "description": "SVTAS.svtas.utils.misc",
        "peekOfCode": "def is_method_overridden(method, base_class, derived_class):\n    \"\"\"Check if a method of base class is overridden in derived class.\n    Args:\n        method (str): the method name to check.\n        base_class (type): the class of the base class.\n        derived_class (type | Any): the class or instance of the derived class.\n    \"\"\"\n    assert isinstance(base_class, type), \\\n        \"base_class doesn't accept instance, Please pass class instead.\"\n    if not isinstance(derived_class, type):",
        "detail": "SVTAS.svtas.utils.misc",
        "documentation": {}
    },
    {
        "label": "has_method",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.misc",
        "description": "SVTAS.svtas.utils.misc",
        "peekOfCode": "def has_method(obj: object, method: str) -> bool:\n    \"\"\"Check whether the object has a method.\n    Args:\n        method (str): The method name to check.\n        obj (object): The object to check.\n    Returns:\n        bool: True if the object has the method else False.\n    \"\"\"\n    return hasattr(obj, method) and callable(getattr(obj, method))",
        "detail": "SVTAS.svtas.utils.misc",
        "documentation": {}
    },
    {
        "label": "to_1tuple",
        "kind": 5,
        "importPath": "SVTAS.svtas.utils.misc",
        "description": "SVTAS.svtas.utils.misc",
        "peekOfCode": "to_1tuple = _ntuple(1)\nto_2tuple = _ntuple(2)\nto_3tuple = _ntuple(3)\nto_4tuple = _ntuple(4)\nto_ntuple = _ntuple\ndef is_str(x):\n    \"\"\"Whether the input is an string instance.\n    Note: This method is deprecated since python 2 is no longer supported.\n    \"\"\"\n    return isinstance(x, str)",
        "detail": "SVTAS.svtas.utils.misc",
        "documentation": {}
    },
    {
        "label": "to_2tuple",
        "kind": 5,
        "importPath": "SVTAS.svtas.utils.misc",
        "description": "SVTAS.svtas.utils.misc",
        "peekOfCode": "to_2tuple = _ntuple(2)\nto_3tuple = _ntuple(3)\nto_4tuple = _ntuple(4)\nto_ntuple = _ntuple\ndef is_str(x):\n    \"\"\"Whether the input is an string instance.\n    Note: This method is deprecated since python 2 is no longer supported.\n    \"\"\"\n    return isinstance(x, str)\ndef import_modules_from_strings(imports, allow_failed_imports=False):",
        "detail": "SVTAS.svtas.utils.misc",
        "documentation": {}
    },
    {
        "label": "to_3tuple",
        "kind": 5,
        "importPath": "SVTAS.svtas.utils.misc",
        "description": "SVTAS.svtas.utils.misc",
        "peekOfCode": "to_3tuple = _ntuple(3)\nto_4tuple = _ntuple(4)\nto_ntuple = _ntuple\ndef is_str(x):\n    \"\"\"Whether the input is an string instance.\n    Note: This method is deprecated since python 2 is no longer supported.\n    \"\"\"\n    return isinstance(x, str)\ndef import_modules_from_strings(imports, allow_failed_imports=False):\n    \"\"\"Import modules from the given list of strings.",
        "detail": "SVTAS.svtas.utils.misc",
        "documentation": {}
    },
    {
        "label": "to_4tuple",
        "kind": 5,
        "importPath": "SVTAS.svtas.utils.misc",
        "description": "SVTAS.svtas.utils.misc",
        "peekOfCode": "to_4tuple = _ntuple(4)\nto_ntuple = _ntuple\ndef is_str(x):\n    \"\"\"Whether the input is an string instance.\n    Note: This method is deprecated since python 2 is no longer supported.\n    \"\"\"\n    return isinstance(x, str)\ndef import_modules_from_strings(imports, allow_failed_imports=False):\n    \"\"\"Import modules from the given list of strings.\n    Args:",
        "detail": "SVTAS.svtas.utils.misc",
        "documentation": {}
    },
    {
        "label": "to_ntuple",
        "kind": 5,
        "importPath": "SVTAS.svtas.utils.misc",
        "description": "SVTAS.svtas.utils.misc",
        "peekOfCode": "to_ntuple = _ntuple\ndef is_str(x):\n    \"\"\"Whether the input is an string instance.\n    Note: This method is deprecated since python 2 is no longer supported.\n    \"\"\"\n    return isinstance(x, str)\ndef import_modules_from_strings(imports, allow_failed_imports=False):\n    \"\"\"Import modules from the given list of strings.\n    Args:\n        imports (list | str | None): The given module names to be imported.",
        "detail": "SVTAS.svtas.utils.misc",
        "documentation": {}
    },
    {
        "label": "is_filepath",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.path",
        "description": "SVTAS.svtas.utils.path",
        "peekOfCode": "def is_filepath(x):\n    return is_str(x) or isinstance(x, Path)\ndef fopen(filepath, *args, **kwargs):\n    if is_str(filepath):\n        return open(filepath, *args, **kwargs)\n    elif isinstance(filepath, Path):\n        return filepath.open(*args, **kwargs)\n    raise ValueError('`filepath` should be a string or a Path')\ndef check_file_exist(filename, msg_tmpl='file \"{}\" does not exist'):\n    if not osp.isfile(filename):",
        "detail": "SVTAS.svtas.utils.path",
        "documentation": {}
    },
    {
        "label": "fopen",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.path",
        "description": "SVTAS.svtas.utils.path",
        "peekOfCode": "def fopen(filepath, *args, **kwargs):\n    if is_str(filepath):\n        return open(filepath, *args, **kwargs)\n    elif isinstance(filepath, Path):\n        return filepath.open(*args, **kwargs)\n    raise ValueError('`filepath` should be a string or a Path')\ndef check_file_exist(filename, msg_tmpl='file \"{}\" does not exist'):\n    if not osp.isfile(filename):\n        raise FileNotFoundError(msg_tmpl.format(filename))\ndef mkdir_or_exist(dir_name, mode=0o777):",
        "detail": "SVTAS.svtas.utils.path",
        "documentation": {}
    },
    {
        "label": "check_file_exist",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.path",
        "description": "SVTAS.svtas.utils.path",
        "peekOfCode": "def check_file_exist(filename, msg_tmpl='file \"{}\" does not exist'):\n    if not osp.isfile(filename):\n        raise FileNotFoundError(msg_tmpl.format(filename))\ndef mkdir_or_exist(dir_name, mode=0o777):\n    if dir_name == '':\n        return\n    dir_name = osp.expanduser(dir_name)\n    os.makedirs(dir_name, mode=mode, exist_ok=True)\ndef symlink(src, dst, overwrite=True, **kwargs):\n    if os.path.lexists(dst) and overwrite:",
        "detail": "SVTAS.svtas.utils.path",
        "documentation": {}
    },
    {
        "label": "mkdir_or_exist",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.path",
        "description": "SVTAS.svtas.utils.path",
        "peekOfCode": "def mkdir_or_exist(dir_name, mode=0o777):\n    if dir_name == '':\n        return\n    dir_name = osp.expanduser(dir_name)\n    os.makedirs(dir_name, mode=mode, exist_ok=True)\ndef symlink(src, dst, overwrite=True, **kwargs):\n    if os.path.lexists(dst) and overwrite:\n        os.remove(dst)\n    os.symlink(src, dst, **kwargs)\ndef scandir(dir_path, suffix=None, recursive=False, case_sensitive=True):",
        "detail": "SVTAS.svtas.utils.path",
        "documentation": {}
    },
    {
        "label": "symlink",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.path",
        "description": "SVTAS.svtas.utils.path",
        "peekOfCode": "def symlink(src, dst, overwrite=True, **kwargs):\n    if os.path.lexists(dst) and overwrite:\n        os.remove(dst)\n    os.symlink(src, dst, **kwargs)\ndef scandir(dir_path, suffix=None, recursive=False, case_sensitive=True):\n    \"\"\"Scan a directory to find the interested files.\n    Args:\n        dir_path (str | :obj:`Path`): Path of the directory.\n        suffix (str | tuple(str), optional): File suffix that we are\n            interested in. Default: None.",
        "detail": "SVTAS.svtas.utils.path",
        "documentation": {}
    },
    {
        "label": "scandir",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.path",
        "description": "SVTAS.svtas.utils.path",
        "peekOfCode": "def scandir(dir_path, suffix=None, recursive=False, case_sensitive=True):\n    \"\"\"Scan a directory to find the interested files.\n    Args:\n        dir_path (str | :obj:`Path`): Path of the directory.\n        suffix (str | tuple(str), optional): File suffix that we are\n            interested in. Default: None.\n        recursive (bool, optional): If set to True, recursively scan the\n            directory. Default: False.\n        case_sensitive (bool, optional) : If set to False, ignore the case of\n            suffix. Default: True.",
        "detail": "SVTAS.svtas.utils.path",
        "documentation": {}
    },
    {
        "label": "find_vcs_root",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.path",
        "description": "SVTAS.svtas.utils.path",
        "peekOfCode": "def find_vcs_root(path, markers=('.git', )):\n    \"\"\"Finds the root directory (including itself) of specified markers.\n    Args:\n        path (str): Path of directory or file.\n        markers (list[str], optional): List of file or directory names.\n    Returns:\n        The directory contained one of the markers or None if not found.\n    \"\"\"\n    if osp.isfile(path):\n        path = osp.dirname(path)",
        "detail": "SVTAS.svtas.utils.path",
        "documentation": {}
    },
    {
        "label": "build_recod",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.recorder",
        "description": "SVTAS.svtas.utils.recorder",
        "peekOfCode": "def build_recod(architecture_type, mode):\n    assert mode in [\"train\", \"validation\", \"test\"]\n    if architecture_type in [\"StreamSegmentation2DWithNeck\"]:\n        if mode == \"train\":\n            return {'batch_time': AverageMeter('batch_cost', '.5f'),\n                    'reader_time': AverageMeter('reader_time', '.5f'),\n                    'loss': AverageMeter('loss', '7.5f'),\n                    'lr': AverageMeter('lr', 'f', need_avg=False),\n                    'F1@0.5': AverageMeter(\"F1@0.50\", '.5f'),\n                    'Acc': AverageMeter(\"Acc\", '.5f'),",
        "detail": "SVTAS.svtas.utils.recorder",
        "documentation": {}
    },
    {
        "label": "mkdir",
        "kind": 2,
        "importPath": "SVTAS.svtas.utils.save_load",
        "description": "SVTAS.svtas.utils.save_load",
        "peekOfCode": "def mkdir(dir):\n    if not os.path.exists(dir):\n        # avoid error when train with multiple gpus\n        try:\n            os.makedirs(dir)\n        except:\n            pass",
        "detail": "SVTAS.svtas.utils.save_load",
        "documentation": {}
    },
    {
        "label": "get_video_clip_list",
        "kind": 2,
        "importPath": "SVTAS.tools.dataset_transform.prepare_video_recognition_data",
        "description": "SVTAS.tools.dataset_transform.prepare_video_recognition_data",
        "peekOfCode": "def get_video_clip_list(label, background_id, fps, total_frames):\n    clip_dict = {}\n    video_ation_clip = []  # start_frame end_frame action_id\n    video_background_clip = []  # start_frame end_frame action_id\n    background_idx_list = []\n    frame_index = 0\n    background_list_idx = 0\n    for action in label:\n        start_frame = int(np.floor(action[\"segment\"][0] * fps))\n        end_frame = int(np.floor(action[\"segment\"][1] * fps)) + 1",
        "detail": "SVTAS.tools.dataset_transform.prepare_video_recognition_data",
        "documentation": {}
    },
    {
        "label": "caculate_video_std_mean",
        "kind": 2,
        "importPath": "SVTAS.tools.dataset_transform.prepare_video_recognition_data",
        "description": "SVTAS.tools.dataset_transform.prepare_video_recognition_data",
        "peekOfCode": "def caculate_video_std_mean(video_path, sample_rate, label_fps, dataset_type):\n    result_dict = {}\n    # read video\n    if dataset_type in ['gtea_rgb', '50salads_rgb',\n    'gtea_flow', '50salads_flow', 'thumos14_flow', 'egtea_flow', 'breakfast_flow']:\n        video_capture = de.VideoReader(video_path)\n    elif dataset_type in ['thumos14_rgb', 'egtea_rgb']:\n        video_capture = de.VideoReader(video_path + \".mp4\")\n    elif dataset_type in ['breakfast_rgb']:\n        video_ptr = video_path.split('/')[-1].split('.')[0].split('_')",
        "detail": "SVTAS.tools.dataset_transform.prepare_video_recognition_data",
        "documentation": {}
    },
    {
        "label": "video_split_to_clip",
        "kind": 2,
        "importPath": "SVTAS.tools.dataset_transform.prepare_video_recognition_data",
        "description": "SVTAS.tools.dataset_transform.prepare_video_recognition_data",
        "peekOfCode": "def video_split_to_clip(video_path, output_path_fix, video_name, label_fps,\n                        sample_rate, video_clip_list, action_dict,\n                        background_id, val_prob, only_norm_flag,\n                        dataset_type):\n    result_dict = {}\n    result_dict[\"rec_label_list\"] = []\n    result_dict[\"rec_val_label_list\"] = []\n    output_path_fix = os.path.join(output_path_fix, video_name)\n    if only_norm_flag is False:\n        isExists = os.path.exists(output_path_fix)",
        "detail": "SVTAS.tools.dataset_transform.prepare_video_recognition_data",
        "documentation": {}
    },
    {
        "label": "resample_background",
        "kind": 2,
        "importPath": "SVTAS.tools.dataset_transform.prepare_video_recognition_data",
        "description": "SVTAS.tools.dataset_transform.prepare_video_recognition_data",
        "peekOfCode": "def resample_background(video_clip_dict, background_id, neg_num):\n    clips_dict = {}\n    key_list = list(video_clip_dict.keys())\n    avg_sample = math.floor(neg_num / len(key_list))\n    cnt_neg = 0\n    if neg_num > 0:\n        if avg_sample >= 1:\n            for key in key_list:\n                temp_dict = {}\n                sample_idx = video_clip_dict[key][\"background_idx\"]",
        "detail": "SVTAS.tools.dataset_transform.prepare_video_recognition_data",
        "documentation": {}
    },
    {
        "label": "load_action_dict",
        "kind": 2,
        "importPath": "SVTAS.tools.dataset_transform.prepare_video_recognition_data",
        "description": "SVTAS.tools.dataset_transform.prepare_video_recognition_data",
        "peekOfCode": "def load_action_dict(data_path):\n    mapping_txt_path = os.path.join(data_path, \"mapping.txt\")\n    with open(mapping_txt_path, \"r\", encoding='utf-8') as f:\n        actions = f.read().split(\"\\n\")[:-1]\n    id2class_map = dict()\n    class2id_map = dict()\n    for a in actions:\n        id2class_map[int(a.split()[0])] = a.split()[1]\n        class2id_map[a.split()[1]] = int(a.split()[0])\n    return id2class_map, class2id_map",
        "detail": "SVTAS.tools.dataset_transform.prepare_video_recognition_data",
        "documentation": {}
    },
    {
        "label": "get_arguments",
        "kind": 2,
        "importPath": "SVTAS.tools.dataset_transform.prepare_video_recognition_data",
        "description": "SVTAS.tools.dataset_transform.prepare_video_recognition_data",
        "peekOfCode": "def get_arguments():\n    \"\"\"\n    parse all the arguments from command line inteface\n    return a list of parsed arguments\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"convert segmentation and localization label\")\n    parser.add_argument(\"label_path\", type=str, help=\"path of a label file\")\n    parser.add_argument(\n        \"video_path\",",
        "detail": "SVTAS.tools.dataset_transform.prepare_video_recognition_data",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "SVTAS.tools.dataset_transform.prepare_video_recognition_data",
        "description": "SVTAS.tools.dataset_transform.prepare_video_recognition_data",
        "peekOfCode": "def main():\n    args = get_arguments()\n    with open(args.label_path, 'r', encoding='utf-8') as json_file:\n        label = json.load(json_file)\n    id2class_map, class2id_map = load_action_dict(args.out_path)\n    background_id = None\n    for ignore_action in ignore_action_list:\n        if ignore_action in class2id_map.keys():\n            background_id = class2id_map[ignore_action]\n    if background_id is None:",
        "detail": "SVTAS.tools.dataset_transform.prepare_video_recognition_data",
        "documentation": {}
    },
    {
        "label": "ignore_action_list",
        "kind": 5,
        "importPath": "SVTAS.tools.dataset_transform.prepare_video_recognition_data",
        "description": "SVTAS.tools.dataset_transform.prepare_video_recognition_data",
        "peekOfCode": "ignore_action_list = [\"background\", \"None\", \"STL\"]\ndef get_video_clip_list(label, background_id, fps, total_frames):\n    clip_dict = {}\n    video_ation_clip = []  # start_frame end_frame action_id\n    video_background_clip = []  # start_frame end_frame action_id\n    background_idx_list = []\n    frame_index = 0\n    background_list_idx = 0\n    for action in label:\n        start_frame = int(np.floor(action[\"segment\"][0] * fps))",
        "detail": "SVTAS.tools.dataset_transform.prepare_video_recognition_data",
        "documentation": {}
    },
    {
        "label": "generate_mapping_list_txt",
        "kind": 2,
        "importPath": "SVTAS.tools.dataset_transform.transform_breakfast_fine_label",
        "description": "SVTAS.tools.dataset_transform.transform_breakfast_fine_label",
        "peekOfCode": "def generate_mapping_list_txt(action_dict, out_path):\n    out_txt_file_path = os.path.join(out_path, \"mapping_fine.txt\")\n    f = open(out_txt_file_path, \"w\", encoding='utf-8')\n    for key, action_name in action_dict.items():\n        str_str = str(key) + \" \" + action_name + \"\\n\"\n        f.write(str_str)\n    # add background\n    str_str = str(len(action_dict)) + \" background\" + \"\\n\"\n    f.write(str_str)\n    f.close()",
        "detail": "SVTAS.tools.dataset_transform.transform_breakfast_fine_label",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "SVTAS.tools.dataset_transform.transform_breakfast_fine_label",
        "description": "SVTAS.tools.dataset_transform.transform_breakfast_fine_label",
        "peekOfCode": "def main():\n    args = get_arguments()\n    Filelist = []\n    for home, dirs, files in os.walk(args.label_path):\n        for filename in files:\n            if filename.endswith(\"txt\"):\n                Filelist.append(os.path.join(home, filename))\n    VideoFileset = set([])\n    for home, dirs, files in os.walk(args.video_path):\n        for filename in files:",
        "detail": "SVTAS.tools.dataset_transform.transform_breakfast_fine_label",
        "documentation": {}
    },
    {
        "label": "get_arguments",
        "kind": 2,
        "importPath": "SVTAS.tools.dataset_transform.transform_breakfast_fine_label",
        "description": "SVTAS.tools.dataset_transform.transform_breakfast_fine_label",
        "peekOfCode": "def get_arguments():\n    \"\"\"\n    parse all the arguments from command line inteface\n    return a list of parsed arguments\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"convert fine segmentation label\")\n    parser.add_argument(\"label_path\", type=str, help=\"path of a label files\")\n    parser.add_argument(\"video_path\", type=str, help=\"path of a video files\")\n    parser.add_argument(",
        "detail": "SVTAS.tools.dataset_transform.transform_breakfast_fine_label",
        "documentation": {}
    },
    {
        "label": "load_action_dict",
        "kind": 2,
        "importPath": "SVTAS.tools.dataset_transform.transform_egtea_label",
        "description": "SVTAS.tools.dataset_transform.transform_egtea_label",
        "peekOfCode": "def load_action_dict(label_path):\n    with open(label_path, \"r\", encoding='utf-8') as f:\n        actions = f.read().split(\"\\n\")[:-1]\n    id2class_map = dict()\n    for a in actions:\n        id2class_map[int(a.split(\" \")[1])] = a.split(\" \")[0]\n    return id2class_map\ndef generate_mapping_list_txt(action_dict, out_path):\n    out_txt_file_path = os.path.join(out_path, \"mapping.txt\")\n    f = open(out_txt_file_path, \"w\", encoding='utf-8')",
        "detail": "SVTAS.tools.dataset_transform.transform_egtea_label",
        "documentation": {}
    },
    {
        "label": "generate_mapping_list_txt",
        "kind": 2,
        "importPath": "SVTAS.tools.dataset_transform.transform_egtea_label",
        "description": "SVTAS.tools.dataset_transform.transform_egtea_label",
        "peekOfCode": "def generate_mapping_list_txt(action_dict, out_path):\n    out_txt_file_path = os.path.join(out_path, \"mapping.txt\")\n    f = open(out_txt_file_path, \"w\", encoding='utf-8')\n    for key, action_name in action_dict.items():\n        str_str = str(key - 1) + \" \" + action_name + \"\\n\"\n        f.write(str_str)\n    # add None\n    str_str = str(len(action_dict)) + \" None\" + \"\\n\"\n    f.write(str_str)\n    f.close()",
        "detail": "SVTAS.tools.dataset_transform.transform_egtea_label",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "SVTAS.tools.dataset_transform.transform_egtea_label",
        "description": "SVTAS.tools.dataset_transform.transform_egtea_label",
        "peekOfCode": "def main():\n    args = get_arguments()\n    fils_name = os.listdir(args.label_path)\n    id2class_map = load_action_dict(args.action_idx_path)\n    generate_mapping_list_txt(id2class_map, args.out_path)\n    test_list = []\n    train_list = []\n    for file in fils_name:\n        if file.startswith(\"test\"):\n            test_list.append(file)",
        "detail": "SVTAS.tools.dataset_transform.transform_egtea_label",
        "documentation": {}
    },
    {
        "label": "get_arguments",
        "kind": 2,
        "importPath": "SVTAS.tools.dataset_transform.transform_egtea_label",
        "description": "SVTAS.tools.dataset_transform.transform_egtea_label",
        "peekOfCode": "def get_arguments():\n    \"\"\"\n    parse all the arguments from command line inteface\n    return a list of parsed arguments\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"convert segmentation and localization label\")\n    parser.add_argument(\"label_path\", type=str, help=\"path of a label files\")\n    parser.add_argument(\n        \"action_idx_path\",",
        "detail": "SVTAS.tools.dataset_transform.transform_egtea_label",
        "documentation": {}
    },
    {
        "label": "generate_mapping_list_txt",
        "kind": 2,
        "importPath": "SVTAS.tools.dataset_transform.transform_segmentation_label",
        "description": "SVTAS.tools.dataset_transform.transform_segmentation_label",
        "peekOfCode": "def generate_mapping_list_txt(action_dict, out_path):\n    out_txt_file_path = os.path.join(out_path, \"mapping.txt\")\n    f = open(out_txt_file_path, \"w\", encoding='utf-8')\n    for key, action_name in action_dict.items():\n        str_str = str(key) + \" \" + action_name + \"\\n\"\n        f.write(str_str)\n    # add None\n    str_str = str(len(action_dict)) + \" None\" + \"\\n\"\n    f.write(str_str)\n    f.close()",
        "detail": "SVTAS.tools.dataset_transform.transform_segmentation_label",
        "documentation": {}
    },
    {
        "label": "segmentation_convert_localization_label",
        "kind": 2,
        "importPath": "SVTAS.tools.dataset_transform.transform_segmentation_label",
        "description": "SVTAS.tools.dataset_transform.transform_segmentation_label",
        "peekOfCode": "def segmentation_convert_localization_label(prefix_data_path, out_path,\n                                            action_dict, fps):\n    label_path = os.path.join(prefix_data_path)\n    label_txt_name_list = os.listdir(label_path)\n    output_dict = {}\n    output_dict[\"fps\"] = fps\n    labels_dict = {}\n    for label_name in tqdm(label_txt_name_list, desc='label convert:'):\n        label_dict = {}\n        # Todos: according video format change",
        "detail": "SVTAS.tools.dataset_transform.transform_segmentation_label",
        "documentation": {}
    },
    {
        "label": "generate_action_dict",
        "kind": 2,
        "importPath": "SVTAS.tools.dataset_transform.transform_segmentation_label",
        "description": "SVTAS.tools.dataset_transform.transform_segmentation_label",
        "peekOfCode": "def generate_action_dict(label):\n    action_list = []\n    for vid, gt in label[\"database\"].items():\n        for action in gt[\"annotations\"]:\n            label_name = action[\"label\"]\n            if label_name not in action_list:\n                action_list.append(label_name)\n    action_dict = {}\n    for idx in range(len(action_list)):\n        action_dict[idx] = action_list[idx]",
        "detail": "SVTAS.tools.dataset_transform.transform_segmentation_label",
        "documentation": {}
    },
    {
        "label": "load_action_dict",
        "kind": 2,
        "importPath": "SVTAS.tools.dataset_transform.transform_segmentation_label",
        "description": "SVTAS.tools.dataset_transform.transform_segmentation_label",
        "peekOfCode": "def load_action_dict(data_path):\n    mapping_txt_path = os.path.join(data_path, \"mapping.txt\")\n    with open(mapping_txt_path, \"r\", encoding='utf-8') as f:\n        actions = f.read().split(\"\\n\")[:-1]\n    class2id_map = dict()\n    for a in actions:\n        class2id_map[a.split()[1]] = int(a.split()[0])\n    return class2id_map\ndef localization_convert_segmentation_label(label, prefix_data_path, out_path, fps):\n    path = os.path.join(out_path, \"groundTruth\")",
        "detail": "SVTAS.tools.dataset_transform.transform_segmentation_label",
        "documentation": {}
    },
    {
        "label": "localization_convert_segmentation_label",
        "kind": 2,
        "importPath": "SVTAS.tools.dataset_transform.transform_segmentation_label",
        "description": "SVTAS.tools.dataset_transform.transform_segmentation_label",
        "peekOfCode": "def localization_convert_segmentation_label(label, prefix_data_path, out_path, fps):\n    path = os.path.join(out_path, \"groundTruth\")\n    isExists = os.path.exists(path)\n    if not isExists:\n        os.makedirs(path)\n        print(path + ' created successful')\n    # fps = float(label[\"fps\"])\n    video_val_list = []\n    video_test_list = []\n    for vid, gt in tqdm(label[\"database\"].items(), desc='label convert:'):",
        "detail": "SVTAS.tools.dataset_transform.transform_segmentation_label",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "SVTAS.tools.dataset_transform.transform_segmentation_label",
        "description": "SVTAS.tools.dataset_transform.transform_segmentation_label",
        "peekOfCode": "def main():\n    args = get_arguments()\n    if args.mode in [\"segmentation\", \"localization\"]:\n        if args.mode == \"segmentation\":\n            with open(args.label_path, 'r', encoding='utf-8') as json_file:\n                label = json.load(json_file)\n            action_dict = generate_action_dict(label)\n            generate_mapping_list_txt(action_dict, args.out_path)\n            localization_convert_segmentation_label(label, args.data_path,\n                                                    args.out_path, args.fps)",
        "detail": "SVTAS.tools.dataset_transform.transform_segmentation_label",
        "documentation": {}
    },
    {
        "label": "get_arguments",
        "kind": 2,
        "importPath": "SVTAS.tools.dataset_transform.transform_segmentation_label",
        "description": "SVTAS.tools.dataset_transform.transform_segmentation_label",
        "peekOfCode": "def get_arguments():\n    \"\"\"\n    parse all the arguments from command line inteface\n    return a list of parsed arguments\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"convert segmentation and localization label\")\n    parser.add_argument(\"label_path\", type=str, help=\"path of a label file\")\n    parser.add_argument(\n        \"data_path\",",
        "detail": "SVTAS.tools.dataset_transform.transform_segmentation_label",
        "documentation": {}
    },
    {
        "label": "ignore_action_list",
        "kind": 5,
        "importPath": "SVTAS.tools.dataset_transform.transform_segmentation_label",
        "description": "SVTAS.tools.dataset_transform.transform_segmentation_label",
        "peekOfCode": "ignore_action_list = [\"background\", \"None\"]\ndef generate_mapping_list_txt(action_dict, out_path):\n    out_txt_file_path = os.path.join(out_path, \"mapping.txt\")\n    f = open(out_txt_file_path, \"w\", encoding='utf-8')\n    for key, action_name in action_dict.items():\n        str_str = str(key) + \" \" + action_name + \"\\n\"\n        f.write(str_str)\n    # add None\n    str_str = str(len(action_dict)) + \" None\" + \"\\n\"\n    f.write(str_str)",
        "detail": "SVTAS.tools.dataset_transform.transform_segmentation_label",
        "documentation": {}
    },
    {
        "label": "input_constructor",
        "kind": 2,
        "importPath": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "description": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "peekOfCode": "def input_constructor(input_shape):\n    x_shape, mask_shape = input_shape\n    x = torch.randn([1] + x_shape).cuda()\n    mask = torch.randn([1] + mask_shape).cuda()\n    idx = torch.randn([1] + [1]).cuda()\n    return dict(imgs=x, masks=mask, idx=idx)\ndummy_input = input_constructor(input_shape)\nmodel = InceptionI3d(num_classes=11, in_channels=2)\n# mmcv caculate param and flops\nprint(\"=\"*20)",
        "detail": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "documentation": {}
    },
    {
        "label": "path",
        "kind": 5,
        "importPath": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "description": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "peekOfCode": "path = os.path.join(os.getcwd())\nsys.path.append(path)\nfrom mmcv.cnn.utils.flops_counter import get_model_complexity_info\nfrom fvcore.nn import FlopCountAnalysis, flop_count_table\nfrom thop import clever_format\nfrom svtas.model.backbones.video import InceptionI3d\n# I3D model param flops caculate\nclip_seg_num = 64\nsample_rate = 4\nx_shape = [2, clip_seg_num, 244, 244]",
        "detail": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "documentation": {}
    },
    {
        "label": "clip_seg_num",
        "kind": 5,
        "importPath": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "description": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "peekOfCode": "clip_seg_num = 64\nsample_rate = 4\nx_shape = [2, clip_seg_num, 244, 244]\nmask_shape = [clip_seg_num * sample_rate]\ninput_shape = (x_shape, mask_shape)\ndef input_constructor(input_shape):\n    x_shape, mask_shape = input_shape\n    x = torch.randn([1] + x_shape).cuda()\n    mask = torch.randn([1] + mask_shape).cuda()\n    idx = torch.randn([1] + [1]).cuda()",
        "detail": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "documentation": {}
    },
    {
        "label": "sample_rate",
        "kind": 5,
        "importPath": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "description": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "peekOfCode": "sample_rate = 4\nx_shape = [2, clip_seg_num, 244, 244]\nmask_shape = [clip_seg_num * sample_rate]\ninput_shape = (x_shape, mask_shape)\ndef input_constructor(input_shape):\n    x_shape, mask_shape = input_shape\n    x = torch.randn([1] + x_shape).cuda()\n    mask = torch.randn([1] + mask_shape).cuda()\n    idx = torch.randn([1] + [1]).cuda()\n    return dict(imgs=x, masks=mask, idx=idx)",
        "detail": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "documentation": {}
    },
    {
        "label": "x_shape",
        "kind": 5,
        "importPath": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "description": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "peekOfCode": "x_shape = [2, clip_seg_num, 244, 244]\nmask_shape = [clip_seg_num * sample_rate]\ninput_shape = (x_shape, mask_shape)\ndef input_constructor(input_shape):\n    x_shape, mask_shape = input_shape\n    x = torch.randn([1] + x_shape).cuda()\n    mask = torch.randn([1] + mask_shape).cuda()\n    idx = torch.randn([1] + [1]).cuda()\n    return dict(imgs=x, masks=mask, idx=idx)\ndummy_input = input_constructor(input_shape)",
        "detail": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "documentation": {}
    },
    {
        "label": "mask_shape",
        "kind": 5,
        "importPath": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "description": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "peekOfCode": "mask_shape = [clip_seg_num * sample_rate]\ninput_shape = (x_shape, mask_shape)\ndef input_constructor(input_shape):\n    x_shape, mask_shape = input_shape\n    x = torch.randn([1] + x_shape).cuda()\n    mask = torch.randn([1] + mask_shape).cuda()\n    idx = torch.randn([1] + [1]).cuda()\n    return dict(imgs=x, masks=mask, idx=idx)\ndummy_input = input_constructor(input_shape)\nmodel = InceptionI3d(num_classes=11, in_channels=2)",
        "detail": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "documentation": {}
    },
    {
        "label": "input_shape",
        "kind": 5,
        "importPath": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "description": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "peekOfCode": "input_shape = (x_shape, mask_shape)\ndef input_constructor(input_shape):\n    x_shape, mask_shape = input_shape\n    x = torch.randn([1] + x_shape).cuda()\n    mask = torch.randn([1] + mask_shape).cuda()\n    idx = torch.randn([1] + [1]).cuda()\n    return dict(imgs=x, masks=mask, idx=idx)\ndummy_input = input_constructor(input_shape)\nmodel = InceptionI3d(num_classes=11, in_channels=2)\n# mmcv caculate param and flops",
        "detail": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "documentation": {}
    },
    {
        "label": "dummy_input",
        "kind": 5,
        "importPath": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "description": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "peekOfCode": "dummy_input = input_constructor(input_shape)\nmodel = InceptionI3d(num_classes=11, in_channels=2)\n# mmcv caculate param and flops\nprint(\"=\"*20)\nprint('Use mmcv get_model_complexity_info function')\nflops_number, params_number = get_model_complexity_info(model, input_shape=input_shape, input_constructor=input_constructor, print_per_layer_stat=False, as_strings=False)\nflops_per_image_number = flops_number / cfg.DATASET.test.clip_seg_num\nflops, params = clever_format([flops_number, params_number], \"%.6f\")\nflops_per_image, params = clever_format([flops_per_image_number, params_number], \"%.6f\")\nprint(\"Hitp: This FLOPs is caculation by {clip_seg_num:d} imgs\".format(clip_seg_num=cfg.DATASET.test.clip_seg_num))",
        "detail": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "description": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "peekOfCode": "model = InceptionI3d(num_classes=11, in_channels=2)\n# mmcv caculate param and flops\nprint(\"=\"*20)\nprint('Use mmcv get_model_complexity_info function')\nflops_number, params_number = get_model_complexity_info(model, input_shape=input_shape, input_constructor=input_constructor, print_per_layer_stat=False, as_strings=False)\nflops_per_image_number = flops_number / cfg.DATASET.test.clip_seg_num\nflops, params = clever_format([flops_number, params_number], \"%.6f\")\nflops_per_image, params = clever_format([flops_per_image_number, params_number], \"%.6f\")\nprint(\"Hitp: This FLOPs is caculation by {clip_seg_num:d} imgs\".format(clip_seg_num=cfg.DATASET.test.clip_seg_num))\nprint(\"Per Image FLOPs:\"+ flops_per_image + \", Total FLOPs:\" + flops + \", Total params:\" + params)",
        "detail": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "documentation": {}
    },
    {
        "label": "flops_per_image_number",
        "kind": 5,
        "importPath": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "description": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "peekOfCode": "flops_per_image_number = flops_number / cfg.DATASET.test.clip_seg_num\nflops, params = clever_format([flops_number, params_number], \"%.6f\")\nflops_per_image, params = clever_format([flops_per_image_number, params_number], \"%.6f\")\nprint(\"Hitp: This FLOPs is caculation by {clip_seg_num:d} imgs\".format(clip_seg_num=cfg.DATASET.test.clip_seg_num))\nprint(\"Per Image FLOPs:\"+ flops_per_image + \", Total FLOPs:\" + flops + \", Total params:\" + params)\nprint(\"=\"*20)\n# fvcore caculate param and flops\nprint('Use fvcore FlopCountAnalysis function')\ninputs = (dummy_input['input_data'])\nflops = FlopCountAnalysis(model, inputs)",
        "detail": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "documentation": {}
    },
    {
        "label": "inputs",
        "kind": 5,
        "importPath": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "description": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "peekOfCode": "inputs = (dummy_input['input_data'])\nflops = FlopCountAnalysis(model, inputs)\nprint(flop_count_table(flops))\nflops_number = flops.total()\nflops_per_image_number = flops_number / cfg.DATASET.test.clip_seg_num\nflops = clever_format([flops_number], \"%.6f\")\nflops_per_image = clever_format([flops_per_image_number], \"%.6f\")\nprint(\"Hitp: This FLOPs is caculation by {clip_seg_num:d} imgs\".format(clip_seg_num=cfg.DATASET.test.clip_seg_num))\nprint(\"Per Image FLOPs:\"+ flops_per_image + \", Total FLOPs:\" + flops)\nprint(\"=\"*20)",
        "detail": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "documentation": {}
    },
    {
        "label": "flops",
        "kind": 5,
        "importPath": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "description": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "peekOfCode": "flops = FlopCountAnalysis(model, inputs)\nprint(flop_count_table(flops))\nflops_number = flops.total()\nflops_per_image_number = flops_number / cfg.DATASET.test.clip_seg_num\nflops = clever_format([flops_number], \"%.6f\")\nflops_per_image = clever_format([flops_per_image_number], \"%.6f\")\nprint(\"Hitp: This FLOPs is caculation by {clip_seg_num:d} imgs\".format(clip_seg_num=cfg.DATASET.test.clip_seg_num))\nprint(\"Per Image FLOPs:\"+ flops_per_image + \", Total FLOPs:\" + flops)\nprint(\"=\"*20)\n# model fps caculate",
        "detail": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "documentation": {}
    },
    {
        "label": "flops_number",
        "kind": 5,
        "importPath": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "description": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "peekOfCode": "flops_number = flops.total()\nflops_per_image_number = flops_number / cfg.DATASET.test.clip_seg_num\nflops = clever_format([flops_number], \"%.6f\")\nflops_per_image = clever_format([flops_per_image_number], \"%.6f\")\nprint(\"Hitp: This FLOPs is caculation by {clip_seg_num:d} imgs\".format(clip_seg_num=cfg.DATASET.test.clip_seg_num))\nprint(\"Per Image FLOPs:\"+ flops_per_image + \", Total FLOPs:\" + flops)\nprint(\"=\"*20)\n# model fps caculate\ndummy_input = dummy_input['input_data']\nprint('Caculate model fps (single frame infer times)')",
        "detail": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "documentation": {}
    },
    {
        "label": "flops_per_image_number",
        "kind": 5,
        "importPath": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "description": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "peekOfCode": "flops_per_image_number = flops_number / cfg.DATASET.test.clip_seg_num\nflops = clever_format([flops_number], \"%.6f\")\nflops_per_image = clever_format([flops_per_image_number], \"%.6f\")\nprint(\"Hitp: This FLOPs is caculation by {clip_seg_num:d} imgs\".format(clip_seg_num=cfg.DATASET.test.clip_seg_num))\nprint(\"Per Image FLOPs:\"+ flops_per_image + \", Total FLOPs:\" + flops)\nprint(\"=\"*20)\n# model fps caculate\ndummy_input = dummy_input['input_data']\nprint('Caculate model fps (single frame infer times)')\nstarter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)",
        "detail": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "documentation": {}
    },
    {
        "label": "flops",
        "kind": 5,
        "importPath": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "description": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "peekOfCode": "flops = clever_format([flops_number], \"%.6f\")\nflops_per_image = clever_format([flops_per_image_number], \"%.6f\")\nprint(\"Hitp: This FLOPs is caculation by {clip_seg_num:d} imgs\".format(clip_seg_num=cfg.DATASET.test.clip_seg_num))\nprint(\"Per Image FLOPs:\"+ flops_per_image + \", Total FLOPs:\" + flops)\nprint(\"=\"*20)\n# model fps caculate\ndummy_input = dummy_input['input_data']\nprint('Caculate model fps (single frame infer times)')\nstarter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\nrepetitions = 300",
        "detail": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "documentation": {}
    },
    {
        "label": "flops_per_image",
        "kind": 5,
        "importPath": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "description": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "peekOfCode": "flops_per_image = clever_format([flops_per_image_number], \"%.6f\")\nprint(\"Hitp: This FLOPs is caculation by {clip_seg_num:d} imgs\".format(clip_seg_num=cfg.DATASET.test.clip_seg_num))\nprint(\"Per Image FLOPs:\"+ flops_per_image + \", Total FLOPs:\" + flops)\nprint(\"=\"*20)\n# model fps caculate\ndummy_input = dummy_input['input_data']\nprint('Caculate model fps (single frame infer times)')\nstarter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\nrepetitions = 300\ntimings = np.zeros((repetitions, 1))",
        "detail": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "documentation": {}
    },
    {
        "label": "dummy_input",
        "kind": 5,
        "importPath": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "description": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "peekOfCode": "dummy_input = dummy_input['input_data']\nprint('Caculate model fps (single frame infer times)')\nstarter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\nrepetitions = 300\ntimings = np.zeros((repetitions, 1))\n#GPU-WARM-UP\nfor _ in range(10):\n    _ = model(dummy_input)\n# MEASURE PERFORMANCE\nwith torch.no_grad():",
        "detail": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "documentation": {}
    },
    {
        "label": "repetitions",
        "kind": 5,
        "importPath": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "description": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "peekOfCode": "repetitions = 300\ntimings = np.zeros((repetitions, 1))\n#GPU-WARM-UP\nfor _ in range(10):\n    _ = model(dummy_input)\n# MEASURE PERFORMANCE\nwith torch.no_grad():\n    for rep in range(repetitions):\n        starter.record()\n        _ = model(dummy_input)",
        "detail": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "documentation": {}
    },
    {
        "label": "timings",
        "kind": 5,
        "importPath": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "description": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "peekOfCode": "timings = np.zeros((repetitions, 1))\n#GPU-WARM-UP\nfor _ in range(10):\n    _ = model(dummy_input)\n# MEASURE PERFORMANCE\nwith torch.no_grad():\n    for rep in range(repetitions):\n        starter.record()\n        _ = model(dummy_input)\n        ender.record()",
        "detail": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "documentation": {}
    },
    {
        "label": "mean_syn",
        "kind": 5,
        "importPath": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "description": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "peekOfCode": "mean_syn = np.sum(timings) / repetitions\nstd_syn = np.std(timings)\nmean_fps = 1000. / mean_syn * clip_seg_num\nprint('Mean@1 {mean_syn:.3f}ms, Std@5 {std_syn:.3f}ms, FPS@1 {mean_fps:.2f}'.format(mean_syn=mean_syn, std_syn=std_syn, mean_fps=mean_fps))\nprint('Model single forward infer time(ms) {mean_syn:.3f}ms'.format(mean_syn=mean_syn))\nprint(\"=\"*20)\n# model latency time\nprint('Caculate model Throughput')\nrepetitions=100\ntotal_time = 0",
        "detail": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "documentation": {}
    },
    {
        "label": "std_syn",
        "kind": 5,
        "importPath": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "description": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "peekOfCode": "std_syn = np.std(timings)\nmean_fps = 1000. / mean_syn * clip_seg_num\nprint('Mean@1 {mean_syn:.3f}ms, Std@5 {std_syn:.3f}ms, FPS@1 {mean_fps:.2f}'.format(mean_syn=mean_syn, std_syn=std_syn, mean_fps=mean_fps))\nprint('Model single forward infer time(ms) {mean_syn:.3f}ms'.format(mean_syn=mean_syn))\nprint(\"=\"*20)\n# model latency time\nprint('Caculate model Throughput')\nrepetitions=100\ntotal_time = 0\n# it should be modify by every model",
        "detail": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "documentation": {}
    },
    {
        "label": "mean_fps",
        "kind": 5,
        "importPath": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "description": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "peekOfCode": "mean_fps = 1000. / mean_syn * clip_seg_num\nprint('Mean@1 {mean_syn:.3f}ms, Std@5 {std_syn:.3f}ms, FPS@1 {mean_fps:.2f}'.format(mean_syn=mean_syn, std_syn=std_syn, mean_fps=mean_fps))\nprint('Model single forward infer time(ms) {mean_syn:.3f}ms'.format(mean_syn=mean_syn))\nprint(\"=\"*20)\n# model latency time\nprint('Caculate model Throughput')\nrepetitions=100\ntotal_time = 0\n# it should be modify by every model\noptimal_batch_size=1",
        "detail": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "documentation": {}
    },
    {
        "label": "total_time",
        "kind": 5,
        "importPath": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "description": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "peekOfCode": "total_time = 0\n# it should be modify by every model\noptimal_batch_size=1\ndummy_input = input_constructor(input_shape, optimal_batch_size=optimal_batch_size)['input_data']\nwith torch.no_grad():\n    for rep in range(repetitions):\n        starter, ender = torch.cuda.Event(enable_timing=True),torch.cuda.Event(enable_timing=True)\n        starter.record()\n        _ = model(dummy_input)\n        ender.record()",
        "detail": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "documentation": {}
    },
    {
        "label": "dummy_input",
        "kind": 5,
        "importPath": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "description": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "peekOfCode": "dummy_input = input_constructor(input_shape, optimal_batch_size=optimal_batch_size)['input_data']\nwith torch.no_grad():\n    for rep in range(repetitions):\n        starter, ender = torch.cuda.Event(enable_timing=True),torch.cuda.Event(enable_timing=True)\n        starter.record()\n        _ = model(dummy_input)\n        ender.record()\n        torch.cuda.synchronize()\n        curr_time = starter.elapsed_time(ender) / 1000\n        total_time += curr_time",
        "detail": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "documentation": {}
    },
    {
        "label": "Throughput",
        "kind": 5,
        "importPath": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "description": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "peekOfCode": "Throughput = (repetitions * optimal_batch_size) / total_time\nprint(\"Final Throughput: {Throughput:.2f} V/s, Measuring by batch_size: {Batch_size:d}\".format(Throughput=Throughput, Batch_size=optimal_batch_size))\nprint(\"=\"*20)",
        "detail": "SVTAS.tools.data_anlysis.caculate_model_complex_info",
        "documentation": {}
    },
    {
        "label": "get_arguments",
        "kind": 2,
        "importPath": "SVTAS.tools.data_anlysis.statistic_labels_num",
        "description": "SVTAS.tools.data_anlysis.statistic_labels_num",
        "peekOfCode": "def get_arguments() -> argparse.Namespace:\n    \"\"\"\n    parse all the arguments from command line inteface\n    return a list of parsed arguments\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"convert pred and gt list to images.\")\n    parser.add_argument(\n        \"file_list_path\",\n        type=str,\n        help=\"path to dataset file list\",",
        "detail": "SVTAS.tools.data_anlysis.statistic_labels_num",
        "documentation": {}
    },
    {
        "label": "load_action_dict",
        "kind": 2,
        "importPath": "SVTAS.tools.data_anlysis.statistic_labels_num",
        "description": "SVTAS.tools.data_anlysis.statistic_labels_num",
        "peekOfCode": "def load_action_dict(label_path):\n    with open(label_path, \"r\", encoding='utf-8') as f:\n        actions = f.read().split(\"\\n\")[:-1]\n    id2class_map = dict()\n    for a in actions:\n        id2class_map[int(a.split(\" \")[0])] = a.split(\" \")[1]\n    return id2class_map\ndef parse_file_paths(input_path):\n    file_ptr = open(input_path, 'r')\n    info = file_ptr.read().split('\\n')[:-1]",
        "detail": "SVTAS.tools.data_anlysis.statistic_labels_num",
        "documentation": {}
    },
    {
        "label": "parse_file_paths",
        "kind": 2,
        "importPath": "SVTAS.tools.data_anlysis.statistic_labels_num",
        "description": "SVTAS.tools.data_anlysis.statistic_labels_num",
        "peekOfCode": "def parse_file_paths(input_path):\n    file_ptr = open(input_path, 'r')\n    info = file_ptr.read().split('\\n')[:-1]\n    file_ptr.close()\n    return info\ndef main() -> None:\n    args = get_arguments()\n    file_list = parse_file_paths(args.file_list_path)\n    id2class_map = load_action_dict(args.mapping_txt_path)\n    num_dict = {}",
        "detail": "SVTAS.tools.data_anlysis.statistic_labels_num",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "SVTAS.tools.data_anlysis.statistic_labels_num",
        "description": "SVTAS.tools.data_anlysis.statistic_labels_num",
        "peekOfCode": "def main() -> None:\n    args = get_arguments()\n    file_list = parse_file_paths(args.file_list_path)\n    id2class_map = load_action_dict(args.mapping_txt_path)\n    num_dict = {}\n    total_frame_cnt = {}\n    for file_name in tqdm(file_list, desc=\"label count\"):\n        video_name = file_name.split('.')[0]\n        label_path = os.path.join(args.labels_path, video_name + '.txt')\n        file_ptr = open(label_path, 'r')",
        "detail": "SVTAS.tools.data_anlysis.statistic_labels_num",
        "documentation": {}
    },
    {
        "label": "extractor",
        "kind": 2,
        "importPath": "SVTAS.tools.extract.extract_features",
        "description": "SVTAS.tools.extract.extract_features",
        "peekOfCode": "def extractor(cfg, outpath, flow_extract):\n    if flow_extract:\n        out_path = os.path.join(outpath, \"flow_features\")\n    else:\n        out_path = os.path.join(outpath, \"features\")\n    isExists = os.path.exists(out_path)\n    if not isExists:\n        os.makedirs(out_path)\n        print(out_path + ' created successful')\n    logger = get_logger(\"SVTAS\")",
        "detail": "SVTAS.tools.extract.extract_features",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "SVTAS.tools.extract.extract_features",
        "description": "SVTAS.tools.extract.extract_features",
        "peekOfCode": "def parse_args():\n    parser = argparse.ArgumentParser(\"SVTAS extract video feature script\")\n    parser.add_argument('-c',\n                        '--config',\n                        type=str,\n                        default='configs/example.yaml',\n                        help='config file path')\n    parser.add_argument('-o',\n                        '--out_path',\n                        type=str,",
        "detail": "SVTAS.tools.extract.extract_features",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "SVTAS.tools.extract.extract_features",
        "description": "SVTAS.tools.extract.extract_features",
        "peekOfCode": "def main():\n    args = parse_args()\n    setup_logger(f\"./output/etract_feature\", name=\"SVTAS\", level=\"INFO\", tensorboard=False)\n    cfg = Config.fromfile(args.config)\n    extractor(cfg, args.out_path, args.flow_extract)\nif __name__ == '__main__':\n    main()",
        "detail": "SVTAS.tools.extract.extract_features",
        "documentation": {}
    },
    {
        "label": "path",
        "kind": 5,
        "importPath": "SVTAS.tools.extract.extract_features",
        "description": "SVTAS.tools.extract.extract_features",
        "peekOfCode": "path = os.path.join(os.getcwd())\nsys.path.append(path)\nimport torch\nimport numpy as np\nimport svtas.model.builder as model_builder\nimport svtas.loader.builder as dataset_builder\nimport argparse\nfrom svtas.utils.config import Config\nfrom svtas.utils.logger import get_logger, setup_logger\nfrom mmcv.runner import load_state_dict",
        "detail": "SVTAS.tools.extract.extract_features",
        "documentation": {}
    },
    {
        "label": "extractor",
        "kind": 2,
        "importPath": "SVTAS.tools.extract.extract_flow",
        "description": "SVTAS.tools.extract.extract_flow",
        "peekOfCode": "def extractor(cfg, args):\n    logger = get_logger(\"SVTAS\")\n    # construct model\n    model = model_builder.build_model(cfg.MODEL).cuda()\n    # construct dataloader\n    num_workers = cfg.DATASET.get('num_workers', 0)\n    test_num_workers = cfg.DATASET.get('test_num_workers', num_workers)\n    temporal_clip_batch_size = cfg.DATASET.get('temporal_clip_batch_size', 3)\n    video_batch_size = cfg.DATASET.get('video_batch_size', 1)\n    assert video_batch_size == 1, \"Only support 1 batch size\"",
        "detail": "SVTAS.tools.extract.extract_flow",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "SVTAS.tools.extract.extract_flow",
        "description": "SVTAS.tools.extract.extract_flow",
        "peekOfCode": "def parse_args():\n    parser = argparse.ArgumentParser(\"SVTAS extract optical flow script\")\n    parser.add_argument('-c',\n                        '--config',\n                        type=str,\n                        default='configs/example.yaml',\n                        help='config file path')\n    parser.add_argument('-o',\n                        '--out_path',\n                        type=str,",
        "detail": "SVTAS.tools.extract.extract_flow",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "SVTAS.tools.extract.extract_flow",
        "description": "SVTAS.tools.extract.extract_flow",
        "peekOfCode": "def main():\n    args = parse_args()\n    cfg = Config.fromfile(args.config)\n    setup_logger(f\"./output/etract_flow\", name=\"SVTAS\", level=\"INFO\", tensorboard=False)\n    extractor(cfg, args)\nif __name__ == '__main__':\n    main()",
        "detail": "SVTAS.tools.extract.extract_flow",
        "documentation": {}
    },
    {
        "label": "path",
        "kind": 5,
        "importPath": "SVTAS.tools.extract.extract_flow",
        "description": "SVTAS.tools.extract.extract_flow",
        "peekOfCode": "path = os.path.join(os.getcwd())\nsys.path.append(path)\nimport torch\nimport svtas.model.builder as model_builder\nimport argparse\nfrom svtas.utils.logger import get_logger, setup_logger\nfrom svtas.utils.config import Config\nimport svtas.loader.builder as dataset_builder\nfrom svtas.runner.extract_runner import ExtractOpticalFlowRunner\n@torch.no_grad()",
        "detail": "SVTAS.tools.extract.extract_flow",
        "documentation": {}
    },
    {
        "label": "export_model_to_onnx",
        "kind": 2,
        "importPath": "SVTAS.tools.infer.export_model_to_onnx",
        "description": "SVTAS.tools.infer.export_model_to_onnx",
        "peekOfCode": "def export_model_to_onnx(cfg,\n                         args):\n    logger = get_logger(\"SVTAS\")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    # construct torch model\n    model = model_builder.build_model(cfg.MODEL).to(device)\n    checkpoint = torch.load(args.weights)\n    state_dicts = checkpoint['model_state_dict']\n    model.load_state_dict(state_dicts)\n    # export path construct",
        "detail": "SVTAS.tools.infer.export_model_to_onnx",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "SVTAS.tools.infer.export_model_to_onnx",
        "description": "SVTAS.tools.infer.export_model_to_onnx",
        "peekOfCode": "def parse_args():\n    parser = argparse.ArgumentParser(\"SVTAS export model script\")\n    parser.add_argument('-c',\n                        '--config',\n                        type=str,\n                        default='configs/example.yaml',\n                        help='config file path')\n    parser.add_argument('--export_path',\n                        type=str,\n                        default='output/infer/',",
        "detail": "SVTAS.tools.infer.export_model_to_onnx",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "SVTAS.tools.infer.export_model_to_onnx",
        "description": "SVTAS.tools.infer.export_model_to_onnx",
        "peekOfCode": "def main():\n    args = parse_args()\n    cfg = get_config(args.config, overrides=args.override, tensorboard=False, logger_path = \"output/infer\")\n    export_model_to_onnx(cfg, args)\nif __name__ == '__main__':\n    main()",
        "detail": "SVTAS.tools.infer.export_model_to_onnx",
        "documentation": {}
    },
    {
        "label": "path",
        "kind": 5,
        "importPath": "SVTAS.tools.infer.export_model_to_onnx",
        "description": "SVTAS.tools.infer.export_model_to_onnx",
        "peekOfCode": "path = os.path.join(os.getcwd())\nsys.path.append(path)\nimport svtas.model.builder as model_builder\nfrom svtas.utils.logger import get_logger\nimport torch\nimport numpy as np\nimport onnx\nimport onnxruntime\nfrom svtas.utils.config import get_config\n@torch.no_grad()",
        "detail": "SVTAS.tools.infer.export_model_to_onnx",
        "documentation": {}
    },
    {
        "label": "load_capture",
        "kind": 2,
        "importPath": "SVTAS.tools.infer.infer",
        "description": "SVTAS.tools.infer.infer",
        "peekOfCode": "def load_capture(args):\n    capture = cv2.VideoCapture(args.input)\n    return capture\ndef make_palette(num_classes):\n    \"\"\"\n    Maps classes to colors in the style of PASCAL VOC.\n    Close values are mapped to far colors for segmentation visualization.\n    See http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#devkit\n    Takes:\n        num_classes: the number of classes",
        "detail": "SVTAS.tools.infer.infer",
        "documentation": {}
    },
    {
        "label": "make_palette",
        "kind": 2,
        "importPath": "SVTAS.tools.infer.infer",
        "description": "SVTAS.tools.infer.infer",
        "peekOfCode": "def make_palette(num_classes):\n    \"\"\"\n    Maps classes to colors in the style of PASCAL VOC.\n    Close values are mapped to far colors for segmentation visualization.\n    See http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#devkit\n    Takes:\n        num_classes: the number of classes\n    Gives:\n        palette: the colormap as a k x 3 array of RGB colors\n    \"\"\"",
        "detail": "SVTAS.tools.infer.infer",
        "documentation": {}
    },
    {
        "label": "draw_action_label",
        "kind": 2,
        "importPath": "SVTAS.tools.infer.infer",
        "description": "SVTAS.tools.infer.infer",
        "peekOfCode": "def draw_action_label(img, palette, action_dict, label):\n    fix_buffer = 12\n    for i in range(len(label)):\n        k = label[i]\n        color_plate = (int(palette[k][2]), int(palette[k][1]), int(palette[k][0]))\n        img = cv2.rectangle(img, (5, 15 + fix_buffer * i), (25, 5 + fix_buffer * i), color_plate, thickness=-1)\n        cv2.putText(img, action_dict[k], (30, 12 + fix_buffer * i), cv2.FONT_HERSHEY_COMPLEX, 0.25, color_plate, 1)\n    return img\ndef label_arr2img(label_queue, palette):\n    data = list(copy.deepcopy(label_queue.queue))",
        "detail": "SVTAS.tools.infer.infer",
        "documentation": {}
    },
    {
        "label": "label_arr2img",
        "kind": 2,
        "importPath": "SVTAS.tools.infer.infer",
        "description": "SVTAS.tools.infer.infer",
        "peekOfCode": "def label_arr2img(label_queue, palette):\n    data = list(copy.deepcopy(label_queue.queue))\n    array = np.array(data).transpose()\n    arr = array.astype(np.uint8)\n    arr = np.tile(arr, (20, 1))\n    img = Image.fromarray(arr)\n    img = img.convert(\"P\")\n    img.putpalette(palette)\n    return img\ndef parse_args():",
        "detail": "SVTAS.tools.infer.infer",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "SVTAS.tools.infer.infer",
        "description": "SVTAS.tools.infer.infer",
        "peekOfCode": "def parse_args():\n    parser = argparse.ArgumentParser(\"SVTAS infer script\")\n    parser.add_argument('-m',\n                        '--model',\n                        type=str,\n                        default='model.onnx',\n                        help='onnx model path')\n    parser.add_argument('-i',\n                        '--input',\n                        type=str,",
        "detail": "SVTAS.tools.infer.infer",
        "documentation": {}
    },
    {
        "label": "infer",
        "kind": 2,
        "importPath": "SVTAS.tools.infer.infer",
        "description": "SVTAS.tools.infer.infer",
        "peekOfCode": "def infer():\n    args = parse_args()\n    #! set mean and std\n    memory_factor = 3\n    mean = [[[0.551, 0.424, 0.179]]]\n    std = [[[0.133, 0.141, 0.124]]]\n    mean = np.array(mean)[:,:,::-1].transpose((2,0,1))\n    std = np.array(std)[:,:,::-1].transpose((2,0,1))\n    # load model\n    ort_session = onnxruntime.InferenceSession(args.model)",
        "detail": "SVTAS.tools.infer.infer",
        "documentation": {}
    },
    {
        "label": "cam_forward",
        "kind": 2,
        "importPath": "SVTAS.tools.visualize.cam_forward_fn",
        "description": "SVTAS.tools.visualize.cam_forward_fn",
        "peekOfCode": "def cam_forward(self, input_data):\n    \"\"\"\n        Use Like \n        ```\n        feature = debugger.debug(feature, 'backbone_output') \n        ```\n        for Debug\n    \"\"\"\n    input_data = input_data.reshape([-1, 8]+list(input_data.shape[-3:]))\n    masks = torch.full([input_data.shape[0], input_data.shape[1] * self.sample_rate], 1.0).to(input_data.device)",
        "detail": "SVTAS.tools.visualize.cam_forward_fn",
        "documentation": {}
    },
    {
        "label": "CAMPostProcessing",
        "kind": 6,
        "importPath": "SVTAS.tools.visualize.cam_visualization",
        "description": "SVTAS.tools.visualize.cam_visualization",
        "peekOfCode": "class CAMPostProcessing():\n    def __init__(self,\n                 sample_rate,\n                 ignore_index=-100):\n        self.init_flag = False\n        self.ignore_index = ignore_index\n        self.sample_rate = sample_rate\n    def init_scores(self):\n        self.imgs_list = []\n        self.labels_list = []",
        "detail": "SVTAS.tools.visualize.cam_visualization",
        "documentation": {}
    },
    {
        "label": "get_args",
        "kind": 2,
        "importPath": "SVTAS.tools.visualize.cam_visualization",
        "description": "SVTAS.tools.visualize.cam_visualization",
        "peekOfCode": "def get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--use-cuda', action='store_true', default=True,\n                        help='Use NVIDIA GPU acceleration')\n    parser.add_argument('-c',\n                        '--config',\n                        type=str,\n                        default='configs/example.yaml',\n                        help='config file path')\n    parser.add_argument('-o',",
        "detail": "SVTAS.tools.visualize.cam_visualization",
        "documentation": {}
    },
    {
        "label": "path",
        "kind": 5,
        "importPath": "SVTAS.tools.visualize.cam_visualization",
        "description": "SVTAS.tools.visualize.cam_visualization",
        "peekOfCode": "path = os.path.join(os.getcwd())\nsys.path.append(path)\nimport argparse\nimport numpy as np\nimport torch\nfrom types import MethodType \nfrom svtas.utils.config import Config\nimport svtas.model.builder as model_builder\nimport svtas.loader.builder as dataset_builder\nfrom mmcv.runner import load_state_dict",
        "detail": "SVTAS.tools.visualize.cam_visualization",
        "documentation": {}
    },
    {
        "label": "get_arguments",
        "kind": 2,
        "importPath": "SVTAS.tools.visualize.convert_pred2img",
        "description": "SVTAS.tools.visualize.convert_pred2img",
        "peekOfCode": "def get_arguments() -> argparse.Namespace:\n    \"\"\"\n    parse all the arguments from command line inteface\n    return a list of parsed arguments\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"convert pred and gt list to images.\")\n    parser.add_argument(\n        \"input_dir\",\n        type=str,\n        help=\"path to a files you want to convert\",",
        "detail": "SVTAS.tools.visualize.convert_pred2img",
        "documentation": {}
    },
    {
        "label": "convert_arr2img",
        "kind": 2,
        "importPath": "SVTAS.tools.visualize.convert_pred2img",
        "description": "SVTAS.tools.visualize.convert_pred2img",
        "peekOfCode": "def convert_arr2img(file_path, palette, actions_dict):\n    \"\"\"\n    Args:\n        str: file path\n        palette: color palette\n    \"\"\"\n    file_ptr = open(file_path, 'r')\n    list = file_ptr.read().split('\\n')[:-1]\n    file_ptr.close()\n    array = np.array([actions_dict[name] for name in list])",
        "detail": "SVTAS.tools.visualize.convert_pred2img",
        "documentation": {}
    },
    {
        "label": "make_palette",
        "kind": 2,
        "importPath": "SVTAS.tools.visualize.convert_pred2img",
        "description": "SVTAS.tools.visualize.convert_pred2img",
        "peekOfCode": "def make_palette(num_classes):\n    \"\"\"\n    Maps classes to colors in the style of PASCAL VOC.\n    Close values are mapped to far colors for segmentation visualization.\n    See http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#devkit\n    Takes:\n        num_classes: the number of classes\n    Gives:\n        palette: the colormap as a k x 3 array of RGB colors\n    \"\"\"",
        "detail": "SVTAS.tools.visualize.convert_pred2img",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "SVTAS.tools.visualize.convert_pred2img",
        "description": "SVTAS.tools.visualize.convert_pred2img",
        "peekOfCode": "def main() -> None:\n    args = get_arguments()\n    action_dict_path = args.action_dict_path\n    # actions dict generate\n    file_ptr = open(action_dict_path, 'r')\n    actions = file_ptr.read().split('\\n')[:-1]\n    file_ptr.close()\n    actions_dict = dict()\n    for a in actions:\n        actions_dict[a.split()[1]] = int(a.split()[0])",
        "detail": "SVTAS.tools.visualize.convert_pred2img",
        "documentation": {}
    },
    {
        "label": "parse_args",
        "kind": 2,
        "importPath": "SVTAS.tools.launch",
        "description": "SVTAS.tools.launch",
        "peekOfCode": "def parse_args():\n    parser = argparse.ArgumentParser(\"SVTAS train script\")\n    parser.add_argument('-c',\n                        '--config',\n                        type=str,\n                        default='configs/example.yaml',\n                        help='config file path')\n    parser.add_argument('--mode',\n                        '--m',\n                        choices=[\"train\", \"test\", \"infer\"],",
        "detail": "SVTAS.tools.launch",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "SVTAS.tools.launch",
        "description": "SVTAS.tools.launch",
        "peekOfCode": "def main():\n    args = parse_args()\n    cfg = get_config(args.config, overrides=args.override, tensorboard=args.use_tensorboard)\n    # init distributed env first, since logger depends on the dist info.\n    if args.launcher == 'none':\n        nprocs = 1\n    else:\n        nprocs = torch.cuda.device_count()\n    # set seed if specified\n    seed = args.seed",
        "detail": "SVTAS.tools.launch",
        "documentation": {}
    },
    {
        "label": "path",
        "kind": 5,
        "importPath": "SVTAS.tools.launch",
        "description": "SVTAS.tools.launch",
        "peekOfCode": "path = os.path.join(os.getcwd())\nsys.path.append(path)\nimport random\nimport numpy as np\nimport torch\nfrom svtas.tasks.infer import infer\nfrom svtas.tasks.test import test\nfrom svtas.tasks.train import train\nfrom svtas.utils.config import get_config\ndef parse_args():",
        "detail": "SVTAS.tools.launch",
        "documentation": {}
    }
]